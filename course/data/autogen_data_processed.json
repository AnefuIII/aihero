[
  {
    "content": "<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->\n\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\n\n## Why are these changes needed?\n\n<!-- Please give a short summary of the change and the problem this solves. -->\n\n## Related issue number\n\n<!-- For example: \"Closes #1234\" -->\n\n## Checks\n\n- [ ] I've included any doc changes needed for <https://microsoft.github.io/autogen/>. See <https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md> to build and test documentation locally.\n- [ ] I've added tests (if relevant) corresponding to the changes introduced in this PR.\n- [ ] I've made sure all auto checks have passed.",
    "filename": ".github/PULL_REQUEST_TEMPLATE.md"
  },
  {
    "content": "# AutoGen Multi-Agent AI Framework\n\nAutoGen is a multi-language framework for creating AI agents that can act autonomously or work alongside humans. The project has separate Python and .NET implementations with their own development workflows.\n\nAlways reference these instructions first and fallback to search or bash commands only when you encounter unexpected information that does not match the info here.\n\n## Working Effectively\n\n### Prerequisites and Environment Setup\n\n**CRITICAL**: Install both .NET 8.0 and 9.0 for full compatibility:\n- Install uv package manager: `python3 -m pip install uv` \n- Install .NET 9.0 SDK: `wget https://dot.net/v1/dotnet-install.sh && chmod +x dotnet-install.sh && ./dotnet-install.sh --channel 9.0`\n- Install .NET 8.0 runtime: `./dotnet-install.sh --channel 8.0 --runtime dotnet && ./dotnet-install.sh --channel 8.0 --runtime aspnetcore`\n- Update PATH: `export PATH=\"$HOME/.dotnet:$PATH\"`\n\n### Python Development Workflow\n\n**Bootstrap and build Python environment:**\n```bash\ncd /home/runner/work/autogen/autogen/python\nuv sync --all-extras  # NEVER CANCEL: Takes 2 minutes. Set timeout to 300+ seconds.\nsource .venv/bin/activate\n```\n\n**Validate Python development:**\n```bash\n# Quick validation (under 1 second each)\npoe format  # Code formatting\npoe lint    # Linting with ruff\n\n# Type checking - NEVER CANCEL these commands\npoe mypy     # Takes 6 minutes. Set timeout to 600+ seconds.\npoe pyright  # Takes 41 seconds. Set timeout to 120+ seconds.\n\n# Individual package testing (core package example)\npoe --directory ./packages/autogen-core test  # Takes 10 seconds. Set timeout to 60+ seconds.\n\n# Documentation - NEVER CANCEL\npoe docs-build  # Takes 1 minute 16 seconds. Set timeout to 300+ seconds.\n```\n\n**CRITICAL TIMING EXPECTATIONS:**\n- **NEVER CANCEL**: Python environment setup takes 2 minutes minimum\n- **NEVER CANCEL**: mypy type checking takes 6 minutes \n- **NEVER CANCEL**: Documentation build takes 1+ minutes\n- Format/lint tasks complete in under 1 second\n- Individual package tests typically complete in 10-60 seconds\n\n### .NET Development Workflow\n\n**Bootstrap and build .NET environment:**\n```bash\ncd /home/runner/work/autogen/autogen/dotnet\nexport PATH=\"$HOME/.dotnet:$PATH\"\ndotnet restore  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\ndotnet build --configuration Release  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\n```\n\n**Validate .NET development:**\n```bash\n# Unit tests - NEVER CANCEL\ndotnet test --configuration Release --filter \"Category=UnitV2\" --no-build  # Takes 25 seconds. Set timeout to 120+ seconds.\n\n# Format check (if build fails) \ndotnet format --verify-no-changes\n\n# Run samples\ncd samples/Hello\ndotnet run\n```\n\n**CRITICAL TIMING EXPECTATIONS:**\n- **NEVER CANCEL**: .NET restore takes 53 seconds minimum\n- **NEVER CANCEL**: .NET build takes 53 seconds minimum  \n- **NEVER CANCEL**: .NET unit tests take 25 seconds minimum\n- All build and test commands require appropriate timeouts\n\n### Complete Validation Workflow\n\n**Run full check suite (Python):**\n```bash\ncd /home/runner/work/autogen/autogen/python\nsource .venv/bin/activate\npoe check  # NEVER CANCEL: Runs all checks. Takes 7+ minutes total. Set timeout to 900+ seconds.\n```\n\n## Validation Scenarios\n\n### Manual Validation Requirements\nAlways manually validate changes by running complete user scenarios after making modifications:\n\n**Python validation scenarios:**\n1. **Import test**: Verify core imports work:\n   ```python\n   from autogen_agentchat.agents import AssistantAgent\n   from autogen_core import AgentRuntime\n   from autogen_ext.models.openai import OpenAIChatCompletionClient\n   ```\n\n2. **AutoGen Studio test**: Verify web interface can start:\n   ```bash\n   autogenstudio ui --help  # Should show help without errors\n   ```\n\n3. **Documentation test**: Build and verify docs generate without errors:\n   ```bash\n   poe docs-build && ls docs/build/index.html\n   ```\n\n**.NET validation scenarios:**\n1. **Sample execution**: Run Hello sample to verify runtime works:\n   ```bash\n   cd dotnet/samples/Hello && dotnet run --help\n   ```\n\n2. **Build validation**: Ensure all projects compile:\n   ```bash\n   dotnet build --configuration Release --no-restore\n   ```\n\n3. **Test execution**: Run unit tests to verify functionality:\n   ```bash\n   dotnet test --filter \"Category=UnitV2\" --configuration Release --no-build\n   ```\n\n## Common Issues and Workarounds\n\n### Network-Related Issues\n- **Python tests may fail** with network errors (tiktoken downloads, Playwright browser downloads) in sandboxed environments - this is expected\n- **Documentation intersphinx warnings** due to inability to reach external documentation sites - this is expected\n- **Individual package tests work better** than full test suite in network-restricted environments\n\n### .NET Runtime Issues  \n- **Requires both .NET 8.0 and 9.0**: Build uses 9.0 SDK but tests need 8.0 runtime\n- **Global.json specifies 9.0.100**: Must install exact .NET 9.0 version or later\n- **Path configuration critical**: Ensure `$HOME/.dotnet` is in PATH before system .NET\n\n### Python Package Issues\n- **Use uv exclusively**: Do not use pip/conda for dependency management\n- **Virtual environment required**: Always activate `.venv` before running commands\n- **Package workspace structure**: Project uses uv workspace with multiple packages\n\n## Timing Reference\n\n### Python Commands\n| Command | Expected Time | Timeout | Notes |\n|---------|---------------|---------|-------|\n| `uv sync --all-extras` | 2 minutes | 300+ seconds | NEVER CANCEL |\n| `poe mypy` | 6 minutes | 600+ seconds | NEVER CANCEL |\n| `poe pyright` | 41 seconds | 120+ seconds | NEVER CANCEL |\n| `poe docs-build` | 1 min 16 sec | 300+ seconds | NEVER CANCEL |\n| `poe format` | <1 second | 30 seconds | Quick |\n| `poe lint` | <1 second | 30 seconds | Quick |\n| Individual package test | 10 seconds | 60+ seconds | May have network failures |\n\n### .NET Commands  \n| Command | Expected Time | Timeout | Notes |\n|---------|---------------|---------|-------|\n| `dotnet restore` | 53 seconds | 300+ seconds | NEVER CANCEL |\n| `dotnet build --configuration Release` | 53 seconds | 300+ seconds | NEVER CANCEL |\n| `dotnet test --filter \"Category=UnitV2\"` | 25 seconds | 120+ seconds | NEVER CANCEL |\n| `dotnet format --verify-no-changes` | 5-10 seconds | 60 seconds | Quick validation |\n\n## Repository Structure\n\n### Python Packages (`python/packages/`)\n- `autogen-core`: Core agent runtime, model interfaces, and base components\n- `autogen-agentchat`: High-level multi-agent conversation APIs  \n- `autogen-ext`: Extensions for specific model providers and tools\n- `autogen-studio`: Web-based IDE for agent workflows\n- `agbench`: Benchmarking suite for agent performance\n- `magentic-one-cli`: Multi-agent team CLI application\n\n### .NET Projects (`dotnet/src/`)\n- `AutoGen`: Legacy 0.2-style .NET packages (being deprecated)\n- `Microsoft.AutoGen.*`: New event-driven .NET packages\n- `AutoGen.Core`: Core .NET agent functionality\n- Multiple provider packages: OpenAI, Anthropic, Ollama, etc.\n\n### Key Configuration Files\n- `python/pyproject.toml`: Python workspace and tool configuration\n- `dotnet/global.json`: .NET SDK version requirements  \n- `dotnet/AutoGen.sln`: .NET solution file\n- `python/uv.lock`: Locked Python dependencies\n\n## Development Best Practices\n\n### Before Committing Changes\n**ALWAYS run these validation steps:**\n\n**Python:**\n```bash\ncd python && source .venv/bin/activate\npoe format    # Fix formatting\npoe lint      # Check code quality  \npoe mypy      # Type checking (6 minutes)\npoe docs-build # Verify docs build (1+ minutes)\n```\n\n**.NET:**\n```bash  \ncd dotnet && export PATH=\"$HOME/.dotnet:$PATH\"\ndotnet format --verify-no-changes  # Check formatting\ndotnet build --configuration Release --no-restore  # Build (53 seconds)\ndotnet test --configuration Release --filter \"Category=UnitV2\" --no-build  # Test (25 seconds)\n```\n\n### Key Directories Reference\n```\nautogen/\n\u251c\u2500\u2500 python/                    # Python implementation\n\u2502   \u251c\u2500\u2500 packages/             # Individual Python packages\n\u2502   \u251c\u2500\u2500 docs/                 # Sphinx documentation\n\u2502   \u251c\u2500\u2500 samples/              # Example code\n\u2502   \u2514\u2500\u2500 pyproject.toml        # Workspace configuration\n\u251c\u2500\u2500 dotnet/                   # .NET implementation  \n\u2502   \u251c\u2500\u2500 src/                  # Source projects\n\u2502   \u251c\u2500\u2500 test/                 # Test projects\n\u2502   \u251c\u2500\u2500 samples/              # Sample applications\n\u2502   \u2514\u2500\u2500 AutoGen.sln          # Solution file\n\u251c\u2500\u2500 .github/workflows/        # CI/CD pipelines\n\u2514\u2500\u2500 docs/                     # Additional documentation\n```\n\nThis framework supports creating both simple single-agent applications and complex multi-agent workflows with support for various LLM providers, tools, and deployment patterns.",
    "filename": ".github/copilot-instructions.md"
  },
  {
    "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns",
    "filename": "CODE_OF_CONDUCT.md"
  },
  {
    "content": "# Contributing\n\nThe project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project's capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Possible contributions include but not limited to:\n\n- Pushing patches.\n- Code review of pull requests.\n- Documentation, examples and test cases.\n- Readability improvement, e.g., improvement on docstr and comments.\n- Community participation in [issues](https://github.com/microsoft/autogen/issues), [discussions](https://github.com/microsoft/autogen/discussions), [twitter](https://twitter.com/pyautogen), and [Discord](https://aka.ms/autogen-discord).\n- Tutorials, blog posts, talks that promote the project.\n- Sharing application scenarios and/or related research.\n\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nIf you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Running CI checks locally\n\nIt is important to use `uv` when running CI checks locally as it ensures that the correct dependencies and versions are used.\n\nPlease follow the instructions [here](./python/README.md#setup) to get set up.\n\nFor common tasks that are helpful during development and run in CI, see [here](./python/README.md#common-tasks).\n\n## Roadmap\n\nWe use GitHub issues and milestones to track our roadmap. You can view the upcoming milestones [here]([Roadmap Issues](https://aka.ms/autogen-roadmap)).\n\n## Versioning\n\nThe set of `autogen-*` packages are generally all versioned together. When a change is made to one package, all packages are updated to the same version. This is to ensure that all packages are in sync with each other.\n\nWe will update verion numbers according to the following rules:\n\n- Increase minor version (0.X.0) upon breaking changes\n- Increase patch version (0.0.X) upon new features or bug fixes\n\n## Release process\n\n1. Create a PR that updates the version numbers across the codebase ([example](https://github.com/microsoft/autogen/pull/4359))\n2. The docs CI will fail for the PR, but this is expected and will be resolved in the next step\n3. After merging the PR, create and push a tag that corresponds to the new verion. For example, for `0.4.0.dev13`:\n    - `git tag v0.4.0.dev13 && git push origin v0.4.0.dev13`\n4. Restart the docs CI by finding the failed [job corresponding to the `push` event](https://github.com/microsoft/autogen/actions/workflows/docs.yml) and restarting all jobs\n5. Run [this](https://github.com/microsoft/autogen/actions/workflows/single-python-package.yml) workflow for each of the packages that need to be released and get an approval for the release for it to run\n\n## Triage process\n\nTo help ensure the health of the project and community the AutoGen committers have a weekly triage process to ensure that all issues and pull requests are reviewed and addressed in a timely manner. The following documents the responsibilites while on triage duty:\n\n- Issues\n  - Review all new issues - these will be tagged with [`needs-triage`](https://github.com/microsoft/autogen/issues?q=is%3Aissue%20state%3Aopen%20label%3Aneeds-triage).\n  - Apply appropriate labels:\n    - One of `proj-*` labels based on the project the issue is related to\n    - `documentation`: related to documentation\n    - `x-lang`: related to cross language functionality\n    - `dotnet`: related to .NET\n  - Add the issue to a relevant milestone if necessary\n  - If you can resolve the issue or reply to the OP please do.\n  - If you cannot resolve the issue, assign it to the appropriate person.\n  - If awaiting a reply add the tag `awaiting-op-response` (this will be auto removed when the OP replies).\n  - Bonus: there is a backlog of old issues that need to be reviewed - if you have time, review these as well and close or refresh as many as you can.\n- PRs\n  - The UX on GH flags all recently updated PRs. Draft PRs can be ignored, otherwise review all recently updated PRs.\n  - If a PR is ready for review and you can provide one please go ahead. If you cant, please assign someone. You can quickly spin up a codespace with the PR to test it out.\n  - If a PR is needing a reply from the op, please tag it `awaiting-op-response`.\n  - If a PR is approved and passes CI, its ready to merge, please do so.\n  - If it looks like there is a possibly transient CI failure, re-run failed jobs.\n- Discussions\n  - Look for recently updated discussions and reply as needed or find someone on the team to reply.\n- Security\n  - Look through any securty alerts and file issues or dismiss as needed.\n\n## Becoming a Reviewer\n\nThere is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors.\n\n## What makes a good docstring?\n\n- Concise and to the point\n- Describe the expected contract/behavior of the function/class\n- Describe all parameters, return values, and exceptions\n- Provide an example if possible\n\nFor example, this is the docstring for the [TypeSubscription](https://microsoft.github.io/autogen/dev/reference/python/autogen_core.html#autogen_core.TypeSubscription) class:\n\n```python\n\"\"\"This subscription matches on topics based on a prefix of the type and maps to agents using the source of the topic as the agent key.\n\nThis subscription causes each source to have its own agent instance.\n\nExample:\n\n    .. code-block:: python\n\n        from autogen_core import TypePrefixSubscription\n\n        subscription = TypePrefixSubscription(topic_type_prefix=\"t1\", agent_type=\"a1\")\n\n    In this case:\n\n    - A topic_id with type `t1` and source `s1` will be handled by an agent of type `a1` with key `s1`\n    - A topic_id with type `t1` and source `s2` will be handled by an agent of type `a1` with key `s2`.\n    - A topic_id with type `t1SUFFIX` and source `s2` will be handled by an agent of type `a1` with key `s2`.\n\nArgs:\n    topic_type_prefix (str): Topic type prefix to match against\n    agent_type (str): Agent type to handle this subscription\n\"\"\"\n```\n\n## Docs when adding a new API\n\nNow that 0.4.0 is out, we should ensure the docs between versions are easy to navigate. To this end, added or changed APIs should have the following added to their docstrings respectively:\n\n```rst\n.. versionadded:: v0.4.1\n\n   Here's a version added message.\n\n.. versionchanged:: v0.4.1\n\n   Here's a version changed message.\n```\n\nSee [here](https://pydata-sphinx-theme.readthedocs.io/en/stable/examples/kitchen-sink/admonitions.html#versionadded) for how they are rendered.",
    "filename": "CONTRIBUTING.md"
  },
  {
    "content": "## AutoGen FAQs\n\n### What is AutoGen 0.4?\n\nAutoGen v0.4 is a rewrite of AutoGen from the ground up to create a more robust,\nscalable, easier to use, cross-language library for building AI Agents.\nSome key features include asynchronous messaging, support for scalable distributed agents,\nmodular extensible design (bring your own agents, implement behaviors however you like),\ncross-language support, improved observability, and full typing integration.\nIt is a breaking change.\n\n### Why these changes?\n\nWe listened to our AutoGen users, learned from what was working, and adapted to fix what wasn't.\nWe brought together wide-ranging teams working on many different types of AI Agents\nand collaborated to design an improved framework with a more flexible\nprogramming model and better scalability.\n\n### Is this project still maintained?\n\nWe want to reaffirm our commitment to supporting both the original version of AutoGen (0.2) and the redesign (0.4). AutoGen 0.4 is still work-in-progress, and we shared the code now to build with the community. There are no plans to deprecate the original AutoGen anytime soon, and both versions will be actively maintained.\n\n### Who should use it 0.4?\n\nThis code is still experimental, so expect changes and bugs while we work towards a stable 0.4 release. We encourage early adopters to\ntry it out, give us feedback, and contribute.\nFor those looking for a stable version we recommend to continue using 0.2\n\n### I'm using AutoGen 0.2, should I upgrade?\n\nIf you consider yourself an early adopter, you are comfortable making some\nchanges to your code, and are willing to try it out, then yes.\n\n### How do I still use AutoGen 0.2?\n\nAutoGen 0.2 can be installed with:\n\n```sh\npip install autogen-agentchat~=0.2\n```\n\n### Will AutoGen Studio be supported in 0.4?\n\nYes, this is on the [roadmap](#roadmap).\nOur current plan is to enable an implementation of AutoGen Studio\non the AgentChat high level API which implements a set of agent functionalities\n(agents, teams, etc).\n\n### How do I migrate?\n\nFor users familiar with AutoGen, the AgentChat library in 0.4 provides similar concepts.\nWe are working on a migration guide.\n\n### Is 0.4 done?\n\nWe are still actively developing AutoGen 0.4. One exciting new feature is the emergence of new SDKs for .NET. The python SDKs are further ahead at this time but our goal is to achieve parity. We aim to add additional languages in future releases.\n\n### What is happening next? When will this release be ready?\n\nWe are still working on improving the documentation, samples, and enhancing the code. We are hoping to release before the end of the year when things are ready.\n\n### What is the history of this project?\n\nThe rearchitecture of the framework started with multiple Microsoft teams coming together\nto address the gaps and learnings from AutoGen 0.2 - merging ideas from several predecessor projects.\nThe team worked on this internally for some time to ensure alignment before moving work back to the open in October 2024.\n\n### What is the official channel for support?\n\nUse GitHub [Issues](https://github.com/microsoft/autogen/issues) for bug reports and feature requests.\nUse GitHub [Discussions](https://github.com/microsoft/autogen/discussions) for general questions and discussions.\n\n### Do you use Discord for communications?\n\nWe are unable to use the old Discord for project discussions, many of the maintainers no longer have viewing or posting rights there. Therefore, we request that all discussions take place on <https://github.com/microsoft/autogen/discussions/>  or the [new discord server](https://aka.ms/autogen-discord).\n\n### What about forks?\n\n<https://github.com/microsoft/autogen/> remains the only official repo for development and support of AutoGen.\nWe are aware that there are thousands of forks of AutoGen, including many for personal development and startups building with or on top of the library. We are not involved with any of these forks and are not aware of any plans related to them.\n\n### What is the status of the license and open source?\n\nOur project remains fully open-source and accessible to everyone. We understand that some forks use different licenses to align with different interests. We will continue to use the most permissive license (MIT) for the project.\n\n### Can you clarify the current state of the packages?\n\nCurrently, we are unable to make releases to the `pyautogen` package via Pypi due to a change to package ownership that was done without our involvement. Additionally, we are moving to using multiple packages to align with the new design. Please see details [here](https://microsoft.github.io/autogen/dev/packages/index.html).\n\n### Can I still be involved?\n\nWe are grateful to all the contributors to AutoGen 0.2 and we look forward to continuing to collaborate with everyone in the AutoGen community.",
    "filename": "FAQ.md"
  },
  {
    "content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n<img src=\"https://microsoft.github.io/autogen/0.2/img/ag.svg\" alt=\"AutoGen Logo\" width=\"100\">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540)\n[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)\n[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)\n[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)\n\n</div>\n\n# AutoGen\n\n**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n## Installation\n\nAutoGen requires **Python 3.10 or later**.\n\n```bash\n# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\n```\n\nThe current stable version can be found in the [releases](https://github.com/microsoft/autogen/releases). If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.\n\n```bash\n# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"\n```\n\n## Quickstart\n\n### Hello World\n\nCreate an assistant agent using OpenAI's GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n### MCP Server\n\nCreate a web browsing assistant agent that uses the Playwright MCP server.\n\n```python\n# First run `npm install -g @playwright/mcp@latest` to install the MCP server.\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    server_params = StdioServerParams(\n        command=\"npx\",\n        args=[\n            \"@playwright/mcp@latest\",\n            \"--headless\",\n        ],\n    )\n    async with McpWorkbench(server_params) as mcp:\n        agent = AssistantAgent(\n            \"web_browsing_assistant\",\n            model_client=model_client,\n            workbench=mcp, # For multiple MCP servers, put them in a list.\n            model_client_stream=True,\n            max_tool_iterations=10,\n        )\n        await Console(agent.run_stream(task=\"Find out how many contributors for the microsoft/autogen repository\"))\n\n\nasyncio.run(main())\n```\n\n> **Warning**: Only connect to trusted MCP servers as they may execute commands\n> in your local environment or expose sensitive information.\n\n### Multi-Agent Orchestration\n\nYou can use `AgentTool` to create a basic multi-agent orchestration setup.\n\n```python\nimport asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.tools import AgentTool\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n\n    math_agent = AssistantAgent(\n        \"math_expert\",\n        model_client=model_client,\n        system_message=\"You are a math expert.\",\n        description=\"A math expert assistant.\",\n        model_client_stream=True,\n    )\n    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)\n\n    chemistry_agent = AssistantAgent(\n        \"chemistry_expert\",\n        model_client=model_client,\n        system_message=\"You are a chemistry expert.\",\n        description=\"A chemistry expert assistant.\",\n        model_client_stream=True,\n    )\n    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)\n\n    agent = AssistantAgent(\n        \"assistant\",\n        system_message=\"You are a general assistant. Use expert tools when needed.\",\n        model_client=model_client,\n        model_client_stream=True,\n        tools=[math_agent_tool, chemistry_agent_tool],\n        max_tool_iterations=10,\n    )\n    await Console(agent.run_stream(task=\"What is the integral of x^2?\"))\n    await Console(agent.run_stream(task=\"What is the molecular weight of water?\"))\n\n\nasyncio.run(main())\n```\n\nFor more advanced multi-agent orchestrations and workflows, read\n[AgentChat documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html).\n\n### AutoGen Studio\n\nUse AutoGen Studio to prototype and run multi-agent workflows without writing code.\n\n```bash\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n## Why Use AutoGen?\n\n<div align=\"center\">\n  <img src=\"autogen-landing.jpg\" alt=\"AutoGen Landing\" width=\"500\">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated\u00a0API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png\" alt=\"AutoGen Studio Screenshot\" width=\"500\">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align=\"center\">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                        |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                         | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |\n| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                         | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)      |\n| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                          | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                 | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |\n| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) <br> [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) <br> [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) <br> [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) <br> [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) <br> [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                          |\n\n</div>\n\nInterested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!\n\nHave questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don't find what you're looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n    \u2191 Back to Top \u2191\n  </a>\n</p>",
    "filename": "README.md"
  },
  {
    "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc).\n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->",
    "filename": "SECURITY.md"
  },
  {
    "content": "# Support\n\n## How to file issues and get help\n\nThis project uses [GitHub Issues](https://github.com/microsoft/autogen/issues)\nto track bugs and feature requests. Please search the existing\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or\nfeature request as a new Issue.\n\nFor help and questions about using this project, please use\n[GitHub Discussion](https://github.com/microsoft/autogen/discussions).\nFollow [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\nwhen participating in the forum.\n\n## Microsoft Support Policy\n\nSupport for this project is limited to the resources listed above.",
    "filename": "SUPPORT.md"
  },
  {
    "content": "# AutoGen: Responsible AI FAQs\n\n## What is AutoGen?\nAutoGen is a framework for simplifying the orchestration, optimization, and automation of LLM workflows. It offers customizable and conversable agents that leverage the strongest capabilities of the most advanced LLMs, like GPT-4, while addressing their limitations by integrating with humans and tools and having conversations between multiple agents via automated chat.\n\n## What can AutoGen do?\nAutoGen is an experimentational framework for building a complex multi-agent conversation system by:\n- Defining a set of agents with specialized capabilities and roles.\n-\tDefining the interaction behavior between agents, i.e., what to reply when an agent receives messages from another agent.\n\nThe agent conversation-centric design has numerous benefits, including that it:\n-\tNaturally handles ambiguity, feedback, progress, and collaboration.\n-\tEnables effective coding-related tasks, like tool use with back-and-forth troubleshooting.\n-\tAllows users to seamlessly opt in or opt out via an agent in the chat.\n-\tAchieves a collective goal with the cooperation of multiple specialists.\n\n## \tWhat is/are AutoGen\u2019s intended use(s)?\nPlease note that AutoGen is an open-source library under active development and intended for  use for research purposes. It should not be used in any downstream applications without additional detailed evaluation of robustness, safety issues and assessment of any potential harm or bias in the proposed application.\n\nAutoGen is a generic infrastructure that can be used in multiple scenarios. The system\u2019s intended uses include:\n\n-\tBuilding LLM workflows that solve more complex tasks: Users can create agents that interleave reasoning and tool use capabilities of the latest LLMs such as GPT-4. To solve complex tasks, multiple agents can converse to work together (e.g., by partitioning a complex problem into simpler steps or by providing different viewpoints or perspectives).\n-\tApplication-specific agent topologies: Users can create application specific agent topologies and patterns for agents to interact. The exact topology may depend on the domain\u2019s complexity and semantic capabilities of the LLM available.\n-\tCode generation and execution: Users can implement agents that can assume the roles of writing code and other agents that can execute code. Agents can do this with varying levels of human involvement. Users can add more agents and program the conversations to enforce constraints on code and output.\n-\tQuestion answering: Users can create agents that can help answer questions using retrieval augmented generation.\n-\tEnd user and multi-agent chat and debate: Users can build chat applications where they converse with multiple agents at the same time.\n\nWhile AutoGen automates LLM workflows, decisions about how to use specific LLM outputs should always have a human in the loop. For example, you should not use AutoGen to automatically post LLM generated content to social media.\n\n## How was AutoGen evaluated? What metrics are used to measure performance?\n-   We performed testing for Responsible AI harm e.g., cross-domain prompt injection and all tests returned the expected results with no signs of jailbreak.\n-   AutoGen was evaluated on six applications to illustrate its potential in simplifying the development of high-performance multi-agent applications. These applications are selected based on their real-world relevance, problem difficulty and problem-solving capabilities enabled by AutoGen, and innovative potential. These applications involve using AutoGen to solve math problems, question answering, decision making in text world environments, supply chain optimization, etc. For each of these domains AutoGen was evaluated on various success-based metrics (i.e., how often the AutoGen based implementation solved the task). And, in some cases, AutoGen based approach was also evaluated on implementation efficiency (e.g., to track reductions in developer effort to build). More details can be found at: https://aka.ms/autogen-pdf.\n-   We evaluated [a team of AutoGen agents](https://github.com/microsoft/autogen/tree/gaia_multiagent_v01_march_1st/samples/tools/autogenbench/scenarios/GAIA/Templates/Orchestrator) on the [GAIA benchmark](https://arxiv.org/abs/2311.12983), and got [SOTA results](https://huggingface.co/spaces/gaia-benchmark/leaderboard) as of March 1, 2024.\n\n\n## What are the limitations of AutoGen? How can users minimize the impact of AutoGen\u2019s limitations when using the system?\nAutoGen relies on existing LLMs. Experimenting with AutoGen would retain common limitations of large language models; including:\n\n- Data Biases: Large language models, trained on extensive data, can inadvertently carry biases present in the source data. Consequently, the models may generate outputs that could be potentially biased or unfair.\n-\tLack of Contextual Understanding: Despite their impressive capabilities in language understanding and generation, these models exhibit limited real-world understanding, resulting in potential inaccuracies or nonsensical responses.\n-\tLack of Transparency: Due to the complexity and size, large language models can act as `black boxes,' making it difficult to comprehend the rationale behind specific outputs or decisions.\n-\tContent Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them. It is recommended to leverage various content moderation services provided by different companies and institutions.\n-\tInaccurate or ungrounded content: It is important to be aware and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models to fabricate content without high authority input sources.\n-\tPotential for Misuse: Without suitable safeguards, there is a risk that these models could be maliciously used for generating disinformation or harmful content.\n\n\nAdditionally, AutoGen\u2019s multi-agent framework may amplify or introduce additional risks, such as:\n-\tPrivacy and Data Protection: The framework allows for human participation in conversations between agents. It is important to ensure that user data and conversations are protected and that developers use appropriate measures to safeguard privacy.\n-\tAccountability and Transparency: The framework involves multiple agents conversing and collaborating, it is important to establish clear accountability and transparency mechanisms. Users should be able to understand and trace the decision-making process of the agents involved in order to ensure accountability and address any potential issues or biases.\n-\tTrust and reliance: The framework leverages human understanding and intelligence while providing automation through conversations between agents. It is important to consider the impact of this interaction on user experience, trust, and reliance on AI systems. Clear communication and user education about the capabilities and limitations of the system will be essential.\n-\tSecurity & unintended consequences: The use of multi-agent conversations and automation in complex tasks may have unintended consequences. Especially, allowing LLM agents to make changes in external environments through code execution or function calls, such as install packages, could pose significant risks. Developers should carefully consider the potential risks and ensure that appropriate safeguards are in place to prevent harm or negative outcomes, including keeping a human in the loop for decision making.\n\n## What operational factors and settings allow for effective and responsible use of AutoGen?\n-\tCode execution: AutoGen recommends using docker containers so that code execution can happen in a safer manner. Users can use function call instead of free-form code to execute pre-defined functions only. That helps increase the reliability and safety. Users can customize the code execution environment to tailor to their requirements.\n-\tHuman involvement: AutoGen prioritizes human involvement in multi agent conversation. The overseers can step in to give feedback to agents and steer them in the correct direction. In all examples, users confirm code before it is executed.\n-\tAgent modularity: Modularity allows agents to have different levels of information access. Additional agents can assume roles that help keep other agents in check. For example, one can easily add a dedicated agent to play the role of safeguard.\n-\tLLMs: Users can choose the LLM that is optimized for responsible use. The default LLM in all examples is GPT-4o which inherits the existing RAI mechanisms and filters from the LLM provider. We encourage developers to review [OpenAI\u2019s Usage policies](https://openai.com/policies/usage-policies) and [Azure OpenAI\u2019s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct) when using GPT-4o. We encourage developers experimenting with agents to add content moderation and/or use safety metaprompts when using agents, like they would do when using LLMs.",
    "filename": "TRANSPARENCY_FAQS.md"
  },
  {
    "content": "# Programming Model\n\nUnderstanding your workflow and mapping it to agents is the key to building an agent system in AutoGen.\n\nThe programming model is basically publish-subscribe. Agents subscribe to events they care about and also can publish events that other agents may care about. Agents may also have additonal assets such as Memory, prompts, data sources, and skills (external APIs).\n\n## Events Delivered as CloudEvents\n\nEach event in the system is defined using the [CloudEvents Specification](https://cloudevents.io/). This allows for a common event format that can be used across different systems and languages. In CloudEvents, each event has \"Context Attributes\" that must include:\n\n1. *id* - A unique id (eg. a UUID).\n2. *source* - A URI or URN indicating the event's origin.\n3. *type* - The namespace of the event - prefixed with a reverse-DNS name.\n   - The prefixed domain dictates the organization which defines the semantics of this event type: e.g (`com.github.pull_request.opened` or `com.example.object.deleted.v2`), and optionally fields describing the data schema/content-type or extensions.\n\n## Event Handlers\n\nEach agent has a set of event handlers, that are bound to a specific match against a CloudEvents *type*. Event Handlers could match against an exact type or match for a pattern of events of a particular level in the type heirarchy (eg: `com.Microsoft.AutoGen.Agents.System.*` for all Events in the `System` namespace) Each event handler is a function that can change state, call models, access memory, call external tools, emit other events, and flow data to/from other systems. Each event handler can be a simple function or a more complex function that uses a state machine or other control logic.\n\n## Orchestrating Agents\n\nIt is possible to build a functional and scalable agent system that only reacts to external events. In many cases, however, you will want to orchestrate the agents to achieve a specific goal or follow a pre-determined workflow. In this case, you will need to build an orchestrator agent that manages the flow of events between agents.\n\n## Built-in Event Types\n\nThe AutoGen system comes with a set of built-in event types that are used to manage the system. These include:\n\n- *System Events* - Events that are used to manage the system itself. These include events for starting and stopping the Agents, sending messages to all agents, and other system-level events.\n- *Insert other types here*\n\n## Agent Contracts\n\nYou may want to leverage more prescriptive agent behavior contracts, and AutoGen also includes base agents that implement different approaches to agent behavior, including layering request/response patterns on top of the event-driven model. For an example of this see the ChatAgents in the Python examples. In this case your agent will have a known set of events which it must implement and specific behaviors expected of those events.",
    "filename": "docs/design/01 - Programming Model.md"
  },
  {
    "content": "# Topics\n\nThis document describes the semantics and components of publishing messages and subscribing to topics.\n\n## Overview\n\nTopics are used as the primitive to manage which agents receive a given published message. Agents subscribe to topics. There is an application defined mapping from topic to agent instance.\n\nThese concepts intentionally map to the [CloudEvents](https://cloudevents.io/) specification. This allows for easy integration with existing systems and tools.\n\n### Non-goals\n\nThis document does not specify RPC/direct messaging\n\n## Identifiers\n\nA topic is identified by two components (called a `TopicId`):\n\n- [`type`](https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#type) - represents the type of event that occurs, this is static and defined in code\n  - SHOULD use reverse domain name notation to avoid naming conflicts. For example: `com.example.my-topic`.\n  - Allowed values MUST match the regex: `^[\\w\\-\\.\\:\\=]+\\Z`\n  - Notably, this is the same as agent type with the addition of `=` and `:` characters\n- [`source`](https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#source-1) - represents where the event originated from, this is dynamic and based on the message itself\n  - SHOULD be a URI\n\nAgent instances are identified by two components (called an `AgentId`):\n\n- `type` - represents the type of agent, this is static and defined in code\n  - Allowed values MUST match the regex: `^[\\w\\-\\.]+\\Z`\n- `key` - represents the instance of the agent type for the key\n  - SHOULD be a URI\n\nFor example: `GraphicDesigner:1234`\n\n## Subscriptions\n\nSubscriptions define which agents receive messages published to a topic. Subscriptions are dynamic and can be added or removed at any time.\n\nA subscription defines two things:\n\n- Matcher func of type `TopicId -> bool`, telling us \"does this subscription match this topic\"\n- Mapper func of type `TopicId -> AgentId`, telling us \"given this subscription matches this topic, which agent does it map to\"\n\nThese functions MUST be be free of side effects such that the evaluation can be cached.\n\n### Agent instance creation\n\nIf a message is received on a topic that maps to an agent that does not yet exist the runtime will instantiate an agent to fullfil the request.\n\n## Message types\n\nAgents are able to handle certain types of messages. This is an internal detail of an agent's implementation. All agents in a channel will receive all messages, but will ignore messages that it cannot handle.\n\n> [!NOTE]\n> This might be revisited based on scaling and performance considerations.\n\n## Well known topic types\n\nAgents should subscribe via a prefix subscription to the `{AgentType}:` topic as a direct message channel for the agent type.\n\nFor this subscription source should map directly to agent key.\n\nThis subscription will therefore receive all events for the following well known topics:\n\n- `{AgentType}:` - General purpose direct messages. These should be routed to the appropriate message handler.\n- `{AgentType}:rpc_request={RequesterAgentType}` - RPC request messages. These should be routed to the appropriate RPC handler, and RequesterAgentType used to publish the response\n- `{AgentType}:rpc_response={RequestId}` - RPC response messages. These should be routed back to the response future of the caller.\n- `{AgentType}:error={RequestId}` - Error message that corresponds to the given request.",
    "filename": "docs/design/02 - Topics.md"
  },
  {
    "content": "# Agent Worker Protocol\n\n## System architecture\n\nThe system consists of multiple processes, each being either a _service_ process or a _worker_ process.\nWorker processes host application code (agents) and connect to a service process.\nWorkers advertise the agents which they support to the service, so the service can decide which worker to place agents on.\nService processes coordinate placement of agents on worker processes and facilitate communication between agents.\n\nAgent instances are identified by the tuple of `(namespace: str, name: str)`.\nBoth _namespace_ and _name_ are application-defined.\nThe _namespace_ has no semantics implied by the system: it is free-form, and any semantics are implemented by application code.\nThe _name_ is used to route requests to a worker which supports agents with that name.\nWorkers advertise the set of agent names which they are capable of hosting to the service.\nWorkers activate agents in response to messages received from the service.\nThe service uses the _name_ to determine where to place currently-inactive agents, maintaining a mapping from agent name to a set of workers which support that agent.\nThe service maintains a _directory_ mapping active agent ids to worker processes which host the identified agent.\n\n### Agent lifecycle\n\nAgents are never explicitly created or destroyed. When a request is received for an agent which is not currently active, it is the responsibility of the service to select a worker which is capable of hosting that agent, and to route the request to that worker.\n\n## Worker protocol flow\n\nThe worker protocol has three phases, following the lifetime of the worker: initialization, operation, and termination.\n\n### Initialization\n\nWhen the worker process starts, it initiates a connection to a service process, establishing a bi-directional communication channel which messages are passed across.\nNext, the worker issues zero or more `RegisterAgentType(name: str)` messages, which tell the service the names of the agents which it is able to host.\n\n* TODO: What other metadata should the worker give to the service?\n* TODO: Should we give the worker a unique id which can be used to identify it for its lifetime? Should we allow this to be specified by the worker process itself?\n\n### Operation\n\nOnce the connection is established, and the service knows which agents the worker is capable of hosting, the worker may begin receiving requests for agents which it must host.\nPlacement of agents happens in response to an `Event(...)` or `RpcRequest(...)` message.\nThe worker maintains a _catalog_ of locally active agents: a mapping from agent id to agent instance.\nIf a message arrives for an agent which does not have a corresponding entry in the catalog, the worker activates a new instance of that agent and inserts it into the catalog.\nThe worker dispatches the message to the agent:\n\n* For an `Event`, the agent processes the message and no response is generated.\n* For an `RpcRequest` message, the agent processes the message and generates a response of type `RpcResponse`. The worker routes the response to the original sender.\n\nThe worker maintains a mapping of outstanding requests, identified by `RpcRequest.id`, to a promise for a future `RpcResponse`.\nWhen an `RpcResponse` is received, the worker finds the corresponding request id and fulfils the promise using that response.\nIf no response is received in a specified time frame (eg, 30s), the worker breaks the promise with a timeout error.\n\n### Termination\n\nWhen the worker is ready to shutdown, it closes the connection to the service and terminates. The service de-registers the worker and all agent instances which were hosted on it.",
    "filename": "docs/design/03 - Agent Worker Protocol.md"
  },
  {
    "content": "# Agent and Topic ID Specs\n\nThis document describes the structure, constraints, and behavior of Agent IDs and Topic IDs.\n\n## Agent ID\n\n### Required Attributes\n\n#### type\n\n- Type: `string`\n- Description: The agent type is not an agent class. It associates an agent with a specific factory function, which produces instances of agents of the same agent `type`. For example, different factory functions can produce the same agent class but with different constructor perameters.\n- Constraints: UTF8 and only contain alphanumeric letters (a-z) and (0-9), or underscores (\\_). A valid identifier cannot start with a number, or contain any spaces.\n- Examples:\n  - `code_reviewer`\n  - `WebSurfer`\n  - `UserProxy`\n\n#### key\n\n- Type: `string`\n- Description: The agent key is an instance identifier for the given agent `type`\n- Constraints: UTF8 and only contain characters between (inclusive) ascii 32 (space) and 126 (~).\n- Examples:\n  - `default`\n  - A memory address\n  - a UUID string\n\n## Topic ID\n\n### Required Attributes\n\n#### type\n\n- Type: `string`\n- Description: Topic type is usually defined by application code to mark the type of messages the topic is for.\n- Constraints: UTF8 and only contain alphanumeric letters (a-z) and (0-9), ':', '=', or underscores (\\_). A valid identifier cannot start with a number, or contain any spaces.\n- Examples:\n  - `GitHub_Issues`\n\n#### source\n\n- Type: `string`\n- Description: Topic source is the unique identifier for a topic within a topic type. It is typically defined by application data.\n- Constraints: UTF8 and only contain characters between (inclusive) ascii 32 (space) and 126 (~).\n- Examples:\n  - `github.com/{repo_name}/issues/{issue_number}`",
    "filename": "docs/design/04 - Agent and Topic ID Specs.md"
  },
  {
    "content": "# AutoGen Services\n\n## Overview\n\nEach AutoGen agent system has one or more Agent Workers and a set of services for managing/supporting the agents. The services and workers can all be hosted in the same process or in a distributed system.  When in the same process communication and event delivery is in-memory. When distributed, workers communicate with the service over gRPC. In all cases, events are packaged as CloudEvents. There are multiple options for the backend services:\n\n- In-Memory: the Agent Workers and Services are all hosted in the same process and communicate over in-memory channels. Available for python and .NET.\n- Python only: Agent workers communicate with a python hosted service that implements an in-memory message bus and agent registry.\n- Micrososft Orleans: a distributed actor system that can host the services and workers, enables distributed state with persistent storage, can leverage multiple event bus types, and cross-language agent communication.\n- *Roadmap: support for other languages distributed systems such as dapr or Akka.*\n\nThe Services in the system include:\n\n- Worker: Hosts the Agents and is a client to the Gateway\n- Gateway:\n-- RPC gateway for the other services APIs\n-- Provides an RPC bridge between the workers and the Event Bus\n-- Message Session state (track message queues/delivery)\n- Registry: keeps track of the {agents:agent types}:{Subscription/Topics} in the system and which events they can handle\n-- *Roadmap: add lookup api in gateway*\n- AgentState: persistent state for agents\n- Routing: delivers events to agents based on their subscriptions+topics\n-- *Roadmap: add subscription management APIs*\n- *Roadmap: Management APIs for the Agent System*\n- *Roadmap: Scheduling: manages placement of agents*\n- *Roadmap: Discovery: allows discovery of agents and services*",
    "filename": "docs/design/05 - Services.md"
  },
  {
    "content": "# Docs\n\nYou can find the project documentation [here](https://microsoft.github.io/autogen/dev/).",
    "filename": "docs/design/readme.md"
  },
  {
    "content": "# How to build and run the website\n\n## Prerequisites\n\n- dotnet 8.0 or later\n\n## Build\n\nFirstly, go to autogen/dotnet folder and run the following command to build the website:\n\n```bash\ndotnet tool restore\ndotnet tool run docfx ../docs/dotnet/docfx.json --serve\n```\n\nAfter the command is executed, you can open your browser and navigate to `http://localhost:8080` to view the website.",
    "filename": "docs/dotnet/README.md"
  },
  {
    "content": "# Differences from Python\n\n## Publishing to a topic that an agent is also subscribed to\n\n> [!NOTE]\n> TLDR; Default behavior is identical.\n\nWhen an agent publishes a message to a topic to which it also listens, the message will not be received by the agent that sent it. This is also the behavior in the Python runtime. However to support previous usage, in @Microsoft.AutoGen.Core.InProcessRuntime, you can set the @Microsoft.AutoGen.Core.InProcessRuntime.DeliverToSelf property to true in the TopicSubscription attribute to allow an agent to receive messages it sends.",
    "filename": "docs/dotnet/core/differences-from-python.md"
  },
  {
    "content": "# AutoGen Core\n\nAutoGen Core for .NET follows the same concepts and conventions of its Python counterpart. In fact, in order to understand the concepts in the .NET version, we recommend reading the [Python documentation](https://microsoft.github.io/autogen/stable/) first. Unless otherwise stated, the concepts in the Python version map to .NET.\n\nAny important differences between the language versions are documented in the [Differences from Python](./differences-from-python.md) section. For things that only affect a given language, such as dependency injection or host builder patterns, these will not be specified in the differences document.\n\n## Getting Started\n\nYou can obtain the SDK as a nuget package or by cloning the repository. The SDK is available on [NuGet](https://www.nuget.org/packages/Microsoft.AutoGen).\nMinimally you will need the following:\n\n```bash\ndotnet add package Microsoft.AutoGen.Contracts\ndotnet add package Microsoft.AutoGen.Core\n```\n\nSee [Installation](./installation.md) for more detailed notes on installing all the related packages. \n\nYou can quickly get started by looking at the samples in the [samples](https://github.com/microsoft/autogen/tree/main/dotnet/samples) directory of the repository.\n\n### Creating an Agent\n\nTo create an agent, you can inherit from BaseAgent and implement event handlers for the events you care about. Here is a minimal example demonstrating how to inherit from BaseAgent and implement an event handler:\n\n```csharp\npublic class MyAgent : BaseAgent, IHandle<MyMessage>\n{\n    // ...\n    public async ValueTask HandleAsync(MyMessage item, MessageContext context)\n    {\n        // ...logic here...\n    }\n}\n```\n\nBy overriding BaseAgent, you gain access to the runtime and logging utilities, and by implementing IHandle<T>, you can easily define event-handling methods for your custom messages.\n\n### Running an Agent in an Application\n\nTo run your agent in an application, you can use the `AgentsAppBuilder` class. Here is an example of how to run an agent 'HelloAgent' in an application:\n\n```csharp\nAgentsAppBuilder appBuilder = new AgentsAppBuilder()\n    .UseInProcessRuntime(deliverToSelf: true)\n    .AddAgent<HelloAgent>(\"HelloAgent\");\n\nvar app = await appBuilder.BuildAsync();\n\n// start the app by publishing a message to the runtime\nawait app.PublishMessageAsync(new NewMessageReceived\n{\n    Message = \"Hello from .NET\"\n}, new TopicId(\"HelloTopic\"));\n\n// Wait for shutdown\nawait app.WaitForShutdownAsync();\n```\n\n## .NET SDK Runtimes\n\nThe .NET SDK includes both an InMemory Single Process Runtime and a Remote, Distributed Runtime meant for running your agents in the cloud. The Distributed Runtime supports running agents in python and in .NET, allowing those agents to talk to one another. The distributed runtime uses Microsoft Orleans to provide resilience, persistence, and integration with messaging services such as Azure Event Hubs.  The xlang functionality requires that your agent's Messages are serializable as CloudEvents.  The messages are exchanged as CloudEvents over Grpc, and the runtime takes care of ensuring that the messages are delivered to the correct agents. \n\nTo use the Distributed Runtime, you will need to add the following package to your project:\n\n```bash\ndotnet add package Microsoft.AutoGen.Core.Grpc\n```\n\nThis is the package that runs in the application with your agent(s) and connects to the distributed system. \n\nTo Run the backend/server side you need:\n\n```bash\ndotnet add package Microsoft.AutoGen.RuntimeGateway\ndotnet add package Microsoft.AutoGen.AgentHost\n```\n\nYou can run the backend on its own:\n\n```bash\ndotnet run --project Microsoft.AutoGen.AgentHost\n```\n\nor you can include it inside your own application:\n\n```csharp\nusing Microsoft.AutoGen.RuntimeGateway;\nusing Microsoft.AutoGen.AgentHost;\nvar autogenBackend = await Microsoft.AutoGen.RuntimeGateway.Grpc.Host.StartAsync(local: false, useGrpc: true).ConfigureAwait(false);\n```\n\nYou can also install the runtime as a dotnet tool:\n\n```\ndotnet pack --no-build --configuration Release --output './output/release' -bl\\n\ndotnet tool install --add-source ./output/release Microsoft.AutoGen.AgentHost\n# run the tool\n# dotnet agenthost \n# or just...  \nagenthost \n```\n\n### Running Multiple Agents and the Runtime in separate processes with .NET Aspire\n\nThe [Hello.AppHost project](https://github.com/microsoft/autogen/blob/50d7587a4649504af3bb79ab928b2a3882a1a394/dotnet/samples/Hello/Hello.AppHost/Program.cs#L4) illustrates how to orchestrate a distributed system with multiple agents and the runtime in separate processes using .NET Aspire. It also points to a [python agent that illustrates how to run agents in different languages in the same distributed system](https://github.com/microsoft/autogen/blob/50d7587a4649504af3bb79ab928b2a3882a1a394/python/samples/core_xlang_hello_python_agent/README.md#L1).\n\n```csharp\n// Copyright (c) Microsoft Corporation. All rights reserved.\n// Program.cs\n\nusing Microsoft.Extensions.Hosting;\n\nvar builder = DistributedApplication.CreateBuilder(args);\nvar backend = builder.AddProject<Projects.Microsoft_AutoGen_AgentHost>(\"backend\").WithExternalHttpEndpoints();\nvar client = builder.AddProject<Projects.HelloAgent>(\"HelloAgentsDotNET\")\n    .WithReference(backend)\n    .WithEnvironment(\"AGENT_HOST\", backend.GetEndpoint(\"https\"))\n    .WithEnvironment(\"STAY_ALIVE_ON_GOODBYE\", \"true\")\n    .WaitFor(backend);\n// xlang is over http for now - in prod use TLS between containers\nbuilder.AddPythonApp(\"HelloAgentsPython\", \"../../../../python/samples/core_xlang_hello_python_agent\", \"hello_python_agent.py\", \"../../.venv\")\n    .WithReference(backend)\n    .WithEnvironment(\"AGENT_HOST\", backend.GetEndpoint(\"http\"))\n    .WithEnvironment(\"STAY_ALIVE_ON_GOODBYE\", \"true\")\n    .WithEnvironment(\"GRPC_DNS_RESOLVER\", \"native\")\n    .WithOtlpExporter()\n    .WaitFor(client);\nusing var app = builder.Build();\nawait app.StartAsync();\nvar url = backend.GetEndpoint(\"http\").Url;\nConsole.WriteLine(\"Backend URL: \" + url);\nawait app.WaitForShutdownAsync();\n```\n\nYou can find more examples of how to use Aspire and XLang agents in the [Microsoft.AutoGen.Integration.Tests.AppHost](https://github.com/microsoft/autogen/blob/acd7e864300e24a3ee67a89a916436e8894bb143/dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/) directory. \n\n### Configuring Logging\n\nThe SDK uses the Microsoft.Extensions.Logging framework for logging. Here is an example appsettings.json file with some useful defaults:\n\n```json\n{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Warning\",\n      \"Microsoft.Hosting.Lifetime\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Information\",\n      \"Microsoft\": \"Information\",\n      \"Microsoft.Orleans\": \"Warning\",\n      \"Orleans.Runtime\": \"Error\",\n      \"Grpc\": \"Information\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"Kestrel\": {\n    \"EndpointDefaults\": {\n      \"Protocols\": \"Http2\"\n    }\n  }\n}\n```\n\n### Defining Message Types in Protocol Buffers\n\nA convenient way to define common event or message types to be used in both python and .NET agents is to define your events. This is covered here: [Using Protocol Buffers to Define Message Types](./protobuf-message-types.md).",
    "filename": "docs/dotnet/core/index.md"
  },
  {
    "content": "# Installation\n\nInstall via `.NET cli`\n\n```sh\ndotnet add package Microsoft.AutoGen.Contracts --version 0.4.0-dev.1\ndotnet add package Microsoft.AutoGen.Core --version 0.4.0-dev.1\n```\n\nOr, install via `Package Manager`\n\n```pwsh\nPM> NuGet\\Install-Package Microsoft.AutoGen.Contracts -Version 0.4.0-dev.1\nPM> NuGet\\Install-Package Microsoft.AutoGen.Core -Version 0.4.0-dev.1\n```\n\nOr, add via `<PackageReference>`\n\n```xml\n<PackageReference Include=\"Microsoft.AutoGen.Contracts\" Version=\"0.4.0-dev.1\" />\n<PackageReference Include=\"Microsoft.AutoGen.Core\" Version=\"0.4.0-dev.1\" />\n```\n\n# Additional Packages\n\nThe *Core* and *Contracts* packages will give you what you need for writing and running agents using the Core API within a single process. \n\n- *Microsoft.AutoGen.AgentChat* - An implementation of the AgentChat package for building chat-centric agent orchestration on top of the Core SDK\n- *Microsoft.AutoGen.Agents* - a package that has a small number of default agents you can use. \n- *Microsoft.AutoGen.Extensions* - Extensions to support closely related projects including Aspire, Microsoft.Extensions.AI, and Semantic Kernel\n\n```sh\ndotnet add package Microsoft.AutoGen.AgentChat --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.Agents --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.Extensions --version 0.4.0-dev-1\n```\n\nTo enable running a system with agents in different processes that allows for x-language communication between python and .NET agents, there are additional packages:\n\n- *Microsoft.AutoGen.Core.Grpc* - the .NET client runtime for agents in a distributed system. It has the same API as *Microsoft.AutoGen.Core*. \n- *Microsoft.AutoGen.RuntimeGatewway.Grpc* - the .NET server side of the distributed system that allows you to run multiple gateways to manage fleets of agents and enables x-language interoperability. \n- *Microsoft.AutoGen.AgentHost* - A .NET Aspire project that hosts the Grpc Service\n\n```sh\ndotnet add package Microsoft.AutoGen.Core.Grpc --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.RuntimeGateway.Grpc --version 0.4.0-dev-1\ndotnet add package Microsoft.AutoGen.AgentHost --version 0.4.0-dev-1\n```",
    "filename": "docs/dotnet/core/installation.md"
  },
  {
    "content": "# Using Protocol Buffers to Define Message Types\n\nFor a message to be sent using a runtime other than the @Microsoft.AutoGen.Core.InProcessRuntime, it must be defined as a Protocol Buffers message. This is because the message is serialized and deserialized using Protocol Buffers. This requirement may be relaxed in future by allowing for converters, custom serialization, or other mechanisms.\n\n## How to include Protocol Buffers in a .NET project\n\nThe .proto file which defines the message types must be included in the project, which will automatically generate the C# classes for the messages.\n\n1. Include `Grpc.Tools` package in your `.csproj` file:\n\n```xml\n  <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n```\n\n2. Create an include a `.proto` file in the project:\n\n```xml\n<ItemGroup>\n  <Protobuf Include=\"messages.proto\" GrpcServices=\"Client;Server\" Link=\"messages.proto\" />\n</ItemGroup>\n```\n\n3. define your messages as specified in the [Protocol Buffers Language Guide](https://protobuf.dev/programming-guides/proto3/)\n\n```proto\nsyntax = \"proto3\";\n\npackage HelloAgents;\n\noption csharp_namespace = \"MyAgentsProtocol\";\n\nmessage TextMessage {\n    string Source = 1;\n    string Content = 2;\n}\n```\n\n4. Code against the generated class for handling, sending and publishing messages:\n\n```csharp\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\nusing MyAgentsProtocol;\n\n[TypeSubscription(\"default\")]\npublic class Checker(\n    AgentId id,\n    IAgentRuntime runtime,\n    ) :\n        BaseAgent(id, runtime, \"MyAgent\", null),\n        IHandle<TextMessage>\n{\n    public async ValueTask HandleAsync(TextMessage item, MessageContext messageContext)\n    {\n        Console.WriteLine($\"Received message from {item.Source}: {item.Content}\");\n    }\n}\n```",
    "filename": "docs/dotnet/core/protobuf-message-types.md"
  },
  {
    "content": "# Tutorial\n\n> [!TIP]\n> If you'd prefer to just see the code the entire sample is available as a [project here](https://github.com/microsoft/autogen/tree/main/dotnet/samples/GettingStarted).\n\nIn this tutorial we are going to define two agents, `Modifier` and `Checker`, that will count down from 10 to 1. The `Modifier` agent will modify the count and the `Checker` agent will check the count and stop the application when the count reaches 1.\n\n## Defining the message types\n\nThe first thing we need to do is to define the messages that will be passed between the agents, we're simply going to define them as classes.\n\nWe're going to use `CountMessage` to pass the current count and `CountUpdate` to pass the updated count.\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/CountMessage.cs#snippet_CountMessage)]\n[!code-csharp[](../../../dotnet/samples/GettingStarted/CountUpdate.cs#snippet_CountUpdate)]\n\nBy separating out the message types into strongly typed classes, we can build a workflow where agents react to certain types and produce certain types.\n\n## Creating the agents\n\n### Inherit from `BaseAgent`\n\nIn AutoGen an agent is a class that can receive and send messages. The agent defines its own logic of what to do with the messages it receives. To define an agent, create a class that inherits from @Microsoft.AutoGen.Core.BaseAgent, like so:\n\n```csharp\nusing Microsoft.AutoGen.Contracts;\nusing Microsoft.AutoGen.Core;\n\npublic class Modifier(\n    AgentId id,\n    IAgentRuntime runtime,\n    ) :\n        BaseAgent(id, runtime, \"MyAgent\", null),\n{\n}\n```\n\nWe will see how to pass arguments to an agent when it is constructed, but for now you just need to know that @Microsoft.AutoGen.Contracts.AgentId and @Microsoft.AutoGen.Core.IAgentRuntime will always be passed to the constructor, and those should be forwarded to the base class constructor. The other two arguments are a description of the agent and an optional logger.\n\nLearn more about what an Agent ID is [here](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle.html#agent-id).\n\n### Create a handler\n\nNow, we want `Modifier` to receive `CountMessage` and produce `CountUpdate` after it modifies the count. To do this, we need to implement the `IHandle<CountMessage>` interface:\n\n```csharp\npublic class Modifier(\n    // ...\n    ) :\n        BaseAgent(...),\n        IHandle<CountMessage>\n{\n\n    public async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n    {\n        // ...\n    }\n}\n```\n\n### Add a subscription\n\nWe've defined a function that will be called when a `CountMessage` is delivered to this agent, but there is still one step before the message will actually be delivered to the agent. The agent must subscribe to the topic to the message is published to. We can do this by adding the `TypeSubscription` attribute to the class:\n\n```csharp\n[TypeSubscription(\"default\")]\npublic class Modifier(\n    // ...\n```\n\nLearn more about topics and subscriptions [here](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/topic-and-subscription.html).\n\n### Publish a message\n\nNow that we have a handler for `CountMessage`, and we have the subscription in place we can publish a result out of the handler.\n\n```csharp\npublic async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n{\n    int newValue = item.Content - 1;\n    Console.WriteLine($\"\\nModifier:\\nModified {item.Content} to {newValue}\");\n\n    CountUpdate updateMessage = new CountUpdate { NewCount = newValue };\n    await this.PublishMessageAsync(updateMessage, topic: new TopicId(\"default\"));\n}\n```\n\nYou'll notice that when we publish the message, we specify the topic to publish to. We're using a topic called `default` in this case, which is the same topic which we subscribed to. We could have used a different topic, but in this case we're keeping it simple.\n\n### Passing arguments to the agent\n\nLet's extend our agent to make what we do to the count configurable. We'll do this by passing a function to the agent that will be used to modify the count.\n\n```csharp\nusing ModifyF = System.Func<int, int>;\n\n// ...\n\n[TypeSubscription(\"default\")]\npublic class Modifier(\n    AgentId id,\n    IAgentRuntime runtime,\n    ModifyF modifyFunc // <-- Add this\n    ) :\n        BaseAgent(...),\n        IHandle<CountMessage>\n{\n\n    public async ValueTask HandleAsync(CountMessage item, MessageContext messageContext)\n    {\n        int newValue = modifyFunc(item.Content); // <-- use it here\n\n        // ...\n    }\n}\n\n```\n\n### Final Modifier implementation\n\nHere is the final implementation of the Modifier agent:\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Modifier.cs#snippet_Modifier)]\n\n### Checker\n\nWe'll also define a Checker agent that will check the count and stop the application when the count reaches 1. Additionally, we'll use dependency injection to get a reference to the `IHostApplicationLifetime` service, which we can use to stop the application.\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Checker.cs#snippet_Checker)]\n\n## Putting it all together\n\nNow that we have our agents defined, we can put them together in a simple application that will count down from 10 to 1.\n\nAfter includes, the first thing to do is to define the two functions for modifying and checking for completion.\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Program.cs#snippet_Program_funcs)]\n\nThen, we create a builder and do the following things:\n\n- Specify that we are using the in process runtime\n- Register our functions as services\n- Register the agent classes we defined earlier\n- Finally, build and start our app\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Program.cs#snippet_Program_builder)]\n\nThe app is now running, but we need to kick off the process with a message. We do this by publishing a `CountMessage` with an initial value of 10.\nImportantly we publish this to the \"default\" topic which is what our agents are subscribed to. Finally, we wait for the application to stop.\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Program.cs#snippet_Program_publish)]\n\nThat's it! You should see the count down from 10 to 1 in the console.\n\nHere's the full code for the `Program` class:\n\n[!code-csharp[](../../../dotnet/samples/GettingStarted/Program.cs#snippet_Program)]\n\n## Things to try\n\nHere are some ideas to try with this sample:\n\n- Change the initial count\n- Create a new modifier function that counts up instead. (Don't forget to change the checker too!)\n- Create an agent that outputs to the console instead of the modifier or checker agent doing it themselves (hint: use a new message type)",
    "filename": "docs/dotnet/core/tutorial.md"
  },
  {
    "_disableAffix": true,
    "content": "<div class=\"center\">\n    <h1>AutoGen .NET</h1>\n    <p class=\"subheader\">\n    A <i>.NET</i> framework for building AI agents and applications\n    </p>\n</div>\n\n<div class=\"row\">\n  <div class=\"col-sm-6\">\n    <div class=\"card\">\n      <div class=\"card-body\">\n        <h5 class=\"card-title\">Core</h5>\n<p>\n\n[![dotnet-ci](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml)\n[![NuGet version](https://badge.fury.io/nu/Microsoft.AutoGen.Contracts.svg)](https://badge.fury.io/nu/Microsoft.AutoGen.Contracts)\n[![NuGet version](https://badge.fury.io/nu/Microsoft.AutoGen.Core.svg)](https://badge.fury.io/nu/Microsoft.AutoGen.Core)\n[![NuGet version](https://badge.fury.io/nu/Microsoft.AutoGen.Core.Grpc.svg)](https://badge.fury.io/nu/Microsoft.AutoGen.Core.Grpc)\n[![NuGet version](https://badge.fury.io/nu/Microsoft.AutoGen.RuntimeGateway.Grpc.svg)](https://badge.fury.io/nu/Microsoft.AutoGen.RuntimeGateway.Grpc)\n[![NuGet version](https://badge.fury.io/nu/Microsoft.AutoGen.AgentHost.svg)](https://badge.fury.io/nu/Microsoft.AutoGen.AgentHost)\n\n</p>\n        <p class=\"card-text\">An event-driven programming framework for building scalable multi-agent AI systems.</p>\n\n- Deterministic and dynamic agentic workflows for business processes\n- Research on multi-agent collaboration\n- Distributed agents for multi-language applications\n- integration with event-driven, cloud native applications\n\n*Start here if you are building workflows or distributed agent systems*\n\n<p>\n<div class=\"highlight\">\n<pre id=\"codecell0\" tabindex=\"0\">\n\n```bash\ndotnet add package Microsoft.AutoGen.Contracts\ndotnet add package Microsoft.AutoGen.Core\n\n# optionally - for distributed agent systems:\ndotnet add package Microsoft.AutoGen.RuntimeGateway.Grpc\ndotnet add package Microsoft.AutoGen.AgentHost\n\n# other optional packages\ndotnet add package Microsoft.AutoGen.Agents\ndotnet add package Microsoft.AutoGen.Extensions.Aspire\ndotnet add package Microsoft.AutoGen.Extensions.MEAI\ndotnet add package Microsoft.AutoGen.Extensions.SemanticKernel\n```\n\n</pre></div></p>\n<p>\n        <a href=\"core/index.md\" class=\"btn btn-primary\">Get started</a>\n      </div>\n    </div>\n  </div>\n  <div class=\"col-sm-6\">\n    <div class=\"card\">\n      <div class=\"card-body\">\n        <h5 class=\"card-title\">AgentChat</h5>\n        <p class=\"card-text\">A programming framework for building conversational single and multi-agent applications. Built on Core.</p>\n        <a href=\"#\" class=\"btn btn-primary disabled\">Coming soon</a>\n      </div>\n    </div>\n  </div>\n</div>",
    "filename": "docs/dotnet/index.md"
  },
  {
    "content": "# Packaging AutoGen.NET\n\nThis document describes the steps to pack the `AutoGen.NET` project.\n\n## Prerequisites\n\n- .NET SDK\n\n## Create Package\n\n1. **Restore and Build the Project**\n```bash\ndotnet restore\ndotnet build --configuration Release --no-restore\n```\n\n\n2. **Create the NuGet Package**\n```bash\ndotnet pack --configuration Release --no-build\n```\n\nThis will generate both the `.nupkg` file and the `.snupkg` file in the `./artifacts/package/release` directory.\n\nFor more details, refer to the [official .NET documentation](https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-pack).\n\n## Add new project to package list.\nBy default, when you add a new project to `AutoGen.sln`, it will not be included in the package list. To include the new project in the package, you need to add the following line to the new project's `.csproj` file\n\ne.g.\n\n```xml\n<Import Project=\"$(RepoRoot)/nuget/nuget-package.props\" />\n```\n\nThe `nuget-packages.props` enables `IsPackable` to `true` for the project, it also sets nenecessary metadata for the package.\n\nFor more details, refer to the [NuGet folder](./nuget/README.md).\n\n## Package versioning\nThe version of the package is defined by `VersionPrefix` and `VersionPrefixForAutoGen0_2` in [MetaInfo.props](./eng/MetaInfo.props). If the name of your project starts with `AutoGen.`, the version will be set to `VersionPrefixForAutoGen0_2`, otherwise it will be set to `VersionPrefix`.",
    "filename": "dotnet/PACKAGING.md"
  },
  {
    "content": "# AutoGen for .NET\n\nThre are two sets of packages here:\nAutoGen.\\* the older packages derived from AutoGen 0.2 for .NET - these will gradually be deprecated and ported into the new packages\nMicrosoft.AutoGen.* the new packages for .NET that use the event-driven model - These APIs are not yet stable and are subject to change.\n\nTo get started with the new packages, please see the [samples](./samples/) and in particular the [Hello](./samples/Hello) sample.\n\nYou can install both new and old packages from the following feeds:\n\n[![dotnet-ci](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml)\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\n> [!NOTE]\n> Nightly build is available at:\n>\n> - [![Static Badge](https://img.shields.io/badge/azure_devops-grey?style=flat)](https://dev.azure.com/AGPublish/AGPublic/_artifacts/feed/AutoGen-Nightly) : <https://pkgs.dev.azure.com/AGPublish/AGPublic/_packaging/AutoGen-Nightly/nuget/v3/index.json>\n\nFirstly, following the [installation guide](./website/articles/Installation.md) to install AutoGen packages.\n\nThen you can start with the following code snippet to create a conversable agent and chat with it.\n\n```csharp\nusing AutoGen;\nusing AutoGen.OpenAI;\n\nvar openAIKey = Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\") ?? throw new Exception(\"Please set OPENAI_API_KEY environment variable.\");\nvar gpt35Config = new OpenAIConfig(openAIKey, \"gpt-3.5-turbo\");\n\nvar assistantAgent = new AssistantAgent(\n    name: \"assistant\",\n    systemMessage: \"You are an assistant that help user to do some tasks.\",\n    llmConfig: new ConversableAgentConfig\n    {\n        Temperature = 0,\n        ConfigList = [gpt35Config],\n    })\n    .RegisterPrintMessage(); // register a hook to print message nicely to console\n\n// set human input mode to ALWAYS so that user always provide input\nvar userProxyAgent = new UserProxyAgent(\n    name: \"user\",\n    humanInputMode: HumanInputMode.ALWAYS)\n    .RegisterPrintMessage();\n\n// start the conversation\nawait userProxyAgent.InitiateChatAsync(\n    receiver: assistantAgent,\n    message: \"Hey assistant, please do me a favor.\",\n    maxRound: 10);\n```\n\n## Samples\n\nYou can find more examples under the [sample project](https://github.com/microsoft/autogen/tree/dotnet/samples/AgentChat/Autogen.Basic.Sample).\n\n## Functionality\n\n- ConversableAgent\n  - [x] function call\n  - [x] code execution (dotnet only, powered by [`dotnet-interactive`](https://github.com/dotnet/interactive))\n\n- Agent communication\n  - [x] Two-agent chat\n  - [x] Group chat\n\n- [ ] Enhanced LLM Inferences\n\n- Exclusive for dotnet\n  - [x] Source generator for type-safe function definition generation",
    "filename": "dotnet/README.md"
  },
  {
    "content": "### About AutoGen for .NET\n`AutoGen for .NET` is the official .NET SDK for [AutoGen](https://github.com/microsoft/autogen). It enables you to create LLM agents and construct multi-agent workflows with ease. It also provides integration with popular platforms like OpenAI, Semantic Kernel, and LM Studio.\n\n### Gettings started\n- Find documents and examples on our [document site](https://microsoft.github.io/autogen-for-net/)\n- Report a bug or request a feature by creating a new issue in our [github repo](https://github.com/microsoft/autogen)\n- Consume the nightly build package from one of the [nightly build feeds](https://microsoft.github.io/autogen-for-net/articles/Installation.html#nighly-build)",
    "filename": "dotnet/nuget/NUGET.md"
  },
  {
    "content": "# NuGet Directory\n\nThis directory contains resources and metadata for packaging the AutoGen.NET SDK as a NuGet package.\n\n## Files\n\n- **icon.png**: The icon used for the NuGet package.\n- **NUGET.md**: The readme file displayed on the NuGet package page.\n- **NUGET-PACKAGE.PROPS**: The MSBuild properties file that defines the packaging settings for the NuGet package.\n\n## Purpose\n\nThe files in this directory are used to configure and build the NuGet package for the AutoGen.NET SDK, ensuring that it includes necessary metadata, documentation, and resources.",
    "filename": "dotnet/nuget/README.md"
  },
  {
    "content": "# AutoGen 0.4 .NET Hello World Sample\n\nThis [sample](Program.cs) demonstrates how to create a simple .NET console application that listens for an event and then orchestrates a series of actions in response.\n\n## Prerequisites\n\nTo run this sample, you'll need: [.NET 8.0](https://dotnet.microsoft.com/en-us/) or later.\nAlso recommended is the [GitHub CLI](https://cli.github.com/).\n\n## Instructions to run the sample\n\n```bash\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n## Key Concepts\n\nThis sample illustrates how to create your own agent that inherits from a base agent and listens for an event. It also shows how to use the SDK's App Runtime locally to start the agent and send messages.\n\nFlow Diagram:\n\n```mermaid\n%%{init: {'theme':'forest'}}%%\ngraph LR;\n    A[Main] --> |\"PublishEventAsync(NewMessage('World'))\"| B{\"Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Hello, World***'))\"| C[ConsoleAgent]\n    C --> D{\"WriteConsole()\"}\n    B --> |\"PublishEventAsync(ConversationClosed('Goodbye'))\"| E{\"Handle(ConversationClosed item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Goodbye***'))\"| C\n    E --> F{\"Shutdown()\"}\n\n```\n\n### Writing Event Handlers\n\nThe heart of an autogen application are the event handlers. Agents select a ```TopicSubscription``` to listen for events on a specific topic. When an event is received, the agent's event handler is called with the event data.\n\nWithin that event handler you may optionally *emit* new events, which are then sent to the event bus for other agents to process. The EventTypes are declared gRPC ProtoBuf messages that are used to define the schema of the event.  The default protos are available via the ```Microsoft.AutoGen.Contracts;``` namespace and are defined in [autogen/protos](/autogen/protos). The EventTypes are registered in the agent's constructor using the ```IHandle``` interface.\n\n```csharp\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n### Inheritance and Composition\n\nThis sample also illustrates inheritance in AutoGen. The `HelloAgent` class inherits from `ConsoleAgent`, which is a base class that provides a `WriteConsole` method.\n\n### Starting the Application Runtime\n\nAuotoGen provides a flexible runtime ```Microsoft.AutoGen.Agents.App``` that can be started in a variety of ways. The `Program.cs` file demonstrates how to start the runtime locally and send a message to the agent all in one go using the ```App.PublishMessageAsync``` method.\n\n```csharp\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n### Sending Messages\n\nThe set of possible Messages is defined in gRPC ProtoBuf specs. These are then turned into C# classes by the gRPC tools. You can define your own Message types by creating a new .proto file in your project and including the gRPC tools in your ```.csproj``` file:\n\n```proto\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n```xml\n  <ItemGroup>\n    <PackageReference Include=\"Google.Protobuf\" />\n    <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n    <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n  </ItemGroup>\n```\n\nYou can send messages using the [```Microsoft.AutoGen.Agents.AgentWorker``` class](autogen/dotnet/src/Microsoft.AutoGen/Agents/AgentWorker.cs). Messages are wrapped in [the CloudEvents specification](https://cloudevents.io) and sent to the event bus.",
    "filename": "dotnet/samples/Hello/HelloAgent/README.md"
  },
  {
    "content": "# AutoGen 0.4 .NET Hello World Sample\n\nThis [sample](Program.cs) demonstrates how to create a simple .NET console application that listens for an event and then orchestrates a series of actions in response.\n\n## Prerequisites\n\nTo run this sample, you'll need: [.NET 8.0](https://dotnet.microsoft.com/en-us/) or later.\nAlso recommended is the [GitHub CLI](https://cli.github.com/).\n\n## Instructions to run the sample\n\n```bash\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n## Key Concepts\n\nThis sample illustrates how to create your own agent that inherits from a base agent and listens for an event. It also shows how to use the SDK's App Runtime locally to start the agent and send messages.\n\nFlow Diagram:\n\n```mermaid\n%%{init: {'theme':'forest'}}%%\ngraph LR;\n    A[Main] --> |\"PublishEventAsync(NewMessage('World'))\"| B{\"Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Hello, World***'))\"| C[ConsoleAgent]\n    C --> D{\"WriteConsole()\"}\n    B --> |\"PublishEventAsync(ConversationClosed('Goodbye'))\"| E{\"Handle(ConversationClosed item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Goodbye***'))\"| C\n    E --> F{\"Shutdown()\"}\n\n```\n\n### Writing Event Handlers\n\nThe heart of an autogen application are the event handlers. Agents select a ```TopicSubscription``` to listen for events on a specific topic. When an event is received, the agent's event handler is called with the event data.\n\nWithin that event handler you may optionally *emit* new events, which are then sent to the event bus for other agents to process. The EventTypes are declared gRPC ProtoBuf messages that are used to define the schema of the event.  The default protos are available via the ```Microsoft.AutoGen.Contracts;``` namespace and are defined in [autogen/protos](/autogen/protos). The EventTypes are registered in the agent's constructor using the ```IHandle``` interface.\n\n```csharp\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n### Inheritance and Composition\n\nThis sample also illustrates inheritance in AutoGen. The `HelloAgent` class inherits from `ConsoleAgent`, which is a base class that provides a `WriteConsole` method.\n\n### Starting the Application Runtime\n\nAuotoGen provides a flexible runtime ```Microsoft.AutoGen.Agents.App``` that can be started in a variety of ways. The `Program.cs` file demonstrates how to start the runtime locally and send a message to the agent all in one go using the ```App.PublishMessageAsync``` method.\n\n```csharp\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n### Sending Messages\n\nThe set of possible Messages is defined in gRPC ProtoBuf specs. These are then turned into C# classes by the gRPC tools. You can define your own Message types by creating a new .proto file in your project and including the gRPC tools in your ```.csproj``` file:\n\n```proto\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n```xml\n  <ItemGroup>\n    <PackageReference Include=\"Google.Protobuf\" />\n    <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n    <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n  </ItemGroup>\n```\n\nYou can send messages using the [```Microsoft.AutoGen.Agents.AgentWorker``` class](autogen/dotnet/src/Microsoft.AutoGen/Agents/AgentWorker.cs). Messages are wrapped in [the CloudEvents specification](https://cloudevents.io) and sent to the event bus.\n\n### Managing State\n\nThere is a simple API for persisting agent state.\n\n```csharp\n            await Store(new AgentState \n            {\n                AgentId = this.AgentId,\n                TextData = entry\n            }).ConfigureAwait(false);\n```\n\nwhich can be read back using Read:\n\n```csharp\n            State = await Read<AgentState>(this.AgentId).ConfigureAwait(false);\n```",
    "filename": "dotnet/samples/Hello/HelloAgentState/README.md"
  },
  {
    "content": "# Multiproject App Host for HelloAgent\n\nThis is a [.NET Aspire](https://learn.microsoft.com/en-us/dotnet/aspire/get-started/aspire-overview) App Host that starts up the HelloAgent project and the agents backend. Once the project starts up you will be able to view the telemetry and logs in the [Aspire Dashboard](https://learn.microsoft.com/en-us/dotnet/aspire/get-started/aspire-dashboard) using the link provided in the console.\n\n```shell\ncd Hello.AppHost\ndotnet run\n```\n\nFor more info see the HelloAgent [README](../HelloAgent/README.md).",
    "filename": "dotnet/samples/Hello/README.md"
  },
  {
    "content": "# GitHub Dev Team with AI Agents\n\nBuild a Dev Team using event driven agents. This project is an experiment and is not intended to be used in production.\n\n## Background\n\nFrom a natural language specification, set out to integrate a team of AI agents into your team\u2019s dev process, either for discrete tasks on an existing repo (unit tests, pipeline expansions, PRs for specific intents), developing a new feature, or even building an application from scratch.  Starting from an existing repo and a broad statement of intent, work with multiple AI agents, each of which has a different emphasis - from architecture, to task breakdown, to plans for individual tasks, to code output, code review, efficiency, documentation, build, writing tests, setting up pipelines, deployment, integration tests, and then validation.\nThe system will present a view that facilitates chain-of-thought coordination across multiple trees of reasoning with the dev team agents.\n\n\n\n## Get it running\n\nCheck [the getting started guide](./docs/github-flow-getting-started.md).\n\n## Demo\n\nhttps://github.com/microsoft/azure-openai-dev-skills-orchestrator/assets/10728102/cafb1546-69ab-4c27-aaf5-1968313d637f\n\n## Solution overview\n\n![General overview](./docs/images/overview.png)\n\n## How it works\n\n* User begins with creating an issue and then stateing what they want to accomplish, natural language, as simple or as detailed as needed.\n* Product manager agent will respond with a Readme, which can be iterated upon.\n  * User approves the readme or gives feedback via issue comments.\n  * Once the readme is approved, the user closes the issue and the Readme is commited to a PR.\n* Developer lead agent responds with a decomposed plan for development, which also can be iterated upon.\n  * User approves the plan or gives feedback via issue comments.\n  * Once the readme is approved, the user closes the issue and the plan is used to break down the task to different developer agents.\n* Developer agents respond with code, which can be iterated upon.\n  * User approves the code or gives feedback via issue comments.\n  * Once the code is approved, the user closes the issue and the code is commited to a PR.\n\n```mermaid\ngraph TD;\n    NEA([NewAsk event]) -->|Hubber| NEA1[Creation of PM issue, DevLead issue, and new branch];\n    \n    RR([ReadmeRequested event]) -->|ProductManager| PM1[Generation of new README];\n    NEA1 --> RR;\n    PM1 --> RG([ReadmeGenerated event]);\n    RG -->|Hubber| RC[Post the readme as a new comment on the issue];\n    RC --> RCC([ReadmeChainClosed event]);\n    RCC -->|ProductManager| RCR([ReadmeCreated event]);\n    RCR --> |AzureGenie| RES[Store Readme in blob storage];\n    RES --> RES2([ReadmeStored event]);\n    RES2 --> |Hubber| REC[Readme commited to branch and create new PR];\n\n    DPR([DevPlanRequested event]) -->|DeveloperLead| DPG[Generation of new development plan];\n    NEA1 --> DPR;\n    DPG --> DPGE([DevPlanGenerated event]);\n    DPGE -->|Hubber| DPGEC[Posting the plan as a new comment on the issue];\n    DPGEC --> DPCC([DevPlanChainClosed event]);\n    DPCC -->|DeveloperLead| DPCE([DevPlanCreated event]);\n    DPCE --> |Hubber| DPC[Creates a Dev issue for each subtask];\n\n    DPC([CodeGenerationRequested event]) -->|Developer| CG[Generation of new code];\n    CG --> CGE([CodeGenerated event]);\n    CGE -->|Hubber| CGC[Posting the code as a new comment on the issue];\n    CGC --> CCCE([CodeChainClosed event]);\n    CCCE -->|Developer| CCE([CodeCreated event]);\n    CCE --> |AzureGenie| CS[Store code in blob storage and schedule a run in the sandbox];\n    CS --> SRC([SandboxRunCreated event]);\n    SRC --> |Sandbox| SRM[Check every minute if the run finished];\n    SRM --> SRF([SandboxRunFinished event]);\n    SRF --> |Hubber| SRCC[Code files commited to branch];\n```",
    "filename": "dotnet/samples/dev-team/README.md"
  },
  {
    "content": "## Prerequisites\n\n- Access to gpt3.5-turbo or preferably gpt4 - [Get access here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai)\n- [Setup a Github app](#how-do-i-setup-the-github-app)\n- [Install the Github app](https://docs.github.com/en/apps/using-github-apps/installing-your-own-github-app)\n- [Provision the azure resources](#how-do-I-deploy-the-azure-bits)\n- [Create labels for the dev team skills](#which-labels-should-i-create)\n\n### How do I setup the Github app?\n\n- [Register a Github app](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app), with the options listed below:\n    - Give your App a name and add a description\n    - Homepage URL: Can be anything (Example: repository URL)\n    - Add a dummy value for the webhook url, we'll come back to this setting\n    - Enter a webhook secret, which you'll need later on when filling in the `WebhookSecret` property in the `appsettings.json` file\n    - Setup the following permissions\n        - Repository \n            - Contents - read and write\n            - Issues - read and write\n            - Metadata - read only\n            - Pull requests - read and write\n    - Subscribe to the following events:\n        - Issues\n        - Issue comment\n    - Allow this app to be installed by any user or organization\n    \n- After the app is created, generate a private key, we'll use it later for authentication to Github from the app\n\n### Which labels should I create?\n\nIn order for us to know which skill and persona we need to talk with, we are using Labels in Github Issues.\n\nThe default bunch of skills and personnas are as follow:\n- PM.Readme\n- Do.It\n- DevLead.Plan\n- Developer.Implement\n\nAdd them to your repository (They are not there by default).\n\nOnce you start adding your own skills, just remember to add the corresponding label to your repository.\n\n## How do I run this locally?\n\nCodespaces are preset for this repo. For codespaces there is a 'free' tier for individual accounts. See: https://github.com/pricing\nStart by creating a codespace:\nhttps://docs.github.com/en/codespaces/developing-in-a-codespace/creating-a-codespace-for-a-repository\n\n![Alt text](./images/new-codespace.png)\n\nIn this sample's folder there are two files called appsettings.azure.template.json and appsettings.local.template.json. If you run this demo locally, use the local template and if you want to run it within Azure use the Azure template. Rename the selected file to appsettings.json and fill out the config values within the file.\n\n### GitHubOptions\n\nFor the GitHubOptions section, you'll need to fill in the following values:\n- **AppKey (PrivateKey)**: this is a key generated while creating a GitHub App. If you haven't saved it during creation, you'll need to generate a new one. Go to the settings of your GitHub app, scroll down to \"Private keys\" and click on \"Generate a new private key\". It will download a .pem file that contains your App Key. Then copy and paste all the **-----BEGIN RSA PRIVATE KEY---- your key -----END RSA PRIVATE KEY-----** content here, in one line.\n- **AppId**: This can be found on the same page where you created your app. Go to the settings of your GitHub app and you can see the App ID at the top of the page.\n- **InstallationId**: Access to your GitHub app installation and take note of the number (long type) at the end of the URL (which should be in the following format: https://github.com/settings/installations/installation-id).\n- **WebhookSecret**: This is a value that you set when you create your app. In the app settings, go to the \"Webhooks\" section. Here you can find the \"Secret\" field where you can set your Webhook Secret.\n\n### AzureOptions\n\nThe following fields are required and need to be filled in:\n- **SubscriptionId**: The id of the subscription you want to work on.\n- **Location**\n- **ContainerInstancesResourceGroup**: The name of the resource group where container instances will be deployed.\n- **FilesAccountName**: Azure Storage Account name.\n- **FilesShareName**: The name of the File Share.\n- **FilesAccountKey**: The File Account key.\n- **SandboxImage**\n\nIn the Explorer tab in VS Code, find the Solution explorer, right click on the `gh-flow` project and click Debug -> Start new instance\n\n![Alt text](./images/solution-explorer.png)\n\nWe'll need to expose the running application to the GH App webhooks, for example using [DevTunnels](https://learn.microsoft.com/en-us/azure/developer/dev-tunnels/overview), but any tool like ngrok can also work.\nThe following commands will create a persistent tunnel, so we need to only do this once:\n```bash\nTUNNEL_NAME=_name_your_tunnel_here_\ndevtunnel user login\ndevtunnel create -a $TUNNEL_NAME\ndevtunnel port create -p 5244 $TUNNEL_NAME\n```\nand once we have the tunnel created we can just start forwarding with the following command:\n\n```bash\ndevtunnel host $TUNNEL_NAME\n```\n\nCopy the local address (it will look something like https://your_tunnel_name.euw.devtunnels.ms) and append `/api/github/webhooks` at the end. Using this value, update the Github App's webhook URL and you are ready to go!\n\nBefore you go and have the best of times, there is one last thing left to do [load the WAF into the vector DB](#load-the-waf-into-qdrant)\n\nAlso, since this project is relying on Orleans for the Agents implementation, there is a [dashboard](https://github.com/OrleansContrib/OrleansDashboard) available at https://yout_tunnel_name.euw.devtunnels.ms/dashboard, with useful metrics and stats related to the running Agents.\n\n## How do I deploy the azure bits?\n\nThis sample is setup to use  [azd](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/overview) to work with the Azure bits. `azd` is installed in the codespace.\n\nLet's start by logging in to Azure using\n```bash\nazd auth login\n```\n\nAfter we've logged in, we need to create a new environment provision the azure bits.\n\n```bash\nENVIRONMENT=_name_of_your_env\nazd env new $ENVIRONMENT\nazd provision -e $ENVIRONMENT\n```\nAfter the provisioning is done, you can inspect the outputs with the following command\n\n```bash\nazd env get-values -e dev\n```\nAs the last step, we also need to [load the WAF into the vector DB](#load-the-waf-into-qdrant)\n\n### Load the WAF into Qdrant. \n\nIf you are running the app locally, we have [Qdrant](https://qdrant.tech/) setup in the Codespace and if you are running in Azure, Qdrant is deployed to ACA.\nThe loader is a project in the `samples` folder, called `seed-memory`. We need to fill in the `appsettings.json` (after renaming `appsettings.template.json` in `appsettings.json`) file in the `config` folder with the OpenAI details and the Qdrant endpoint, then just run the loader with `dotnet run` and you are ready to go.\n\n\n\n### WIP Local setup\n\n```\ndotnet user-secrets set \"OpenAI:Key\" \"your_key\"\n\ndotnet user-secrets set \"OpenAI:Endpoint\" \"https://your_endpoint.openai.azure.com/\"\n\ndotnet user-secrets set \"Github:AppId\" \"gh_app_id\"\n\ndotnet user-secrets set \"Github:InstallationId\" \"gh_inst_id\"\n\ndotnet user-secrets set \"Github:WebhookSecret\" \"webhook_secret\"\n\ndotnet user-secrets set \"Github:AppKey\" \"gh_app_key\"\n```",
    "filename": "dotnet/samples/dev-team/docs/github-flow-getting-started.md"
  },
  {
    "content": "# TODO",
    "filename": "dotnet/samples/dev-team/seed-memory/README.md"
  },
  {
    "content": "## AutoGen.LMStudio\n\nThis package provides support for consuming openai-like API from LMStudio local server.\n\n## Installation\nTo use `AutoGen.LMStudio`, add the following package to your `.csproj` file:\n\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n## Usage\n```csharp\nusing AutoGen.LMStudio;\nvar localServerEndpoint = \"localhost\";\nvar port = 5000;\nvar lmStudioConfig = new LMStudioConfig(localServerEndpoint, port);\nvar agent = new LMStudioAgent(\n    name: \"agent\",\n    systemMessage: \"You are an agent that help user to do some tasks.\",\n    lmStudioConfig: lmStudioConfig)\n    .RegisterPrintMessage(); // register a hook to print message nicely to console\n\nawait agent.SendAsync(\"Can you write a piece of C# code to calculate 100th of fibonacci?\");\n```\n\n## Update history\n### Update on 0.0.7 (2024-02-11)\n- Add `LMStudioAgent` to support consuming openai-like API from LMStudio local server.",
    "filename": "dotnet/src/AutoGen.LMStudio/README.md"
  },
  {
    "content": "### AutoGen.SourceGenerator\n\nThis package carries a source generator that adds support for type-safe function definition generation. Simply mark a method with `Function` attribute, and the source generator will generate a function definition and a function call wrapper for you.\n\n### Get start\n\nFirst, add the following to your project file and set `GenerateDocumentationFile` property to true\n\n```xml\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.SourceGenerator\" />\n</ItemGroup>\n```\n\n> Nightly Build feed: https://devdiv.pkgs.visualstudio.com/DevDiv/_packaging/AutoGen/nuget/v3/index.json\n\nThen, for the methods you want to generate function definition and function call wrapper, mark them with `Function` attribute:\n\n> Note: For the best of performance, try using primitive types for the parameters and return type.\n\n```csharp\n// file: MyFunctions.cs\n\nusing AutoGen;\n\n// a partial class is required\n// and the class must be public\npublic partial class MyFunctions\n{\n    /// <summary>\n    /// Add two numbers.\n    /// </summary>\n    /// <param name=\"a\">The first number.</param>\n    /// <param name=\"b\">The second number.</param>\n    [Function]\n    public Task<string> AddAsync(int a, int b)\n    {\n        return Task.FromResult($\"{a} + {b} = {a + b}\");\n    }\n}\n```\n\nThe source generator will generate the following code based on the method signature and documentation. It helps you save the effort of writing function definition and keep it up to date with the actual method signature.\n\n```csharp\n// file: MyFunctions.generated.cs\npublic partial class MyFunctions\n{\n    private class AddAsyncSchema\n    {\n\t\tpublic int a {get; set;}\n\t\tpublic int b {get; set;}\n    }\n\n    public Task<string> AddAsyncWrapper(string arguments)\n    {\n        var schema = JsonSerializer.Deserialize<AddAsyncSchema>(\n            arguments, \n            new JsonSerializerOptions\n            {\n                PropertyNamingPolicy = JsonNamingPolicy.CamelCase,\n            });\n        return AddAsync(schema.a, schema.b);\n    }\n\n    public FunctionDefinition AddAsyncFunction\n    {\n        get => new FunctionDefinition\n\t\t{\n\t\t\tName = @\"AddAsync\",\n            Description = \"\"\"\nAdd two numbers.\n\"\"\",\n            Parameters = BinaryData.FromObjectAsJson(new\n            {\n                Type = \"object\",\n                Properties = new\n\t\t\t\t{\n\t\t\t\t    a = new\n\t\t\t\t    {\n\t\t\t\t\t    Type = @\"number\",\n\t\t\t\t\t    Description = @\"The first number.\",\n\t\t\t\t    },\n\t\t\t\t    b = new\n\t\t\t\t    {\n\t\t\t\t\t    Type = @\"number\",\n\t\t\t\t\t    Description = @\"The second number.\",\n\t\t\t\t    },\n                },\n                Required = new []\n\t\t\t\t{\n\t\t\t\t    \"a\",\n\t\t\t\t    \"b\",\n\t\t\t\t},\n            },\n            new JsonSerializerOptions\n\t\t\t{\n\t\t\t\tPropertyNamingPolicy = JsonNamingPolicy.CamelCase,\n\t\t\t})\n        };\n    }\n}\n```\n\nFor more examples, please check out the following project\n- [AutoGen.Basic.Sample](../samples/AgentChat/Autogen.Basic.Sample/)\n- [AutoGen.SourceGenerator.Tests](../../test/AutoGen.SourceGenerator.Tests/)",
    "filename": "dotnet/src/AutoGen.SourceGenerator/README.md"
  },
  {
    "content": "# Microsoft.AutoGen\n\n- [Getting started sample](../../samples/getting-started/)",
    "filename": "dotnet/src/Microsoft.AutoGen/readme.md"
  },
  {
    "content": "# AutoGen 0.4 .NET Hello World Sample\n\nThis [sample](Program.cs) demonstrates how to create a simple .NET console application that listens for an event and then orchestrates a series of actions in response.\n\n## Prerequisites\n\nTo run this sample, you'll need: [.NET 8.0](https://dotnet.microsoft.com/en-us/) or later.\nAlso recommended is the [GitHub CLI](https://cli.github.com/).\n\n## Instructions to run the sample\n\n```bash\n# Clone the repository\ngh repo clone microsoft/autogen\ncd dotnet/samples/Hello\ndotnet run\n```\n\n## Key Concepts\n\nThis sample illustrates how to create your own agent that inherits from a base agent and listens for an event. It also shows how to use the SDK's App Runtime locally to start the agent and send messages.\n\nFlow Diagram:\n\n```mermaid\n%%{init: {'theme':'forest'}}%%\ngraph LR;\n    A[Main] --> |\"PublishEventAsync(NewMessage('World'))\"| B{\"Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Hello, World***'))\"| C[ConsoleAgent]\n    C --> D{\"WriteConsole()\"}\n    B --> |\"PublishEventAsync(ConversationClosed('Goodbye'))\"| E{\"Handle(ConversationClosed item, CancellationToken cancellationToken = default)\"}\n    B --> |\"PublishEventAsync(Output('***Goodbye***'))\"| C\n    E --> F{\"Shutdown()\"}\n\n```\n\n### Writing Event Handlers\n\nThe heart of an autogen application are the event handlers. Agents select a ```TopicSubscription``` to listen for events on a specific topic. When an event is received, the agent's event handler is called with the event data.\n\nWithin that event handler you may optionally *emit* new events, which are then sent to the event bus for other agents to process. The EventTypes are declared gRPC ProtoBuf messages that are used to define the schema of the event.  The default protos are available via the ```Microsoft.AutoGen.Contracts;``` namespace and are defined in [autogen/protos](/autogen/protos). The EventTypes are registered in the agent's constructor using the ```IHandle``` interface.\n\n```csharp\nTopicSubscription(\"HelloAgents\")]\npublic class HelloAgent(\n    iAgentWorker worker,\n    [FromKeyedServices(\"AgentsMetadata\")] AgentsMetadata typeRegistry) : ConsoleAgent(\n        worker,\n        typeRegistry),\n        ISayHello,\n        IHandle<NewMessageReceived>,\n        IHandle<ConversationClosed>\n{\n    public async Task Handle(NewMessageReceived item, CancellationToken cancellationToken = default)\n    {\n        var response = await SayHello(item.Message).ConfigureAwait(false);\n        var evt = new Output\n        {\n            Message = response\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(evt).ConfigureAwait(false);\n        var goodbye = new ConversationClosed\n        {\n            UserId = this.AgentId.Key,\n            UserMessage = \"Goodbye\"\n        }.ToCloudEvent(this.AgentId.Key);\n        await PublishEventAsync(goodbye).ConfigureAwait(false);\n    }\n```\n\n### Inheritance and Composition\n\nThis sample also illustrates inheritance in AutoGen. The `HelloAgent` class inherits from `ConsoleAgent`, which is a base class that provides a `WriteConsole` method.\n\n### Starting the Application Runtime\n\nAuotoGen provides a flexible runtime ```Microsoft.AutoGen.Agents.App``` that can be started in a variety of ways. The `Program.cs` file demonstrates how to start the runtime locally and send a message to the agent all in one go using the ```App.PublishMessageAsync``` method.\n\n```csharp\n// send a message to the agent\nvar app = await App.PublishMessageAsync(\"HelloAgents\", new NewMessageReceived\n{\n    Message = \"World\"\n}, local: true);\n\nawait App.RuntimeApp!.WaitForShutdownAsync();\nawait app.WaitForShutdownAsync();\n```\n\n### Sending Messages\n\nThe set of possible Messages is defined in gRPC ProtoBuf specs. These are then turned into C# classes by the gRPC tools. You can define your own Message types by creating a new .proto file in your project and including the gRPC tools in your ```.csproj``` file:\n\n```proto\nsyntax = \"proto3\";\npackage devteam;\noption csharp_namespace = \"DevTeam.Shared\";\nmessage NewAsk {\n  string org = 1;\n  string repo = 2;\n  string ask = 3;\n  int64 issue_number = 4;\n}\nmessage ReadmeRequested {\n   string org = 1;\n   string repo = 2;\n   int64 issue_number = 3;\n   string ask = 4;\n}\n```\n\n```xml\n  <ItemGroup>\n    <PackageReference Include=\"Google.Protobuf\" />\n    <PackageReference Include=\"Grpc.Tools\" PrivateAssets=\"All\" />\n    <Protobuf Include=\"..\\Protos\\messages.proto\" Link=\"Protos\\messages.proto\" />\n  </ItemGroup>\n```\n\nYou can send messages using the [```Microsoft.AutoGen.Agents.AgentWorker``` class](autogen/dotnet/src/Microsoft.AutoGen/Agents/AgentWorker.cs). Messages are wrapped in [the CloudEvents specification](https://cloudevents.io) and sent to the event bus.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/HelloAgentTests/README.md"
  },
  {
    "content": "# Python and dotnet agents interoperability sample\n\nThis sample demonstrates how to create a Python agent that interacts with a .NET agent.\nTo run the sample, check out the autogen repository.\nThen do the following:\n\n1. Navigate to autogen/dotnet/samples/Hello/Hello.AppHost\n2. Run `dotnet run` to start the .NET Aspire app host, which runs three projects:\n    - Backend (the .NET Agent Runtime)\n    - HelloAgent (the .NET Agent)\n    - this Python agent - hello_python_agent.py\n3. The AppHost will start the Aspire dashboard on [https://localhost:15887](https://localhost:15887).\n\nThe Python agent will interact with the .NET agent by sending a message to the .NET runtime, which will relay the message to the .NET agent.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/README.md"
  },
  {
    "code": false,
    "content": "# Documentation Overview of the Python Script\n\nThis script implements an asynchronous worker that communicates with an agent runtime using gRPC. It primarily listens for messages and sends responses based on predefined structures defined elsewhere in the code. It interacts with external (.proto) message definitions and user input handling, establishing subscriptions to various topics associated with agent communication.\n\n## Import Statements and Library Setup\n\nThe script begins with import statements that pull in several libraries and modules:\n\n- **Standard Libraries:** \n  - `asyncio`: Facilitates asynchronous programming.\n  - `logging`: Provides a flexible logging system for tracking events.\n  - `os`, `sys`: Modules for operating system interaction and manipulating the Python path.\n\n- **External Libraries:**\n  - `dotenv`: Loads environment variables from a `.env` file which sets configurations such as `AGENT_HOST`.\n\n- **Local Modules:**\n  - Classes and functions are imported from `autogen_core`, `autogen_ext.runtimes.grpc`, and proto definitions from `protos.agent_events_pb2`. This indicates that the script is part of a larger application related to agent-based communication.\n\n## Environment Configuration\n\n### Loading Environment Variables\n\nThe `load_dotenv()` function loads environment variables from a `.env` file situated in the code's directory.\n\n### Setting Agent Host\n\nThe `agentHost` variable fetches its value from the environment variable `AGENT_HOST`, defaulting to a localhost address if it is not found. The script ensures the URL is stripped of its protocol prefix (`http://` or `https://`) to yield a clean hostname for gRPC communications.\n\n## Logger Initialization\n\nA logger named \"autogen_core\" is created, which is set to DEBUG level. This will log all information at the DEBUG level and above, aiding in troubleshooting and monitoring the status of the worker.\n\n## gRPC Runtime Initialization\n\nThe script proceeds to create an instance of `GrpcWorkerAgentRuntime`, initializing it with `agentHost` and a specified payload serialization format. \n\n### Starting the Runtime\n\nThe `await runtime.start()` line activates the gRPC worker runtime to begin handling messages.\n\n### Registering Message Serializers\n\nThe script registers a message serializer for the `NewMessageReceived` message type. This prepares the runtime to correctly handle incoming message formats.\n\n## User Proxy Registration and Subscription Management\n\n### Registering User Proxy\n\nThe `UserProxy` is registered with the runtime using a lambda function that returns a new instance of `UserProxy`. This indicates that user interactions may be channeled through the registered proxy for further handling.\n\n### Adding Subscriptions\n\nSeveral subscriptions are added to the runtime:\n- `DefaultSubscription`: Subscribes to messages of type \"HelloAgent\".\n- `TypeSubscription`: Subscribes to various topics such as \"agents.NewMessageReceived\", \"agents.ConversationClosed\", and \"agents.Output\".\n\nThese subscriptions enable the worker to receive messages published to specified topics associated with agent interactions.\n\n## Message Creation and Publishing\n\n### Constructing Messages\n\nTwo message instances are created:\n- A `NewMessageReceived` object with the content \"Hello from Python!\".\n- An `Output` message featuring the text \"^v^v^v---Wild Hello from Python!---^v^v^v\".\n\n### Publishing Messages\n\nBoth messages are published to the \"HelloTopic\", with the sender identifier set as \"HelloAgents\" and \"python\". The use of `DefaultTopicId` and `AgentId` indicates a structured approach to categorizing communication.\n\n## Graceful Shutdown Process\n\nAfter completing the message-publishing procedure, the script awaits a shutdown signal with `await runtime.stop_when_signal()`, allowing for clean termination of the runtime process.\n\n## Main Function Execution \n\nThe script contains a standard Python check: \n```python\nif __name__ == \"__main__\":\n```\nHere, the logging configuration is set up at the DEBUG level, and the main asynchronous function (`main()`) is run using `asyncio.run()`.\n\nThe logger emits a starting message before execution begins, which helps in tracking the initialization and flow of the program. \n\n## Conclusion\n\nThis script sets up a gRPC-based worker agent that performs message I/O with flexibility and proper logging. It configures both the runtime environment and handling of message events, effectively allowing it to function within an agent-based architecture.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/hello_python_agent.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: autogen_core.worker.protos\n\nThe `autogen_core.worker.protos` module is designed to facilitate communication between agents and workers using Google Protobuf (Protocol Buffers). This document provides a high-level overview of the module's functionality and structure.\n\n## Overview\n\nThis module serves as an interface for data exchange in a distributed system where agents and workers communicate. Using Google Protobuf allows for efficient serialization of structured data, providing benefits like performance improvements and reducing network latency. The classes defined in this module help define the message types that can be exchanged between these components.\n\n## Imports\n\nThe module begins by importing standard Python libraries:\n\n```python\nimport os\nimport sys\n```\n\n### os Module\n\n- **Purpose**: This module provides a way to interact with the operating system, enabling the script to access file paths and perform directory operations.\n  \n### sys Module\n\n- **Purpose**: The sys module is utilized to manipulate the Python runtime environment. It allows for the modification of the module search path, which is particularly useful for ensuring that all dependencies are available.\n\n## Path Setup\n\nThe following code adjusts the Python module search path:\n\n```python\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n```\n\n### Purpose\n\n- **Functionality**: This line appends the absolute path of the current module's directory to the beginning of the system path (`sys.path`). \n- **Rationale**: Doing this allows the interpreter to locate any local modules or packages residing in the same directory, which is crucial for the successful import of classes or functions defined elsewhere in the module.\n\n## Purpose of the Module\n\nWhile the provided code doesn't define specific classes or functions itself, the documentation indicates that the module is essential for generating and managing Protocol Buffers specific to the communication between agents and workers.\n\n### Incoming and Outgoing Messages\n\nAlthough the code snippet doesn\u2019t specify the actual Protobuf message structures, they would typically include:\n\n- **Request messages**: Used by agents to request data or actions from workers.\n- **Response messages**: Sent by workers back to agents, confirming actions or providing the requested data.\n\n## Future Implementation\n\nThis module likely acts as a foundational piece of a larger system where actual Protobuf definitions and message handling will be implemented later. The next steps would include defining these message types, implementing serialization/deserialization processes, and providing proper interfaces for agents and workers to manage their communications.\n\n## Conclusion\n\nIn summary, the `autogen_core.worker.protos` module is foundational for structured communication between agents and workers utilizing Google Protobuf for efficient data handling. The initial setup of module paths indicates an approach for well-organized dependency management, preparing the ground for the addition of Protobuf-defined classes and methods in future developments.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/protos/__init__.py"
  },
  {
    "code": false,
    "content": "# Agent Events Protocol Buffers Documentation\n\nThis document provides a high-level overview of the generated protocol buffer code from `agent_events.proto`. It outlines the structure of the messages defined in the file and explains their purposes. Protocol Buffers (protobuf) is a method developed by Google for serializing structured data, commonly used in communication protocols, data storage, and more.\n\n## Overview of the Protocol Buffers Schema\n\nThe protocol buffer schema defines several message types under the `agents` namespace. Each message corresponds to a specific type of event or data exchange between agents. The schema is limited to a single file, `agent_events.proto`, generated using the Protocol Buffer Compiler (`protoc`). \n\nThe messages are structured using various primitive field types, primarily strings, denoted by the `\\t` type in the serializations.\n\n### Defined Message Types\n\n1. **TextMessage**  \n   - **Fields:**\n     - `textMessage` (string): The content of the message.\n     - `source` (string): The origin or source of the message.\n   - **Purpose:** Represents a textual message sent to agents.\n\n2. **Input**  \n   - **Fields:**\n     - `message` (string): A text input from a user or an external source.\n   - **Purpose:** Indicates that an input message has been received for processing.\n\n3. **InputProcessed**  \n   - **Fields:**\n     - `route` (string): A routing identifier for the processed input.\n   - **Purpose:** Denotes that an input message has been processed and is possibly routed for further actions.\n\n4. **Output**  \n   - **Fields:**\n     - `message` (string): An output message generated after processing input.\n   - **Purpose:** Represents data resulting from the input processing.\n\n5. **OutputWritten**  \n   - **Fields:**\n     - `route` (string): The route identifier for the output that was written.\n   - **Purpose:** Indicates successful writing or logging of the output message.\n\n6. **IOError**  \n   - **Fields:**\n     - `message` (string): Describes the I/O error that occurred.\n   - **Purpose:** Represents error messages resulting from input/output operations.\n\n### Additional Message Types\n\n7. **NewMessageReceived**  \n   - **Fields:**\n     - `message` (string): The new message that has been received.\n   - **Purpose:** Indicates that a new message has arrived for processing.\n\n8. **ResponseGenerated**  \n   - **Fields:**\n     - `response` (string): The generated response after processing an input.\n   - **Purpose:** Shows the successful generation of a response based on input.\n\n9. **GoodBye**  \n   - **Fields:**\n     - `message` (string): A farewell message indicating the end of a conversation.\n   - **Purpose:** Represents a closing message in a session.\n\n10. **MessageStored**  \n   - **Fields:**\n     - `message` (string): A message that was successfully stored.\n   - **Purpose:** Confirms that a message was saved in the system.\n\n11. **ConversationClosed**  \n   - **Fields:**\n     - `user_id` (string): Identifier for the user associated with the conversation.\n     - `user_message` (string): Last message from the user before the conversation closed.\n   - **Purpose:** Indicates the conclusion of a conversation session.\n\n12. **Shutdown**  \n   - **Fields:**\n     - `message` (string): Message indicating system shutdown.\n   - **Purpose:** Represents a notification for system shutdown events.\n\n## Internal Mechanics and Serialization\n\nThe code utilizes the Google Protobuf library to define and serialize these message structures. A series of steps initialize the descriptors and message classes. The data structure is serialized to byte format, facilitating easy transmission and storage. The generated Python code includes descriptors for each message type and helper functions to encode and decode these structures efficiently.\n\n### Global Variables and Configuration\n\n- **_globals**: A collection of global variables representing the mapping of message types to their descriptors.\n- **_builder**: Used for building the message descriptors and enums from the defined schema.\n- **DESCRIPTOR**: The core descriptor generated from the serialized file that contains all message types, allowing access and manipulation of the defined structures.\n\n### Conclusion\n\nThis generated protocol buffer code provides a structured way to handle messaging events within an agent-based system. Each message type outlines a specific role in communication, ranging from input and output management to error reporting and conversation control. By using Protocol Buffers, developers can ensure efficient data encoding and decoding, suitable for high-performance applications.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/protos/agent_events_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Generated Code Initialization\n\nThis code snippet is part of a Python script generated by the gRPC (Google Remote Procedure Call) protocol compiler. It defines the essential components required for communication between a gRPC client and server using protocol buffers. The script consists primarily of version checks and relevant imports to ensure that the gRPC package is being used correctly.\n\n## Imports and Dependencies\n\nThe script begins by importing the `grpc` library, which provides the necessary tools for implementing gRPC clients and servers in Python. It also imports the `warnings` module for issuing warnings in case of version mismatches.\n\n```python\nimport grpc\nimport warnings\n```\n\n## Version Constants\n\nTwo constants are defined: `GRPC_GENERATED_VERSION` and `GRPC_VERSION`. \n\n- **`GRPC_GENERATED_VERSION`**: Specifies the minimum version of the `grpc` library that this generated code is compatible with. In this case, it is set to '1.70.0'.\n- **`GRPC_VERSION`**: Retrieves the current version of the `grpc` library that is installed in the environment.\n\n```python\nGRPC_GENERATED_VERSION = '1.70.0'\nGRPC_VERSION = grpc.__version__\n```\n\n## Version Compatibility Check\n\nThe snippet includes a mechanism to check if the installed version of the `grpc` library is compatible with the required version for the generated code. This is done using a utility function `first_version_is_lower`, which compares the two version strings.\n\n```python\n_version_not_supported = False\n\ntry:\n    from grpc._utilities import first_version_is_lower\n    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n    _version_not_supported = True\n```\n\n- If the `first_version_is_lower` function is available and determines that the `GRPC_VERSION` is less than `GRPC_GENERATED_VERSION`, the `_version_not_supported` variable is set to `True`.\n- If the import fails, `_version_not_supported` is also set to `True`, indicating a lack of support.\n\n## Runtime Error Handling\n\nIf it is determined that the installed version of `grpc` does not meet the minimum requirement, a `RuntimeError` is raised. This error message provides the user with details regarding the version mismatch and suggests actions they can take to resolve the issue.\n\n```python\nif _version_not_supported:\n    raise RuntimeError(\n        f'The grpc package installed is at version {GRPC_VERSION},'\n        + f' but the generated code in agent_events_pb2_grpc.py depends on'\n        + f' grpcio>={GRPC_GENERATED_VERSION}.'\n        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n    )\n```\n\n- The error message specifies the installed gRPC version and reminds the user of the requirements for using the generated code effectively.\n\n## Summary\n\nIn summary, this script is a foundational component of a gRPC service, ensuring compatibility between the installed gRPC library and the generated service code. By implementing version checks, it helps to prevent runtime errors that could arise from using incompatible versions. The emphasis on not editing the generated code is also a key point, as it is generated and maintained through tools rather than manual modifications.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/protos/agent_events_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "# Overview of the UserProxy Class\n\nThe provided code defines a Python class `UserProxy` that extends the `RoutedAgent` class from the `autogen_core` library. It is designed to facilitate a conversation where the user plays the role of an agent. The class is structured to handle different types of messages related to the conversation, specifically user inputs, outputs from the conversation, and signaling that the conversation has ended. \n\n## Class Definition\n\n### UserProxy\n\n- **Base Class:** The `UserProxy` class inherits from `RoutedAgent`, indicating that it is part of a routed agent architecture, which likely involves multiple agents communicating with each other.\n- **Purpose:** The primary role of the `UserProxy` class is to mediate user inputs within a conversation context, allowing the user to send messages and receive outputs in real-time.\n\n### Constructor\n\n```python\ndef __init__(self, description: str = DEFAULT_DESCRIPTION) -> None:\n    super().__init__(description)\n```\n\n- This constructor initializes the `UserProxy` instance with a default description (\"A human user\") or a provided description. It calls the constructor of the base class `RoutedAgent` to handle any initializations defined there.\n\n## Message Handling\n\n### Method: handle_user_chat_input\n\n```python\n@message_handler\nasync def handle_user_chat_input(self, message: input_types, ctx: MessageContext) -> None:\n```\n\n- **Decorator:** The `message_handler` decorator indicates that this method is designed to process incoming messages from the conversation context.\n- **Input Types:** The method accepts a message of union type `input_types`, which can be `ConversationClosed`, `Input`, or `Output`. This flexibility allows the agent to respond appropriately based on the type of message received.\n\n#### Logic Flow\n\n1. **User Input Handling:**\n   - If the incoming message is of type `Input`, the method prompts the user for input asynchronously using `ainput`.\n   - The input is logged and stripped of any surrounding whitespace.\n   - A `NewMessageReceived` message is published with the user's response to the conversation, signaling that a new message has been added.\n\n2. **Output Handling:**\n   - If the message is of type `Output`, it logs the content of the received message. This allows the user to see responses from the agent in the conversation.\n\n3. **Conversation Closure:**\n   - If the message type does not match `Input` or `Output`, the method does nothing, effectively ignoring the message.\n\n## User Input Method\n\n### Method: ainput\n\n```python\nasync def ainput(self, prompt: str) -> str:\n    return await asyncio.to_thread(input, f\"{prompt} \")\n```\n\n- **Purpose:** The `ainput` method is an asynchronous wrapper around the built-in `input` function, allowing for non-blocking behavior.\n- It takes a prompt string and uses `asyncio.to_thread` to run the synchronous `input` function in a separate thread, returning user input as a string.\n- This design ensures that user input does not block the event loop, allowing other asynchronous tasks to run concurrently.\n\n## Logging\n\n- The logging is managed using Python\u2019s built-in `logging` module. \n- The logger associated with the name \"autogen_core\" is used to record user inputs and outputs, facilitating debugging and monitoring of the conversation flow.\n\n## Summary\n\nThe `UserProxy` class serves as an intermediary to facilitate real-time communication between a user and an agent in a conversational interface. By handling different message types and managing user inputs asynchronously, it provides a flexible framework for interactive conversations. The logging feature enhances traceability by logging all interactions, which is critical for understanding user-agent exchanges. Overall, this code contributes to the development of an interactive, user-friendly conversation system.",
    "filename": "dotnet/test/Microsoft.AutoGen.Integration.Tests.AppHosts/core_xlang_hello_python_agent/user_input.py"
  },
  {
    "content": "## How to build and run the website\n\n### Prerequisites\n- dotnet 7.0 or later\n\n### Build\nFirstly, go to autogen/dotnet folder and run the following command to build the website:\n```bash\ndotnet tool restore\ndotnet tool run docfx website/docfx.json --serve\n```\n\nAfter the command is executed, you can open your browser and navigate to `http://localhost:8080` to view the website.",
    "filename": "dotnet/website/README.md"
  },
  {
    "content": "`Agent` is one of the most fundamental concepts in AutoGen.Net. In AutoGen.Net, you construct a single agent to process a specific task, and you extend an agent using [Middlewares](./Middleware-overview.md), and you construct a multi-agent workflow using [GroupChat](./Group-chat-overview.md).\n\n> [!NOTE]\n> Every agent in AutoGen.Net implements @AutoGen.Core.IAgent, for agent that supports streaming reply, it also implements @AutoGen.Core.IStreamingAgent.\n\n## Create an agent\n- Create an @AutoGen.AssistantAgent: [Create an assistant agent](./Create-an-agent.md)\n- Create an @AutoGen.OpenAI.OpenAIChatAgent: [Create an OpenAI chat agent](./OpenAIChatAgent-simple-chat.md)\n- Create a @AutoGen.SemanticKernel.SemanticKernelAgent: [Create a semantic kernel agent](./AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md)\n- Create a @AutoGen.LMStudio.LMStudioAgent: [Connect to LM Studio](./Consume-LLM-server-from-LM-Studio.md)\n\n## Chat with an agent\nTo chat with an agent, typically you can invoke @AutoGen.Core.IAgent.GenerateReplyAsync*. On top of that, you can also use one of the extension methods like @AutoGen.Core.AgentExtension.SendAsync* as shortcuts.\n\n> [!NOTE]\n> AutoGen provides a list of built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, @AutoGen.Core.ToolCallMessage, @AutoGen.Core.ToolCallResultMessage, etc. You can use these message types to chat with an agent. For further details, see [built-in messages](./Built-in-messages.md).\n\n- Send a @AutoGen.Core.TextMessage to an agent via @AutoGen.Core.IAgent.GenerateReplyAsync*:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_GenerateReplyAsync)]\n\n- Send a message to an agent via @AutoGen.Core.AgentExtension.SendAsync*:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_SendAsync)]\n\n## Streaming chat\nIf an agent implements @AutoGen.Core.IStreamingAgent, you can use @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* to chat with the agent in a streaming way. You would need to process the streaming updates on your side though.\n\n- Send a @AutoGen.Core.TextMessage to an agent via @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*, and print the streaming updates to console:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/AgentCodeSnippet.cs?name=ChatWithAnAgent_GenerateStreamingReplyAsync)]\n\n## Register middleware to an agent\n@AutoGen.Core.IMiddleware and @AutoGen.Core.IStreamingMiddleware are used to extend the behavior of @AutoGen.Core.IAgent.GenerateReplyAsync* and @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*. You can register middleware to an agent to customize the behavior of the agent on things like function call support, converting message of different types, print message, gather user input, etc.\n\n- Middleware overview: [Middleware overview](./Middleware-overview.md)\n- Write message to console: [Print message middleware](./Print-message-middleware.md)\n- Convert message type: [SemanticKernelChatMessageContentConnector](./AutoGen.SemanticKernel/SemanticKernelAgent-support-more-messages.md) and [OpenAIChatRequestMessageConnector](./OpenAIChatAgent-support-more-messages.md)\n- Create your own middleware: [Create your own middleware](./Create-your-own-middleware.md)\n\n## Group chat\nYou can construct a multi-agent workflow using @AutoGen.Core.IGroupChat. In AutoGen.Net, there are two type of group chat:\n@AutoGen.Core.SequentialGroupChat: Orchestrates the agents in the group chat in a fix, sequential order.\n@AutoGen.Core.GroupChat: Provide more dynamic yet controllable way to orchestrate the agents in the group chat.\n\nFor further details, see [Group chat overview](./Group-chat-overview.md).",
    "filename": "dotnet/website/articles/Agent-overview.md"
  },
  {
    "content": "## AutoGen.Mistral overview\n\nAutoGen.Mistral provides the following agent(s) to connect to [Mistral.AI](https://mistral.ai/) platform.\n- @AutoGen.Mistral.MistralClientAgent: A slim wrapper agent over @AutoGen.Mistral.MistralClient.\n\n### Get started with AutoGen.Mistral\n\nTo get started with AutoGen.Mistral, follow the [installation guide](Installation.md) to make sure you add the AutoGen feed correctly. Then add the `AutoGen.Mistral` package to your project file.\n\n```bash\ndotnet add package AutoGen.Mistral\n```\n\n>[!NOTE]\n> You need to provide an api-key to use Mistral models which will bring additional cost while using. you can get the api key from [Mistral.AI](https://mistral.ai/).\n\n### Example\n\nImport the required namespace\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=using_statement)]\n\nCreate a @AutoGen.Mistral.MistralClientAgent and start chatting!\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=create_mistral_agent)]\n\nUse @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* to stream the chat completion.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=streaming_chat)]",
    "filename": "dotnet/website/articles/AutoGen-Mistral-Overview.md"
  },
  {
    "content": "## AutoGen.OpenAI Overview\n\nAutoGen.OpenAI provides the following agents over openai models:\n- @AutoGen.OpenAI.OpenAIChatAgent: A slim wrapper agent over `OpenAIClient`. This agent only support `IMessage<ChatRequestMessage>` message type. To support more message types like @AutoGen.Core.TextMessage, register the agent with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector.\n- @AutoGen.OpenAI.GPTAgent: An agent that build on top of @AutoGen.OpenAI.OpenAIChatAgent with more message types support like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage and function call support. Essentially, it is equivalent to @AutoGen.OpenAI.OpenAIChatAgent with @AutoGen.Core.FunctionCallMiddleware and @AutoGen.OpenAI.OpenAIChatRequestMessageConnector registered.\n\n### Get start with AutoGen.OpenAI\n\nTo get start with AutoGen.OpenAI, firstly, follow the [installation guide](Installation.md) to make sure you add the AutoGen feed correctly. Then add `AutoGen.OpenAI` package to your project file.\n\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```",
    "filename": "dotnet/website/articles/AutoGen-OpenAI-Overview.md"
  },
  {
    "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to connect to Google AI Gemini and chat with Gemini model.\n\nTo run this example, you need to have a Google AI Gemini API key. For how to get a Google Gemini API key, please refer to [Google Gemini](https://gemini.google.com/).\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs)\n\n> [!NOTE]\n> What's the difference between Google AI Gemini and Vertex AI Gemini?\n>\n> Gemini is a series of large language models developed by Google. You can use it either from Google AI API or Vertex AI API. If you are relatively new to Gemini and wants to explore the feature and build some prototype for your chatbot app, Google AI APIs (with Google AI Studio) is a fast way to get started. While your app and idea matures and you'd like to leverage more MLOps tools that streamline the usage, deployment, and monitoring of models, you can move to Google Cloud Vertex AI which provides Gemini APIs along with many other features. Basically, to help you productionize your app. ([reference](https://stackoverflow.com/questions/78007243/utilizing-gemini-through-vertex-ai-or-through-google-generative-ai))\n\n### Step 1: Install AutoGen.Gemini\n\nFirst, install the AutoGen.Gemini package using the following command:\n\n```bash\ndotnet add package AutoGen.Gemini\n```\n\n### Step 2: Add using statement\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Using)]\n\n### Step 3: Create a Gemini agent\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Create_Gemini_Agent)]\n\n### Step 4: Chat with Gemini\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Google_Gemini.cs?name=Chat_With_Google_Gemini)]",
    "filename": "dotnet/website/articles/AutoGen.Gemini/Chat-with-google-gemini.md"
  },
  {
    "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to connect to Vertex AI Gemini API and chat with Gemini model.\n\nTo run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs)\n\n> [!NOTE]\n> What's the difference between Google AI Gemini and Vertex AI Gemini?\n>\n> Gemini is a series of large language models developed by Google. You can use it either from Google AI API or Vertex AI API. If you are relatively new to Gemini and wants to explore the feature and build some prototype for your chatbot app, Google AI APIs (with Google AI Studio) is a fast way to get started. While your app and idea matures and you'd like to leverage more MLOps tools that streamline the usage, deployment, and monitoring of models, you can move to Google Cloud Vertex AI which provides Gemini APIs along with many other features. Basically, to help you productionize your app. ([reference](https://stackoverflow.com/questions/78007243/utilizing-gemini-through-vertex-ai-or-through-google-generative-ai))\n\n### Step 1: Install AutoGen.Gemini\n\nFirst, install the AutoGen.Gemini package using the following command:\n\n```bash\ndotnet add package AutoGen.Gemini\n```\n\n### Step 2: Add using statement\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Using)]\n\n### Step 3: Create a Gemini agent\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Create_Gemini_Agent)]\n\n\n### Step 4: Chat with Gemini\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Chat_With_Vertex_Gemini.cs?name=Chat_With_Vertex_Gemini)]",
    "filename": "dotnet/website/articles/AutoGen.Gemini/Chat-with-vertex-gemini.md"
  },
  {
    "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent to make function call. This example is modified from [gemini-api function call example](https://ai.google.dev/gemini-api/docs/function-calling)\n\nTo run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).\n\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Gemini.Sample/Function_Call_With_Gemini.cs)\n\n### Step 1: Install AutoGen.Gemini and AutoGen.SourceGenerator\n\nFirst, install the AutoGen.Gemini package using the following command:\n\n```bash\ndotnet add package AutoGen.Gemini\ndotnet add package AutoGen.SourceGenerator\n```\n\nThe AutoGen.SourceGenerator package is required to generate the @AutoGen.Core.FunctionContract. For more information, please refer to [Create-type-safe-function-call](../Create-type-safe-function-call.md)\n\n### Step 2: Add using statement\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Using)]\n\n### Step 3: Create `MovieFunction`\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=MovieFunction)]\n\n### Step 4: Create a Gemini agent\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Create_Gemini_Agent)]\n\n### Step 5: Single turn function call\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Single_turn)]\n\n### Step 6: Multi-turn function call\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Function_call_with_gemini.cs?name=Multi_turn)]",
    "filename": "dotnet/website/articles/AutoGen.Gemini/Function-call-with-gemini.md"
  },
  {
    "content": "This example shows how to use @AutoGen.Gemini.GeminiChatAgent for image chat with Gemini model.\n\nTo run this example, you need to have a project on Google Cloud with access to Vertex AI API. For more information please refer to [Google Vertex AI](https://cloud.google.com/vertex-ai/docs).\n\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs)\n\n### Step 1: Install AutoGen.Gemini\n\nFirst, install the AutoGen.Gemini package using the following command:\n\n```bash\ndotnet add package AutoGen.Gemini\n```\n\n### Step 2: Add using statement\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Using)]\n\n### Step 3: Create a Gemini agent\n\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Create_Gemini_Agent)]\n\n### Step 4: Send image to Gemini\n[!code-csharp[](../../../samples/AutoGen.Gemini.Sample/Image_Chat_With_Vertex_Gemini.cs?name=Send_Image_Request)]",
    "filename": "dotnet/website/articles/AutoGen.Gemini/Image-chat-with-gemini.md"
  },
  {
    "content": "# AutoGen.Gemini Overview\n\nAutoGen.Gemini is a package that provides seamless integration with Google Gemini. It provides the following agent:\n\n- @AutoGen.Gemini.GeminiChatAgent: The agent that connects to Google Gemini or Vertex AI Gemini. It supports chat, multi-modal chat, and function call.\n\nAutoGen.Gemini also provides the following middleware:\n- @AutoGen.Gemini.GeminiMessageConnector: The middleware that converts the Gemini message to AutoGen built-in message type.\n\n## Examples\n\nYou can find more examples under the [gemini sample project](https://github.com/microsoft/autogen/tree/main/dotnet/samples/AutoGen.Gemini.Sample)",
    "filename": "dotnet/website/articles/AutoGen.Gemini/Overview.md"
  },
  {
    "content": "This example shows how to use @AutoGen.Ollama.OllamaAgent to connect to Ollama server and chat with LLaVA model.\n\nTo run this example, you need to have an Ollama server running aside and have `llama3:latest` model installed. For how to setup an Ollama server, please refer to [Ollama](https://ollama.com/).\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs)\n\n### Step 1: Install AutoGen.Ollama\n\nFirst, install the AutoGen.Ollama package using the following command:\n\n```bash\ndotnet add package AutoGen.Ollama\n```\n\nFor how to install from nightly build, please refer to [Installation](../Installation.md).\n\n### Step 2: Add using statement\n\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs?name=Using)]\n\n### Step 3: Create and chat @AutoGen.Ollama.OllamaAgent\n\nIn this step, we create an @AutoGen.Ollama.OllamaAgent and connect it to the Ollama server.\n\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaMA.cs?name=Create_Ollama_Agent)]",
    "filename": "dotnet/website/articles/AutoGen.Ollama/Chat-with-llama.md"
  },
  {
    "content": "This sample shows how to use @AutoGen.Ollama.OllamaAgent to chat with LLaVA model.\n\nTo run this example, you need to have an Ollama server running aside and have `llava:latest` model installed. For how to setup an Ollama server, please refer to [Ollama](https://ollama.com/).\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs)\n\n### Step 1: Install AutoGen.Ollama\n\nFirst, install the AutoGen.Ollama package using the following command:\n\n```bash\ndotnet add package AutoGen.Ollama\n```\n\nFor how to install from nightly build, please refer to [Installation](../Installation.md).\n\n### Step 2: Add using statement\n\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Using)]\n\n### Step 3: Create @AutoGen.Ollama.OllamaAgent\n\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Create_Ollama_Agent)]\n\n### Step 4: Start MultiModal Chat\nLLaVA is a multimodal model that supports both text and image inputs. In this step, we create an image message along with a question about the image.\n\n[!code-csharp[](../../../samples/AutoGen.Ollama.Sample/Chat_With_LLaVA.cs?name=Send_Message)]",
    "filename": "dotnet/website/articles/AutoGen.Ollama/Chat-with-llava.md"
  },
  {
    "content": "## AutoGen.SemanticKernel Overview\n\nAutoGen.SemanticKernel is a package that provides seamless integration with Semantic Kernel. It provides the following agents:\n- @AutoGen.SemanticKernel.SemanticKernelAgent: A slim wrapper agent over `Kernel` that only support original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message type, register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector.\n- @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent: A slim wrapper agent over `Microsoft.SemanticKernel.Agents.ChatCompletionAgent`.\n\nAutoGen.SemanticKernel also provides the following middleware:\n- @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector: A connector that convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa. At the current stage, it only supports conversation between @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage and @AutoGen.Core.MultiModalMessage. Function call message type like @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage are not supported yet.\n- @AutoGen.SemanticKernel.KernelPluginMiddleware: A middleware that allows you to use semantic kernel plugins in other AutoGen agents like @AutoGen.OpenAI.OpenAIChatAgent.\n\n### Get start with AutoGen.SemanticKernel\n\nTo get start with AutoGen.SemanticKernel, firstly, follow the [installation guide](../Installation.md) to make sure you add the AutoGen feed correctly. Then add `AutoGen.SemanticKernel` package to your project file.\n\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.SemanticKernel\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```",
    "filename": "dotnet/website/articles/AutoGen.SemanticKernel/AutoGen-SemanticKernel-Overview.md"
  },
  {
    "content": "You can chat with @AutoGen.SemanticKernel.SemanticKernelAgent using both streaming and non-streaming methods and use native `ChatMessageContent` type via `IMessage<ChatMessageContent>`.\n\nThe following example shows how to create an @AutoGen.SemanticKernel.SemanticKernelAgent and chat with it using non-streaming method:\n\n[!code-csharp[](../../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent)]\n\n@AutoGen.SemanticKernel.SemanticKernelAgent also supports streaming chat via @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*.\n\n[!code-csharp[](../../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/SemanticKernelCodeSnippet.cs?name=create_semantic_kernel_agent_streaming)]",
    "filename": "dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md"
  },
  {
    "content": "@AutoGen.SemanticKernel.SemanticKernelAgent only supports the original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, you can register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa.\n> [!NOTE]\n> At the current stage, @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector only supports conversation for the followng built-in @AutoGen.Core.IMessage\n> - @AutoGen.Core.TextMessage\n> - @AutoGen.Core.ImageMessage\n> - @AutoGen.Core.MultiModalMessage\n>\n> Function call message type like @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage are not supported yet.\n\n[!code-csharp[](../../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/SemanticKernelCodeSnippet.cs?name=register_semantic_kernel_chat_message_content_connector)]",
    "filename": "dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelAgent-support-more-messages.md"
  },
  {
    "content": "`AutoGen.SemanticKernel` provides built-in support for `ChatCompletionAgent` via @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent. By default the @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent only supports the original `ChatMessageContent` type via `IMessage<ChatMessageContent>`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage, you can register the agent with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa.\n\nThe following step-by-step example shows how to create an @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent and chat with it:\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs).\n\n### Step 1: add using statement\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Using)]\n\n### Step 2: create kernel\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_Kernel)]\n\n### Step 3: create ChatCompletionAgent\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_ChatCompletionAgent)]\n\n### Step 4: create @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent\nIn this step, we create an @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent and register it with @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector. The @AutoGen.SemanticKernel.SemanticKernelChatMessageContentConnector will convert the message from AutoGen built-in message types to `ChatMessageContent` and vice versa.\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Create_SemanticKernelChatCompletionAgent)]\n\n### Step 5: chat with @AutoGen.SemanticKernel.SemanticKernelChatCompletionAgent\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Create_Semantic_Kernel_Chat_Agent.cs?name=Send_Message)]",
    "filename": "dotnet/website/articles/AutoGen.SemanticKernel/SemanticKernelChatAgent-simple-chat.md"
  },
  {
    "content": "In semantic kernel, a kernel plugin is a collection of kernel functions that can be invoked during LLM calls. Semantic kernel provides a list of built-in plugins, like [core plugins](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Plugins/Plugins.Core), [web search plugin](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Plugins/Plugins.Web) and many more. You can also create your own plugins and use them in semantic kernel. Kernel plugins greatly extend the capabilities of semantic kernel and can be used to perform various tasks like web search, image search, text summarization, etc.\n\n`AutoGen.SemanticKernel` provides a middleware called @AutoGen.SemanticKernel.KernelPluginMiddleware that allows you to use semantic kernel plugins in other AutoGen agents like @AutoGen.OpenAI.OpenAIChatAgent. The following example shows how to define a simple plugin with a single `GetWeather` function and use it in @AutoGen.OpenAI.OpenAIChatAgent.\n\n> [!NOTE]\n> You can find the complete sample code [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs)\n\n### Step 1: add using statement\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Using)]\n\n### Step 2: create plugin\n\nIn this step, we create a simple plugin with a single `GetWeather` function that takes a location as input and returns the weather information for that location.\n\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Create_plugin)]\n\n### Step 3: create OpenAIChatAgent and use the plugin\n\nIn this step, we firstly create a @AutoGen.SemanticKernel.KernelPluginMiddleware and register the previous plugin with it. The `KernelPluginMiddleware` will load the plugin and make the functions available for use in other agents. Followed by creating an @AutoGen.OpenAI.OpenAIChatAgent and register it with the `KernelPluginMiddleware`.\n\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Use_plugin)]\n\n### Step 4: chat with OpenAIChatAgent\n\nIn this final step, we start the chat with the @AutoGen.OpenAI.OpenAIChatAgent by asking the weather in Seattle. The `OpenAIChatAgent` will use the `GetWeather` function from the plugin to get the weather information for Seattle.\n\n[!code-csharp[](../../../samples/AutoGen.SemanticKernel.Sample/Use_Kernel_Functions_With_Other_Agent.cs?name=Send_message)]",
    "filename": "dotnet/website/articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md"
  },
  {
    "content": "## An overview of built-in @AutoGen.Core.IMessage types\n\nStart from 0.0.9, AutoGen introduces the @AutoGen.Core.IMessage and @AutoGen.Core.IMessage`1 types to provide a unified message interface for different agents. The @AutoGen.Core.IMessage is a non-generic interface that represents a message. The @AutoGen.Core.IMessage`1 is a generic interface that represents a message with a specific `T` where `T` can be any type.\n\nBesides, AutoGen also provides a set of built-in message types that implement the @AutoGen.Core.IMessage and @AutoGen.Core.IMessage`1 interfaces. These built-in message types are designed to cover different types of messages as much as possilbe. The built-in message types include:\n\n> [!NOTE]\n> The minimal requirement for an agent to be used as admin in @AutoGen.Core.GroupChat is to support @AutoGen.Core.TextMessage.\n\n> [!NOTE]\n> @AutoGen.Core.Message will be deprecated in 0.0.14. Please replace it with a more specific message type like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc.\n\n- @AutoGen.Core.TextMessage: A message that contains a piece of text.\n- @AutoGen.Core.ImageMessage: A message that contains an image.\n- @AutoGen.Core.MultiModalMessage: A message that contains multiple modalities like text, image, etc.\n- @AutoGen.Core.ToolCallMessage: A message that represents a function call request.\n- @AutoGen.Core.ToolCallResultMessage: A message that represents a function call result.\n- @AutoGen.Core.ToolCallAggregateMessage: A message that contains both @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. This type of message is used by @AutoGen.Core.FunctionCallMiddleware to aggregate both @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage into a single message.\n- @AutoGen.Core.MessageEnvelope`1: A message that represents an envelope that contains a message of any type.\n- @AutoGen.Core.Message: The original message type before 0.0.9. This message type is reserved for backward compatibility. It is recommended to replace it with a more specific message type like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc.\n\n### Streaming message support\nAutoGen also introduces @AutoGen.Core.IStreamingMessage and @AutoGen.Core.IStreamingMessage`1 which are used in streaming call api. The following built-in message types implement the @AutoGen.Core.IStreamingMessage and @AutoGen.Core.IStreamingMessage`1 interfaces:\n\n> [!NOTE]\n> All @AutoGen.Core.IMessage is also a @AutoGen.Core.IStreamingMessage. That means you can return an @AutoGen.Core.IMessage from a streaming call method. It's also recommended to return the final updated result instead of the last update as the last message in the streaming call method to indicate the end of the stream, which saves caller's effort of assembling the final result from multiple updates. \n- @AutoGen.Core.TextMessageUpdate: A message that contains a piece of text update.\n- @AutoGen.Core.ToolCallMessageUpdate: A message that contains a function call request update.\n\n#### Usage\n\nThe below code snippet shows how to print a streaming update to console and update the final result on the caller side.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/BuildInMessageCodeSnippet.cs?name=StreamingCallCodeSnippet)]\n\nIf the agent returns a final result instead of the last update as the last message in the streaming call method, the caller can directly use the final result without assembling the final result from multiple updates.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/BuildInMessageCodeSnippet.cs?name=StreamingCallWithFinalMessage)]",
    "filename": "dotnet/website/articles/Built-in-messages.md"
  },
  {
    "content": "## Consume LLM server from LM Studio\nYou can use @AutoGen.LMStudio.LMStudioAgent from `AutoGen.LMStudio` package to consume openai-like API from LMStudio local server.\n\n### What's LM Studio\n[LM Studio](https://lmstudio.ai/) is an app that allows you to deploy and inference hundreds of thousands of open-source language model on your local machine. It provides an in-app chat ui plus an openai-like API to interact with the language model programmatically.\n\n### Installation\n- Install LM studio if you haven't done so. You can find the installation guide [here](https://lmstudio.ai/)\n- Add `AutoGen.LMStudio` to your project.\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.LMStudio\" Version=\"AUTOGEN_LMSTUDIO_VERSION\" />\n</ItemGroup>\n```\n\n### Usage\nThe following code shows how to use `LMStudioAgent` to write a piece of C# code to calculate 100th of fibonacci. Before running the code, make sure you have local server from LM Studio running on `localhost:1234`.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example08_LMStudio.cs?name=lmstudio_using_statements)]\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example08_LMStudio.cs?name=lmstudio_example_1)]",
    "filename": "dotnet/website/articles/Consume-LLM-server-from-LM-Studio.md"
  },
  {
    "content": "## UserProxyAgent\n\n[`UserProxyAgent`](../api/AutoGen.UserProxyAgent.yml) is a special type of agent that can be used to proxy user input to another agent or group of agents. It supports the following human input modes:\n- `ALWAYS`: Always ask user for input.\n- `NEVER`: Never ask user for input. In this mode, the agent will use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n- `AUTO`: Only ask user for input when conversation is terminated by the other agent(s). Otherwise, use the default response (if any) to respond to the message. Or using underlying LLM model to generate response if provided.\n\n> [!TIP]\n> You can also set up `humanInputMode` when creating `AssistantAgent` to enable/disable human input. `UserProxyAgent` is equivalent to `AssistantAgent` with `humanInputMode` set to `ALWAYS`. Similarly, `AssistantAgent` is equivalent to `UserProxyAgent` with `humanInputMode` set to `NEVER`.\n\n### Create a `UserProxyAgent` with `HumanInputMode` set to `ALWAYS`\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/UserProxyAgentCodeSnippet.cs?name=code_snippet_1)]\n\nWhen running the code, the user proxy agent will ask user for input and use the input as response.\n![code output](../images/articles/CreateUserProxyAgent/image-1.png)",
    "filename": "dotnet/website/articles/Create-a-user-proxy-agent.md"
  },
  {
    "content": "## AssistantAgent\n\n[`AssistantAgent`](../api/AutoGen.AssistantAgent.yml) is a built-in agent in `AutoGen` that acts as an AI assistant. It uses LLM to generate response to user input. It also supports function call if the underlying LLM model supports it (e.g. `gpt-3.5-turbo-0613`).\n\n## Create an `AssistantAgent` using OpenAI model.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/CreateAnAgent.cs?name=code_snippet_1)]\n\n## Create an `AssistantAgent` using Azure OpenAI model.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/CreateAnAgent.cs?name=code_snippet_2)]",
    "filename": "dotnet/website/articles/Create-an-agent.md"
  },
  {
    "content": "## Create type-safe function call using AutoGen.SourceGenerator\n\n`AutoGen` provides a source generator to easness the trouble of manually craft function definition and function call wrapper from a function. To use this feature, simply add the `AutoGen.SourceGenerator` package to your project and decorate your function with @AutoGen.Core.FunctionAttribute.\n\n```bash\ndotnet add package AutoGen.SourceGenerator\n```\n\n> [!NOTE]\n> It's recommended to enable structural xml document support by setting `GenerateDocumentationFile` property to true in your project file. This allows source generator to leverage the documentation of the function when generating the function definition.\n\n```xml\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\nThen, create a `public partial` class to host the methods you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods is defined, mark them with @AutoGen.FunctionAttribute attribute:\n\n> [!NOTE]\n> A `public partial` class is required for the source generator to generate code.\n> The method has to be a `public` instance method and its return type must be `Task<string>`.\n> Mark the method with @AutoGen.Core.FunctionAttribute attribute.\n\nFirstly, import the required namespaces:\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_using_statement)]\n\nThen, create a `WeatherReport` function and mark it with @AutoGen.Core.FunctionAttribute:\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report)]\n\nThe source generator will generate the @AutoGen.Core.FunctionContract and function call wrapper for `WeatherReport` in another partial class based on its signature and structural comments. The @AutoGen.Core.FunctionContract is introduced by [#1736](https://github.com/microsoft/autogen/pull/1736) and contains all the necessary metadata such as function name, parameters, and return type. It is LLM independent and can be used to generate openai function definition or semantic kernel function. The function call wrapper is a helper class that provides a type-safe way to call the function.\n\n> [!NOTE]\n> If you are using VSCode as your editor, you may need to restart the editor to see the generated code.\n\nThe following code shows how to generate openai function definition from the @AutoGen.Core.FunctionContract and call the function using the function call wrapper.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report_consume)]",
    "filename": "dotnet/website/articles/Create-type-safe-function-call.md"
  },
  {
    "content": "## Coming soon",
    "filename": "dotnet/website/articles/Create-your-own-agent.md"
  },
  {
    "content": "## Coming soon",
    "filename": "dotnet/website/articles/Create-your-own-middleware.md"
  },
  {
    "content": "# Coming soon",
    "filename": "dotnet/website/articles/Function-call-middleware.md"
  },
  {
    "content": "## Overview of function call\n\nIn some LLM models, you can provide a list of function definitions to the model. The function definition is usually essentially an OpenAPI schema object which describes the function, its parameters and return value. And these function definitions tells the model what \"functions\" are available to be used to resolve the user's request. This feature greatly extend the capability of LLM models by enabling them to \"execute\" arbitrary function as long as it can be described as a function definition.\n\nBelow is an example of a function definition for getting weather report for a city:\n\n> [!NOTE]\n> To use function call, the underlying LLM model must support function call as well for the best experience.\n> The model used in the example below is `gpt-3.5-turbo-0613`.\n```json\n{\n    \"name\": \"GetWeather\",\n    \"description\": \"Get the weather report for a city\",\n    \"parameters\": {\n        \"city\": {\n            \"type\": \"string\",\n            \"description\": \"The city name\"\n        },\n        \"required\": [\"city\"]\n    },\n}\n```\n\n\n\nWhen the model receives a message, it will intelligently decide whether to use function call or not based on the message received. If the model decides to use function call, it will generate a function call which can be used to invoke the actual function. The function call is a json object which contains the function name and its arguments.\n\nBelow is an example of a function call object for getting weather report for Seattle:\n\n```json\n{\n    \"name\": \"GetWeather\",\n    \"arguments\": {\n        \"city\": \"Seattle\"\n    }\n}\n```\n\nAnd when the function call is return to the caller, it can be used to invoke the actual function to get the weather report for Seattle.\n\n### Create type-safe function contract and function call wrapper use AutoGen.SourceGenerator\nAutoGen provides a source generator to easness the trouble of manually craft function contract and function call wrapper from a function. To use this feature, simply add the `AutoGen.SourceGenerator` package to your project and decorate your function with `Function` attribute.\n\nFor more information, please check out [Create type-safe function](Create-type-safe-function-call.md).\n\n### Use function call in an agent\nAutoGen provides first-class support for function call in its agent story. Usually there are three ways to enable a function call in an agent.\n- Pass function definitions when creating an agent. This only works if the agent supports pass function call from its constructor.\n- Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent\n- Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls.\n\nFor more information, please check out [Use function call in an agent](Use-function-call.md).",
    "filename": "dotnet/website/articles/Function-call-overview.md"
  },
  {
    "content": "This example shows how to use function call with local LLM models where [Ollama](https://ollama.com/) as local model provider and [LiteLLM](https://docs.litellm.ai/docs/) proxy server which provides an openai-api compatible interface.\n\n[![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs)\n\nTo run this example, the following prerequisites are required:\n- Install [Ollama](https://ollama.com/) and [LiteLLM](https://docs.litellm.ai/docs/) on your local machine.\n- A local model that supports function call. In this example `dolphincoder:latest` is used.\n\n## Install Ollama and pull `dolphincoder:latest` model\nFirst, install Ollama by following the instructions on the [Ollama website](https://ollama.com/).\n\nAfter installing Ollama, pull the `dolphincoder:latest` model by running the following command:\n```bash\nollama pull dolphincoder:latest\n```\n\n## Install LiteLLM and start the proxy server\n\nYou can install LiteLLM by following the instructions on the [LiteLLM website](https://docs.litellm.ai/docs/).\n```bash\npip install 'litellm[proxy]'\n```\n\nThen, start the proxy server by running the following command:\n\n```bash\nlitellm --model ollama_chat/dolphincoder --port 4000\n```\n\nThis will start an openai-api compatible proxy server at `http://localhost:4000`. You can verify if the server is running by observing the following output in the terminal:\n\n```bash\n#------------------------------------------------------------#\n#                                                            #\n#         'The worst thing about this product is...'          #\n#        https://github.com/BerriAI/litellm/issues/new        #\n#                                                            #\n#------------------------------------------------------------#\n\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n```\n\n## Install AutoGen and AutoGen.SourceGenerator\nIn your project, install the AutoGen and AutoGen.SourceGenerator package using the following command:\n\n```bash\ndotnet add package AutoGen\ndotnet add package AutoGen.SourceGenerator\n```\n\nThe `AutoGen.SourceGenerator` package is used to automatically generate type-safe `FunctionContract` instead of manually defining them. For more information, please check out [Create type-safe function](Create-type-safe-function-call.md).\n\nAnd in your project file, enable structural xml document support by setting the `GenerateDocumentationFile` property to `true`:\n\n```xml\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\n## Define `WeatherReport` function and create @AutoGen.Core.FunctionCallMiddleware\n\nCreate a `public partial` class to host the methods you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods are defined, mark them with `AutoGen.Core.FunctionAttribute` attribute.\n\n[!code-csharp[Define WeatherReport function](../../samples/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Function)]\n\nThen create a @AutoGen.Core.FunctionCallMiddleware and add the `WeatherReport` function to the middleware. The middleware will pass the `FunctionContract` to the agent when generating a response, and process the tool call response when receiving a `ToolCallMessage`.\n[!code-csharp[Define WeatherReport function](../../samples/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Create_tools)]\n\n## Create @AutoGen.OpenAI.OpenAIChatAgent with `GetWeatherReport` tool and chat with it\n\nBecause LiteLLM proxy server is openai-api compatible, we can use @AutoGen.OpenAI.OpenAIChatAgent to connect to it as a third-party openai-api provider. The agent is also registered with a @AutoGen.Core.FunctionCallMiddleware which contains the `WeatherReport` tool. Therefore, the agent can call the `WeatherReport` tool when generating a response.\n\n[!code-csharp[Create an agent with tools](../../samples/AutoGen.OpenAI.Sample/Tool_Call_With_Ollama_And_LiteLLM.cs?name=Create_Agent)]\n\nThe reply from the agent will similar to the following:\n```bash\nAggregateMessage from assistant\n--------------------\nToolCallMessage:\nToolCallMessage from assistant\n--------------------\n- GetWeatherAsync: {\"city\": \"new york\"}\n--------------------\n\nToolCallResultMessage:\nToolCallResultMessage from assistant\n--------------------\n- GetWeatherAsync: The weather in new york is 72 degrees and sunny.\n--------------------\n```",
    "filename": "dotnet/website/articles/Function-call-with-ollama-and-litellm.md"
  },
  {
    "content": "@AutoGen.Core.IGroupChat is a fundamental feature in AutoGen. It provides a way to organize multiple agents under the same context and work together to resolve a given task.\n\nIn AutoGen, there are two types of group chat:\n- @AutoGen.Core.RoundRobinGroupChat : This group chat runs agents in a round-robin sequence. The chat history plus the most recent reply from the previous agent will be passed to the next agent.\n- @AutoGen.Core.GroupChat : This group chat provides a more dynamic yet controlable way to determine the next speaker agent. You can either use a llm agent as group admin, or use a @AutoGen.Core.Graph, which is introduced by [this PR](https://github.com/microsoft/autogen/pull/1761), or both to determine the next speaker agent.\n\n> [!NOTE]\n> In @AutoGen.Core.GroupChat, when only the group admin is used to determine the next speaker agent, it's recommented to use a more powerful llm model, such as `gpt-4` to ensure the best experience.",
    "filename": "dotnet/website/articles/Group-chat-overview.md"
  },
  {
    "content": "@AutoGen.Core.GroupChat invokes agents in a dynamic way. On one hand, It relies on its admin agent to intellegently determines the next speaker based on conversation context, and on the other hand, it also allows you to control the conversation flow by using a @AutoGen.Core.Graph. This makes it a more dynamic yet controlable way to determine the next speaker agent. You can use @AutoGen.Core.GroupChat to create a dynamic group chat with multiple agents working together to resolve a given task.\n\n> [!NOTE]\n> In @AutoGen.Core.GroupChat, when only the group admin is used to determine the next speaker agent, it's recommented to use a more powerful llm model, such as `gpt-4` to ensure the best experience.\n\n## Use @AutoGen.Core.GroupChat to implement a code interpreter chat flow\nThe following example shows how to create a dynamic group chat with @AutoGen.Core.GroupChat. In this example, we will create a dynamic group chat with 4 agents: `admin`, `coder`, `reviewer` and `runner`. Each agent has its own role in the group chat:\n\n### Code interpreter group chat\n- `admin`: create task for group to work on and terminate the conversation when task is completed. In this example, the task to resolve is to calculate the 39th Fibonacci number.\n- `coder`: a dotnet coder who can write code to resolve tasks.\n- `reviewer`: a dotnet code reviewer who can review code written by `coder`. In this example, `reviewer` will examine if the code written by `coder` follows the condition below:\n  - has only one csharp code block.\n  - use top-level statements.\n  - is dotnet code snippet.\n  - print the result of the code snippet to console.\n- `runner`: a dotnet code runner who can run code written by `coder` and print the result.\n\n```mermaid\nflowchart LR\n  subgraph Group Chat\n    B[Amin]\n    C[Coder]\n    D[Reviewer]\n    E[Runner]\n  end\n```\n\n> [!NOTE]\n> The complete code of this example can be found in `Example07_Dynamic_GroupChat_Calculate_Fibonacci`\n\n### Create group chat\n\nThe code below shows how to create a dynamic group chat with @AutoGen.Core.GroupChat. In this example, we will create a dynamic group chat with 4 agents: `admin`, `coder`, `reviewer` and `runner`. In this case we don't pass a workflow to the group chat, so the group chat will use driven by the admin agent.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_group_chat)]\n\n> [!TIP]\n> You can set up initial context for the group chat using @AutoGen.Core.GroupChatExtension.SendIntroduction*. The initial context can help group admin orchestrates the conversation flow.\n\nOutput:\n\n![GroupChat](../images/articles/DynamicGroupChat/dynamicChat.gif)\n\n### Below are break-down of how agents are created and their roles in the group chat.\n\n- Create admin agent\n\nThe code below shows how to create `admin` agent. `admin` agent will create a task for group to work on and terminate the conversation when task is completed.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_admin)]\n\n- Create coder agent\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_coder)]\n\n- Create reviewer agent\n\nThe code below shows how to create `reviewer` agent. `reviewer` agent is a dotnet code reviewer who can review code written by `coder`. In this example, a `function` is used to examine if the code written by `coder` follows the condition.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=reviewer_function)]\n\n> [!TIP]\n> You can use @AutoGen.Core.FunctionAttribute to generate type-safe function definition and function call wrapper for the function. For more information, please check out [Create type safe function call](./Create-type-safe-function-call.md).\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_reviewer)]\n\n- Create runner agent\n\n> [!TIP]\n> `AutoGen` provides a built-in support for running code snippet. For more information, please check out [Execute code snippet](./Run-dotnet-code.md).\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_runner)]",
    "filename": "dotnet/website/articles/Group-chat.md"
  },
  {
    "content": "### Current version:\n\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\nAutoGen.Net provides the following packages, you can choose to install one or more of them based on your needs:\n\n- `AutoGen`: The one-in-all package. This package has dependencies over `AutoGen.Core`, `AutoGen.OpenAI`, `AutoGen.LMStudio`, `AutoGen.SemanticKernel` and `AutoGen.SourceGenerator`.\n- `AutoGen.Core`: The core package, this package provides the abstraction for message type, agent and group chat.\n- `AutoGen.OpenAI`: This package provides the integration agents over openai models.\n- `AutoGen.Mistral`: This package provides the integration agents for Mistral.AI models.\n- `AutoGen.Ollama`: This package provides the integration agents for [Ollama](https://ollama.com/).\n- `AutoGen.Anthropic`: This package provides the integration agents for [Anthropic](https://www.anthropic.com/api)\n- `AutoGen.LMStudio`: This package provides the integration agents from LM Studio.\n- `AutoGen.SemanticKernel`: This package provides the integration agents over semantic kernel.\n- `AutoGen.Gemini`: This package provides the integration agents from [Google Gemini](https://gemini.google.com/).\n- `AutoGen.AzureAIInference`: This package provides the integration agents for [Azure AI Inference](https://www.nuget.org/packages/Azure.AI.Inference).\n- `AutoGen.SourceGenerator`: This package carries a source generator that adds support for type-safe function definition generation.\n- `AutoGen.DotnetInteractive`: This packages carries dotnet interactive support to execute code snippets. The current supported language is C#, F#, powershell and python.\n\n>[!Note]\n> Help me choose\n> - If you just want to install one package and enjoy the core features of AutoGen, choose `AutoGen`.\n> - If you want to leverage AutoGen's abstraction only and want to avoid introducing any other dependencies, like `Azure.AI.OpenAI` or `Semantic Kernel`, choose `AutoGen.Core`. You will need to implement your own agent, but you can still use AutoGen core features like group chat, built-in message type, workflow and middleware.\n>- If you want to use AutoGen with openai, choose `AutoGen.OpenAI`, similarly, choose `AutoGen.LMStudio` or `AutoGen.SemanticKernel` if you want to use agents from LM Studio or semantic kernel.\n>- If you just want the type-safe source generation for function call and don't want any other features, which even include the AutoGen's abstraction, choose `AutoGen.SourceGenerator`.\n\nThen, install the package using the following command:\n\n```bash\ndotnet add package AUTOGEN_PACKAGES\n```\n\n### Consume nightly build\nTo consume nightly build, you can add one of the following feeds to your `NuGet.config` or global nuget config:\n> - [![Static Badge](https://img.shields.io/badge/azure_devops-grey?style=flat)](https://dev.azure.com/AGPublish/AGPublic/_artifacts/feed/AutoGen-Nightly) : <https://pkgs.dev.azure.com/AGPublish/AGPublic/_packaging/AutoGen-Nightly/nuget/v3/index.json>\n\nTo add a local `NuGet.config`, create a file named `NuGet.config` in the root of your project and add the following content:\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<configuration>\n  <packageSources>\n    <clear />\n    <add key=\"AutoGen\" value=\"$(FEED_URL)\" /> <!-- replace $(FEED_URL) with the feed url -->\n    <!-- other feeds -->\n  </packageSources>\n  <disabledPackageSources />\n</configuration>\n```\n\nTo add the feed to your global nuget config. You can do this by running the following command in your terminal:\n```bash\ndotnet nuget add source FEED_URL --name AutoGen\n\n# dotnet-tools contains Microsoft.DotNet.Interactive.VisualStudio package, which is used by AutoGen.DotnetInteractive\ndotnet nuget add source https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json --name dotnet-tools\n```\n\nOnce you have added the feed, you can install the nightly-build package using the following command:\n```bash\ndotnet add package AUTOGEN_PACKAGES VERSION\n```",
    "filename": "dotnet/website/articles/Installation.md"
  },
  {
    "content": "`Middleware` is a key feature in AutoGen.Net that enables you to customize the behavior of @AutoGen.Core.IAgent.GenerateReplyAsync*. It's similar to the middleware concept in ASP.Net and is widely used in AutoGen.Net for various scenarios, such as function call support, converting message of different types, print message, gather user input, etc.\n\nHere are a few examples of how middleware is used in AutoGen.Net:\n- @AutoGen.AssistantAgent is essentially an agent with @AutoGen.Core.FunctionCallMiddleware, @AutoGen.HumanInputMiddleware and default reply middleware.\n- @AutoGen.OpenAI.GPTAgent is essentially an @AutoGen.OpenAI.OpenAIChatAgent with @AutoGen.Core.FunctionCallMiddleware and @AutoGen.OpenAI.OpenAIChatRequestMessageConnector.\n\n## Use middleware in an agent\nTo use middleware in an existing agent, you can either create a @AutoGen.Core.MiddlewareAgent on top of the original agent or register middleware functions to the original agent.\n\n### Create @AutoGen.Core.MiddlewareAgent on top of the original agent\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=create_middleware_agent_with_original_agent)]\n\n### Register middleware functions to the original agent\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=register_middleware_agent)]\n\n## Short-circuit the next agent\nThe example below shows how to short-circuit the inner agent\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=short_circuit_middleware_agent)]\n\n> [!Note]\n> When multiple middleware functions are registered, the order of middleware functions is first registered, last invoked.\n\n## Streaming middleware\nYou can also modify the behavior of @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync* by registering streaming middleware to it. One example is @AutoGen.OpenAI.OpenAIChatRequestMessageConnector which converts `StreamingChatCompletionsUpdate` to one of `AutoGen.Core.TextMessageUpdate` or `AutoGen.Core.ToolCallMessageUpdate`.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MiddlewareAgentCodeSnippet.cs?name=register_streaming_middleware)]",
    "filename": "dotnet/website/articles/Middleware-overview.md"
  },
  {
    "content": "The following example shows how to create a `MistralAITokenCounterMiddleware` @AutoGen.Core.IMiddleware and count the token usage when chatting with @AutoGen.Mistral.MistralClientAgent.\n\n### Overview\nTo collect the token usage for the entire chat session, one easy solution is simply collect all the responses from agent and sum up the token usage for each response. To collect all the agent responses, we can create a middleware which simply saves all responses to a list and register it with the agent. To get the token usage information for each response, because in the example we are using @AutoGen.Mistral.MistralClientAgent, we can simply get the token usage from the response object.\n\n> [!NOTE]\n> You can find the complete example in the [Example13_OpenAIAgent_JsonMode](https://github.com/microsoft/autogen/tree/main/dotnet/samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs).\n\n- Step 1: Adding using statement\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=using_statements)]\n\n- Step 2: Create a `MistralAITokenCounterMiddleware` class which implements @AutoGen.Core.IMiddleware. This middleware will collect all the responses from the agent and sum up the token usage for each response.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=token_counter_middleware)]\n\n- Step 3: Create a `MistralClientAgent`\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=create_mistral_client_agent)]\n\n- Step 4: Register the `MistralAITokenCounterMiddleware` with the `MistralClientAgent`. Note that the order of each middlewares matters. The token counter middleware needs to be registered before `mistralMessageConnector` because it collects response only when the responding message type is `IMessage<ChatCompletionResponse>` while the `mistralMessageConnector` will convert `IMessage<ChatCompletionResponse>` to one of @AutoGen.Core.TextMessage, @AutoGen.Core.ToolCallMessage or @AutoGen.Core.ToolCallResultMessage.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=register_middleware)]\n\n- Step 5: Chat with the `MistralClientAgent` and get the token usage information from the response object.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example14_MistralClientAgent_TokenCount.cs?name=chat_with_agent)]\n\n### Output\nWhen running the example, the completion token count will be printed to the console.\n```bash\nCompletion token count: 1408 # might be different based on the response\n```",
    "filename": "dotnet/website/articles/MistralChatAgent-count-token-usage.md"
  },
  {
    "content": "## Use tool in MistralChatAgent\n\nThe following example shows how to enable tool support in @AutoGen.Mistral.MistralClientAgent by creating a `GetWeatherAsync` function and passing it to the agent.\n\nFirstly, you need to install the following packages:\n```bash\ndotnet add package AutoGen.Mistral\ndotnet add package AutoGen.SourceGenerator\n```\n\n> [!Note]\n> Tool support is only available in some mistral models. Please refer to the [link](https://docs.mistral.ai/capabilities/function_calling/#available-models) for tool call support in mistral models.\n\n> [!Note]\n> The `AutoGen.SourceGenerator` package carries a source generator that adds support for type-safe function definition generation. For more information, please check out [Create type-safe function](./Create-type-safe-function-call.md).\n\n> [!NOTE]\n> If you are using VSCode as your editor, you may need to restart the editor to see the generated code.\n\nImport the required namespace\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=using_statement)]\n\nThen define a public partial `MistralAgentFunction` class and `GetWeather` method. The `GetWeather` method is a simple function that returns the weather of a given location that marked with @AutoGen.Core.FunctionAttribute. Marking the class as `public partial` together with the @AutoGen.Core.FunctionAttribute attribute allows the source generator to generate the @AutoGen.Core.FunctionContract for the `GetWeather` method.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=weather_function)]\n\nThen create an @AutoGen.Mistral.MistralClientAgent and register it with @AutoGen.Mistral.Extension.MistralAgentExtension.RegisterMessageConnector* so it can support @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. These message types are necessary to use @AutoGen.Core.FunctionCallMiddleware, which provides support for processing and invoking function calls.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=create_mistral_function_call_agent)]\n\nThen create an @AutoGen.Core.FunctionCallMiddleware with `GetWeather` function When creating the middleware, we also pass a `functionMap` object which means the function will be automatically invoked when the agent replies a `GetWeather` function call.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=create_get_weather_function_call_middleware)]\n\nAfter the function call middleware is created, register it with the agent so the `GetWeather` function will be passed to agent during chat completion.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=register_function_call_middleware)]\n\nFinally, you can chat with the @AutoGen.Mistral.MistralClientAgent about weather! The agent will automatically invoke the `GetWeather` function to \"get\" the weather information and return the result.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/MistralAICodeSnippet.cs?name=send_message_with_function_call)]",
    "filename": "dotnet/website/articles/MistralChatAgent-use-function-call.md"
  },
  {
    "content": "The following example shows how to connect to third-party OpenAI API using @AutoGen.OpenAI.OpenAIChatAgent.\n\n[![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs)\n\n## Overview\nA lot of LLM applications/platforms support spinning up a chat server that is compatible with OpenAI API, such as LM Studio, Ollama, Mistral etc. This means that you can connect to these servers using the @AutoGen.OpenAI.OpenAIChatAgent.\n\n> [!NOTE]\n> Some platforms might not support all the features of OpenAI API. For example, Ollama does not support `function call` when using it's openai API according to its [document](https://github.com/ollama/ollama/blob/main/docs/openai.md#v1chatcompletions) (as of 2024/05/07).\n> That means some of the features of OpenAI API might not work as expected when using these platforms with the @AutoGen.OpenAI.OpenAIChatAgent.\n> Please refer to the platform's documentation for more information.\n\n## Prerequisites\n- Install the following packages:\n```bash\ndotnet add package AutoGen.OpenAI --version AUTOGEN_VERSION\n```\n\n- Spin up a chat server that is compatible with OpenAI API.\nThe following example uses Ollama as the chat server, and llama3 as the llm model.\n```bash\nollama serve\n```\n\n## Steps\n- Import the required namespaces:\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=using_statement)]\n\n- Create a `CustomHttpClientHandler` class.\n\nThe `CustomHttpClientHandler` class is used to customize the HttpClientHandler. In this example, we override the `SendAsync` method to redirect the request to local Ollama server, which is running on `http://localhost:11434`.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=CustomHttpClientHandler)]\n\n- Create an `OpenAIChatAgent` instance and connect to the third-party API.\n\nThen create an @AutoGen.OpenAI.OpenAIChatAgent instance and connect to the OpenAI API from Ollama. You can customize the transport behavior of `OpenAIClient` by passing a customized `HttpClientTransport` instance. In the customized `HttpClientTransport` instance, we pass the `CustomHttpClientHandler` we just created which redirects all openai chat requests to the local Ollama server.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=create_agent)]\n\n- Chat with the `OpenAIChatAgent`.\nFinally, you can start chatting with the agent. In this example, we send a coding question to the agent and get the response.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Connect_To_Ollama.cs?name=send_message)]\n\n## Sample Output\nThe following is the sample output of the code snippet above:\n\n![output](../images/articles/ConnectTo3PartyOpenAI/output.gif)",
    "filename": "dotnet/website/articles/OpenAIChatAgent-connect-to-third-party-api.md"
  },
  {
    "content": "The following example shows how to create an @AutoGen.OpenAI.OpenAIChatAgent and chat with it.\n\nFirsly, import the required namespaces:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]\n\nThen, create an @AutoGen.OpenAI.OpenAIChatAgent and chat with it:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=create_openai_chat_agent)]\n\n@AutoGen.OpenAI.OpenAIChatAgent also supports streaming chat via @AutoGen.Core.IAgent.GenerateStreamingReplyAsync*.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=create_openai_chat_agent_streaming)]",
    "filename": "dotnet/website/articles/OpenAIChatAgent-simple-chat.md"
  },
  {
    "content": "By default, @AutoGen.OpenAI.OpenAIChatAgent only supports the @AutoGen.Core.IMessage<T> type where `T` is original request or response message from `Azure.AI.OpenAI`. To support more AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, @AutoGen.Core.MultiModalMessage and so on, you can register the agent with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector. The @AutoGen.OpenAI.OpenAIChatRequestMessageConnector will convert the message from AutoGen built-in message types to `Azure.AI.OpenAI.ChatRequestMessage` and vice versa.\n\nimport the required namespaces:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=register_openai_chat_message_connector)]",
    "filename": "dotnet/website/articles/OpenAIChatAgent-support-more-messages.md"
  },
  {
    "content": "The following example shows how to create a `GetWeatherAsync` function and pass it to @AutoGen.OpenAI.OpenAIChatAgent.\n\nFirstly, you need to install the following packages:\n```xml\n<ItemGroup>\n    <PackageReference Include=\"AutoGen.OpenAI\" Version=\"AUTOGEN_VERSION\" />\n    <PackageReference Include=\"AutoGen.SourceGenerator\" Version=\"AUTOGEN_VERSION\" />\n</ItemGroup>\n```\n\n> [!Note]\n> The `AutoGen.SourceGenerator` package carries a source generator that adds support for type-safe function definition generation. For more information, please check out [Create type-safe function](./Create-type-safe-function-call.md).\n\n> [!NOTE]\n> If you are using VSCode as your editor, you may need to restart the editor to see the generated code.\n\nFirstly, import the required namespaces:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=using_statement)]\n\nThen, define a public partial class: `Function` with `GetWeather` method\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=weather_function)]\n\nThen, create an @AutoGen.OpenAI.OpenAIChatAgent and register it with @AutoGen.OpenAI.OpenAIChatRequestMessageConnector so it can support @AutoGen.Core.ToolCallMessage and @AutoGen.Core.ToolCallResultMessage. These message types are necessary to use @AutoGen.Core.FunctionCallMiddleware, which provides support for processing and invoking function calls.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=openai_chat_agent_get_weather_function_call)]\n\nThen, create an @AutoGen.Core.FunctionCallMiddleware with `GetWeather` function and register it with the agent above. When creating the middleware, we also pass a `functionMap` to @AutoGen.Core.FunctionCallMiddleware, which means the function will be automatically invoked when the agent replies a `GetWeather` function call.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=create_function_call_middleware)]\n\nFinally, you can chat with the @AutoGen.OpenAI.OpenAIChatAgent and invoke the `GetWeather` function.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/OpenAICodeSnippet.cs?name=chat_agent_send_function_call)]",
    "filename": "dotnet/website/articles/OpenAIChatAgent-use-function-call.md"
  },
  {
    "content": "The following example shows how to enable JSON mode in @AutoGen.OpenAI.OpenAIChatAgent.\n\n[![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Use_Json_Mode.cs)\n\n## What is JSON mode?\nJSON mode is a new feature in OpenAI which allows you to instruct model to always respond with a valid JSON object. This is useful when you want to constrain the model output to JSON format only.\n\n> [!NOTE]\n> Currently, JOSN mode is only supported by `gpt-4-turbo-preview` and `gpt-3.5-turbo-0125`. For more information (and limitations) about JSON mode, please visit [OpenAI API documentation](https://platform.openai.com/docs/guides/text-generation/json-mode).\n\n## How to enable JSON mode in OpenAIChatAgent.\n\nTo enable JSON mode for @AutoGen.OpenAI.OpenAIChatAgent, set `responseFormat` to `ChatCompletionsResponseFormat.JsonObject` when creating the agent. Note that when enabling JSON mode, you also need to instruct the agent to output JSON format in its system message.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=create_agent)]\n\nAfter enabling JSON mode, the `openAIClientAgent` will always respond in JSON format when it receives a message.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=chat_with_agent)]\n\nWhen running the example, the output from `openAIClientAgent` will be a valid JSON object which can be parsed as `Person` class defined below. Note that in the output, the `address` field is missing because the address information is not provided in user input.\n\n[!code-csharp[](../../samples/AutoGen.OpenAI.Sample/Use_Json_Mode.cs?name=person_class)]\n\nThe output will be:\n```bash\nName: John\nAge: 25\nDone\n```",
    "filename": "dotnet/website/articles/OpenAIChatAgent-use-json-mode.md"
  },
  {
    "content": "@AutoGen.Core.PrintMessageMiddleware is a built-in @AutoGen.Core.IMiddleware that pretty print @AutoGen.Core.IMessage to console.\n\n> [!NOTE]\n> @AutoGen.Core.PrintMessageMiddleware support the following @AutoGen.Core.IMessage types:\n> - @AutoGen.Core.TextMessage\n> - @AutoGen.Core.MultiModalMessage\n> - @AutoGen.Core.ToolCallMessage\n> - @AutoGen.Core.ToolCallResultMessage\n> - @AutoGen.Core.Message\n> - (streaming) @AutoGen.Core.TextMessageUpdate\n> - (streaming) @AutoGen.Core.ToolCallMessageUpdate\n\n## Use @AutoGen.Core.PrintMessageMiddleware in an agent\nYou can use @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* to register the @AutoGen.Core.PrintMessageMiddleware to an agent.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=PrintMessageMiddleware)]\n\n@AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* will format the message and print it to console\n![image](../images/articles/PrintMessageMiddleware/printMessage.png)\n\n## Streaming message support\n\n@AutoGen.Core.PrintMessageMiddleware also supports streaming message types like @AutoGen.Core.TextMessageUpdate and @AutoGen.Core.ToolCallMessageUpdate. If you register @AutoGen.Core.PrintMessageMiddleware to a @AutoGen.Core.IStreamingAgent, it will format the streaming message and print it to console if the message is of supported type.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/PrintMessageMiddlewareCodeSnippet.cs?name=print_message_streaming)]\n\n![image](../images/articles/PrintMessageMiddleware/streamingoutput.gif)",
    "filename": "dotnet/website/articles/Print-message-middleware.md"
  },
  {
    "content": "@AutoGen.Core.RoundRobinGroupChat is a group chat that invokes agents in a round-robin order. It's useful when you want to call multiple agents in a fixed sequence. For example, asking search agent to retrieve related information followed by a summarization agent to summarize the information. Beside, it also used by @AutoGen.Core.AgentExtension.SendAsync(AutoGen.Core.IAgent,AutoGen.Core.IAgent,System.String,System.Collections.Generic.IEnumerable{AutoGen.Core.IMessage},System.Int32,System.Threading.CancellationToken) in two agent chat.\n\n### Use @AutoGen.Core.RoundRobinGroupChat to implement a search-summarize chat flow\n\n```mermaid\nflowchart LR\n    A[User] -->|Ask a question| B[Search Agent]\n    B -->|Retrieve information| C[Summarization Agent]\n    C -->|Summarize result| A[User]\n```\n\n> [!NOTE]\n> Complete code can be found in [Example11_Sequential_GroupChat_Example](https://github.com/microsoft/autogen/blob/dotnet/dotnet/samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs);\n\nStep 1: Add required using statements\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=using_statement)]\n\nStep 2: Create a `bingSearch` agent using @AutoGen.SemanticKernel.SemanticKernelAgent\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=CreateBingSearchAgent)]\n\nStep 3: Create a `summarization` agent using @AutoGen.SemanticKernel.SemanticKernelAgent\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=CreateSummarizerAgent)]\n\nStep 4: Create a @AutoGen.Core.RoundRobinGroupChat and add `bingSearch` and `summarization` agents to it\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example11_Sequential_GroupChat_Example.cs?name=Sequential_GroupChat_Example)]\n\nOutput:\n\n![Searcher-Summarizer](../images/articles/SequentialGroupChat/SearcherSummarizer.gif)",
    "filename": "dotnet/website/articles/Roundrobin-chat.md"
  },
  {
    "content": "`AutoGen` provides a built-in feature to run code snippet from agent response. Currently the following languages are supported:\n- dotnet\n\nMore languages will be supported in the future.\n\n## What is a code snippet?\nA code snippet in agent response is a code block with a language identifier. For example:\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_3)]\n\n## Why running code snippet is useful?\nThe ability of running code snippet can greatly extend the ability of an agent. Because it enables agent to resolve tasks by writing code and run it, which is much more powerful than just returning a text response.\n\nFor example, in data analysis scenario, agent can resolve tasks like \"What is the average of the sales amount of the last 7 days?\" by firstly write a code snippet to query the sales amount of the last 7 days, then calculate the average and then run the code snippet to get the result.\n\n> [!WARNING]\n> Running arbitrary code snippet from agent response could bring risks to your system. Using this feature with caution.\n\n## Use dotnet interactive kernel to execute code snippet?\nThe built-in feature of running dotnet code snippet is provided by [dotnet-interactive](https://github.com/dotnet/interactive). To run dotnet code snippet, you need to install the following package to your project, which provides the intergraion with dotnet-interactive:\n\n```xml\n<PackageReference Include=\"AutoGen.DotnetInteractive\" />\n```\n\nThen you can use @AutoGen.DotnetInteractive.DotnetInteractiveKernelBuilder* to create a in-process dotnet-interactive composite kernel with C# and F# kernels.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_1)]\n\nAfter that, use @AutoGen.DotnetInteractive.Extension.RunSubmitCodeCommandAsync* method to run code snippet. The method will return the result of the code snippet.\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_2)]\n\n## Run python code snippet\nTo run python code, firstly you need to have python installed on your machine, then you need to set up ipykernel and jupyter in your environment.\n\n```bash\npip install ipykernel\npip install jupyter\n```\n\nAfter `ipykernel` and `jupyter` are installed, you can confirm the ipykernel is installed correctly by running the following command:\n\n```bash\njupyter kernelspec list\n```\n\nThe output should contain all available kernels, including `python3`.\n\n```bash\nAvailable kernels:\n    python3    /usr/local/share/jupyter/kernels/python3\n    ...\n```\n\nThen you can add the python kernel to the dotnet-interactive composite kernel by calling `AddPythonKernel` method.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/RunCodeSnippetCodeSnippet.cs?name=code_snippet_1_4)]\n\n## Further reading\nYou can refer to the following examples for running code snippet in agentic workflow:\n- Dynamic_GroupChat_Coding_Task:  [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Basic.Sample/Example04_Dynamic_GroupChat_Coding_Task.cs)\n- Dynamic_GroupChat_Calculate_Fibonacci: [![](https://img.shields.io/badge/Open%20on%20Github-grey?logo=github)](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs)",
    "filename": "dotnet/website/articles/Run-dotnet-code.md"
  },
  {
    "content": "In `AutoGen`, you can start a conversation between two agents using @AutoGen.Core.AgentExtension.InitiateChatAsync* or one of @AutoGen.Core.AgentExtension.SendAsync* APIs. When conversation starts, the sender agent will firstly send a message to receiver agent, then receiver agent will generate a reply and send it back to sender agent. This process will repeat until either one of the agent sends a termination message or the maximum number of turns is reached.\n\n> [!NOTE]\n> A termination message is an @AutoGen.Core.IMessage which content contains the keyword: @AutoGen.Core.GroupChatExtension.TERMINATE. To determine if a message is a terminate message, you can use @AutoGen.Core.GroupChatExtension.IsGroupChatTerminateMessage*.\n\n## A basic example\n\nThe following example shows how to start a conversation between the teacher agent and student agent, where the student agent starts the conversation by asking teacher to create math questions.\n\n> [!TIP]\n> You can use @AutoGen.Core.PrintMessageMiddlewareExtension.RegisterPrintMessage* to pretty print the message replied by the agent.\n\n> [!NOTE]\n> The conversation is terminated when teacher agent sends a message containing the keyword: @AutoGen.Core.GroupChatExtension.TERMINATE.\n\n> [!NOTE]\n> The teacher agent uses @AutoGen.Core.MiddlewareExtension.RegisterPostProcess* to register a post process function which returns a hard-coded termination message when a certain condition is met. Comparing with putting the @AutoGen.Core.GroupChatExtension.TERMINATE keyword in the prompt, this approach is more robust especially when a weaker LLM model is used.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example02_TwoAgent_MathChat.cs?name=code_snippet_1)]",
    "filename": "dotnet/website/articles/Two-agent-chat.md"
  },
  {
    "content": "## Use function call in AutoGen agent\n\nTypically, there are three ways to pass a function definition to an agent to enable function call:\n- Pass function definitions when creating an agent. This only works if the agent supports pass function call from its constructor.\n- Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent\n- Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls.\n\n> [!NOTE]\n> To use function call, the underlying LLM model must support function call as well for the best experience. If the model does not support function call, it's likely that the function call will be ignored and the model will reply with a normal response even if a function call is passed to it.\n\n## Pass function definitions when creating an agent\nIn some agents like @AutoGen.AssistantAgent or @AutoGen.OpenAI.GPTAgent, you can pass function definitions when creating the agent\n\nSuppose the `TypeSafeFunctionCall` is defined in the following code snippet:\n[!code-csharp[TypeSafeFunctionCall](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/TypeSafeFunctionCallCodeSnippet.cs?name=weather_report)]\n\nYou can then pass the `WeatherReport` to the agent when creating it:\n[!code-csharp[assistant agent](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_4)]\n\n## Passing function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent\nYou can also pass function definitions in @AutoGen.Core.GenerateReplyOptions when invoking an agent. This is useful when you want to override the function definitions passed to the agent when creating it.\n\n[!code-csharp[assistant agent](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=overrider_function_contract)]\n\n## Register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls\nYou can also register an agent with @AutoGen.Core.FunctionCallMiddleware to process and invoke function calls. This is useful when you want to process and invoke function calls in a more flexible way.\n\n[!code-csharp[assistant agent](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=register_function_call_middleware)]\n\n## Invoke function call inside an agent\nTo invoke a function instead of returning the function call object, you can pass its function call wrapper to the agent via `functionMap`.\n\nYou can then pass the `WeatherReportWrapper` to the agent via `functionMap`:\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_6)]\n\nWhen a function call object is returned, the agent will invoke the function and uses the return value as response rather than returning the function call object.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=code_snippet_6_1)]\n\n## Invoke function call by another agent\nYou can also use another agent to invoke the function call from one agent. This is a useful pattern in two-agent chat, where one agent is used as a function proxy to invoke the function call from another agent. Once the function call is invoked, the result can be returned to the original agent for further processing.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/FunctionCallCodeSnippet.cs?name=two_agent_weather_chat)]",
    "filename": "dotnet/website/articles/Use-function-call.md"
  },
  {
    "content": "Sometimes, you may want to add more control on how the next agent is selected in a @AutoGen.Core.GroupChat based on the task you want to resolve. For example, in the previous [code writing example](./Group-chat.md), the original code interpreter workflow can be improved by the following diagram because it's not necessary for `admin` to directly talk to `reviewer`, nor it's necessary for `coder` to talk to `runner`.\n\n```mermaid\nflowchart TD\n    A[Admin] -->|Ask coder to write code| B[Coder]\n    B -->|Ask Reviewer to review code| C[Reviewer]\n    C -->|Ask Runner to run code| D[Runner]\n    D -->|Send result if succeed| A[Admin]\n    D -->|Ask coder to fix if failed| B[Coder]\n    C -->|Ask coder to fix if not approved| B[Coder]\n```\n\nBy having @AutoGen.Core.GroupChat to follow a specific graph flow, we can bring prior knowledge to group chat and make the conversation more efficient and robust. This is where @AutoGen.Core.Graph comes in.\n\n### Create a graph\nThe following code shows how to create a graph that represents the diagram above. The graph doesn't need to be a finite state machine where each state can only have one legitimate next state. Instead, it can be a directed graph where each state can have multiple legitimate next states. And if there are multiple legitimate next states, the `admin` agent of @AutoGen.Core.GroupChat will decide which one to go based on the conversation context.\n\n> [!TIP]\n> @AutoGen.Core.Graph supports conditional transitions. To create a conditional transition, you can pass a lambda function to `canTransitionAsync` when creating a @AutoGen.Core.Transition. The lambda function should return a boolean value indicating if the transition can be taken.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_workflow)]\n\nOnce the graph is created, you can pass it to the group chat. The group chat will then use the graph along with admin agent to orchestrate the conversation flow.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/Example07_Dynamic_GroupChat_Calculate_Fibonacci.cs?name=create_group_chat_with_workflow)]",
    "filename": "dotnet/website/articles/Use-graph-in-group-chat.md"
  },
  {
    "content": "### Function comparison between Python AutoGen and AutoGen\\.Net\n\n\n#### Agentic pattern\n\n| Feature | AutoGen | AutoGen\\.Net |\n| :---------------- | :------ | :---- |\n| Code interpreter | run python code in local/docker/notebook executor | run csharp code in dotnet interactive executor |\n| Single agent chat pattern | \u2714\ufe0f | \u2714\ufe0f |\n| Two agent chat pattern | \u2714\ufe0f | \u2714\ufe0f |\n| group chat (include FSM)| \u2714\ufe0f | \u2714\ufe0f (using workflow for FSM groupchat) |\n| Nest chat| \u2714\ufe0f | \u2714\ufe0f (using middleware pattern)|\n|Sequential chat | \u2714\ufe0f | \u274c (need to manually create task in code) |\n| Tool | \u2714\ufe0f | \u2714\ufe0f |\n\n\n#### LLM platform support\n\n\u2139\ufe0f Note \n\n``` Other than the platforms list below, AutoGen.Net also supports all the platforms that semantic kernel supports via AutoGen.SemanticKernel as a bridge ```\n\n| Feature | AutoGen | AutoGen\\.Net |\n| :---------------- | :------ | :---- |\n| OpenAI (include third-party) | \u2714\ufe0f | \u2714\ufe0f |\n| Mistral |\t\u2714\ufe0f|\t\u2714\ufe0f|\n| Ollama |\t\u2714\ufe0f|\t\u2714\ufe0f|\n|Claude\t|\u2714\ufe0f\t|\u2714\ufe0f|\n|Gemini (Include Vertex) | \u2714\ufe0f | \u2714\ufe0f |\n\n#### Popular Contrib Agent support\n\n\n| Feature | AutoGen | AutoGen\\.Net |\n| :---------------- | :------ | :---- |\n| Rag Agent |\t\u2714\ufe0f|\t\u274c |\n| Web surfer |\t\u2714\ufe0f|\t\u274c |",
    "filename": "dotnet/website/articles/function-comparison-page-between-python-AutoGen-and-autogen.net.md"
  },
  {
    "content": "### Get start with AutoGen for dotnet\n[![dotnet-ci](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/dotnet-build.yml)\n[![NuGet version](https://badge.fury.io/nu/AutoGen.Core.svg)](https://badge.fury.io/nu/AutoGen.Core)\n\nFirstly, add `AutoGen` package to your project.\n\n```bash\ndotnet add package AutoGen\n```\n\n> [!NOTE]\n> For more information about installing packages, please check out the [installation guide](Installation.md).\n\nThen you can start with the following code snippet to create a conversable agent and chat with it.\n\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/GetStartCodeSnippet.cs?name=snippet_GetStartCodeSnippet)]\n[!code-csharp[](../../samples/AgentChat/Autogen.Basic.Sample/CodeSnippet/GetStartCodeSnippet.cs?name=code_snippet_1)]\n\n### Tutorial\nGetting started with AutoGen.Net by following the [tutorial](../tutorial/Chat-with-an-agent.md) series.\n### Examples\nYou can find more examples under the [sample project](https://github.com/microsoft/autogen/tree/dotnet/dotnet/samples/AgentChat/Autogen.Basic.Sample).\n\n### Report a bug or request a feature\nYou can report a bug or request a feature by creating a new issue in the [github issue](https://github.com/microsoft/autogen/issues) and specifying label the label \"donet\"",
    "filename": "dotnet/website/articles/getting-start.md"
  },
  {
    "content": "[!INCLUDE [](./articles/getting-start.md)]",
    "filename": "dotnet/website/index.md"
  },
  {
    "content": "# AutoGen.Net 0.0.16 Release Notes\n\nWe are excited to announce the release of **AutoGen.Net 0.0.16**. This release includes several new features, bug fixes, improvements, and important updates. Below are the detailed release notes:\n\n**[Milestone: AutoGen.Net 0.0.16](https://github.com/microsoft/autogen/milestone/4)**\n\n## \ud83d\udce6 New Features\n1. **Deprecate `IStreamingMessage`** ([#3045](https://github.com/microsoft/autogen/issues/3045)) - Replaced `IStreamingMessage` and `IStreamingMessage<T>` with `IMessage` and `IMessage<T>`.\n2. **Add example for using ollama + LiteLLM for function call** ([#3014](https://github.com/microsoft/autogen/issues/3014)) - Added a new tutorial to the website for integrating ollama with LiteLLM for function calls.\n3. **Add ReAct sample** ([#2978](https://github.com/microsoft/autogen/issues/2978)) - Added a new sample demonstrating the ReAct pattern.\n4. **Support tools Anthropic Models** ([#2771](https://github.com/microsoft/autogen/issues/2771)) - Introduced support for tools like `AnthropicClient`, `AnthropicClientAgent`, and `AnthropicMessageConnector`.\n5. **Propose Orchestrator for managing group chat/agentic workflow** ([#2695](https://github.com/microsoft/autogen/issues/2695)) - Introduced a customizable orchestrator interface for managing group chats and agent workflows.\n6. **Run Agent as Web API** ([#2519](https://github.com/microsoft/autogen/issues/2519)) - Introduced the ability to start an OpenAI-chat-compatible web API from an arbitrary agent.\n\n## \ud83d\udc1b Bug Fixes\n1. **SourceGenerator doesn't work when function's arguments are empty** ([#2976](https://github.com/microsoft/autogen/issues/2976)) - Fixed an issue where the SourceGenerator failed when function arguments were empty.\n2. **Add content field in ToolCallMessage** ([#2975](https://github.com/microsoft/autogen/issues/2975)) - Added a content property in `ToolCallMessage` to handle text content returned by the OpenAI model during tool calls.\n3. **AutoGen.SourceGenerator doesn\u2019t encode `\"` in structural comments** ([#2872](https://github.com/microsoft/autogen/issues/2872)) - Fixed an issue where structural comments containing `\"` were not properly encoded, leading to compilation errors.\n\n## \ud83d\ude80 Improvements\n1. **Sample update - Add getting-start samples for Basic.Sample project** ([#2859](https://github.com/microsoft/autogen/issues/2859)) - Re-organized the `AutoGen.Basic.Sample` project to include only essential getting-started examples, simplifying complex examples.\n2. **Graph constructor should consider null transitions** ([#2708](https://github.com/microsoft/autogen/issues/2708)) - Updated the Graph constructor to handle cases where transitions\u2019 values are null.\n\n## \u26a0\ufe0f API-Breakchange\n1. **Deprecate `IStreamingMessage`** ([#3045](https://github.com/microsoft/autogen/issues/3045)) - **Migration guide:** Deprecating `IStreamingMessage` will introduce breaking changes, particularly for `IStreamingAgent` and `IStreamingMiddleware`. Replace all `IStreamingMessage` and `IStreamingMessage<T>` with `IMessage` and `IMessage<T>`.\n\n## \ud83d\udcda Document Update\n1. **Add example for using ollama + LiteLLM for function call** ([#3014](https://github.com/microsoft/autogen/issues/3014)) - Added a tutorial to the website for using ollama with LiteLLM.\n\nThank you to all the contributors for making this release possible. We encourage everyone to upgrade to AutoGen.Net 0.0.16 to take advantage of these new features and improvements. If you encounter any issues or have any feedback, please let us know.\n\nHappy coding! \ud83d\ude80",
    "filename": "dotnet/website/release_note/0.0.16.md"
  },
  {
    "content": "# AutoGen.Net 0.0.17 Release Notes\n\n## \ud83c\udf1f What's New\n\n1. **.NET Core Target Framework Support** ([#3203](https://github.com/microsoft/autogen/issues/3203))\n   - \ud83d\ude80 Added support for .NET Core to ensure compatibility and enhanced performance of AutoGen packages across different platforms.\n\n2. **Kernel Support in Interactive Service Constructor** ([#3181](https://github.com/microsoft/autogen/issues/3181))\n   - \ud83e\udde0 Enhanced the Interactive Service to accept a kernel in its constructor, facilitating usage in notebook environments.\n\n3. **Constructor Options for OpenAIChatAgent** ([#3126](https://github.com/microsoft/autogen/issues/3126))\n   - \u2699\ufe0f Added new constructor options for `OpenAIChatAgent` to allow full control over chat completion flags/options.\n\n4. **Step-by-Step Execution for Group Chat** ([#3075](https://github.com/microsoft/autogen/issues/3075))\n   - \ud83d\udee0\ufe0f Introduced an `IAsyncEnumerable` extension API to run group chat step-by-step, enabling developers to observe internal processes or implement early stopping mechanisms.\n\n## \ud83d\ude80 Improvements\n\n1. **Cancellation Token Addition in Graph APIs** ([#3111](https://github.com/microsoft/autogen/issues/3111))\n   - \ud83d\udd04 Added cancellation tokens to async APIs in the `AutoGen.Core.Graph` class to follow best practices and enhance the control flow.\n\n## \u26a0\ufe0f API Breaking Changes\n\n1. **FunctionDefinition Generation Stopped in Source Generator** ([#3133](https://github.com/microsoft/autogen/issues/3133))\n   - \ud83d\uded1 Stopped generating `FunctionDefinition` from `Azure.AI.OpenAI` in the source generator to eliminate unnecessary package dependencies. Migration guide:\n     - \u27a1\ufe0f Use `ToOpenAIFunctionDefinition()` extension from `AutoGen.OpenAI` for generating `FunctionDefinition` from `AutoGen.Core.FunctionContract`.\n     - \u27a1\ufe0f Use `FunctionContract` for metadata such as function name or parameters.\n\n2. **Namespace Renaming for AutoGen.WebAPI** ([#3152](https://github.com/microsoft/autogen/issues/3152))\n   - \u270f\ufe0f Renamed the namespace of `AutoGen.WebAPI` from `AutoGen.Service` to `AutoGen.WebAPI` to maintain consistency with the project name.\n\n3. **Semantic Kernel Version Update** ([#3118](https://github.com/microsoft/autogen/issues/3118))\n   - \ud83d\udcc8 Upgraded the Semantic Kernel version to 1.15.1 for enhanced functionality and performance improvements. This might introduce break change for those who use a lower-version semantic kernel.\n\n## \ud83d\udcda Documentation\n\n1. **Consume AutoGen.Net Agent in AG Studio** ([#3142](https://github.com/microsoft/autogen/issues/3142))\n   - Added detailed documentation on using AutoGen.Net Agent as a model in AG Studio, including examples of starting an OpenAI chat backend and integrating third-party OpenAI models.\n\n2. **Middleware Overview Documentation Errors Fixed** ([#3129](https://github.com/microsoft/autogen/issues/3129))\n   - Corrected logic and compile errors in the example code provided in the Middleware Overview documentation to ensure it runs without issues.\n\n---\n\nWe hope you enjoy the new features and improvements in AutoGen.Net 0.0.17! If you encounter any issues or have feedback, please open a new issue on our [GitHub repository](https://github.com/microsoft/autogen/issues).",
    "filename": "dotnet/website/release_note/0.0.17.md"
  },
  {
    "content": "# \ud83c\udf89 Release Notes: AutoGen.Net 0.1.0 \ud83c\udf89\n\n## \ud83d\udce6 New Packages\n\n1. **Add AutoGen.AzureAIInference Package**\n   - **Issue**: [.Net][Feature Request] [#3323](https://github.com/microsoft/autogen/issues/3323)\n   - **Description**: The new `AutoGen.AzureAIInference` package includes the `ChatCompletionClientAgent`.\n\n## \u2728 New Features\n\n1. **Enable Step-by-Step Execution for Two Agent Chat API**\n   - **Issue**: [.Net][Feature Request] [#3339](https://github.com/microsoft/autogen/issues/3339)\n   - **Description**: The `AgentExtension.SendAsync` now returns an `IAsyncEnumerable`, allowing conversations to be driven step by step, similar to how `GroupChatExtension.SendAsync` works.\n\n2. **Support Python Code Execution in AutoGen.DotnetInteractive**\n   - **Issue**: [.Net][Feature Request] [#3316](https://github.com/microsoft/autogen/issues/3316)\n   - **Description**: `dotnet-interactive` now supports Jupyter kernel connection, allowing Python code execution in `AutoGen.DotnetInteractive`.\n\n3. **Support Prompt Cache in Claude**\n   - **Issue**: [.Net][Feature Request] [#3359](https://github.com/microsoft/autogen/issues/3359)\n   - **Description**: Claude now supports prompt caching, which dramatically lowers the bill if the cache is hit. Added the corresponding option in the Claude client.\n\n## \ud83d\udc1b Bug Fixes\n\n1. **GroupChatExtension.SendAsync Doesn\u2019t Terminate Chat When `IOrchestrator` Returns Null as Next Agent**\n   - **Issue**: [.Net][Bug] [#3306](https://github.com/microsoft/autogen/issues/3306)\n   - **Description**: Fixed an issue where `GroupChatExtension.SendAsync` would continue until the max_round is reached even when `IOrchestrator` returns null as the next speaker.\n\n2. **InitializedMessages Are Added Repeatedly in GroupChatExtension.SendAsync Method**\n   - **Issue**: [.Net][Bug] [#3268](https://github.com/microsoft/autogen/issues/3268)\n   - **Description**: Fixed an issue where initialized messages from group chat were being added repeatedly in every iteration of the `GroupChatExtension.SendAsync` API.\n\n3. **Remove `Azure.AI.OpenAI` Dependency from `AutoGen.DotnetInteractive`**\n   - **Issue**: [.Net][Feature Request] [#3273](https://github.com/microsoft/autogen/issues/3273)\n   - **Description**: Fixed an issue by removing the `Azure.AI.OpenAI` dependency from `AutoGen.DotnetInteractive`, simplifying the package and reducing dependencies.\n\n## \ud83d\udcc4 Documentation Updates\n\n1. **Add Function Comparison Page Between Python AutoGen and AutoGen.Net**\n   - **Issue**: [.Net][Document] [#3184](https://github.com/microsoft/autogen/issues/3184)\n   - **Description**: Added comparative documentation for features between AutoGen and AutoGen.Net across various functionalities and platform supports.",
    "filename": "dotnet/website/release_note/0.1.0.md"
  },
  {
    "content": "# Release Notes for AutoGen.Net v0.2.0 \ud83d\ude80\n\n## New Features \ud83c\udf1f\n- **OpenAI Structural Format Output**: Added support for structural output format in the OpenAI integration. You can check out the example [here](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Structural_Output.cs) ([#3482](https://github.com/microsoft/autogen/issues/3482)).\n- **Structural Output Configuration**: Introduced a property for overriding the structural output schema when generating replies with `GenerateReplyOption` ([#3436](https://github.com/microsoft/autogen/issues/3436)).\n\n## Bug Fixes \ud83d\udc1b\n- **Fixed Error Code 500**: Resolved an issue where an error occurred when the message history contained multiple different tool calls with the `name` field ([#3437](https://github.com/microsoft/autogen/issues/3437)).\n\n## Improvements \ud83d\udd27\n- **Leverage OpenAI V2.0 in AutoGen.OpenAI  package**: The `AutoGen.OpenAI` package now uses OpenAI v2.0, providing improved functionality and performance. In the meantime, the original `AutoGen.OpenAI` is still available and can be accessed by `AutoGen.OpenAI.V1`. This allows users who prefer to continue to use `Azure.AI.OpenAI v1` package in their project. ([#3193](https://github.com/microsoft/autogen/issues/3193)).\n- **Deprecation of GPTAgent**: `GPTAgent` has been deprecated in favor of `OpenAIChatAgent` and `OpenAIMessageConnector` ([#3404](https://github.com/microsoft/autogen/issues/3404)).\n\n## Documentation \ud83d\udcda\n- **Tool Call Instructions**: Added detailed documentation on using tool calls with `ollama` and `OpenAIChatAgent` ([#3248](https://github.com/microsoft/autogen/issues/3248)).\n\n### Migration Guides \ud83d\udd04\n\n#### For the Deprecation of `GPTAgent` ([#3404](https://github.com/microsoft/autogen/issues/3404)):\n**Before:**\n```csharp\nvar agent = new GPTAgent(...);\n```\n**After:**\n```csharp\nvar agent = new OpenAIChatAgent(...)\n    .RegisterMessageConnector();\n```\n\n#### For Using Azure.AI.OpenAI v2.0 ([#3193](https://github.com/microsoft/autogen/issues/3193)):\n**Previous way of creating `OpenAIChatAgent`:**\n```csharp\nvar openAIClient = new OpenAIClient(apiKey);\nvar openAIClientAgent = new OpenAIChatAgent(\n            openAIClient: openAIClient,\n            model: \"gpt-4o-mini\",\n            // Other parameters...\n            );\n```\n\n**New way of creating `OpenAIChatAgent`:**\n```csharp\nvar openAIClient = new OpenAIClient(apiKey);\nvar openAIClientAgent = new OpenAIChatAgent(\n            chatClient: openAIClient.GetChatClient(\"gpt-4o-mini\"),\n            // Other parameters...\n            );\n```",
    "filename": "dotnet/website/release_note/0.2.0.md"
  },
  {
    "content": "\ufeff# Release Notes for AutoGen.Net v0.2.1 \ud83d\ude80\n\n## New Features \ud83c\udf1f\n- **Support for OpenAi o1-preview** : Added support for OpenAI o1-preview model ([#3522](https://github.com/microsoft/autogen/issues/3522))\n\n## Example \ud83d\udcda\n- **OpenAI o1-preview**: [Connect_To_OpenAI_o1_preview](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AutoGen.OpenAI.Sample/Connect_To_OpenAI_o1_preview.cs)",
    "filename": "dotnet/website/release_note/0.2.1.md"
  },
  {
    "content": "\ufeff# Release Notes for AutoGen.Net v0.2.2 \ud83d\ude80\n\n## Improvements \ud83c\udf1f\n- **Update OpenAI and Semantick Kernel to the latest version** : Updated OpenAI and Semantick Kernel to the latest version ([#3792](https://github.com/microsoft/autogen/pull/3792)",
    "filename": "dotnet/website/release_note/0.2.2.md"
  },
  {
    "content": "##### Update on 0.0.15 (2024-06-13) Milestone: [AutoGen.Net 0.0.15](https://github.com/microsoft/autogen/milestone/3)\n\n###### Highlights\n- [Issue 2851](https://github.com/microsoft/autogen/issues/2851) `AutoGen.Gemini` package for Gemini support. Examples can be found [here](https://github.com/microsoft/autogen/tree/main/dotnet/samples/AutoGen.Gemini.Sample)\n\n##### Update on 0.0.14 (2024-05-28)\n###### New features\n- [Issue 2319](https://github.com/microsoft/autogen/issues/2319) Add `AutoGen.Ollama` package for Ollama support. Special thanks to @iddelacruz for the effort.\n- [Issue 2608](https://github.com/microsoft/autogen/issues/2608) Add `AutoGen.Anthropic` package for Anthropic support. Special thanks to @DavidLuong98 for the effort.\n- [Issue 2647](https://github.com/microsoft/autogen/issues/2647) Add `ToolCallAggregateMessage` for function call middleware.\n\n###### API Breaking Changes\n- [Issue 2648](https://github.com/microsoft/autogen/issues/2648) Deprecate `Message` type.\n- [Issue 2649](https://github.com/microsoft/autogen/issues/2649) Deprecate `Workflow` type.\n###### Bug Fixes\n- [Issue 2735](https://github.com/microsoft/autogen/issues/2735) Fix tool call issue in AutoGen.Mistral package.\n- [Issue 2722](https://github.com/microsoft/autogen/issues/2722) Fix parallel funciton call in function call middleware.\n- [Issue 2633](https://github.com/microsoft/autogen/issues/2633) Set up `name` field in `OpenAIChatMessageConnector`\n- [Issue 2660](https://github.com/microsoft/autogen/issues/2660) Fix dotnet interactive restoring issue when system language is Chinese\n- [Issue 2687](https://github.com/microsoft/autogen/issues/2687) Add `global::` prefix to generated code to avoid conflict with user-defined types. \n##### Update on 0.0.13 (2024-05-09)\n###### New features\n- [Issue 2593](https://github.com/microsoft/autogen/issues/2593) Consume SK plugins in Agent.\n- [Issue 1893](https://github.com/microsoft/autogen/issues/1893) Support inline-data in ImageMessage\n- [Issue 2481](https://github.com/microsoft/autogen/issues/2481) Introduce `ChatCompletionAgent` to `AutoGen.SemanticKernel`\n###### API Breaking Changes\n- [Issue 2470](https://github.com/microsoft/autogen/issues/2470) Update the return type of `IStreamingAgent.GenerateStreamingReplyAsync` from `Task<IAsyncEnumerable<IStreamingMessage>>` to `IAsyncEnumerable<IStreamingMessage>`\n- [Issue 2470](https://github.com/microsoft/autogen/issues/2470) Update the return type of `IStreamingMiddleware.InvokeAsync` from `Task<IAsyncEnumerable<IStreamingMessage>>` to `IAsyncEnumerable<IStreamingMessage>`\n- Mark `RegisterReply`, `RegisterPreProcess` and `RegisterPostProcess` as obsolete. You can replace them with `RegisterMiddleware`\n\n###### Bug Fixes\n- Fix [Issue 2609](https://github.com/microsoft/autogen/issues/2609) Constructor of conversableAgentConfig does not accept LMStudioConfig as ConfigList\n\n##### Update on 0.0.12 (2024-04-22)\n- Add AutoGen.Mistral package to support Mistral.AI models\n##### Update on 0.0.11 (2024-04-10)\n- Add link to Discord channel in nuget's readme.md\n- Document improvements\n- In `AutoGen.OpenAI`, update `Azure.AI.OpenAI` to 1.0.0-beta.15 and add support for json mode and deterministic output in `OpenAIChatAgent` [Issue #2346](https://github.com/microsoft/autogen/issues/2346)\n- In `AutoGen.SemanticKernel`, update `SemanticKernel` package to 1.7.1\n- [API Breaking Change] Rename `PrintMessageMiddlewareExtension.RegisterPrintFormatMessageHook' to `PrintMessageMiddlewareExtension.RegisterPrintMessage`.\n##### Update on 0.0.10 (2024-03-12)\n- Rename `Workflow` to `Graph`\n- Rename `AddInitializeMessage` to `SendIntroduction`\n- Rename `SequentialGroupChat` to `RoundRobinGroupChat`\n##### Update on 0.0.9 (2024-03-02)\n- Refactor over @AutoGen.Message and introducing `TextMessage`, `ImageMessage`, `MultiModalMessage` and so on. PR [#1676](https://github.com/microsoft/autogen/pull/1676)\n- Add `AutoGen.SemanticKernel` to support seamless integration with Semantic Kernel\n- Move the agent contract abstraction to `AutoGen.Core` package. The `AutoGen.Core` package provides the abstraction for message type, agent and group chat and doesn't contain dependencies over `Azure.AI.OpenAI` or `Semantic Kernel`. This is useful when you want to leverage AutoGen's abstraction only and want to avoid introducing any other dependencies.\n- Move `GPTAgent`, `OpenAIChatAgent` and all openai-dependencies to `AutoGen.OpenAI`\n##### Update on 0.0.8 (2024-02-28)\n- Fix [#1804](https://github.com/microsoft/autogen/pull/1804)\n- Streaming support for IAgent [#1656](https://github.com/microsoft/autogen/pull/1656)\n- Streaming support for middleware via `MiddlewareStreamingAgent` [#1656](https://github.com/microsoft/autogen/pull/1656)\n- Graph chat support with conditional transition workflow [#1761](https://github.com/microsoft/autogen/pull/1761)\n- AutoGen.SourceGenerator: Generate `FunctionContract` from `FunctionAttribute` [#1736](https://github.com/microsoft/autogen/pull/1736)\n##### Update on 0.0.7 (2024-02-11)\n- Add `AutoGen.LMStudio` to support comsume openai-like API from LMStudio local server\n##### Update on 0.0.6 (2024-01-23)\n- Add `MiddlewareAgent`\n- Use `MiddlewareAgent` to implement existing agent hooks (RegisterPreProcess, RegisterPostProcess, RegisterReply)\n- Remove `AutoReplyAgent`, `PreProcessAgent`, `PostProcessAgent` because they are replaced by `MiddlewareAgent`\n##### Update on 0.0.5\n- Simplify `IAgent` interface by removing `ChatLLM` Property\n- Add `GenerateReplyOptions` to `IAgent.GenerateReplyAsync` which allows user to specify or override the options when generating reply\n\n##### Update on 0.0.4\n- Move out dependency of Semantic Kernel\n- Add type `IChatLLM` as connector to LLM\n\n##### Update on 0.0.3\n- In AutoGen.SourceGenerator, rename FunctionAttribution to FunctionAttribute\n- In AutoGen, refactor over ConversationAgent, UserProxyAgent, and AssistantAgent\n\n##### Update on 0.0.2\n- update Azure.OpenAI.AI to 1.0.0-beta.12\n- update Semantic kernel to 1.0.1",
    "filename": "dotnet/website/release_note/update.md"
  },
  {
    "content": "This tutorial shows how to generate response using an @AutoGen.Core.IAgent by taking @AutoGen.OpenAI.OpenAIChatAgent as an example.\n\n> [!NOTE]\n> AutoGen.Net provides the following agents to connect to different LLM platforms. Generating responses using these agents is similar to the example shown below.\n> - @AutoGen.OpenAI.OpenAIChatAgent\n> - @AutoGen.SemanticKernel.SemanticKernelAgent\n> - @AutoGen.LMStudio.LMStudioAgent\n> - @AutoGen.Mistral.MistralClientAgent\n> - @AutoGen.Anthropic.AnthropicClientAgent\n> - @AutoGen.Ollama.OllamaAgent\n> - @AutoGen.Gemini.GeminiChatAgent\n\n> [!NOTE]\n> The complete code example can be found in [Chat_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs)\n\n## Step 1: Install AutoGen\n\nFirst, install the AutoGen package using the following command:\n\n```bash\ndotnet add package AutoGen\n```\n\n## Step 2: add Using Statements\n\n[!code-csharp[Using Statements](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Using)]\n\n## Step 3: Create an @AutoGen.OpenAI.OpenAIChatAgent\n\n> [!NOTE]\n> The @AutoGen.OpenAI.Extension.OpenAIAgentExtension.RegisterMessageConnector* method registers an @AutoGen.OpenAI.OpenAIChatRequestMessageConnector middleware which converts OpenAI message types to AutoGen message types. This step is necessary when you want to use AutoGen built-in message types like @AutoGen.Core.TextMessage, @AutoGen.Core.ImageMessage, etc.\n> For more information, see [Built-in-messages](../articles/Built-in-messages.md)\n\n[!code-csharp[Create an OpenAIChatAgent](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Create_Agent)]\n\n## Step 4: Generate Response\nTo generate response, you can use one of the overloaded method of @AutoGen.Core.AgentExtension.SendAsync* method. The following code shows how to generate response with text message:\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Chat_With_Agent)]\n\nTo generate response with chat history, you can pass the chat history to the @AutoGen.Core.AgentExtension.SendAsync* method:\n\n[!code-csharp[Generate Response with Chat History](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Chat_With_History)]\n\nTo streamingly generate response, use @AutoGen.Core.IStreamingAgent.GenerateStreamingReplyAsync*\n\n[!code-csharp[Generate Streaming Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Chat_With_Agent.cs?name=Streaming_Chat)]\n\n## Further Reading\n- [Chat with google gemini](../articles/AutoGen.Gemini/Chat-with-google-gemini.md)\n- [Chat with vertex gemini](../articles/AutoGen.Gemini/Chat-with-vertex-gemini.md)\n- [Chat with Ollama](../articles/AutoGen.Ollama/Chat-with-llama.md)\n- [Chat with Semantic Kernel Agent](../articles/AutoGen.SemanticKernel/SemanticKernelAgent-simple-chat.md)",
    "filename": "dotnet/website/tutorial/Chat-with-an-agent.md"
  },
  {
    "content": "This tutorial shows how to use tools in an agent.\n\n## What is tool\nTools are pre-defined functions in user's project that agent can invoke. Agent can use tools to perform actions like search web, perform calculations, etc. With tools, it can greatly extend the capabilities of an agent.\n\n> [!NOTE]\n> To use tools with agent, the backend LLM model used by the agent needs to support tool calling. Here are some of the LLM models that support tool calling as of 06/21/2024\n> - GPT-3.5-turbo with version >= 0613\n> - GPT-4 series\n> - Gemini series\n> - OPEN_MISTRAL_7B\n> - ...\n>\n> This tutorial uses the latest `GPT-3.5-turbo` as example.\n\n> [!NOTE]\n> The complete code example can be found in [Use_Tools_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs)\n\n## Key Concepts\n- @AutoGen.Core.FunctionContract: The contract of a function that agent can invoke. It contains the function name, description, parameters schema, and return type.\n- @AutoGen.Core.ToolCallMessage: A message type that represents a tool call request in AutoGen.Net.\n- @AutoGen.Core.ToolCallResultMessage: A message type that represents a tool call result in AutoGen.Net.\n- @AutoGen.Core.ToolCallAggregateMessage: An aggregate message type that represents a tool call request and its result in a single message in AutoGen.Net.\n- @AutoGen.Core.FunctionCallMiddleware: A middleware that pass the @AutoGen.Core.FunctionContract to the agent when generating response, and process the tool call response when receiving a @AutoGen.Core.ToolCallMessage.\n\n> [!Tip]\n> You can Use AutoGen.SourceGenerator to automatically generate type-safe @AutoGen.Core.FunctionContract instead of manually defining them. For more information, please check out [Create type-safe function](../articles/Create-type-safe-function-call.md).\n\n## Install AutoGen and AutoGen.SourceGenerator\nFirst, install the AutoGen and AutoGen.SourceGenerator package using the following command:\n\n```bash\ndotnet add package AutoGen\ndotnet add package AutoGen.SourceGenerator\n```\n\nAlso, you might need to enable structural xml document support by setting `GenerateDocumentationFile` property to true in your project file. This allows source generator to leverage the documentation of the function when generating the function definition.\n\n```xml\n<PropertyGroup>\n    <!-- This enables structural xml document support -->\n    <GenerateDocumentationFile>true</GenerateDocumentationFile>\n</PropertyGroup>\n```\n\n## Add Using Statements\n\n[!code-csharp[Using Statements](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Using)]\n\n## Create agent\n\nCreate an @AutoGen.OpenAI.OpenAIChatAgent with `GPT-3.5-turbo` as the backend LLM model.\n\n[!code-csharp[Create an agent with tools](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Create_Agent)]\n\n## Define `Tool` class and create tools\nCreate a `public partial` class to host the tools you want to use in AutoGen agents. The method has to be a `public` instance method and its return type must be `Task<string>`. After the methods is defined, mark them with @AutoGen.Core.FunctionAttribute attribute.\n\nIn the following example, we define a `GetWeather` tool that returns the weather information of a city.\n\n[!code-csharp[Define Tool class](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Tools)]\n[!code-csharp[Create tools](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Create_tools)]\n\n## Tool call without auto-invoke\nIn this case, when receiving a @AutoGen.Core.ToolCallMessage, the agent will not automatically invoke the tool. Instead, the agent will return the original message back to the user. The user can then decide whether to invoke the tool or not.\n\n![single-turn tool call without auto-invoke](../images/articles/CreateAgentWithTools/single-turn-tool-call-without-auto-invoke.png)\n\nTo implement this, you can create the @AutoGen.Core.FunctionCallMiddleware without passing the `functionMap` parameter to the constructor so that the middleware will not automatically invoke the tool once it receives a @AutoGen.Core.ToolCallMessage from its inner agent.\n\n[!code-csharp[Single-turn tool call without auto-invoke](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Create_no_invoke_middleware)]\n\nAfter creating the function call middleware, you can register it to the agent using `RegisterMiddleware` method, which will return a new agent which can use the methods defined in the `Tool` class.\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Single_Turn_No_Invoke)]\n\n## Tool call with auto-invoke\nIn this case, the agent will automatically invoke the tool when receiving a @AutoGen.Core.ToolCallMessage and return the @AutoGen.Core.ToolCallAggregateMessage which contains both the tool call request and the tool call result.\n\n![single-turn tool call with auto-invoke](../images/articles/CreateAgentWithTools/single-turn-tool-call-with-auto-invoke.png)\n\nTo implement this, you can create the @AutoGen.Core.FunctionCallMiddleware with the `functionMap` parameter so that the middleware will automatically invoke the tool once it receives a @AutoGen.Core.ToolCallMessage from its inner agent.\n\n[!code-csharp[Single-turn tool call with auto-invoke](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Create_auto_invoke_middleware)]\n\nAfter creating the function call middleware, you can register it to the agent using `RegisterMiddleware` method, which will return a new agent which can use the methods defined in the `Tool` class.\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Single_Turn_Auto_Invoke)]\n\n## Send the tool call result back to LLM to generate further response\nIn some cases, you may want to send the tool call result back to the LLM to generate further response. To do this, you can send the tool call response from agent back to the LLM by calling the `SendAsync` method of the agent.\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=Multi_Turn_Tool_Call)]\n\n## Parallel tool call\nSome LLM models support parallel tool call, which returns multiple tool calls in one single message. Note that @AutoGen.Core.FunctionCallMiddleware has already handled the parallel tool call for you. When it receives a @AutoGen.Core.ToolCallMessage that contains multiple tool calls, it will automatically invoke all the tools in the sequantial order and return the @AutoGen.Core.ToolCallAggregateMessage which contains all the tool call requests and results.\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Use_Tools_With_Agent.cs?name=parallel_tool_call)]\n\n## Further Reading\n- [Function call with openai](../articles/OpenAIChatAgent-use-function-call.md)\n- [Function call with gemini](../articles/AutoGen.Gemini/Function-call-with-gemini.md)\n- [Function call with local model](../articles/Function-call-with-ollama-and-litellm.md)\n- [Use kernel plugin in other agents](../articles/AutoGen.SemanticKernel/Use-kernel-plugin-in-other-agents.md)\n- [function call in mistral](../articles/MistralChatAgent-use-function-call.md)",
    "filename": "dotnet/website/tutorial/Create-agent-with-tools.md"
  },
  {
    "content": "This tutorial shows how to perform image chat with an agent using the @AutoGen.OpenAI.OpenAIChatAgent as an example.\n\n> [!NOTE]\n> To chat image with an agent, the model behind the agent needs to support image input. Here is a partial list of models that support image input:\n> - gpt-4o\n> - gemini-1.5\n> - llava\n> - claude-3\n> - ...\n>\n> In this example, we are using the gpt-4o model as the backend model for the agent.\n\n> [!NOTE]\n> The complete code example can be found in [Image_Chat_With_Agent.cs](https://github.com/microsoft/autogen/blob/main/dotnet/samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs)\n\n## Step 1: Install AutoGen\n\nFirst, install the AutoGen package using the following command:\n\n```bash\ndotnet add package AutoGen\n```\n\n## Step 2: Add Using Statements\n\n[!code-csharp[Using Statements](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Using)]\n\n## Step 3: Create an @AutoGen.OpenAI.OpenAIChatAgent\n\n[!code-csharp[Create an OpenAIChatAgent](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Create_Agent)]\n\n## Step 4: Prepare Image Message\n\nIn AutoGen, you can create an image message using either @AutoGen.Core.ImageMessage or @AutoGen.Core.MultiModalMessage. The @AutoGen.Core.ImageMessage takes a single image as input, whereas the @AutoGen.Core.MultiModalMessage allows you to pass multiple modalities like text or image.\n\nHere is how to create an image message using @AutoGen.Core.ImageMessage:\n[!code-csharp[Create Image Message](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Image_Input)]\n\nHere is how to create a multimodal message using @AutoGen.Core.MultiModalMessage:\n[!code-csharp[Create MultiModal Message](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Prepare_Multimodal_Input)]\n\n## Step 5: Generate Response\n\nTo generate response, you can use one of the overloaded methods of @AutoGen.Core.AgentExtension.SendAsync* method. The following code shows how to generate response with an image message:\n\n[!code-csharp[Generate Response](../../samples/AgentChat/Autogen.Basic.Sample/GettingStart/Image_Chat_With_Agent.cs?name=Chat_With_Agent)]\n\n## Further Reading\n- [Image chat with gemini](../articles/AutoGen.Gemini/Image-chat-with-gemini.md)\n- [Image chat with llava](../articles/AutoGen.Ollama/Chat-with-llava.md)",
    "filename": "dotnet/website/tutorial/Image-chat-with-agent.md"
  },
  {
    "content": "This tutorial shows how to use AutoGen.Net agent as model in AG Studio\n\n## Step 1. Create Dotnet empty web app and install AutoGen and AutoGen.WebAPI package\n\n```bash\ndotnet new web\ndotnet add package AutoGen\ndotnet add package AutoGen.WebAPI\n```\n\n## Step 2. Replace the Program.cs with following code\n\n```bash\nusing AutoGen.Core;\nusing AutoGen.Service;\n\nvar builder = WebApplication.CreateBuilder(args);\nvar app = builder.Build();\n\nvar helloWorldAgent = new HelloWorldAgent();\napp.UseAgentAsOpenAIChatCompletionEndpoint(helloWorldAgent);\n\napp.Run();\n\nclass HelloWorldAgent : IAgent\n{\n    public string Name => \"HelloWorld\";\n\n    public Task<IMessage> GenerateReplyAsync(IEnumerable<IMessage> messages, GenerateReplyOptions? options = null, CancellationToken cancellationToken = default)\n    {\n        return Task.FromResult<IMessage>(new TextMessage(Role.Assistant, \"Hello World!\", from: this.Name));\n    }\n}\n```\n\n## Step 3: Start the web app\n\nRun the following command to start web api\n\n```bash\ndotnet RUN\n```\n\nThe web api will listen at `http://localhost:5264/v1/chat/completion\n\n![terminal](../images/articles/UseAutoGenAsModelinAGStudio/Terminal.png)\n\n## Step 4: In another terminal, start autogen-studio\n\n```bash\nautogenstudio ui\n```\n\n## Step 5: Navigate to AutoGen Studio UI and add hello world agent as openai Model\n\n### Step 5.1: Go to model tab\n\n![The Model Tab](../images/articles/UseAutoGenAsModelinAGStudio/TheModelTab.png)\n\n### Step 5.2: Select \"OpenAI model\" card\n\n![Open AI model Card](../images/articles/UseAutoGenAsModelinAGStudio/Step5.2OpenAIModel.png)\n\n### Step 5.3: Fill the model name and url\n\nThe model name needs to be same with agent name\n\n![Fill the model name and url](../images/articles/UseAutoGenAsModelinAGStudio/Step5.3ModelNameAndURL.png)\n\n## Step 6: Create a hello world agent that uses the hello world model\n\n![Create a hello world agent that uses the hello world model](../images/articles/UseAutoGenAsModelinAGStudio/Step6.png)\n\n![Agent Configuration](../images/articles/UseAutoGenAsModelinAGStudio/Step6b.png)\n\n## Final Step: Use the hello world agent in workflow\n\n![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsA.png)\n\n![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsA.png)\n\n![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsB.png)\n\n![Use the hello world agent in workflow](../images/articles/UseAutoGenAsModelinAGStudio/FinalStepsC.png)",
    "filename": "dotnet/website/tutorial/Use-AutoGen.Net-agent-as-model-in-AG-Studio.md"
  },
  {
    "content": "# AutoGen Python Development Guide\n\n[![Docs (dev)](https://img.shields.io/badge/Docs-dev-blue)](https://microsoft.github.io/autogen/dev/)\n[![Docs (latest release)](https://img.shields.io/badge/Docs-latest%20release-blue)](https://microsoft.github.io/autogen/dev/)\n[![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/)\n\nThis directory works as a single `uv` workspace containing all project packages, including:\n\n- `packages/autogen-core`: interface definitions and reference implementations of agent runtime, model, tool, workbench, memory, tracing.\n- `packages/autogen-agentchat`: single and multi-agent workflows built on top of `autogen-core`.\n- `packages/autogen-ext`: implementations for ecosystem integrations. For example, `autogen-ext[openai]` provides the OpenAI model client.\n- `packages/autogen-studio`: a web-based IDE for building and running AutoGen agents.\n\n## Migrating from 0.2.x?\n\nPlease refer to the [migration guide](./migration_guide.md) for how to migrate your code from 0.2.x to 0.4.x.\n\n## Quick Start\n\n**TL;DR**, run all checks with:\n\n```sh\nuv sync --all-extras\nsource .venv/bin/activate\npoe check\n```\n\n## Setup\n\n`uv` is a package manager that assists in creating the necessary environment and installing packages to run AutoGen.\n\n- [Install `uv`](https://docs.astral.sh/uv/getting-started/installation/).\n\nTo upgrade `uv` to the latest version, run:\n\n```sh\nuv self update\n```\n\n## Virtual Environment\n\nDuring development, you may need to test changes made to any of the packages.\\\nTo do so, create a virtual environment where the AutoGen packages are installed based on the current state of the directory.\\\nRun the following commands at the root level of the Python directory:\n\n```sh\nuv sync --all-extras\nsource .venv/bin/activate\n```\n\n- `uv sync --all-extras` will create a `.venv` directory at the current level and install packages from the current directory along with any other dependencies. The `all-extras` flag adds optional dependencies.\n- `source .venv/bin/activate` activates the virtual environment.\n\n## Common Tasks\n\nTo create a pull request (PR), ensure the following checks are met. You can run each check individually:\n\n- Format: `poe format`\n- Lint: `poe lint`\n- Test: `poe test`\n- Mypy: `poe mypy`\n- Pyright: `poe pyright`\n- Build docs: `poe docs-build`\n- Check docs: `poe docs-check`\n- Clean docs: `poe docs-clean`\n- Check code blocks in API references: `poe docs-check-examples`\n- Auto rebuild+serve docs: `poe docs-serve`\n- Check samples in `python/samples`: `poe samples-code-check`\n  Alternatively, you can run all the checks with:\n- `poe check`\n\n> [!NOTE]\n> These need to be run in the virtual environment.\n\n## Syncing Dependencies\n\nWhen you pull new changes, you may need to update the dependencies.\nTo do so, first make sure you are in the virtual environment, and then in the `python` directory, run:\n\n```sh\nuv sync --all-extras\n```\n\nThis will update the dependencies in the virtual environment.\n\n## Building Documentation\n\nThe documentation source directory is located at `docs/src/`.\n\nTo build the documentation, run this from the root of the Python directory:\n\n```sh\npoe docs-build\n```\n\nTo serve the documentation locally, run:\n\n```sh\npoe docs-serve\n```\n\nWhen you make changes to the doc strings or add new modules, you may need to\nrefresh the API references in the documentation by first cleaning the docs and\nthen building them again:\n\n```sh\npoe docs-clean # This will remove the build directory and the reference directory\npoe docs-build # This will rebuild the documentation from scratch\n```\n\n## Writing Documentation\n\nWhen you add a new public class or function, you should always add a docstring\nto it. The docstring should follow the\n[Google style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) layout\nand the Sphinx RST format for Python docstrings.\n\nThe docstring for a public class or function should include:\n\n- A short description of the class or function at the beginning immediately after the `\"\"\"`.\n- A longer description if necessary, explaining the purpose and usage.\n- A list of arguments with their types and descriptions, using the `Args` section.\n  Each argument should be listed with its name, type, and a brief description.\n- A description of the return value and its type, using the `Returns` section.\n  If the function does not return anything, you can omit this section.\n- A list of exceptions that the function may raise, with descriptions,\n  using the `Raises` section. This is optional but recommended if the function can raise exceptions that users should be aware of.\n- Examples of how to use the class or function, using the `Examples` section,\n  and formatted using `.. code-block:: python` directive. Optionally, also include the output of the example using\n  `.. code-block:: text` directive.\n\nHere is an example of a docstring for `McpWorkbench` class:\n\n```python\nclass McpWorkbench(Workbench, Component[McpWorkbenchConfig]):\n    \"\"\"A workbench that wraps an MCP server and provides an interface\n    to list and call tools provided by the server.\n\n    This workbench should be used as a context manager to ensure proper\n    initialization and cleanup of the underlying MCP session.\n\n    Args:\n        server_params (McpServerParams): The parameters to connect to the MCP server.\n            This can be either a :class:`StdioServerParams` or :class:`SseServerParams`.\n        tool_overrides (Optional[Dict[str, ToolOverride]]): Optional mapping of original tool\n            names to override configurations for name and/or description. This allows\n            customizing how server tools appear to consumers while maintaining the underlying\n            tool functionality.\n\n    Raises:\n        ValueError: If there are conflicts in tool override names.\n\n    Examples:\n\n        Here is a simple example of how to use the workbench with a `mcp-server-fetch` server:\n\n        .. code-block:: python\n\n            import asyncio\n\n            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n\n            async def main() -> None:\n                params = StdioServerParams(\n                    command=\"uvx\",\n                    args=[\"mcp-server-fetch\"],\n                    read_timeout_seconds=60,\n                )\n\n                # You can also use `start()` and `stop()` to manage the session.\n                async with McpWorkbench(server_params=params) as workbench:\n                    tools = await workbench.list_tools()\n                    print(tools)\n                    result = await workbench.call_tool(tools[0][\"name\"], {\"url\": \"https://github.com/\"})\n                    print(result)\n\n\n            asyncio.run(main())\n```\n\nThe code blocks with `.. code-block:: python` is checked by the `docs-check-examples` task using Pyright,\nso make sure the code is valid. Running the code as a script and checking it using `pyright`\nis a good way to ensure the code examples are correct.\n\nWhen you reference a class, method, or function in the docstring, you should always\nuse the `:class:`, `:meth:`, or `:func:` directive to create a link to the class or function.\nAlways use the fully qualified name of the class or function, including the package name, but\nprefix it with a `~` for shorter rendering in the documentation.\nFor example, if you are referencing the `AssistantAgent` class in the `autogen-agentchat` package,\nyou should write it as `:class:~autogen_agentchat.AssistantAgent`.\n\nFor a public data class, including those that are Pydantic models, you should also include docstrings\nfor each field in the class.\n\n## Writing Tests\n\nWhen you add a new public class or function, you should also always add tests for it.\nWe track test coverage and aim for not reducing the coverage percentage with new changes.\n\nWe use `pytest` for testing, and you should always use fixtures to set up the test dependencies.\n\nUse mock objects to simulate dependencies and avoid making real API calls or database queries in tests.\nSee existing tests for examples of how to use fixtures and mocks.\n\nFor model clients, use `autogen_ext.models.replay.ReplayChatCompletionClient` as a\ndrop-in replacement for the model client to simulate responses without making real API calls.\n\nWhen certain tests requires interaction with actual model APIs or other external services,\nyou should configure the tests to be skipped if the required services are not available.\nFor example, if you are testing a model client that requires an OpenAI API key,\nyou can use the `pytest.mark.skipif` decorator to skip the test if the environment variable for the API key is not set.\n\n## Creating a New Package\n\nTo create a new package, similar to `autogen-core` or `autogen-chat`, use the following:\n\n```sh\nuv sync --python 3.12\nsource .venv/bin/activate\ncookiecutter ./templates/new-package/\n```",
    "filename": "python/README.md"
  },
  {
    "code": false,
    "content": "# Documentation: Syntax Checker for Markdown Code Blocks\n\n## Overview\nThis script is designed to analyze Markdown files and check for syntax errors specifically in Python code blocks. By leveraging the Pygments library for syntax highlighting and the `pyright` tool for checking Python code, it provides a logging mechanism to report findings to the user. This functionality is useful for developers who frequently document their code in Markdown and want to ensure the integrity of the code blocks included.\n\n## Imports\n- **argparse**: Used for parsing command-line arguments.\n- **logging**: Facilitates detailed logging, which helps track the script's operations and error reporting.\n- **tempfile**: Creates temporary files to store the extracted code blocks for syntax checking.\n- **typing**: Provides type hints to enhance readability and maintainability of the code.\n- **pygments**: A syntax highlighting library that formats the extracted code for better readability in logs.\n- **sphinx.util.console**: Offers colored text output for better visual cues in terminal logs.\n\n## Logger Initialization\nThe logger is set up to log messages at the INFO level. A stream handler is attached, which outputs messages directly to the console, ensuring visibility of the status and any syntax errors encountered during execution.\n\n## Function: `extract_python_code_blocks`\n```python\ndef extract_python_code_blocks(markdown_file_path: str) -> List[Tuple[str, int]]:\n```\n### Purpose\nThis function is responsible for extracting Python code blocks from a given Markdown file.\n\n### Role\n- It opens the Markdown file and reads its content line-by-line.\n- It identifies lines that start with ```python and stores the corresponding blocks as tuples of code and their initial line numbers.\n- It returns a list of those tuples, facilitating further checks for syntax errors in the next steps.\n\n### Implementation Details\n- The method toggles a flag (`in_code_block`) to track whether the current line is within a Python code block.\n- It accumulates lines into `current_block` until it encounters the closing marker ```.\n  \n## Function: `check_code_blocks`\n```python\ndef check_code_blocks(markdown_file_paths: List[str]) -> None:\n```\n### Purpose\nThis function checks the extracted Python code blocks from the specified Markdown files for syntax errors using the `pyright` static type checker.\n\n### Role\n- For each Markdown file path provided, it invokes the `extract_python_code_blocks` to retrieve the embedded code blocks.\n- It processes each code block by temporarily storing it in a file and running `pyright` on it.\n- If errors are found, it logs the error details along with the highlighted code block for user clarity.\n\n### Error Handling\n- If any Markdown file contains code blocks with syntax errors, a `RuntimeError` is raised, notifying the user of the specific files that have issues.\n\n### Logic Flow\n- It first checks if any code blocks contain specific imports related to `autogen_agentchat`, `autogen_core`, or `autogen_ext` and ignores those blocks.\n- The function uses `subprocess.run` to execute the `pyright` command and captures output, processing both success and error messages accordingly.\n\n## Main Execution Block\n```python\nif __name__ == \"__main__\":\n```\n### Purpose\nThis portion of the script is executed when the file is run as a standalone program.\n\n### Role\n- It sets up an argument parser that allows users to specify one or more Markdown files (including glob patterns).\n- The provided Markdown file paths are then passed to the `check_code_blocks` function for processing.\n\n## Application\nThe utility of this script is apparent for developers and documentation writers who maintain Markdown files with Python code. By running this tool, users can be assured that their documented code samples are free of syntax errors, enhancing the overall quality of their documentation.\n\n## Usage\nTo utilize this script, run it from the command line, passing the desired Markdown files as arguments. Example:\n```bash\npython script.py *.md\n``` \n\nThis command will check all Markdown files in the current directory, reporting any syntax errors found in the contained Python code blocks.",
    "filename": "python/check_md_code_blocks.py"
  },
  {
    "content": "## Building the AutoGen Documentation\n\nAutoGen documentation is based on the sphinx documentation system and uses the myst-parser to render markdown files. It uses the [pydata-sphinx-theme](https://pydata-sphinx-theme.readthedocs.io/en/latest/) to style the documentation.\n\n### Prerequisites\n\nEnsure you have all of the dev dependencies for the `autogen-core` package installed. You can install them by running the following command from the root of the python repository:\n\n```bash\nuv sync\nsource .venv/bin/activate\n```\n\n## Building Docs\n\nTo build the documentation, run the following command from the root of the python directory:\n\n```bash\npoe docs-build\n```\n\nTo serve the documentation locally, run the following command from the root of the python directory:\n\n```bash\npoe docs-serve\n```\n\n[!NOTE]\nSphinx will only rebuild files that have changed since the last build. If you want to force a full rebuild, you can delete the `./docs/build` directory before running the `docs-build` command.",
    "filename": "python/docs/README.md"
  },
  {
    "code": false,
    "content": "# Redirects Script Documentation\n\nThis document provides a high-level overview of a Python script designed to generate HTML redirect pages based on provided URL mappings. The script utilizes a template file for generating HTML content and reads URL mappings from a separate text file.\n\n## Overview\n\nThe script's primary function is to read a list of old-to-new URL mappings and generate redirect pages for each entry. It utilizes the `pathlib` module for file operations and the `string.Template` class for creating HTML content. The script expects a command-line argument specifying a base directory where the redirect pages will be generated.\n\n## Dependencies\n\nThe script imports the following modules:\n\n- **`pathlib`**: Used for handling filesystem paths.\n- **`string`**: Utilizes the `Template` class for processing text templates.\n- **`sys`**: Provides access to command-line arguments.\n\n## Constants\n\n- **`THIS_FILE_DIR`**: Stores the directory of the current script.\n- **`HTML_PAGE_TEMPLATE_FILE`**: Path to the HTML template file used for redirects.\n- **`HTML_REDIRECT_TEMPLATE`**: Contains the contents of the redirect HTML template.\n- **`REDIRECT_URLS_FILE`**: Path to the file that contains the old-to-new URL mappings.\n\n## Function: `generate_redirect`\n\n### Purpose\n\nThe `generate_redirect` function creates an HTML file that redirects from an old URL to a new URL. It ensures that the proper file structure is set up before writing the redirect.\n\n### Parameters\n\n- **`file_to_write (str)`**: The relative path where the redirect page will be created.\n- **`new_url (str)`**: The destination URL to which visitors will be redirected.\n- **`base_dir (Path)`**: The base directory where the redirect pages will be stored.\n\n### Functionality\n\n1. **Template Substitution**: Uses the `Template` class to substitute `$to_url` in the HTML template with the provided `new_url`.\n2. **File Path Preparation**: Ensures that the path for the redirect page ends with `index.html` and is constructed properly.\n3. **Directory Creation**: Utilizes the `mkdir` method to create any necessary directories if they do not already exist.\n4. **File Writing**: Writes the generated HTML redirect content to the specified file path.\n\n## Function: `main`\n\n### Purpose\n\nThe `main` function serves as the entry point for the script, handling command-line arguments, reading the URL mappings, and invoking the redirect generation process.\n\n### Functionality\n\n1. **Argument Check**: Validates that exactly one command-line argument is provided, which is the base directory for output files.\n2. **Directory Setup**: Converts the base directory argument into a `Path` object.\n3. **Reading Redirect Mappings**: Opens the `redirect_urls.txt` file and reads all lines.\n4. **Processing Lines**: Iterates through each line:\n   - Splits the line into old and new URL components.\n   - Replaces the base path `/autogen/` with `/` in the old URL for generating the file path.\n   - Calls `generate_redirect` to create the corresponding redirect page.\n\n## Command-Line Usage\n\nTo execute the script, use the following command structure:\n\n```bash\npython redirects.py <base_dir>\n```\n\n- `<base_dir>`: The path to the directory where the HTML redirect files will be created.\n\n## Conclusion\n\nThis script automates the generation of HTML redirects based on predefined mappings. By separating concerns into functions and utilizing templates, it allows for clean code organization and ease of maintenance. The use of command-line arguments makes it flexible for different directory structures, making it a valuable tool for web developers handling URL redirects.",
    "filename": "python/docs/redirects/redirects.py"
  },
  {
    "code": false,
    "content": "# Code Linter Documentation\n\n## Overview\nThis code defines a Sphinx extension that acts as a code linter for Python code blocks contained within documentation. The `CodeLinter` class inherits from Sphinx's `Builder` and takes advantage of the `pyright` tool to analyze the code for potential errors. The key functionalities include identifying code blocks, checking their validity, and reporting any found issues in a structured format.\n\n## Dependencies\nThe code relies on several external libraries and modules:\n- `docutils`: Provides node structures for Sphinx documents.\n- `sphinx`: A documentation generator that supports extensions.\n- `pygments`: Used for syntax highlighting of code snippets.\n- `tempfile`: For creating temporary files to hold code snippets.\n- `subprocess`: To run external command line tools (in this case, `pyright`) for linting.\n\n## Class: CodeLinter\n### Description\nThe `CodeLinter` class is the core component of this extension. It processes Sphinx documents to lint Python code blocks using the `pyright` static type checker. \n\n### Attributes\n- `name`: Specifies the builder's name as \"code_lint\".\n- `allow_parallel`: Indicates that the builder can run in parallel with other operations.\n\n### Methods\n#### init\n```python\ndef init(self) -> None:\n```\nThe `init` method initializes any necessary attributes for the builder. In this case, it sets `_had_errors` to `False`, which will track if any linting issues are found during the document processing.\n\n#### get_outdated_docs\n```python\ndef get_outdated_docs(self) -> str | Iterable[str]:\n```\nThis method determines which documents are outdated and need rebuilding. It returns the found document names from the Sphinx environment.\n\n#### get_target_uri\n```python\ndef get_target_uri(self, docname: str, typ: str | None = None) -> str:\n```\nThis method generates a target URI for a given document name, although in this implementation, it always returns an empty string.\n\n#### prepare_writing\n```python\ndef prepare_writing(self, docnames: AbstractSet[str]) -> None:\n```\nThis method is a placeholder intended to run any setup before the writing process begins. In this case, it currently does nothing.\n\n#### write_doc\n```python\ndef write_doc(self, docname: str, doctree: nodes.Node) -> None:\n```\nThe `write_doc` method handles the core logic of linting. It:\n1. Checks if the document name starts with the specified `path_prefix`.\n2. Finds all `literal_block` nodes in the document.\n3. For supported languages (e.g., Python), it checks for an \"ignore\" class.\n4. If not ignored, it creates a temporary file, writes the code block into it, and runs `pyright` on that file.\n5. If `pyright` returns an error, it logs the error details and the corresponding code snippet for debugging.\n\n#### finish\n```python\ndef finish(self) -> None:\n```\nThis method wraps up the processing. If any linting errors were encountered, it raises a `RuntimeError` to alert the user.\n\n## Function: setup\n```python\ndef setup(app: Sphinx) -> dict[str, Any]:\n```\nThe `setup` function is called when the Sphinx application starts. It:\n1. Registers the `CodeLinter` as a new builder.\n2. Adds a configuration value for the `code_lint_path_prefix`, which determines which documents should be linted.\n\nThis function returns a dictionary containing the version of the extension and flags indicating that it is safe for parallel operations.\n\n## Summary\nIn conclusion, this code provides a comprehensive structure for a Sphinx extension that lints Python code blocks within documentation. By leveraging `pyright`, it aims to improve the quality and reliability of code examples included in documentation. The logging and error reporting features facilitate easy debugging and error tracking throughout the build process.",
    "filename": "python/docs/src/_extension/code_lint.py"
  },
  {
    "code": false,
    "content": "# Image Gallery Directive Documentation\n\nThis documentation provides a high-level overview of a Sphinx directive designed to generate a gallery of images from structured data defined in YAML format. This directive simplifies the process of creating image galleries within documentation by encapsulating the functionality into a single directive that leverages YAML configuration.\n\n## Introduction\n\nGenerating standardized image galleries is a common requirement in documentation. The `GalleryGridDirective` addresses the challenges of doing this programmatically, allowing developers to specify their gallery structure through a YAML file or directly within the directive. This directive is particularly beneficial for maintainers of the `pydata-sphinx-theme` but may also be useful across other contexts. If this proves successful, the functionality may be abstracted into a standalone package.\n\n## Imports and Dependencies\n\nThe directive relies on several libraries and modules:\n\n- **Path from pathlib**: Used for file path manipulations.\n- **Any, ClassVar, Dict, List from typing**: Enables type hinting for improved code clarity.\n- **Nodes, directives from docutils**: Import nodes for document tree manipulation and directive definitions.\n- **Sphinx**: The application context for Sphinx extensions.\n- **Logging from Sphinx**: Used to log information and errors.\n- **safe_load from yaml**: Parses YAML content safely into Python data structures.\n\n## Template Definitions\n\nTwo templates, `TEMPLATE_GRID` and `GRID_CARD`, define the structure for the gallery and individual items:\n\n- **TEMPLATE_GRID**: This template formats a grid layout, which can have flexible columns.\n- **GRID_CARD**: This template specifies the structure for individual grid items, allowing customization through parameters.\n\nThese templates leverage a Markdown flavor for formatting, which is crucial for rendering content appropriately within the document.\n\n## GalleryGridDirective Class\n\n### Purpose\n\nThe `GalleryGridDirective` inherits from `SphinxDirective` and is responsible for creating an image gallery that can be rendered in a Bootstrap-style grid.\n\n### Attributes\n\n- **name**: Sets the directive name as \"gallery-grid\".\n- **has_content**: Indicates that the directive can contain additional content.\n- **required_arguments**: Specifies that no arguments are required.\n- **optional_arguments**: Defines that one optional argument (the YAML file path) may be specified.\n- **option_spec**: Defines various options, like `grid-columns`, `class-container`, and `class-card` that customize the gallery's appearance.\n\n### Method: run\n\nThe `run` method executes the core functionality of the directive:\n\n1. **Argument Handling**: If an argument is supplied, it is treated as a path to a YAML file. The method resolves this path and reads its content. If not found, it logs an error.\n  \n2. **YAML Parsing**: If no YAML file is provided, the method uses content supplied within the directive's definition.\n\n3. **Card Generation**: For each item in the YAML data, it generates a card using the `GRID_CARD` template, populating it with values for title, header, image, and other custom options. Each card can have an additional CSS class applied if specified.\n\n4. **Grid Template Preparation**: It creates a grid directive from the card contents and applies options like column count and container classes.\n\n5. **Content Parsing**: Finally, it uses Sphinx's parsing capabilities to process the grid directive and return the constructed content.\n\n## Setup Function\n\n### Purpose\n\nThe `setup` function initializes the extension within the Sphinx application.\n\n### Parameters\n\n- **app**: The Sphinx application instance to which the directive is added.\n\n### Returns\n\nIt returns a dictionary indicating the directive's support for parallel reading and writing. This ensures that the directive is safe to use in multi-threaded contexts.\n\n## Usage and Restrictions\n\n### Usage\n\nTo use this directive, add the following to your document:\n\n```rst\n.. gallery-grid:: path/to/config.yaml\n   :grid-columns: 3\n   :class-container: custom-container\n```\n\nThis statement reads a YAML configuration and generates a gallery grid accordingly.\n\n### Restrictions\n\nA notable restriction is that this directive is intended for use within MyST (Markdown for Sphinx) documentation pages. Since it employs Markdown formatting, it is crucial to ensure the supporting framework is in place.\n\n## Conclusion\n\nIn summary, the `GalleryGridDirective` facilitates the creation of organized image galleries within Sphinx documentation. By leveraging YAML configuration, it simplifies the process of maintaining visual content, enhancing both usability and aesthetic consistency across documentation projects. Future enhancements may include making it more adaptable for broader use cases beyond the pydata-sphinx-theme.",
    "filename": "python/docs/src/_extension/gallery_directive.py"
  },
  {
    "code": false,
    "content": "# Sphinx Configuration Documentation\n\nThis document provides an overview of a Sphinx configuration file that sets up documentation for the `autogen_core` project. The configuration defines project metadata, general settings, HTML output options, and custom behaviors through functions.\n\n## Project Information\n\nThe initial section of the configuration specifies essential project metadata:\n\n```python\nproject = \"autogen_core\"\ncopyright = \"2024, Microsoft\"\nauthor = \"Microsoft\"\nversion = \"0.4\"\n```\n\n- **project**: This variable holds the name of the project, which is `autogen_core`.\n- **copyright**: Indicates the copyright holder and year.\n- **author**: Specifies the author of the project.\n- **version**: Denotes the version number of the project, which is set to `0.4`.\n\nThe release of the documentation can be overridden by an environment variable `SPHINX_RELEASE_OVERRIDE`. If it is not set, the release version is taken from `autogen_core.__version__`.\n\n## General Configuration \n\nThis section includes the general configuration necessary for the Sphinx builder:\n\n```python\nextensions = [\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n    ...\n]\nsuppress_warnings = [\"myst.header\"]\n```\n\n- **extensions**: A list of Sphinx extensions being used to enhance the documentation, such as `autodoc` for auto-generating documentation from docstrings and `napoleon` for Google-style docstrings support.\n- **suppress_warnings**: A list that contains warnings Sphinx should ignore; in this case, it suppresses header warnings from MyST markdown.\n\n## Path and Templates\n\nThe configuration continues by setting paths for templates and static files:\n\n```python\ntemplates_path = [\"_templates\"]\nhtml_static_path = [\"_static\"]\n```\n\n- **templates_path**: The directory where custom templates are stored for rendering.\n- **html_static_path**: Paths where static files, such as CSS and images, are stored.\n\n## HTML Output Options\n\nThe configuration specifies various settings related to the HTML output generated by Sphinx:\n\n```python\nhtml_theme = \"pydata_sphinx_theme\"\nhtml_logo = \"_static/images/logo/logo.svg\"\nhtml_favicon = \"_static/images/logo/favicon-512x512.png\"\n```\n\n- **html_theme**: The theme used for the documentation, set to `pydata_sphinx_theme`.\n- **html_logo**: Specifies the path of a logo used in the documentation header.\n- **html_favicon**: Path for the favicon displayed in the browser tab.\n\n### Theme Options\n\nThe `html_theme_options` dictionary configures various aspects of the theme:\n\n```python\nhtml_theme_options = {\n    \"header_links_before_dropdown\": 6,\n    ...\n}\n```\n\nThrough this dictionary, customization such as header alignment, version switchers, and additional footer elements are defined.\n\n## JavaScript and Sidebars\n\nAdditional settings include JavaScript files and sidebar configurations:\n\n```python\nhtml_js_files = [\"custom-icon.js\", \"banner-override.js\", \"custom.js\"]\nhtml_sidebars = {\n    \"packages/index\": [],\n    ...\n}\n```\n\n- **html_js_files**: Lists JavaScript files to be included in the documentation.\n- **html_sidebars**: Specifies which sidebar templates to use for specific pages in the documentation. Certain user-guide sections have designated sidebar templates.\n\n## API Reference Generation \n\nThe `generate_api_reference` function is crucial for ensuring up-to-date API documentation:\n\n```python\ndef generate_api_reference() -> None:\n    ...\n```\n\nThis function checks if a `reference` directory exists; if not, it triggers a script to generate API documentation. Any output from this process is logged, and errors are captured but do not fail the overall build.\n\n## Setup Function\n\nThe `setup` function integrates with the Sphinx application to customize the build process:\n\n```python\ndef setup(app: Sphinx) -> Dict[str, Any]:\n    ...\n```\n\n- Upon 'builder-inited', it calls `generate_api_reference()` to create updated API docs.\n- Modifies the context to add a `to_main` function that generates links pointing to the main branch of the documentation on GitHub.\n- Adds JavaScript directly to the app for user analytics.\n\n### Core and Custom Configurations\n\nThe function's return statement includes parameters that dictate Sphinx's behavior regarding parallel reading and writing. This allows certain configurations to run in parallel, optimizing the build process.\n\n## Conclusion\n\nThis Sphinx configuration file provides a robust framework for documenting the `autogen_core` project. By leveraging extensions, customizing themes, and defining data paths, it ensures a professional and user-friendly documentation experience. The inclusion of functions for API generation and context-specific utilities enhances maintainability and usability, making it easier to manage documentation across versions.",
    "filename": "python/docs/src/conf.py"
  },
  {
    "code": false,
    "content": "# API Documentation Generator for AutoGen\n\n## Overview\n\nThis Python script automates the generation of the API reference table of contents for the AutoGen project. It identifies documented Python packages, organizes their modules, and produces corresponding reStructuredText (`.rst`) files. Finally, it generates an `index.md` file that consolidates all information into a coherent API documentation structure.\n\n## Dependencies and Imports\n\nThe script relies on several standard Python libraries:\n\n- `os` and `Path` from `pathlib`: These handle file system interactions.\n- `typing`: This is used for type annotations, specifying data structures used throughout the code.\n- `re`: This module aids in pattern matching, particularly for excluding certain submodules.\n\n## Key Constants\n\n### Documented Packages\n\nThe script explicitly lists the packages to be documented:\n\n```python\nDOCUMENTED_PACKAGES = [\"autogen_core\", \"autogen_agentchat\", \"autogen_ext\"]\n```\n\n### Package Sections\n\nMappings represent different sections of the documentation:\n\n```python\nPACKAGE_SECTIONS = {\n    \"autogen_agentchat\": \"AutoGen AgentChat\",\n    \"autogen_core\": \"AutoGen Core\", \n    \"autogen_ext\": \"AutoGen Extensions\"\n}\n```\n\n### Exclusion Patterns\n\nThese patterns prevent the documentation of certain submodules that are re-exported. This is crucial for avoiding duplicate entries in the documentation.\n\n## Functionality Overview\n\n### Module Hierarchy and Filtering\n\nThe script defines several functions to traverse and filter module hierarchies:\n\n- **`find_python_packages()`**: This function scans the project\u2019s directory structure to find all relevant packages by checking for the existence of their `src` directories.\n  \n- **`get_module_hierarchy(package_root: Path)`**: This function recursively scans through a package, identifies non-private modules (those that don't start with `_`), and builds a hierarchical structure of these modules. It returns a dictionary wherein the key is the package name and the value is a set of module names.\n\n### Exclusion of Submodules\n\n- **`should_exclude_submodule(module_name: str, all_modules: Set[str])`**: This function checks whether a detected module should be excluded based on its name matching pre-defined exclusion patterns.\n\n### RST File Generation\n\n- **`clean_rst_files(reference_dir: Path)`**: It cleans the existing reStructuredText files in the specified directory to ensure a fresh start for new file generation.\n\n- **`generate_rst_files(package_roots: List[Path], reference_dir: Path) -> Set[str]`**: This crucial function generates `.rst` files for each identified module, making sure to exclude any that may lead to duplicates. The `.rst` file content is structured to allow automatic documentation generation using Sphinx or similar tools.\n\n### TOC Generation\n\n- **`generate_toctree_from_rst_files(reference_dir: Path) -> Dict[str, List[str]]`**: This function collects the generated `.rst` files and organizes them into sections based on their package. The resulting structure is utilized to create the final index.\n\n## Documentation Content Generation\n\n### Index Content Creation\n\n- **`generate_index_content(toctree_sections: Dict[str, List[str]]) -> str`**: Utilizes the TOC sections to create the final `index.md` content. It formats the document appropriately, ensuring that only modules with available documentation are included.\n\n### Main Execution Flow\n\n- **`main()`**: This primary function orchestrates the entire script. It scans for packages, generates `.rst` files, creates TOC entries, and writes the `index.md` file. The function provides various print statements for user feedback during the execution.\n\n## Summary of Results\n\nThe script concludes by printing a summary of the processed modules, including the total count for each package section. This ensures that users are informed of the documentation state.\n\n## Conclusion\n\nOverall, this script efficiently manages the generation of API documentation for the AutoGen project by automating the task of scanning, filtering, and formatting module information, resulting in a well-organized and comprehensive reference guide.",
    "filename": "python/docs/src/generate_api_reference.py"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "Top-level documentation for AutoGen, a framework for developing applications using AI agents\n"
      }
    },
    "html_theme.sidebar_secondary.remove": false,
    "sd_hide_title": true,
    "content": "<style>\n.hero-title {\n  font-size: 60px;\n  font-weight: bold;\n  margin: 2rem auto 0;\n}\n\n.wip-card {\n  border: 1px solid var(--pst-color-success);\n  background-color: var(--pst-color-success-bg);\n  border-radius: .25rem;\n  padding: 0.3rem;\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  margin-bottom: 1rem;\n}\n</style>\n\n# AutoGen\n\n<div class=\"container\">\n<div class=\"row text-center\">\n<div class=\"col-sm-12\">\n<h1 class=\"hero-title\">\nAutoGen\n</h1>\n<h3>\nA framework for building AI agents and applications\n</h3>\n</div>\n</div>\n</div>\n\n<div style=\"margin-top: 2rem;\">\n\n::::{grid}\n:gutter: 2\n\n:::{grid-item-card} {fas}`palette;pst-color-primary` Studio [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-blue?logo=pypi)](https://pypi.org/project/autogenstudio/)\n:shadow: none\n:margin: 2 0 0 0\n:columns: 12 12 12 12\n\nAn web-based UI for prototyping with agents without writing code.\nBuilt on AgentChat.\n\n```bash\npip install -U autogenstudio\nautogenstudio ui --port 8080 --appdir ./myapp\n```\n\n_Start here if you are new to AutoGen and want to prototype with agents without writing code._\n\n+++\n\n```{button-ref} user-guide/autogenstudio-user-guide/index\n:color: secondary\n\nGet Started\n```\n\n:::\n\n:::{grid-item-card}\n:shadow: none\n:margin: 2 0 0 0\n:columns: 12 12 12 12\n\n<div class=\"sd-card-title sd-font-weight-bold docutils\">\n\n{fas}`people-group;pst-color-primary` AgentChat\n[![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/)\n\n</div>\nA programming framework for building conversational single and multi-agent applications.\nBuilt on Core. Requires Python 3.10+.\n\n```python\n# pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    agent = AssistantAgent(\"assistant\", OpenAIChatCompletionClient(model=\"gpt-4o\"))\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n\nasyncio.run(main())\n```\n\n_Start here if you are prototyping with agents using Python. [Migrating from AutoGen 0.2?](./user-guide/agentchat-user-guide/migration-guide.md)._\n\n+++\n\n```{button-ref} user-guide/agentchat-user-guide/quickstart\n:color: secondary\n\nGet Started\n```\n\n:::\n\n:::{grid-item-card} {fas}`cube;pst-color-primary` Core [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/)\n:shadow: none\n:margin: 2 0 0 0\n:columns: 12 12 12 12\n\nAn event-driven programming framework for building scalable multi-agent AI systems. Example scenarios:\n\n* Deterministic and dynamic agentic workflows for business processes.\n* Research on multi-agent collaboration.\n* Distributed agents for multi-language applications.\n\n_Start here if you are getting serious about building multi-agent systems._\n\n+++\n\n```{button-ref} user-guide/core-user-guide/quickstart\n:color: secondary\n\nGet Started\n```\n\n:::\n\n:::{grid-item-card} {fas}`puzzle-piece;pst-color-primary` Extensions [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/)\n:shadow: none\n:margin: 2 0 0 0\n:columns: 12 12 12 12\n\nImplementations of Core and AgentChat components that interface with external services or other libraries.\nYou can find and use community extensions or create your own. Examples of built-in extensions:\n\n* {py:class}`~autogen_ext.tools.mcp.McpWorkbench` for using Model-Context Protocol (MCP) servers.\n* {py:class}`~autogen_ext.agents.openai.OpenAIAssistantAgent` for using Assistant API.\n* {py:class}`~autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor` for running model-generated code in a Docker container.\n* {py:class}`~autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime` for distributed agents.\n\n+++\n\n<a class=\"sd-sphinx-override sd-btn sd-text-wrap sd-btn-secondary reference internal\" href=\"user-guide/extensions-user-guide/discover.html\"><span class=\"doc\">Discover Community Extensions</span></a>\n<a class=\"sd-sphinx-override sd-btn sd-text-wrap sd-btn-secondary reference internal\" href=\"user-guide/extensions-user-guide/create-your-own.html\"><span class=\"doc\">Create New Extension</span></a>\n\n:::\n\n::::\n\n</div>\n\n```{toctree}\n:maxdepth: 3\n:hidden:\n\nuser-guide/agentchat-user-guide/index\nuser-guide/core-user-guide/index\nuser-guide/extensions-user-guide/index\nStudio <user-guide/autogenstudio-user-guide/index>\nreference/index\n```",
    "filename": "python/docs/src/index.md"
  },
  {
    "code": false,
    "content": "# Custom Agents Documentation\n\nThis document outlines how to create custom agents in the AgentChat framework. Custom agents allow for specialized behaviors that are not covered by predefined agent types. This guide covers the fundamentals of creating custom agents, illustrated through examples, as well as advanced features such as integrating custom model clients and making agents declarative.\n\n## Overview of Custom Agents\n\nCustom agents in AgentChat inherit from the `BaseChatAgent` class. To build a custom agent, you need to implement the following core abstract methods and properties:\n\n- **`on_messages(messages: Sequence[BaseChatMessage])`**: This method defines the agent's behavior in response to incoming messages. It is called when the agent is required to generate a response during execution.\n- **`on_reset()`**: This method resets the agent to its initial state.\n- **`produced_message_types`**: This property must return a list of message types that the agent can produce in its responses.\n\nAdditionally, you may optionally implement the `on_messages_stream()` method for streaming messages as they are generated.\n\n## CountDownAgent Example\n\nThe `CountDownAgent` is a simple agent that counts down from a specified number to zero. It illustrates how an agent can produce a sequence of messages dynamically.\n\n```python\nfrom typing import AsyncGenerator, List, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage\nfrom autogen_core import CancellationToken\n\nclass CountDownAgent(BaseChatAgent):\n    def __init__(self, name: str, count: int = 3):\n        super().__init__(name, \"A simple agent that counts down.\")\n        self._count = count\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        response: Response | None = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                response = message\n        assert response is not None\n        return response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []\n        for i in range(self._count, 0, -1):\n            msg = TextMessage(content=f\"{i}...\", source=self.name)\n            inner_messages.append(msg)\n            yield msg\n        yield Response(chat_message=TextMessage(content=\"Done!\", source=self.name), inner_messages=inner_messages)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\nasync def run_countdown_agent() -> None:\n    countdown_agent = CountDownAgent(\"countdown\")\n    async for message in countdown_agent.on_messages_stream([], CancellationToken()):\n        if isinstance(message, Response):\n            print(message.chat_message)\n        else:\n            print(message)\n\nawait run_countdown_agent()\n```\n\n### Explanation\n\nIn the `CountDownAgent` class, the `__init__` method establishes the countdown value. The `on_messages` method coordinates the response generation, while `on_messages_stream` produces each message in the countdown sequence. The final message is yielded to indicate task completion.\n\n## ArithmeticAgent Example\n\nNext, we develop the `ArithmeticAgent` that can perform basic arithmetic operations. It uses a callable function passed to its constructor to apply various transformations on a given integer.\n\n```python\nfrom typing import Callable, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nclass ArithmeticAgent(BaseChatAgent):\n    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n        super().__init__(name, description=description)\n        self._operator_func = operator_func\n        self._message_history: List[BaseChatMessage] = []\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        self._message_history.extend(messages)\n        assert isinstance(self._message_history[-1], TextMessage)\n        number = int(self._message_history[-1].content)\n        result = self._operator_func(number)\n        response_message = TextMessage(content=str(result), source=self.name)\n        self._message_history.append(response_message)\n        return Response(chat_message=response_message)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n```\n\n### Explanation\n\nThe `ArithmeticAgent` modifies an integer based on its `operator_func`. The `on_messages` method processes incoming messages and applies the arithmetic operation, yielding a result. It also maintains a history of prior messages for proper context handling.\n\n### Creating a Selector Group Chat\n\nTo organize multiple `ArithmeticAgent` instances, the `SelectorGroupChat` is leveraged. Each agent performs different operations on the provided integer.\n\n```python\nasync def run_number_agents() -> None:\n    # Create agents for number operations.\n    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n\n    termination_condition = MaxMessageTermination(10)\n\n    selector_group_chat = SelectorGroupChat(\n        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n        termination_condition=termination_condition,\n        allow_repeated_speaker=True,\n        selector_prompt=(\n            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n            \"Current conversation history:\\n{history}\\n\"\n            \"Please select the most appropriate role for the next message, and only return the role name.\"\n        ),\n    )\n\n    task: List[BaseChatMessage] = [\n        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n        TextMessage(content=\"10\", source=\"user\"),\n    ]\n    stream = selector_group_chat.run_stream(task=task)\n    await Console(stream)\n\nawait run_number_agents()\n```\n\n### Explanation\n\nIn this example, the agents perform distinct arithmetic functions on an initial number. The `SelectorGroupChat` manages the flow of operations and allows for iterative transformations until the termination condition is met.\n\n## Using Custom Model Clients in Custom Agents\n\nFor cases requiring unique model behavior, you may implement a custom model client. This section walks through creating a custom agent that uses Google's Gemini SDK for message responses.\n\nFirst, ensure the Gemini SDK is installed:\n\n```bash\npip install google-genai\n```\n\n### Gemini Assistant Agent Implementation\n\n```python\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, RequestUsage, UserMessage\nfrom google import genai\nfrom google.genai import types\n\nclass GeminiAssistantAgent(BaseChatAgent):\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n        yield Response(chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage), inner_messages=[])\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        await self._model_context.clear()\n```\n\n### Explanation\n\nThis custom agent utilizes the Gemini model to process messages. It manages context using `UnboundedChatCompletionContext` and the `genai.Client` for model interactions. Its responses are generated based on conversation history, demonstrating how to leverage an external API within a custom agent.\n\n### Integrating Custom Agent into Team\n\nThe custom Gemini agent can also participate in team settings similar to other AgentChat components.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\ngemini_critic_agent = GeminiAssistantAgent(\n    \"gemini_critic\",\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\ntermination = TextMentionTermination(\"APPROVE\") | MaxMessageTermination(10)\n\nteam = RoundRobinGroupChat([primary_agent, gemini_critic_agent], termination_condition=termination)\n\nawait Console(team.run_stream(task=\"Write a Haiku poem with 4 lines about the fall season.\"))\nawait model_client.close()\n```\n\n### Explanation\n\nThis final example integrates the custom agent within a `RoundRobinGroupChat`, pairing it with a primary agent. The team collaborates until either a set termination condition is met or explicit feedback is provided.\n\n## Making the Custom Agent Declarative\n\nYou can also make agents declarative, allowing for easy configuration management. This is achieved by inheriting from the `Component` class and implementing serialization methods.\n\n```python\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken, Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\nclass GeminiAssistantAgentConfig(BaseModel):\n    name: str\n    description: str = \"An agent that provides assistance with ability to use tools.\"\n    model: str = \"gemini-1.5-flash-002\"\n    system_message: str | None = None\n\nclass GeminiAssistantAgent(BaseChatAgent, Component[GeminiAssistantAgentConfig]):\n    component_config_schema = GeminiAssistantAgentConfig\n\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        await self._model_context.clear()\n\n    @classmethod\n    def _from_config(cls, config: GeminiAssistantAgentConfig) -> Self:\n        return cls(\n            name=config.name, description=config.description, model=config.model, system_message=config.system_message\n        )\n\n    def _to_config(self) -> GeminiAssistantAgentConfig:\n        return GeminiAssistantAgentConfig(\n            name=self.name,\n            description=self.description,\n            model=self._model,\n            system_message=self._system_message,\n        )\n```\n\n### Explanation\n\nThe `GeminiAssistantAgent` now supports serialization via the `Component` interface, making it easy to save and load configurations. The `_from_config` and `_to_config` methods allow for flexible configuration management.\n\n```python\ngemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nconfig = gemini_assistant.dump_component()\nprint(config.model_dump_json(indent=2))\nloaded_agent = GeminiAssistantAgent.load_component(config)\nprint(loaded_agent)\n```\n\n### Conclusion\n\nThis document provided an overview of creating custom agents in the AgentChat framework, including practical examples like the `CountDownAgent` and `ArithmeticAgent`, the use of custom model clients, and making agents declarative. These techniques facilitate building tailored chat agents that can meet specific use cases and integrate effectively into a broader chat architecture.\n\n## Next Steps\n\n- Explore extending model clients for function calling capabilities.\n- Package custom agents for use in environments like AgentChat Studio.\n- Experiment with different agent behaviors and combinations to optimize response generation and user interactions.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/custom-agents.ipynb"
  },
  {
    "code": false,
    "content": "# Company Research Documentation\n\nConducting company research, or competitive analysis, is a critical part of any business strategy. In this document, we will demonstrate how to create a team of agents to effectively address this task. We will explore a sequential approach, where we create agents corresponding to steps in the research process and equip them with tools required to execute their tasks efficiently.\n\n## Overview of Agents\n\nWe will be creating three types of agents:\n\n1. **Search Agent**: This agent utilizes a search engine API to gather information about a selected company.\n2. **Stock Analysis Agent**: This agent retrieves stock data using a financial data API, computes essential statistics such as current price, 52-week high, and generates a plot of the stock price year-to-date.\n3. **Report Agent**: This agent generates a comprehensive report using the information collected by the Search and Stock Analysis Agents.\n\n## Importing Necessary Modules\n\nBefore we can define the agents and their tools, we need to import several necessary libraries and modules.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.tools import FunctionTool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nThis code block imports various components essential for creating agents, defining team functionality, and establishing a user interface for interaction.\n\n## Defining Tools\n\nNext, we will create the tools that support the agents' functionalities: a Google search tool and a stock analysis tool. \n\n### Google Search Tool\n\nThe `google_search` function uses the Google Search API to retrieve information about a company. It needs an API key and search engine ID, which should be stored in a `.env` file.\n\n```python\ndef google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:\n    # ... [Function implementation] ...\n```\n\nIn this function:\n- It sends a query to the Google Search API and retrieves search results.\n- It enriches the results by fetching page content using the `BeautifulSoup` library.\n- The results are returned as a list of dictionaries containing the title, link, snippet, and body content.\n\n### Stock Analysis Tool\n\nThe `analyze_stock` function retrieves stock information using the `yfinance` library and generates a plot of the stock prices.\n\n```python\ndef analyze_stock(ticker: str) -> dict:\n    # ... [Function implementation] ...\n```\n\nIn this function:\n- It collects historical stock data and calculates essential statistics including current price, 52-week highs/lows, moving averages, and volatility.\n- It generates a line plot to visualize the stock's closing price over the past year, saving the plot to a file for future reference.\n\nTo allow the agents to use these functions, we wrap them in the `FunctionTool` class.\n\n```python\ngoogle_search_tool = FunctionTool(\n    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\n)\nstock_analysis_tool = FunctionTool(analyze_stock, description=\"Analyze stock data and generate a plot\")\n```\n\n## Defining Agents\n\nWith the tools established, we can now define our agents. Each agent will perform its designated role using the tools we've just created.\n\n### Search Agent\n\nThe `search_agent` utilizes the Google Search tool to find company-related information.\n\n```python\nsearch_agent = AssistantAgent(\n    name=\"Google_Search_Agent\",\n    model_client=model_client,\n    tools=[google_search_tool],\n    description=\"Search Google for information, returns top 2 results with a snippet and body content\",\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n)\n```\n\n### Stock Analysis Agent\n\nThe `stock_analysis_agent` employs the stock analysis tool to gather and analyze stock data.\n\n```python\nstock_analysis_agent = AssistantAgent(\n    name=\"Stock_Analysis_Agent\",\n    model_client=model_client,\n    tools=[stock_analysis_tool],\n    description=\"Analyze stock data and generate a plot\",\n    system_message=\"Perform data analysis.\",\n)\n```\n\n### Report Agent\n\nThe `report_agent` synthesizes the information gathered by both the Search and Stock Analysis Agents into a comprehensive report.\n\n```python\nreport_agent = AssistantAgent(\n    name=\"Report_Agent\",\n    model_client=model_client,\n    description=\"Generate a report based on the search and results of stock analysis\",\n    system_message=\"You are a helpful assistant that can generate a comprehensive report on a given topic based on search and stock analysis. When you are done with generating the report, reply with TERMINATE.\",\n)\n```\n\n## Creating the Team\n\nFinally, we can assemble our agents into a team. We will utilize a round-robin chat to sequentially delegate tasks among the agents.\n\n```python\nteam = RoundRobinGroupChat([stock_analysis_agent, search_agent, report_agent], max_turns=3)\n```\n\nBy setting `max_turns=3`, we ensure that each agent has an equal opportunity to contribute to the task.\n\n## Executing the Team\n\nTo initiate the research process, we can prompt the team to generate a financial report on a specified company, such as American Airlines.\n\n```python\nstream = team.run_stream(task=\"Write a financial report on American airlines\")\nawait Console(stream)\n\nawait model_client.close()\n```\n\nThis block executes the collaboration among the agents and outputs the results in a console format, allowing for real-time monitoring of the process.\n\n---\n\nIn conclusion, this systematic setup allows an organization to execute company research through the use of intelligent agents, enhancing productivity and streamlining data collection and analysis processes.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "Examples built using AgentChat, a high-level api for AutoGen\n"
      }
    },
    "content": "# Examples\n\nA list of examples to help you get started with AgentChat.\n\n:::::{grid} 2 2 2 3\n\n::::{grid-item-card} Travel Planning\n:img-top: ../../../images/example-travel.jpeg\n:img-alt: travel planning example\n:link: ./travel-planning.html\n:link-alt: travel planning: Generating a travel plan using multiple agents.\n\n^^^\nGenerating a travel plan using multiple agents.\n\n::::\n\n::::{grid-item-card} Company Research\n:img-top: ../../../images/example-company.jpg\n:img-alt: company research example\n:link: ./company-research.html\n:link-alt: company research: Generating a company research report using multiple agents with tools.\n\n^^^\nGenerating a company research report using multiple agents with tools.\n\n::::\n\n::::{grid-item-card} Literature Review\n:img-top: ../../../images/example-literature.jpg\n:img-alt: literature review example\n:link: ./literature-review.html\n:link-alt: literature review: Generating a literature review using agents with tools.\n\n^^^\nGenerating a literature review using agents with tools.\n\n::::\n\n:::::\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n\ntravel-planning\ncompany-research\nliterature-review\n\n```",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/examples/index.md"
  },
  {
    "code": false,
    "content": "# Literature Review\n\nConducting a literature review is a common task in research, helping to gather and evaluate previous work in a given area. This documentation describes how to set up a multi-agent team to execute a straightforward literature review using various tools and APIs.\n\n## Overview of Agents\n\nThe following components will facilitate the literature review:\n\n- **Arxiv Search Agent**: Utilizes the Arxiv API to discover research papers related to a specific topic.\n- **Google Search Agent**: Employs the Google Search API to find and return relevant papers.\n- **Report Agent**: Generates a structured report based on the findings from the Arxiv and Google search agents.\n\nThese agents will work together to streamline the literature review process, collecting information and summarizing it into a cohesive document.\n\n## Module Imports\n\nTo begin, we need to import the necessary modules that will support our agents.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.tools import FunctionTool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nThis code imports classes necessary to create agents, define conditions for agent termination, establish team configurations, and enable interactions via a console UI.\n\n## Defining Tools\n\nThe next step is to define the tools the agents will utilize. We will create a function called `search_arxiv`, which uses the `arxiv` library to probe for papers based on the input topic. Additionally, we will encapsulate these functions in a `FunctionTool` class, making them accessible to the agents.\n\nPrior to running the code below, ensure the correct environment variables are set and install the required libraries with the following command:\n\n```bash\n!pip install arxiv\n```\n\n### Google Search Function\n\n```python\ndef google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:\n    import os\n    import time\n    import requests\n    from bs4 import BeautifulSoup\n    from dotenv import load_dotenv\n\n    load_dotenv()\n\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\n    search_engine_id = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n\n    if not api_key or not search_engine_id:\n        raise ValueError(\"API key or Search Engine ID not found in environment variables\")\n\n    url = \"https://www.googleapis.com/customsearch/v1\"\n    params = {\"key\": api_key, \"cx\": search_engine_id, \"q\": query, \"num\": num_results}\n\n    response = requests.get(url, params=params)\n\n    if response.status_code != 200:\n        print(response.json())\n        raise Exception(f\"Error in API request: {response.status_code}\")\n\n    results = response.json().get(\"items\", [])\n\n    def get_page_content(url: str) -> str:\n        try:\n            response = requests.get(url, timeout=10)\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            text = soup.get_text(separator=\" \", strip=True)\n            words = text.split()\n            content = \" \".join(words[:max_chars])\n            return content\n        except Exception as e:\n            print(f\"Error fetching {url}: {str(e)}\")\n            return \"\"\n\n    enriched_results = []\n    for item in results:\n        body = get_page_content(item[\"link\"])\n        enriched_results.append(\n            {\"title\": item[\"title\"], \"link\": item[\"link\"], \"snippet\": item[\"snippet\"], \"body\": body}\n        )\n        time.sleep(1)  # Be respectful to the servers\n\n    return enriched_results\n```\n\nThis function queries Google for relevant papers using the provided API credentials. It retrieves a specified number of results and extracts content from each result page to return enriched information about the found papers.\n\n### Arxiv Search Function\n\n```python\ndef arxiv_search(query: str, max_results: int = 2) -> list:\n    \"\"\"\n    Search Arxiv for papers and return the results including abstracts.\n    \"\"\"\n    import arxiv\n\n    client = arxiv.Client()\n    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n\n    results = []\n    for paper in client.results(search):\n        results.append(\n            {\n                \"title\": paper.title,\n                \"authors\": [author.name for author in paper.authors],\n                \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n                \"abstract\": paper.summary,\n                \"pdf_url\": paper.pdf_url,\n            }\n        )\n\n    return results\n```\n\nThe `arxiv_search` function interacts with the Arxiv API to locate relevant academic papers based on a search query and returns the paper details, including titles, authors, publication dates, abstracts, and PDF URLs.\n\n### Wrapping Functions into Tools\n\n```python\ngoogle_search_tool = FunctionTool(\n    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\n)\narxiv_search_tool = FunctionTool(\n    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n)\n```\n\nThe two functions are encapsulated in `FunctionTool` classes, allowing the agents to utilize these functionalities during their operations. Each tool is equipped with a description that specifies its purpose.\n\n## Defining Agents\n\nHaving defined the tools, we proceed to create agents that will accomplish specific tasks.\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\ngoogle_search_agent = AssistantAgent(\n    name=\"Google_Search_Agent\",\n    tools=[google_search_tool],\n    model_client=model_client,\n    description=\"An agent that can search Google for information, returns results with a snippet and body content\",\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n)\n\narxiv_search_agent = AssistantAgent(\n    name=\"Arxiv_Search_Agent\",\n    tools=[arxiv_search_tool],\n    model_client=model_client,\n    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academic papers.\",\n)\n\nreport_agent = AssistantAgent(\n    name=\"Report_Agent\",\n    model_client=model_client,\n    description=\"Generate a report based on a given topic\",\n    system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high-quality literature review including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references. Your response should end with the word 'TERMINATE'\",\n)\n```\n\nIn this section, we define three agents:\n- **Google Search Agent** to retrieve information from Google.\n- **Arxiv Search Agent** to gather academic papers.\n- **Report Agent** to compile a final literature review based on the aggregated data.\n\n## Creating the Team\n\nFinalizing our setup requires creating a team of these agents and configuring them to work in a round-robin manner.\n\n```python\ntermination = TextMentionTermination(\"TERMINATE\")\nteam = RoundRobinGroupChat(\n    participants=[google_search_agent, arxiv_search_agent, report_agent], termination_condition=termination\n)\n```\n\nThis code establishes a team of agents that will collaborate, with messages exchanged among them until the specified termination condition\u2014when the report agent produces the word \"TERMINATE\"\u2014is met.\n\n## Running the Literature Review\n\nFinally, we can initiate the literature review process using the team of agents configured in the previous steps.\n\n```python\nawait Console(\n    team.run_stream(\n        task=\"Write a literature review on no code tools for building multi-agent AI systems\",\n    )\n)\n\nawait model_client.close()\n```\n\nThis code snippet runs the team within a console, prompting them to generate a literature review on the specified topic. After the task execution, the model client is gracefully closed to release resources.\n\nIn summary, this documentation outlines the structure and functionality of a multi-agent system designed to perform literature reviews efficiently, integrating external APIs for literature discovery and leveraging AI capabilities for report generation.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb"
  },
  {
    "code": false,
    "content": "# Travel Planning Documentation\n\nIn this document, we will walk through the implementation of a comprehensive travel planning system using AgentChat. This system employs a collaborative approach with multiple AI agents, each designed for a specific role in creating a travel itinerary. By leveraging these agents, we can provide users with a well-rounded travel plan that incorporates various perspectives and expertise.\n\n## Importing Necessary Modules\n\nTo begin with, we need to import the modules required for our travel planning system. These modules include essential classes and functions to enable agent interactions and manage the planning process.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nThis block imports the `AssistantAgent` class, which will be used to create our agents. It also brings in conditions for terminating conversations, the framework for managing group chats, a user interface for output, and the OpenAI completion client for generating text responses. \n\n## Defining Agents\n\nNext, we will define the various agents that will comprise our travel planning team. Each agent is assigned a specific role, enhancing the overall effectiveness of the travel planning process.\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nplanner_agent = AssistantAgent(\n    \"planner_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can plan trips.\",\n    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request.\",\n)\n\nlocal_agent = AssistantAgent(\n    \"local_agent\",\n    model_client=model_client,\n    description=\"A local assistant that can suggest local activities or places to visit.\",\n    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.\",\n)\n\nlanguage_agent = AssistantAgent(\n    \"language_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can provide language tips for a given destination.\",\n    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.\",\n)\n\ntravel_summary_agent = AssistantAgent(\n    \"travel_summary_agent\",\n    model_client=model_client,\n    description=\"A helpful assistant that can summarize the travel plan.\",\n    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.\",\n)\n```\n\nHere, we set up four agents, each with a clear role: the planner agent designs the trip, the local agent curates activities, the language agent provides communication tips, and the summary agent compiles the final plan. Each agent is initialized with a system message that guides its function and interaction.\n\n## Setting Up the Group Chat\n\nOnce our agents are defined, we need to create the structure that will allow these agents to interact and collaborate effectively. This is done by establishing a round-robin group chat with a termination condition.\n\n```python\ntermination = TextMentionTermination(\"TERMINATE\")\ngroup_chat = RoundRobinGroupChat(\n    [planner_agent, local_agent, language_agent, travel_summary_agent], termination_condition=termination\n)\n```\n\nIn this code block, we specify a termination condition that allows the conversation to end when one of the agents mentions \"TERMINATE\". The `RoundRobinGroupChat` arranges our agents in a setup that facilitates alternating contributions, promoting collaborative dialogue and ensuring that all perspectives are considered.\n\n## Running the Travel Planning Task\n\nFinally, we will execute the travel planning task. This step invokes the group chat to start brainstorming and developing a travel itinerary based on the user\u2019s request.\n\n```python\nawait Console(group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\"))\n\nawait model_client.close()\n```\n\nIn this last segment, we initiate the chat with a specific travel request\u2014planning a three-day trip to Nepal. The results of the generated itinerary will be displayed in the console interface. After the task is completed, we also close the model client to free up resources.\n\n## Conclusion\n\nThe structured travel planning system outlined in this documentation effectively harnesses the capabilities of multiple AI agents, each dedicated to specific roles. By integrating their expertise, the system not only enhances the user\u2019s travel experience but also ensures comprehensive coverage of travel details. This collaboration results in well-rounded travel itineraries that are both practical and enriching.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/examples/travel-planning.ipynb"
  },
  {
    "code": false,
    "content": "# GraphFlow (Workflows)\n\nIn this section, we'll explore how to create a **multi-agent workflow** using the `GraphFlow` class from the `autogen_agentchat.teams` module. This powerful tool allows for structured execution with precise control over how agents interact to complete tasks.\n\nWe'll guide you through the following:\n- Creating and running a flow\n- Observing and debugging flow behavior\n- Managing execution efficiently\n\n## Overview of GraphFlow\n\nThe `GraphFlow` class provides a framework for directed graph execution:\n- It operates on a directed graph structure (`DiGraph`) to manage the execution order of agents.\n- It supports various behaviors, including sequential execution, parallel processes, conditional branching, and loops.\n\n```{note}\n**When should you use GraphFlow?**\n\nGraphFlow is ideal when you need strict control over the sequence of agent interactions or when different outcomes lead to distinct actions. For simpler conversational flows, consider using simpler teams, such as `RoundRobinGroupChat` or `SelectorGroupChat`. Move to GraphFlow for tasks requiring deterministic control or handling complex multi-step processes.\n```\n\n> **Warning:** The `GraphFlow` feature is currently **experimental**. Its API, behavior, and capabilities may change in future updates.\n\n## Creating and Running a Flow\n\nTo begin, we'll use the `DiGraphBuilder` utility, which simplifies the construction of execution graphs. This builder allows you to create:\n- Sequential chains\n- Parallel branches\n- Conditional paths\n- Loops with defined exit conditions\n\nEach node in the graph represents an agent, and the edges determine valid execution paths. Edges can have conditions based on agent messages, providing flexibility in workflow design.\n\n### Example: Sequential Flow\n\nLet's start with a simple workflow where a **writer** drafts a paragraph, and a **reviewer** provides feedback. The flow automatically identifies source and leaf nodes, commencing execution at the sources and concluding when all tasks are finished.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI model client\nclient = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n\n# Create the writer agent\nwriter = AssistantAgent(\"writer\", model_client=client, system_message=\"Draft a short paragraph on climate change.\")\n\n# Create the reviewer agent\nreviewer = AssistantAgent(\"reviewer\", model_client=client, system_message=\"Review the draft and suggest improvements.\")\n\n# Build the graph\nbuilder = DiGraphBuilder()\nbuilder.add_node(writer).add_node(reviewer)\nbuilder.add_edge(writer, reviewer)\n\n# Build and validate the graph\ngraph = builder.build()\n\n# Create the flow\nflow = GraphFlow([writer, reviewer], graph=graph)\n```\n\nIn this code snippet, we create an OpenAI model client, define a **writer** and **reviewer** agent, and establish a directed graph for execution. The flow now includes both agents and tracks the execution order from the writer to the reviewer.\n\n```python\n# Running the flow with asyncio\n# Use `asyncio.run(...)` and wrap the below in a async function when running in a script.\nstream = flow.run_stream(task=\"Write a short paragraph about climate change.\")\nasync for event in stream:  # type: ignore\n    print(event)\n# Use Console(flow.run_stream(...)) for better formatting in console.\n```\n\nBy executing the flow, we harness the asynchronous nature of the agents, allowing the writer to draft content that the reviewer will assess.\n\n### Example: Parallel Flow with Join\n\nNext, we\u2019ll create a more intricate workflow:\n- A **writer** drafts a paragraph.\n- Two **editors** work on grammar and style independently (parallel fan-out).\n- A **final reviewer** consolidates their edits.\n\nExecution begins with the **writer**, branches out to both editors, and finally convenes at the **final reviewer**.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI model client\nclient = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n\n# Create the writer agent\nwriter = AssistantAgent(\"writer\", model_client=client, system_message=\"Draft a short paragraph on climate change.\")\n\n# Create two editor agents\neditor1 = AssistantAgent(\"editor1\", model_client=client, system_message=\"Edit the paragraph for grammar.\")\neditor2 = AssistantAgent(\"editor2\", model_client=client, system_message=\"Edit the paragraph for style.\")\n\n# Create the final reviewer agent\nfinal_reviewer = AssistantAgent(\"final_reviewer\", model_client=client, system_message=\"Consolidate the grammar and style edits into a final version.\")\n\n# Build the workflow graph\nbuilder = DiGraphBuilder()\nbuilder.add_node(writer).add_node(editor1).add_node(editor2).add_node(final_reviewer)\n\n# Define the flow paths\nbuilder.add_edge(writer, editor1)\nbuilder.add_edge(writer, editor2)\nbuilder.add_edge(editor1, final_reviewer)\nbuilder.add_edge(editor2, final_reviewer)\n\n# Build and validate the graph\ngraph = builder.build()\n\n# Create the flow\nflow = GraphFlow(participants=builder.get_participants(), graph=graph)\n\n# Run the workflow\nawait Console(flow.run_stream(task=\"Write a short paragraph about climate change.\"))\n```\n\nIn this workflow, the simultaneous execution of the two editing tasks demonstrates the parallel functionality of `GraphFlow`, culminating in a unified review process.\n\n## Message Filtering\n\n### Understanding Execution and Message Graphs\n\nIn `GraphFlow`, the **execution graph** is constructed using `DiGraph`, which dictates agent execution order. However, the execution graph alone does not control message dispatch; by default, all messages are sent to every agent involved in the graph. \n\n**Message filtering** allows you to specify which messages each agent receives. Setting up a message graph helps:\n- Reduce irrelevant information (hallucinations)\n- Control memory use\n- Concentrate agents on significant details\n\nYou can implement message filtering using `MessageFilterAgent`, `MessageFilterConfig`, and `PerSourceFilter` to establish custom rules.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent, MessageFilterAgent, MessageFilterConfig, PerSourceFilter\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Model client\nclient = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n\n# Create agents\nresearcher = AssistantAgent(\"researcher\", model_client=client, system_message=\"Summarize key facts about climate change.\")\nanalyst = AssistantAgent(\"analyst\", model_client=client, system_message=\"Review the summary and suggest improvements.\")\npresenter = AssistantAgent(\"presenter\", model_client=client, system_message=\"Prepare a presentation slide based on the final summary.\")\n\n# Set up message filtering\nfiltered_analyst = MessageFilterAgent(\n    name=\"analyst\",\n    wrapped_agent=analyst,\n    filter=MessageFilterConfig(per_source=[PerSourceFilter(source=\"researcher\", position=\"last\", count=1)]),\n)\n\nfiltered_presenter = MessageFilterAgent(\n    name=\"presenter\",\n    wrapped_agent=presenter,\n    filter=MessageFilterConfig(per_source=[PerSourceFilter(source=\"analyst\", position=\"last\", count=1)]),\n)\n\n# Build the flow\nbuilder = DiGraphBuilder()\nbuilder.add_node(researcher).add_node(filtered_analyst).add_node(filtered_presenter)\nbuilder.add_edge(researcher, filtered_analyst).add_edge(filtered_analyst, filtered_presenter)\n\n# Create the flow\nflow = GraphFlow(participants=builder.get_participants(), graph=builder.build())\n\n# Run the flow\nawait Console(flow.run_stream(task=\"Summarize key facts about climate change.\"))\n```\n\nIn this snippet, the researcher\u2019s message is filtered so that only the relevant summary reaches the analyst, enhancing focus and reducing extraneous information.\n\n## Advanced Example: Conditional Loop with Filtered Summary\n\nThis advanced scenario illustrates a workflow where:\n- There\u2019s a loop between a generator and a reviewer, exiting once \"APPROVE\" is communicated.\n- A summarizer agent only accesses the initial user input and the last message from the reviewer.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent, MessageFilterAgent, MessageFilterConfig, PerSourceFilter\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Agents\ngenerator = AssistantAgent(\"generator\", model_client=model_client, system_message=\"Generate a list of creative ideas.\")\nreviewer = AssistantAgent(\"reviewer\", model_client=model_client, system_message=\"Review ideas and provide feedbacks, or just 'APPROVE' for final approval.\")\nsummarizer_core = AssistantAgent(\"summary\", model_client=model_client, system_message=\"Summarize the user request and the final feedback.\")\n\n# Create a filtered summarizer\nfiltered_summarizer = MessageFilterAgent(\n    name=\"summary\",\n    wrapped_agent=summarizer_core,\n    filter=MessageFilterConfig(\n        per_source=[\n            PerSourceFilter(source=\"user\", position=\"first\", count=1),\n            PerSourceFilter(source=\"reviewer\", position=\"last\", count=1),\n        ]\n    ),\n)\n\n# Build the graph with a conditional loop\nbuilder = DiGraphBuilder()\nbuilder.add_node(generator).add_node(reviewer).add_node(filtered_summarizer)\nbuilder.add_edge(generator, reviewer)\nbuilder.add_edge(reviewer, filtered_summarizer, condition=lambda msg: \"APPROVE\" in msg.to_model_text())\nbuilder.add_edge(reviewer, generator, condition=lambda msg: \"APPROVE\" not in msg.to_model_text())\nbuilder.set_entry_point(generator)  # Required entry point for graph execution without source nodes\ngraph = builder.build()\n\ntermination_condition = MaxMessageTermination(10)\n\n# Create the flow\nflow = GraphFlow(participants=builder.get_participants(), graph=graph, termination_condition=termination_condition)\n\n# Run and print output\nawait Console(flow.run_stream(task=\"Brainstorm ways to reduce plastic waste.\"))\n```\n\nThe above code implements a loop that allows the reviewer to approve suggestions or provide feedback, dynamically adjusting the flow based on interactions.\n\n## Advanced Example: Cycles With Activation Group Examples\n\nIn this section, we\u2019ll discuss complex dependency patterns in cyclic graphs utilizing `activation_group` and `activation_condition` attributes.\n\n### Example 1: Loop with Multiple Paths - \"All\" Activation\n\nThis example showcases a structure where node B receives input from both A and C, necessitating all inputs to trigger further processing. \n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Model client\nclient = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create agents\nagent_a = AssistantAgent(\"A\", model_client=client, system_message=\"Start the process and provide initial input.\")\nagent_b = AssistantAgent(\"B\", model_client=client, system_message=\"Process input from A or feedback from C. Say 'CONTINUE' if it's from A or 'STOP' if it's from C.\")\nagent_c = AssistantAgent(\"C\", model_client=client, system_message=\"Review B's output and provide feedback.\")\nagent_e = AssistantAgent(\"E\", model_client=client, system_message=\"Finalize the process.\")\n\n# Build the graph with activation groups\nbuilder = DiGraphBuilder()\nbuilder.add_node(agent_a).add_node(agent_b).add_node(agent_c).add_node(agent_e)\n\n# Define edges for the workflow\nbuilder.add_edge(agent_a, agent_b, activation_group=\"initial\")\nbuilder.add_edge(agent_b, agent_c, condition=\"CONTINUE\")\nbuilder.add_edge(agent_c, agent_b, activation_group=\"feedback\")\nbuilder.add_edge(agent_b, agent_e, condition=\"STOP\")\n\ntermination_condition = MaxMessageTermination(10)\ngraph = builder.build()\nflow = GraphFlow(participants=[agent_a, agent_b, agent_c, agent_e], graph=graph, termination_condition=termination_condition)\n\nprint(\"=== Example 1: A\u2192B\u2192C\u2192B with 'All' Activation ===\")\nprint(\"B will exit when it receives a message from C.\")\n# await Console(flow.run_stream(task=\"Start a review process for a document.\"))\n```\n\n### Example 2: Loop with Multiple Paths - \"Any\" Activation\n\nThis scenario illustrates a flow where A initiates a task that B coordinates, fanning out to both C1 and C2. B proceeds to the next step as soon as either C1 or C2 completes.\n\n```python\n# Create agents for A\u2192B\u2192(C1,C2)\u2192B scenario\nagent_a2 = AssistantAgent(\"A\", model_client=client, system_message=\"Initiate a task that needs parallel processing.\")\nagent_b2 = AssistantAgent(\"B\", model_client=client, system_message=\"Coordinate parallel tasks. Say 'PROCESS' to start parallel work or 'DONE' to finish.\")\nagent_c1 = AssistantAgent(\"C1\", model_client=client, system_message=\"Handle task type 1. Say 'C1_COMPLETE' when done.\")\nagent_c2 = AssistantAgent(\"C2\", model_client=client, system_message=\"Handle task type 2. Say 'C2_COMPLETE' when done.\")\nagent_e = AssistantAgent(\"E\", model_client=client, system_message=\"Finalize the process.\")\n\n# Build the graph with \"any\" activation\nbuilder2 = DiGraphBuilder()\nbuilder2.add_node(agent_a2).add_node(agent_b2).add_node(agent_c1).add_node(agent_c2).add_node(agent_e)\n\n# Define the workflow edges\nbuilder2.add_edge(agent_a2, agent_b2)\nbuilder2.add_edge(agent_b2, agent_c1, condition=\"PROCESS\")\nbuilder2.add_edge(agent_b2, agent_c2, condition=\"PROCESS\")\nbuilder2.add_edge(agent_b2, agent_e, condition=lambda msg: \"DONE\" in msg.to_model_text())\n\n# Loop back conditions\nbuilder2.add_edge(agent_c1, agent_b2, activation_group=\"loop_back_group\", activation_condition=\"any\", condition=\"C1_COMPLETE\")\nbuilder2.add_edge(agent_c2, agent_b2, activation_group=\"loop_back_group\", activation_condition=\"any\", condition=\"C2_COMPLETE\")\n\n# Build and create flow\ngraph2 = builder2.build()\nflow2 = GraphFlow(participants=[agent_a2, agent_b2, agent_c1, agent_c2, agent_e], graph=graph2)\n\nprint(\"=== Example 2: A\u2192B\u2192(C1,C2)\u2192B with 'Any' Activation ===\")\nprint(\"B will execute as soon as EITHER C1 OR C2 completes (whichever finishes first).\")\n# await Console(flow2.run_stream(task=\"Start a parallel processing task.\"))\n```\n\n### Example 3: Mixed Activation Groups\n\nHere, we analyze how different activation conditions can interplay within the same graph, accommodating varied urgency levels for each input.\n\n```python\n# Create agents for mixed activation scenario\nagent_a3 = AssistantAgent(\"A\", model_client=client, system_message=\"Provide critical input that must be processed.\")\nagent_b3 = AssistantAgent(\"B\", model_client=client, system_message=\"Provide secondary critical input.\")\nagent_c3 = AssistantAgent(\"C\", model_client=client, system_message=\"Provide optional quick input.\")\nagent_d3 = AssistantAgent(\"D\", model_client=client, system_message=\"Process inputs based on different priority levels.\")\n\n# Build graph with mixed activation groups\nbuilder3 = DiGraphBuilder()\nbuilder3.add_node(agent_a3).add_node(agent_b3).add_node(agent_c3).add_node(agent_d3)\n\n# Critical inputs that must ALL be present\nbuilder3.add_edge(agent_a3, agent_d3, activation_group=\"critical\", activation_condition=\"all\")\nbuilder3.add_edge(agent_b3, agent_d3, activation_group=\"critical\", activation_condition=\"all\")\n\n# Optional input that can trigger execution independently\nbuilder3.add_edge(agent_c3, agent_d3, activation_group=\"optional\", activation_condition=\"any\")\n\n# Build and create flow\ngraph3 = builder3.build()\nflow3 = GraphFlow(participants=[agent_a3, agent_b3, agent_c3, agent_d3], graph=graph3)\n\nprint(\"=== Example 3: Mixed Activation Groups ===\")\nprint(\"D will execute when:\")\nprint(\"- BOTH A AND B complete (critical group with 'all' activation), OR\")\nprint(\"- C completes (optional group with 'any' activation)\")\n```\n\n### Key Takeaways for Activation Groups\n\n1. **`activation_group`**: This configures how edges pointing to a target node function, allowing you to define different types of dependency patterns.\n  \n2. **`activation_condition`**: \n   - **\"all\" (default)**: The target node awaits all conditions in the group.\n   - **\"any\"**: Executes as soon as any condition is fulfilled.\n\n3. **Use Cases**:\n   - Managing cycles or loops with distinct triggers\n   - Implementing priority-driven execution\n   - Enabling parallel tasks with prompt progression based on rapid completions\n\n4. **Best Practices**:\n   - Clearly label activation groups (e.g., \"critical\", \"optional\").\n   - Maintain consistent conditions within groups for predictable behavior.\n   - Thoroughly test workflows to validate the logic of execution paths.\n\nThese strategies empower you to orchestrate sophisticated workflows with clarity and precision.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/graph-flow.ipynb"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AgentChat, a high-level API for AutoGen\n"
      }
    },
    "content": "# AgentChat\n\nAgentChat is a high-level API for building multi-agent applications.\nIt is built on top of the [`autogen-core`](../core-user-guide/index.md) package.\nFor beginner users, AgentChat is the recommended starting point.\nFor advanced users, [`autogen-core`](../core-user-guide/index.md)'s event-driven\nprogramming model provides more flexibility and control over the underlying components.\n\nAgentChat provides intuitive defaults, such as **Agents** with preset\nbehaviors and **Teams** with predefined [multi-agent design patterns](../core-user-guide/design-patterns/intro.md).\n\n::::{grid} 2 2 2 2\n:gutter: 3\n\n:::{grid-item-card} {fas}`download;pst-color-primary` Installation\n:link: ./installation.html\n:link-alt: Installation: How to install AgentChat\n\nHow to install AgentChat\n:::\n\n:::{grid-item-card} {fas}`rocket;pst-color-primary` Quickstart\n:link: ./quickstart.html\n:link-alt: Quickstart: Build your first agent\n\nBuild your first agent\n:::\n\n:::{grid-item-card} {fas}`school;pst-color-primary` Tutorial\n:link: ./tutorial/index.html\n:link-alt: Tutorial: Step-by-step guide to using AgentChat, learn about agents, teams, and more\n\nStep-by-step guide to using AgentChat, learn about agents, teams, and more\n:::\n\n:::{grid-item-card} {fas}`wrench;pst-color-primary` Custom Agents\n:link: ./custom-agents.html\n:link-alt: Custom Agents: Create your own agents with custom behaviors\n\nCreate your own agents with custom behaviors\n:::\n\n:::{grid-item-card} {fas}`sitemap;pst-color-primary` Selector Group Chat\n:link: ./selector-group-chat.html\n:link-alt: Selector Group Chat: Multi-agent coordination through a shared context and centralized, customizable selector\n\nMulti-agent coordination through a shared context and centralized, customizable selector\n:::\n\n:::{grid-item-card} {fas}`dove;pst-color-primary` Swarm\n:link: ./swarm.html\n:link-alt: Swarm: Multi-agent coordination through a shared context and localized, tool-based selector\n\nMulti-agent coordination through a shared context and localized, tool-based selector\n:::\n\n:::{grid-item-card} {fas}`book;pst-color-primary` Magentic-One\n:link: ./magentic-one.html\n:link-alt: Magentic-One: Get started with Magentic-One\n\nGet started with Magentic-One\n:::\n\n:::{grid-item-card} {fas}`sitemap;pst-color-primary` GraphFlow (Workflow)\n:link: ./graph-flow.html\n:link-alt: GraphFlow: Multi-agent workflows through a directed graph of agents.\n\nMulti-agent workflows through a directed graph of agents.\n:::\n\n:::{grid-item-card} {fas}`brain;pst-color-primary` Memory\n:link: ./memory.html\n:link-alt: Memory: Add memory capabilities to your agents\n\nAdd memory capabilities to your agents\n:::\n\n:::{grid-item-card} {fas}`file;pst-color-primary` Logging\n:link: ./logging.html\n:link-alt: Logging: Log traces and internal messages\n\nLog traces and internal messages\n:::\n\n:::{grid-item-card} {fas}`save;pst-color-primary` Serialize Components\n:link: ./serialize-components.html\n:link-alt: Serialize Components: Serialize and deserialize components\n\nSerialize and deserialize components\n:::\n\n:::{grid-item-card} {fas}`code;pst-color-primary` Examples\n:link: ./examples/index.html\n:link-alt: Examples: Sample code and use cases\n\nSample code and use cases\n:::\n\n:::{grid-item-card} {fas}`truck-moving;pst-color-primary` Migration Guide\n:link: ./migration-guide.html\n:link-alt: Migration Guide: How to migrate from AutoGen 0.2.x to 0.4.x.\n\nHow to migrate from AutoGen 0.2.x to 0.4.x.\n:::\n::::\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n\ninstallation\nquickstart\nmigration-guide\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Tutorial\n\ntutorial/index\ntutorial/models\ntutorial/messages\ntutorial/agents\ntutorial/teams\ntutorial/human-in-the-loop\ntutorial/termination\ntutorial/state\n\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Advanced\n\ncustom-agents\nselector-group-chat\nswarm\nmagentic-one\ngraph-flow\nmemory\nlogging\nserialize-components\ntracing\n\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: More\n\nexamples/index\n```",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/index.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "Installing AutoGen AgentChat\n"
      }
    },
    "content": "# Installation\n\n## Create a Virtual Environment (optional)\n\nWhen installing AgentChat locally, we recommend using a virtual environment for the installation. This will ensure that the dependencies for AgentChat are isolated from the rest of your system.\n\n``````{tab-set}\n\n`````{tab-item} venv\n\nCreate and activate:\n\nLinux/Mac:\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nWindows command-line:\n```batch\n# The command may be `python3` instead of `python` depending on your setup\npython -m venv .venv\n.venv\\Scripts\\activate.bat\n```\n\nTo deactivate later, run:\n\n```bash\ndeactivate\n```\n\n`````\n\n`````{tab-item} conda\n\n[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.\n\n\nCreate and activate:\n\n```bash\nconda create -n autogen python=3.12\nconda activate autogen\n```\n\nTo deactivate later, run:\n\n```bash\nconda deactivate\n```\n\n\n`````\n\n\n\n``````\n\n## Install Using pip\n\nInstall the `autogen-agentchat` package using pip:\n\n```bash\n\npip install -U \"autogen-agentchat\"\n```\n\n```{note}\nPython 3.10 or later is required.\n```\n\n## Install OpenAI for Model Client\n\nTo use the OpenAI and Azure OpenAI models, you need to install the following\nextensions:\n\n```bash\npip install \"autogen-ext[openai]\"\n```\n\nIf you are using Azure OpenAI with AAD authentication, you need to install the following:\n\n```bash\npip install \"autogen-ext[azure]\"\n```",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/installation.md"
  },
  {
    "content": "# Logging\n\nAutoGen uses Python's built-in [`logging`](https://docs.python.org/3/library/logging.html) module.\n\nTo enable logging for AgentChat, you can use the following code:\n\n```python\nimport logging\n\nfrom autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\n\n# For trace logging.\ntrace_logger = logging.getLogger(TRACE_LOGGER_NAME)\ntrace_logger.addHandler(logging.StreamHandler())\ntrace_logger.setLevel(logging.DEBUG)\n\n# For structured message logging, such as low-level messages between agents.\nevent_logger = logging.getLogger(EVENT_LOGGER_NAME)\nevent_logger.addHandler(logging.StreamHandler())\nevent_logger.setLevel(logging.DEBUG)\n```\n\nTo enable additional logs such as model client calls and agent runtime events,\nplease refer to the [Core Logging Guide](../core-user-guide/framework/logging.md).",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/logging.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AgentChat, a high-level API for AutoGen\n"
      }
    },
    "content": "# Magentic-One\n\n[Magentic-One](https://aka.ms/magentic-one-blog) is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step forward for multi-agent systems, achieving competitive performance on a number of agentic benchmarks (see the [technical report](https://arxiv.org/abs/2411.04468) for full details).\n\nWhen originally released in [November 2024](https://aka.ms/magentic-one-blog) Magentic-One was [implemented directly on the `autogen-core` library](https://github.com/microsoft/autogen/tree/v0.4.4/python/packages/autogen-magentic-one). We have now ported Magentic-One to use `autogen-agentchat`, providing a more modular and easier to use interface.\n\nTo this end, the Magentic-One orchestrator {py:class}`~autogen_agentchat.teams.MagenticOneGroupChat` is now simply an AgentChat team, supporting all standard AgentChat agents and features. Likewise, Magentic-One's {py:class}`~autogen_ext.agents.web_surfer.MultimodalWebSurfer`, {py:class}`~autogen_ext.agents.file_surfer.FileSurfer`, and {py:class}`~autogen_ext.agents.magentic_one.MagenticOneCoderAgent` agents are now broadly available as AgentChat agents, to be used in any AgentChat workflows.\n\nLastly, there is a helper class, {py:class}`~autogen_ext.teams.magentic_one.MagenticOne`, which bundles all of this together as it was in the paper with minimal configuration.\n\nFind additional information about Magentic-one in our [blog post](https://aka.ms/magentic-one-blog) and [technical report](https://arxiv.org/abs/2411.04468).\n\n![Autogen Magentic-One example](../../images/autogen-magentic-one-example.png)\n\n**Example**: The figure above illustrates Magentic-One multi-agent team completing a complex task from the GAIA benchmark. Magentic-One's Orchestrator agent creates a plan, delegates tasks to other agents, and tracks progress towards the goal, dynamically revising the plan as needed. The Orchestrator can delegate tasks to a FileSurfer agent to read and handle files, a WebSurfer agent to operate a web browser, or a Coder or Computer Terminal agent to write or execute code, respectively.\n\n```{caution}\nUsing Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:\n\n1. **Use Containers**: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.\n2. **Virtual Environment**: Use a virtual environment to run the agents and prevent them from accessing sensitive data.\n3. **Monitor Logs**: Closely monitor logs during and after execution to detect and mitigate risky behavior.\n4. **Human Oversight**: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.\n5. **Limit Access**: Restrict the agents' access to the internet and other resources to prevent unauthorized actions.\n6. **Safeguard Data**: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.\nBe aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.\n```\n\n## Getting started\n\nInstall the required packages:\n\n```bash\npip install \"autogen-agentchat\" \"autogen-ext[magentic-one,openai]\"\n\n# If using the MultimodalWebSurfer, you also need to install playwright dependencies:\nplaywright install --with-deps chromium\n```\n\nIf you haven't done so already, go through the AgentChat tutorial to learn about the concepts of AgentChat.\n\nThen, you can try swapping out a {py:class}`autogen_agentchat.teams.SelectorGroupChat` with {py:class}`~autogen_agentchat.teams.MagenticOneGroupChat`.\n\nFor example:\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n    )\n    team = MagenticOneGroupChat([assistant], model_client=model_client)\n    await Console(team.run_stream(task=\"Provide a different proof for Fermat's Last Theorem\"))\n    await model_client.close()\n\n\nasyncio.run(main())\n```\n\nTo use a different model, see [Models](./tutorial/models.ipynb) for more information.\n\nOr, use the Magentic-One agents in a team:\n\n```{caution}\nThe example code may download files from the internet, execute code, and interact with web pages. Ensure you are in a safe environment before running the example code.\n```\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n# from autogen_ext.agents.file_surfer import FileSurfer\n# from autogen_ext.agents.magentic_one import MagenticOneCoderAgent\n# from autogen_agentchat.agents import CodeExecutorAgent\n# from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n\n    team = MagenticOneGroupChat([surfer], model_client=model_client)\n    await Console(team.run_stream(task=\"What is the UV index in Melbourne today?\"))\n\n    # # Note: you can also use  other agents in the team\n    # team = MagenticOneGroupChat([surfer, file_surfer, coder, terminal], model_client=model_client)\n    # file_surfer = FileSurfer( \"FileSurfer\",model_client=model_client)\n    # coder = MagenticOneCoderAgent(\"Coder\",model_client=model_client)\n    # terminal = CodeExecutorAgent(\"ComputerTerminal\",code_executor=LocalCommandLineCodeExecutor())\n\n\nasyncio.run(main())\n```\n\nOr, use the {py:class}`~autogen_ext.teams.magentic_one.MagenticOne` helper class\nwith all the agents bundled together:\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import ApprovalRequest, ApprovalResponse\n\n\ndef approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input before code execution.\"\"\"\n    print(f\"Code to execute:\\n{request.code}\")\n    user_input = input(\"Do you approve this code execution? (y/n): \").strip().lower()\n    if user_input == 'y':\n        return ApprovalResponse(approved=True, reason=\"User approved the code execution\")\n    else:\n        return ApprovalResponse(approved=False, reason=\"User denied the code execution\")\n\n\nasync def example_usage():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # Enable code execution approval for security\n    m1 = MagenticOne(client=client, approval_func=approval_func)\n    task = \"Write a Python script to fetch data from an API.\"\n    result = await Console(m1.run_stream(task=task))\n    print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```\n\n## Architecture\n\n![Autogen Magentic-One architecture](../../images/autogen-magentic-one-agents.png)\n\nMagentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan. This is illustrated in the figure above; the Orchestrator work is thus divided into an outer loop where it updates the Task Ledger and an inner loop to update the Progress Ledger.\n\nOverall, Magentic-One consists of the following agents:\n\n- Orchestrator: the lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed\n- WebSurfer: This is an LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming request, the WebSurfer performs an action on the browser then reports on the new state of the web page The action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search); web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). The WebSurfer relies on the accessibility tree of the browser and on set-of-marks prompting to perform its actions.\n- FileSurfer: This is an LLM-based agent that commands a markdown-based file preview application to read local files of most types. The FileSurfer can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.\n- Coder: This is an LLM-based agent specialized through its system prompt for writing code, analyzing information collected from the other agents, or creating new artifacts.\n- ComputerTerminal: Finally, ComputerTerminal provides the team with access to a console shell where the Coder\u2019s programs can be executed, and where new programming libraries can be installed.\n\nTogether, Magentic-One\u2019s agents provide the Orchestrator with the tools and capabilities that it needs to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.\n\nWhile the default multimodal LLM we use for all agents is GPT-4o, Magentic-One is model agnostic and can incorporate heterogonous models to support different capabilities or meet different cost requirements when getting tasks done. For example, it can use different LLMs and SLMs and their specialized versions to power different agents. We recommend a strong reasoning model for the Orchestrator agent such as GPT-4o. In a different configuration of Magentic-One, we also experiment with using OpenAI o1-preview for the outer loop of the Orchestrator and for the Coder, while other agents continue to use GPT-4o.\n\n## Citation\n\n```\n\n@misc{fourney2024magenticonegeneralistmultiagentsolving,\n      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},\n      author={Adam Fourney and Gagan Bansal and Hussein Mozannar and Cheng Tan and Eduardo Salinas and Erkang and Zhu and Friederike Niedtner and Grace Proebsting and Griffin Bassman and Jack Gerrits and Jacob Alber and Peter Chang and Ricky Loynd and Robert West and Victor Dibia and Ahmed Awadallah and Ece Kamar and Rafah Hosn and Saleema Amershi},\n      year={2024},\n      eprint={2411.04468},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2411.04468},\n}\n\n```",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/magentic-one.md"
  },
  {
    "code": false,
    "content": "# Memory and RAG\n\nMaintaining a dynamic _store_ of facts that can be incorporated into an agent's context before pivotal moments is essential in various applications. A common implementation of this concept is the Retrieval-Augmented Generation (RAG) pattern, where a query retrieves pertinent information from a database, enhancing the agent's context for its tasks.\n\nAgentChat offers the `Memory` protocol, part of the `autogen_core.memory` module, which can be extended to implement this functionality effectively. The critical methods associated with this protocol include:\n\n- **`add`**: Introduces new entries to the memory store.\n- **`query`**: Retrieves relevant data from the memory store.\n- **`update_context`**: Alters an agent's internal model context by incorporating the fetched information, particularly useful in the `AssistantAgent` class.\n- **`clear`**: Erases all entries from the memory store.\n- **`close`**: Cleans up any resources utilized by the memory store.\n\n## ListMemory Example\n\nThe `ListMemory` class serves as an illustrative implementation of the `Memory` protocol. It utilizes a list-based structure to maintain records in chronological order, constantly appending the most recent memories for the agent's context. This implementation is straightforward and predictable, making it user-friendly and easy to debug.\n\nIn the subsequent example, we will implement `ListMemory` to track user preferences, demonstrating how it provides consistent context for the agent\u2019s responses over time.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nIn this code, we are importing necessary classes and modules, which include our `AssistantAgent`, the console for UI display, and the `ListMemory` class for memory handling. \n\n```python\n# Initialize user memory\nuser_memory = ListMemory()\n\n# Add user preferences to memory\nawait user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\nawait user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n```\n\nHere, we instantiate a `ListMemory` object called `user_memory`. We then populate it with user preferences, such as requesting weather information in metric units and specifying dietary requirements (vegan meal recipes).\n\n```python\nasync def get_weather(city: str, units: str = \"imperial\") -> str:\n    if units == \"imperial\":\n        return f\"The weather in {city} is 73 \u00b0F and Sunny.\"\n    elif units == \"metric\":\n        return f\"The weather in {city} is 23 \u00b0C and Sunny.\"\n    else:\n        return f\"Sorry, I don't know the weather in {city}.\"\n```\n\nNext, we define an asynchronous function `get_weather` that returns temperature information based on the city and specified units (either imperial or metric). This serves as a tool our agent can utilize.\n\n```python\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\"),\n    tools=[get_weather],\n    memory=[user_memory],\n)\n```\n\nHere, we create an instance of `AssistantAgent`, linking it to the `OpenAIChatCompletionClient`. This agent is assigned specific tools, including the `get_weather` function and our previously defined `user_memory` for context retention.\n\n```python\n# Run the agent with a task.\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)\n```\n\nAt this point, we initiate the agent's task to inquire about the weather in New York, using the console to display the output. \n\n```python\nawait assistant_agent._model_context.get_messages()\n```\n\nFinally, the agent's context is examined to confirm that it has been updated correctly with the retrieved memory entries. The `transform` method formats these memory entries into a string, which is then used by the agent in conversation.\n\nSubsequently, if we were to ask about generating a meal plan, the agent can retrieve the relevant information from memory and respond accordingly.\n\n```python\nstream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\nawait Console(stream)\n```\n\nThis shows that the agent can integrate contextually relevant responses based on stored user preferences, maintaining a personalized interaction.\n\n## Custom Memory Stores (Vector DBs, etc.)\n\nYou can enhance the `Memory` protocol to build more intricate memory stores. For instance, custom memory stores can utilize vector databases for information storage and retrieval, or machine learning models to generate tailored responses based on user preferences.\n\nTo implement the desired functionality, you should overload the `add`, `query`, and `update_context` methods, integrating your specific memory store into the agent's configuration.\n\nThe following example memory stores are currently available within the `autogen_ext` extensions package:\n\n- **`ChromaDBVectorMemory`**: Utilizes a vector database for information storage and retrieval.\n- **`SentenceTransformerEmbeddingFunctionConfig`**: Configures the SentenceTransformer embedding function used in the `ChromaDBVectorMemory`.\n- **`RedisMemory`**: Employs a Redis vector database for information storage and retrieval.\n\n```python\nimport tempfile\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.chromadb import (\n    ChromaDBVectorMemory,\n    PersistentChromaDBVectorMemoryConfig,\n    SentenceTransformerEmbeddingFunctionConfig,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Use a temporary directory for ChromaDB persistence\nwith tempfile.TemporaryDirectory() as tmpdir:\n    chroma_user_memory = ChromaDBVectorMemory(\n        config=PersistentChromaDBVectorMemoryConfig(\n            collection_name=\"preferences\",\n            persistence_path=tmpdir,  # Temporary directory for storage\n            k=2,  # Return top k results\n            score_threshold=0.4,  # Minimum similarity score\n            embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n                model_name=\"all-MiniLM-L6-v2\"  # Default model for testing\n            ),\n        )\n    )\n```\n\nIn this section, we import necessary modules and create `ChromaDBVectorMemory` for storing user preferences in a temporary directory, setting the parameters for retrieving relevant results.\n\n```python\n    # Add user preferences to memory\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"The weather should be in metric units\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"units\"},\n        )\n    )\n\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"Meal recipe must be vegan\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n        )\n    )\n```\n\nIn this code, we add user preferences to the newly created `chroma_user_memory`. Each entry contains metadata for more contextual information.\n\n```python\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    )\n\n    # Create assistant agent with ChromaDB memory\n    assistant_agent = AssistantAgent(\n        name=\"assistant_agent\",\n        model_client=model_client,\n        tools=[get_weather],\n        memory=[chroma_user_memory],\n    )\n\n    stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\n    await model_client.close()\n    await chroma_user_memory.close()\n```\n\nFinally, we establish an instance of `AssistantAgent`, linking it to the `ChromaDB` memory store, and execute a task to fetch the weather. We also ensure to close the model client and memory after use.\n\n```python\nchroma_user_memory.dump_component().model_dump_json()\n```\n\nThis line indicates that it's possible to serialize the `ChromaDBVectorMemory` and save it to disk, promoting data persistence across sessions.\n\n### Redis Memory\n\nSimilarly, persistent memory storage can be achieved using Redis. Ensure that you have a functioning Redis instance to connect to.\n\nRefer to the `RedisMemory` class for guidelines on running Redis locally or via Docker.\n\n```python\nfrom logging import WARNING, getLogger\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.redis import RedisMemory, RedisMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nlogger = getLogger()\nlogger.setLevel(WARNING)\n\n# Initialize Redis memory\nredis_memory = RedisMemory(\n    config=RedisMemoryConfig(\n        redis_url=\"redis://localhost:6379\",\n        index_name=\"chat_history\",\n        prefix=\"memory\",\n    )\n)\n\n# Add user preferences to memory\nawait redis_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait redis_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n)\n\n# Create assistant agent with Redis memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    memory=[redis_memory],\n)\n\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)\n\nawait model_client.close()\nawait redis_memory.close()\n```\n\nIn this code block, we initialize `RedisMemory`, populate it with user preferences, and create an `AssistantAgent` that utilizes this memory store for context. The process concludes with a display of the weather inquiry.\n\n## RAG Agent: Putting It All Together\n\nThe Retrieval-Augmented Generation (RAG) pattern commonly used in developing AI systems involves two crucial phases:\n\n1. **Indexing**: The process of loading documents, segmenting them, and storing them in a vector database.\n2. **Retrieval**: The task of locating and utilizing relevant segments during conversation execution.\n\nThroughout previous segments, we handcrafted items into memory and provided them to our agents. In practical applications, the indexing process is typically automated, sourcing information from expansive document bases, such as documentation, internal files, or knowledge repositories.\n\n> **Note**: The effectiveness of a RAG system significantly relies on the quality of the chunking and retrieval methodologies (models, embeddings, etc.). Experimentation may be needed with advanced chunking and retrieval models for optimal results.\n\n### Building a Simple RAG Agent\n\nTo start, we will develop a basic document indexer designed to load documents, chunk them, and store them within a `ChromaDBVectorMemory` memory store.\n\n```python\nimport re\nfrom typing import List\n\nimport aiofiles\nimport aiohttp\nfrom autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n\n\nclass SimpleDocumentIndexer:\n    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n\n    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n        self.memory = memory\n        self.chunk_size = chunk_size\n\n    async def _fetch_content(self, source: str) -> str:\n        \"\"\"Fetch content from URL or file.\"\"\"\n        if source.startswith((\"http://\", \"https://\")):\n            async with aiohttp.ClientSession() as session:\n                async with session.get(source) as response:\n                    return await response.text()\n        else:\n            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n                return await f.read()\n\n    def _strip_html(self, text: str) -> str:\n        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n        text = re.sub(r\"<[^>]*>\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n\n    def _split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into fixed-size chunks.\"\"\"\n        chunks: List[str] = []\n        # Split text into fixed-size chunks\n        for i in range(0, len(text), self.chunk_size):\n            chunk = text[i : i + self.chunk_size]\n            chunks.append(chunk.strip())\n        return chunks\n\n    async def index_documents(self, sources: List[str]) -> int:\n        \"\"\"Index documents into memory.\"\"\"\n        total_chunks = 0\n\n        for source in sources:\n            try:\n                content = await self._fetch_content(source)\n\n                # Strip HTML if content appears to be HTML\n                if \"<\" in content and \">\" in content:\n                    content = self._strip_html(content)\n\n                chunks = self._split_text(content)\n\n                for i, chunk in enumerate(chunks):\n                    await self.memory.add(\n                        MemoryContent(\n                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n                        )\n                    )\n\n                total_chunks += len(chunks)\n\n            except Exception as e:\n                print(f\"Error indexing {source}: {str(e)}\")\n\n        return total_chunks\n```\n\nIn the example above, we define a simple document indexer. The class handles fetching content from URLs or files, stripping HTML tags, and dividing text into manageable chunks for storage in memory.\n\nNow we'll utilize this indexer along with `ChromaDBVectorMemory` to build a complete RAG agent:\n\n```python\nimport os\nfrom pathlib import Path\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize vector memory\nrag_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"autogen_docs\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=3,  # Return top 3 results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\nawait rag_memory.clear()  # Clear any existing memory\n\n# Index AutoGen documentation\nasync def index_autogen_docs() -> None:\n    indexer = SimpleDocumentIndexer(memory=rag_memory)\n    sources = [\n        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n    ]\n    chunks: int = await indexer.index_documents(sources)\n    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n\nawait index_autogen_docs()\n```\n\nIn this section, we initialize the vector memory and index AutoGen documentation using our `SimpleDocumentIndexer`. This setup prepares the RAG agent to access indexed knowledge.\n\n```python\n# Create our RAG assistant agent\nrag_assistant = AssistantAgent(\n    name=\"rag_assistant\", model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"), memory=[rag_memory]\n)\n\n# Ask questions about AutoGen\nstream = rag_assistant.run_stream(task=\"What is AgentChat?\")\nawait Console(stream)\n\n# Remember to close the memory when done\nawait rag_memory.close()\n```\n\nAfter creating the `rag_assistant` and linking it to the `ChromaDB` memory, we can query about AutoGen, demonstrating the retrieval and augmentation capabilities of our agent. Post-query, we also ensure to close the memory to release resources.\n\n## Mem0Memory Example\n\nThe `Mem0Memory` class integrates the memory system from `Mem0.ai`, accommodating both cloud and local backends. It offers symbolic memory capabilities for agents, ensuring proper retrieval and context updates.\n\nIn the next example, we'll demonstrate how to employ `Mem0Memory` to sustain persistent memories across conversations:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.mem0 import Mem0Memory\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize Mem0 cloud memory (requires API key)\n# For local deployment, use is_cloud=False with appropriate config\nmem0_memory = Mem0Memory(\n    is_cloud=True,\n    limit=5,  # Maximum number of memories to retrieve\n)\n\n# Add user preferences to memory\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\n# Create assistant with mem0 memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o-2024-08-06\",\n    ),\n    tools=[get_weather],\n    memory=[mem0_memory],\n)\n\n# Ask about the weather\nstream = assistant_agent.run_stream(task=\"What are my dietary preferences?\")\nawait Console(stream)\n```\n\nThis code demonstrates the setup of the `Mem0Memory` system for storing and retrieving user preferences. The setup emphasizes maintaining context across interactions.\n\nThe use of `Mem0Memory` is ideal for long-term agent deployment scenarios, requiring advanced controls over memory management, analytics, and unified processing across teams or systems.\n\n```python\n# Serialize the memory configuration\nconfig_json = mem0_memory.dump_component().model_dump_json()\nprint(f\"Memory config JSON: {config_json[:100]}...\")\n```\n\nThis line highlights the serialization capabilities of `Mem0Memory`, allowing for memory configurations to be saved in JSON format for easier management and retrieval in the future.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/memory.ipynb"
  },
  {
    "content": "# Migration Guide for v0.2 to v0.4\n\nThis is a migration guide for users of the `v0.2.*` versions of `autogen-agentchat`\nto the `v0.4` version, which introduces a new set of APIs and features.\nThe `v0.4` version contains breaking changes. Please read this guide carefully.\nWe still maintain the `v0.2` version in the `0.2` branch; however,\nwe highly recommend you upgrade to the `v0.4` version.\n\n```{note}\nWe no longer have admin access to the `pyautogen` PyPI package, and\nthe releases from that package are no longer from Microsoft since version 0.2.34.\nTo continue use the `v0.2` version of AutoGen, install it using `autogen-agentchat~=0.2`.\nPlease read our [clarification statement](https://github.com/microsoft/autogen/discussions/4217) regarding forks.\n```\n\n## What is `v0.4`?\n\nSince the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen `v0.4`, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale.\n\nThe `v0.4` API is layered:\nthe [Core API](../core-user-guide/index.md) is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows;\nthe [AgentChat API](./index.md) is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen `v0.2`.\n\nMost of this guide focuses on `v0.4`'s AgentChat API; however, you can also build your own high-level framework using just the Core API.\n\n## New to AutoGen?\n\nJump straight to the [AgentChat Tutorial](./tutorial/models.ipynb) to get started with `v0.4`.\n\n## What's in this guide?\n\nWe provide a detailed guide on how to migrate your existing codebase from `v0.2` to `v0.4`.\n\nSee each feature below for detailed information on how to migrate.\n\n- [Migration Guide for v0.2 to v0.4](#migration-guide-for-v02-to-v04)\n  - [What is `v0.4`?](#what-is-v04)\n  - [New to AutoGen?](#new-to-autogen)\n  - [What's in this guide?](#whats-in-this-guide)\n  - [Model Client](#model-client)\n    - [Use component config](#use-component-config)\n    - [Use model client class directly](#use-model-client-class-directly)\n  - [Model Client for OpenAI-Compatible APIs](#model-client-for-openai-compatible-apis)\n  - [Model Client Cache](#model-client-cache)\n  - [Assistant Agent](#assistant-agent)\n  - [Multi-Modal Agent](#multi-modal-agent)\n  - [User Proxy](#user-proxy)\n  - [RAG Agent](#rag-agent)\n  - [Conversable Agent and Register Reply](#conversable-agent-and-register-reply)\n  - [Save and Load Agent State](#save-and-load-agent-state)\n  - [Two-Agent Chat](#two-agent-chat)\n  - [Tool Use](#tool-use)\n  - [Chat Result](#chat-result)\n  - [Conversion between v0.2 and v0.4 Messages](#conversion-between-v02-and-v04-messages)\n  - [Group Chat](#group-chat)\n  - [Group Chat with Resume](#group-chat-with-resume)\n  - [Save and Load Group Chat State](#save-and-load-group-chat-state)\n  - [Group Chat with Tool Use](#group-chat-with-tool-use)\n  - [Group Chat with Custom Selector (Stateflow)](#group-chat-with-custom-selector-stateflow)\n  - [Nested Chat](#nested-chat)\n  - [Sequential Chat](#sequential-chat)\n  - [GPTAssistantAgent](#gptassistantagent)\n  - [Long Context Handling](#long-context-handling)\n  - [Observability and Control](#observability-and-control)\n  - [Code Executors](#code-executors)\n\nThe following features currently in `v0.2`\nwill be provided in the future releases of `v0.4.*` versions:\n\n- Model Client Cost [#4835](https://github.com/microsoft/autogen/issues/4835)\n- Teachable Agent\n- RAG Agent\n\nWe will update this guide when the missing features become available.\n\n## Model Client\n\nIn `v0.2` you configure the model client as follows, and create the `OpenAIWrapper` object.\n\n```python\nfrom autogen.oai import OpenAIWrapper\n\nconfig_list = [\n    {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"},\n    {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"},\n]\n\nmodel_client = OpenAIWrapper(config_list=config_list)\n```\n\n> **Note**: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen.\n\nIn `v0.4`, we offer two ways to create a model client.\n\n### Use component config\n\nAutoGen 0.4 has a [generic component configuration system](../core-user-guide/framework/component-config.ipynb). Model clients are a great use case for this. See below for how to create an OpenAI chat completion client.\n\n```python\n\nfrom autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"OpenAIChatCompletionClient\",\n    \"config\": {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"sk-xxx\" # os.environ[\"...']\n    }\n}\n\nmodel_client = ChatCompletionClient.load_component(config)\n```\n\n### Use model client class directly\n\nOpen AI:\n\n```python\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\")\n```\n\nAzure OpenAI:\n\n```python\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nmodel_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\",\n    model=\"gpt-4o\",\n    api_version=\"2024-09-01-preview\",\n    api_key=\"sk-xxx\",\n)\n```\n\nRead more on {py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient`.\n\n## Model Client for OpenAI-Compatible APIs\n\nYou can use a the `OpenAIChatCompletionClient` to connect to an OpenAI-Compatible API,\nbut you need to specify the `base_url` and `model_info`.\n\n```python\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ncustom_model_client = OpenAIChatCompletionClient(\n    model=\"custom-model-name\",\n    base_url=\"https://custom-model.com/reset/of/the/path\",\n    api_key=\"placeholder\",\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"family\": \"unknown\",\n        \"structured_output\": True,\n    },\n)\n```\n\n> **Note**: We don't test all the OpenAI-Compatible APIs, and many of them\n> works differently from the OpenAI API even though they may claim to suppor it.\n> Please test them before using them.\n\nRead about [Model Clients](./tutorial/models.ipynb)\nin AgentChat Tutorial and more detailed information on [Core API Docs](../core-user-guide/components/model-clients.ipynb)\n\nSupport for other hosted models will be added in the future.\n\n## Model Client Cache\n\nIn `v0.2`, you can set the cache seed through the `cache_seed` parameter in the LLM config.\nThe cache is enabled by default.\n\n```python\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n    \"cache_seed\": 42,\n}\n```\n\nIn `v0.4`, the cache is not enabled by default, to use it you need to use a\n{py:class}`~autogen_ext.models.cache.ChatCompletionCache` wrapper around the model client.\n\nYou can use a {py:class}`~autogen_ext.cache_store.diskcache.DiskCacheStore` or {py:class}`~autogen_ext.cache_store.redis.RedisStore` to store the cache.\n\n```bash\npip install -U \"autogen-ext[openai, diskcache, redis]\"\n```\n\nHere's an example of using `diskcache` for local caching:\n\n```python\nimport asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n        await openai_model_client.close()\n\n\nasyncio.run(main())\n```\n\n## Assistant Agent\n\nIn `v0.2`, you create an assistant agent as follows:\n\n```python\nfrom autogen.agentchat import AssistantAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n)\n```\n\nIn `v0.4`, it is similar, but you need to specify `model_client` instead of `llm_config`.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0)\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    model_client=model_client,\n)\n```\n\nHowever, the usage is somewhat different. In `v0.4`, instead of calling `assistant.send`,\nyou call `assistant.on_messages` or `assistant.on_messages_stream` to handle incoming messages.\nFurthermore, the `on_messages` and `on_messages_stream` methods are asynchronous,\nand the latter returns an async generator to stream the inner thoughts of the agent.\n\nHere is how you can call the assistant agent in `v0.4` directly, continuing from the above example:\n\n```python\nimport asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())\n```\n\nThe {py:class}`~autogen_core.CancellationToken` can be used to cancel the request asynchronously\nwhen you call `cancellation_token.cancel()`, which will cause the `await`\non the `on_messages` call to raise a `CancelledError`.\n\nRead more on [Agent Tutorial](./tutorial/agents.ipynb)\nand {py:class}`~autogen_agentchat.agents.AssistantAgent`.\n\n## Multi-Modal Agent\n\nThe {py:class}`~autogen_agentchat.agents.AssistantAgent` in `v0.4` supports multi-modal inputs if the model client supports it.\nThe `vision` capability of the model client is used to determine if the agent supports multi-modal inputs.\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken, Image\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    message = MultiModalMessage(\n        content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))],\n        source=\"user\",\n    )\n    response = await assistant.on_messages([message], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n## User Proxy\n\nIn `v0.2`, you create a user proxy as follows:\n\n```python\nfrom autogen.agentchat import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config=False,\n    llm_config=False,\n)\n```\n\nThis user proxy would take input from the user through console, and would terminate\nif the incoming message ends with \"TERMINATE\".\n\nIn `v0.4`, a user proxy is simply an agent that takes user input only, there is no\nother special configuration needed. You can create a user proxy as follows:\n\n```python\nfrom autogen_agentchat.agents import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\"user_proxy\")\n```\n\nSee {py:class}`~autogen_agentchat.agents.UserProxyAgent`\nfor more details and how to customize the input function with timeout.\n\n## RAG Agent\n\nIn `v0.2`, there was the concept of teachable agents as well as a RAG agents that could take a database config.\n\n```python\nteachable_agent = ConversableAgent(\n    name=\"teachable_agent\",\n    llm_config=llm_config\n)\n\n# Instantiate a Teachability object. Its parameters are all optional.\nteachability = Teachability(\n    reset_db=False,\n    path_to_db_dir=\"./tmp/interactive/teachability_db\"\n)\n\nteachability.add_to_agent(teachable_agent)\n```\n\nIn `v0.4`, you can implement a RAG agent using the {py:class}`~autogen_core.memory.Memory` class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the [Memory](memory.ipynb) tutorial for more details.\n\nThis clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the `Memory` class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call `memory.add` for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store.\n\n```python\n\n# ...\n# example of a ChromaDBVectorMemory class\nchroma_user_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"preferences\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=2,  # Return top  k results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\n# you can add logic such as a document indexer that adds content to the memory store\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    ),\n    tools=[get_weather],\n    memory=[chroma_user_memory],\n)\n```\n\n## Conversable Agent and Register Reply\n\nIn `v0.2`, you can create a conversable agent and register a reply function as follows:\n\n```python\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nfrom autogen.agentchat import ConversableAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nconversable_agent = ConversableAgent(\n    name=\"conversable_agent\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n    code_execution_config={\"work_dir\": \"coding\"},\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n)\n\ndef reply_func(\n    recipient: ConversableAgent,\n    messages: Optional[List[Dict]] = None,\n    sender: Optional[Agent] = None,\n    config: Optional[Any] = None,\n) -> Tuple[bool, Union[str, Dict, None]]:\n    # Custom reply logic here\n    return True, \"Custom reply\"\n\n# Register the reply function\nconversable_agent.register_reply([ConversableAgent], reply_func, position=0)\n\n# NOTE: An async reply function will only be invoked with async send.\n```\n\nRather than guessing what the `reply_func` does, all its parameters,\nand what the `position` should be, in `v0.4`, we can simply create a custom agent\nand implement the `on_messages`, `on_reset`, and `produced_message_types` methods.\n\n```python\nfrom typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CustomAgent(BaseChatAgent):\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n```\n\nYou can then use the custom agent in the same way as the {py:class}`~autogen_agentchat.agents.AssistantAgent`.\nSee [Custom Agent Tutorial](custom-agents.ipynb)\nfor more details.\n\n## Save and Load Agent State\n\nIn `v0.2` there is no built-in way to save and load an agent's state: you need\nto implement it yourself by exporting the `chat_messages` attribute of `ConversableAgent`\nand importing it back through the `chat_messages` parameter.\n\nIn `v0.4`, you can call `save_state` and `load_state` methods on agents to save and load their state.\n\n```python\nimport asyncio\nimport json\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Save the state.\n    state = await assistant.save_state()\n\n    # (Optional) Write state to disk.\n    with open(\"assistant_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # (Optional) Load it back from disk.\n    with open(\"assistant_state.json\", \"r\") as f:\n        state = json.load(f)\n        print(state) # Inspect the state, which contains the chat history.\n\n    # Carry on the chat.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Load the state, resulting the agent to revert to the previous state before the last message.\n    await assistant.load_state(state)\n\n    # Carry on the same chat again.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\nYou can also call `save_state` and `load_state` on any teams, such as {py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`\nto save and load the state of the entire team.\n\n## Two-Agent Chat\n\nIn `v0.2`, you can create a two-agent chat for code execution as follows:\n\n```python\nfrom autogen.coding import LocalCommandLineCodeExecutor\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")},\n    llm_config=False,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nchat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\")\n# Intermediate messages are printed to the console directly.\nprint(chat_result)\n```\n\nTo get the same behavior in `v0.4`, you can use the {py:class}`~autogen_agentchat.agents.AssistantAgent`\nand {py:class}`~autogen_agentchat.agents.CodeExecutorAgent` together in a {py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`.\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    code_executor = CodeExecutorAgent(\n        name=\"code_executor\",\n        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n    )\n\n    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n\n    # The group chat will alternate between the assistant and the code executor.\n    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    \n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n## Tool Use\n\nIn `v0.2`, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool.\nYou need to initiate a two-agent chat for every user request.\n\n```python\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent, register_function\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\ntool_caller = AssistantAgent(\n    name=\"tool_caller\",\n    system_message=\"You are a helpful assistant. You can call tools to help user.\",\n    llm_config=llm_config,\n    max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.\n)\n\ntool_executor = UserProxyAgent(\n    name=\"tool_executor\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    llm_config=False,\n)\n\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\n# Register the tool function to the tool caller and executor.\nregister_function(get_weather, caller=tool_caller, executor=tool_executor)\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input == \"exit\":\n        break\n    chat_result = tool_executor.initiate_chat(\n        tool_caller,\n        message=user_input,\n        summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n    )\n    print(\"Assistant:\", chat_result.summary)\n```\n\nIn `v0.4`, you really just need one agent -- the {py:class}`~autogen_agentchat.agents.AssistantAgent` -- to handle\nboth the tool calling and tool execution.\n\n```python\nimport asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\ndef get_weather(city: str) -> str: # Async tool is possible too.\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n        model_client=model_client,\n        tools=[get_weather],\n        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    await model_client.close()\n\nasyncio.run(main())\n```\n\nWhen using tool-equipped agents inside a group chat such as\n{py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`,\nyou simply do the same as above to add tools to the agents, and create a\ngroup chat with the agents.\n\n## Chat Result\n\nIn `v0.2`, you get a `ChatResult` object from the `initiate_chat` method.\nFor example:\n\n```python\nchat_result = tool_executor.initiate_chat(\n    tool_caller,\n    message=user_input,\n    summary_method=\"reflection_with_llm\",\n)\nprint(chat_result.summary) # Get LLM-reflected summary of the chat.\nprint(chat_result.chat_history) # Get the chat history.\nprint(chat_result.cost) # Get the cost of the chat.\nprint(chat_result.human_input) # Get the human input solicited by the chat.\n```\n\nSee [ChatResult Docs](https://microsoft.github.io/autogen/0.2/docs/reference/agentchat/chat#chatresult)\nfor more details.\n\nIn `v0.4`, you get a {py:class}`~autogen_agentchat.base.TaskResult` object from a `run` or `run_stream` method.\nThe {py:class}`~autogen_agentchat.base.TaskResult` object contains the `messages` which is the message history\nof the chat, including both agents' private (tool calls, etc.) and public messages.\n\nThere are some notable differences between {py:class}`~autogen_agentchat.base.TaskResult` and `ChatResult`:\n\n- The `messages` list in {py:class}`~autogen_agentchat.base.TaskResult` uses different message format than the `ChatResult.chat_history` list.\n- There is no `summary` field. It is up to the application to decide how to summarize the chat using the `messages` list.\n- `human_input` is not provided in the {py:class}`~autogen_agentchat.base.TaskResult` object, as the user input can be extracted from the `messages` list by filtering with the `source` field.\n- `cost` is not provided in the {py:class}`~autogen_agentchat.base.TaskResult` object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See [community extensions](../extensions-user-guide/discover.md).\n\n## Conversion between v0.2 and v0.4 Messages\n\nYou can use the following conversion functions to convert between a v0.4 message in\n{py:attr}`autogen_agentchat.base.TaskResult.messages` and a v0.2 message in `ChatResult.chat_history`.\n\n```python\nfrom typing import Any, Dict, List, Literal\n\nfrom autogen_agentchat.messages import (\n    BaseAgentEvent,\n    BaseChatMessage,\n    HandoffMessage,\n    MultiModalMessage,\n    StopMessage,\n    TextMessage,\n    ToolCallExecutionEvent,\n    ToolCallRequestEvent,\n    ToolCallSummaryMessage,\n)\nfrom autogen_core import FunctionCall, Image\nfrom autogen_core.models import FunctionExecutionResult\n\n\ndef convert_to_v02_message(\n    message: BaseAgentEvent | BaseChatMessage,\n    role: Literal[\"assistant\", \"user\", \"tool\"],\n    image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\",\n) -> Dict[str, Any]:\n    \"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n\n    Args:\n        message (BaseAgentEvent | BaseChatMessage): The message to convert.\n        role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n        image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n\n    Returns:\n        Dict[str, Any]: The converted AutoGen v0.2 message.\n    \"\"\"\n    v02_message: Dict[str, Any] = {}\n    if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):\n        v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source}\n    elif isinstance(message, MultiModalMessage):\n        v02_message = {\"content\": [], \"role\": role, \"name\": message.source}\n        for modal in message.content:\n            if isinstance(modal, str):\n                v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal})\n            elif isinstance(modal, Image):\n                v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail))\n            else:\n                raise ValueError(f\"Invalid multimodal message content: {modal}\")\n    elif isinstance(message, ToolCallRequestEvent):\n        v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source}\n        for tool_call in message.content:\n            v02_message[\"tool_calls\"].append(\n                {\n                    \"id\": tool_call.id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments},\n                }\n            )\n    elif isinstance(message, ToolCallExecutionEvent):\n        tool_responses: List[Dict[str, str]] = []\n        for tool_result in message.content:\n            tool_responses.append(\n                {\n                    \"tool_call_id\": tool_result.call_id,\n                    \"role\": \"tool\",\n                    \"content\": tool_result.content,\n                }\n            )\n        content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses])\n        v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content}\n    else:\n        raise ValueError(f\"Invalid message type: {type(message)}\")\n    return v02_message\n\n\ndef convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage:\n    \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n    if \"tool_calls\" in message:\n        tool_calls: List[FunctionCall] = []\n        for tool_call in message[\"tool_calls\"]:\n            tool_calls.append(\n                FunctionCall(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"function\"][\"name\"],\n                    arguments=tool_call[\"function\"][\"args\"],\n                )\n            )\n        return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls)\n    elif \"tool_responses\" in message:\n        tool_results: List[FunctionExecutionResult] = []\n        for tool_response in message[\"tool_responses\"]:\n            tool_results.append(\n                FunctionExecutionResult(\n                    call_id=tool_response[\"tool_call_id\"],\n                    content=tool_response[\"content\"],\n                    is_error=False,\n                    name=tool_response[\"name\"],\n                )\n            )\n        return ToolCallExecutionEvent(source=\"tools\", content=tool_results)\n    elif isinstance(message[\"content\"], list):\n        content: List[str | Image] = []\n        for modal in message[\"content\"]:  # type: ignore\n            if modal[\"type\"] == \"text\":  # type: ignore\n                content.append(modal[\"text\"])  # type: ignore\n            else:\n                content.append(Image.from_uri(modal[\"image_url\"][\"url\"]))  # type: ignore\n        return MultiModalMessage(content=content, source=message[\"name\"])\n    elif isinstance(message[\"content\"], str):\n        return TextMessage(content=message[\"content\"], source=message[\"name\"])\n    else:\n        raise ValueError(f\"Unable to convert message: {message}\")\n```\n\n## Group Chat\n\nIn `v0.2`, you need to create a `GroupChat` class and pass it into a\n`GroupChatManager`, and have a participant that is a user proxy to initiate the chat.\nFor a simple scenario of a writer and a critic, you can do the following:\n\n```python\nfrom autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nwriter = AssistantAgent(\n    name=\"writer\",\n    description=\"A writer.\",\n    system_message=\"You are a writer.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"),\n)\n\ncritic = AssistantAgent(\n    name=\"critic\",\n    description=\"A critic.\",\n    system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n    llm_config=llm_config,\n)\n\n# Create a group chat with the writer and critic.\ngroupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)\n\n# Create a group chat manager to manage the group chat, use round-robin selection method.\nmanager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\")\n\n# Initiate the chat with the editor, intermediate messages are printed to the console directly.\nresult = editor.initiate_chat(\n    manager,\n    message=\"Write a short story about a robot that discovers it has feelings.\",\n)\nprint(result.summary)\n```\n\nIn `v0.4`, you can use the {py:class}`~autogen_agentchat.teams.RoundRobinGroupChat` to achieve the same behavior.\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\nFor LLM-based speaker selection, you can use the {py:class}`~autogen_agentchat.teams.SelectorGroupChat` instead.\nSee [Selector Group Chat Tutorial](./selector-group-chat.ipynb)\nand {py:class}`~autogen_agentchat.teams.SelectorGroupChat` for more details.\n\n> **Note**: In `v0.4`, you do not need to register functions on a user proxy to use tools\n> in a group chat. You can simply pass the tool functions to the {py:class}`~autogen_agentchat.agents.AssistantAgent` as shown in the [Tool Use](#tool-use) section.\n> The agent will automatically call the tools when needed.\n> If your tool doesn't output well formed response, you can use the `reflect_on_tool_use` parameter to have the model reflect on the tool use.\n\n## Group Chat with Resume\n\nIn `v0.2`, group chat with resume is a bit complicated. You need to explicitly\nsave the group chat messages and load them back when you want to resume the chat.\nSee [Resuming Group Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/resuming_groupchat) for more details.\n\nIn `v0.4`, you can simply call `run` or `run_stream` again with the same group chat object to resume the chat. To export and load the state, you can use\n`save_state` and `load_state` methods.\n\n```python\nimport asyncio\nimport json\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> RoundRobinGroupChat:\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)\n\n    return group_chat\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    # Create team.\n    group_chat = create_team(model_client)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n\n    # Save the state of the group chat and all participants.\n    state = await group_chat.save_state()\n    with open(\"group_chat_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # Create a new team with the same participants configuration.\n    group_chat = create_team(model_client)\n\n    # Load the state of the group chat and all participants.\n    with open(\"group_chat_state.json\", \"r\") as f:\n        state = json.load(f)\n    await group_chat.load_state(state)\n\n    # Resume the chat.\n    stream = group_chat.run_stream(task=\"Translate the story into Chinese.\")\n    await Console(stream)\n\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n## Save and Load Group Chat State\n\nIn `v0.2`, you need to explicitly save the group chat messages and load them back when you want to resume the chat.\n\nIn `v0.4`, you can simply call `save_state` and `load_state` methods on the group chat object.\nSee [Group Chat with Resume](#group-chat-with-resume) for an example.\n\n## Group Chat with Tool Use\n\nIn `v0.2` group chat, when tools are involved, you need to register the tool functions on a user proxy,\nand include the user proxy in the group chat. The tool calls made by other agents\nwill be routed to the user proxy to execute.\n\nWe have observed numerous issues with this approach, such as the the tool call\nrouting not working as expected, and the tool call request and result cannot be\naccepted by models without support for function calling.\n\nIn `v0.4`, there is no need to register the tool functions on a user proxy,\nas the tools are directly executed within the {py:class}`~autogen_agentchat.agents.AssistantAgent`,\nwhich publishes the response from the tool to the group chat.\nSo the group chat manager does not need to be involved in routing tool calls.\n\nSee [Selector Group Chat Tutorial](./selector-group-chat.ipynb) for an example\nof using tools in a group chat.\n\n## Group Chat with Custom Selector (Stateflow)\n\nIn `v0.2` group chat, when the `speaker_selection_method` is set to a custom function,\nit can override the default selection method. This is useful for implementing\na state-based selection method.\nFor more details, see [Custom Sepaker Selection in v0.2](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection).\n\nIn `v0.4`, you can use the {py:class}`~autogen_agentchat.teams.SelectorGroupChat` with `selector_func` to achieve the same behavior.\nThe `selector_func` is a function that takes the current message thread of the group chat\nand returns the next speaker's name. If `None` is returned, the LLM-based\nselection method will be used.\n\nHere is an example of using the state-based selection method to implement\na web search/analysis scenario.\n\n```python\nimport asyncio\nfrom typing import Sequence\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Note: This example uses mock tools instead of real APIs for demonstration purposes\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat:\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Web search agent: Searches for information\n            Data analyst: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"A web search agent.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"A data analyst agent. Useful for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        \"\"\",\n    )\n\n    # The termination condition is a combination of text mention termination and max message termination.\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    # The selector function is a function that takes the current message thread of the group chat\n    # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used.\n    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n        if messages[-1].source != planning_agent.name:\n            return planning_agent.name # Always return to the planning agent after the other agents have spoken.\n        return None\n\n    team = SelectorGroupChat(\n        [planning_agent, web_search_agent, data_analyst_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector.\n        termination_condition=termination,\n        selector_func=selector_func,\n    )\n    return team\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    team = create_team(model_client)\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    await Console(team.run_stream(task=task))\n\nasyncio.run(main())\n```\n\n## Nested Chat\n\nNested chat allows you to nest a whole team or another agent inside\nan agent. This is useful for creating a hierarchical structure of agents\nor \"information silos\", as the nested agents cannot communicate directly\nwith other agents outside of the same group.\n\nIn `v0.2`, nested chat is supported by using the `register_nested_chats` method\non the `ConversableAgent` class.\nYou need to specify the nested sequence of agents using dictionaries,\nSee [Nested Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/tutorial/conversation-patterns#nested-chats)\nfor more details.\n\nIn `v0.4`, nested chat is an implementation detail of a custom agent.\nYou can create a custom agent that takes a team or another agent as a parameter\nand implements the `on_messages` method to trigger the nested team or agent.\nIt is up to the application to decide how to pass or transform the messages from\nand to the nested team or agent.\n\nThe following example shows a simple nested chat that counts numbers.\n\n```python\nimport asyncio\nfrom typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CountingAgent(BaseChatAgent):\n    \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\"\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        if len(messages) == 0:\n            last_number = 0 # Start from 0 if no messages are given.\n        else:\n            assert isinstance(messages[-1], TextMessage)\n            last_number = int(messages[-1].content) # Otherwise, start from the last number.\n        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nclass NestedCountingAgent(BaseChatAgent):\n    \"\"\"An agent that increments the last number in the input messages\n    multiple times using a nested counting team.\"\"\"\n    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:\n        super().__init__(name, description=\"An agent that counts numbers.\")\n        self._counting_team = counting_team\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Run the inner team with the given messages and returns the last message produced by the team.\n        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)\n        # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.\n        assert isinstance(result.messages[-1], TextMessage)\n        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        # Reset the inner team.\n        await self._counting_team.reset()\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nasync def main() -> None:\n    # Create a team of two counting agents as the inner team.\n    counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\")\n    counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\")\n    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)\n    # Create a nested counting agent that takes the inner team as a parameter.\n    nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team)\n    # Run the nested counting agent with a message starting from 1.\n    response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken())\n    assert response.inner_messages is not None\n    for message in response.inner_messages:\n        print(message)\n    print(response.chat_message)\n\nasyncio.run(main())\n```\n\nYou should see the following output:\n\n```bash\nsource='counting_agent_1' models_usage=None content='2' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='3' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='4' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='5' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='6' type='TextMessage'\n```\n\nYou can take a look at {py:class}`~autogen_agentchat.agents.SocietyOfMindAgent`\nfor a more complex implementation.\n\n## Sequential Chat\n\nIn `v0.2`, sequential chat is supported by using the `initiate_chats` function.\nIt takes input a list of dictionary configurations for each step of the sequence.\nSee [Sequential Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/tutorial/conversation-patterns#sequential-chats)\nfor more details.\n\nBase on the feedback from the community, the `initiate_chats` function\nis too opinionated and not flexible enough to support the diverse set of scenarios that\nusers want to implement. We often find users struggling to get the `initiate_chats` function\nto work when they can easily glue the steps together usign basic Python code.\nTherefore, in `v0.4`, we do not provide a built-in function for sequential chat in the AgentChat API.\n\nInstead, you can create an event-driven sequential workflow using the Core API,\nand use the other components provided the AgentChat API to implement each step of the workflow.\nSee an example of sequential workflow in the [Core API Tutorial](../core-user-guide/design-patterns/sequential-workflow.ipynb).\n\nWe recognize that the concept of workflow is at the heart of many applications,\nand we will provide more built-in support for workflows in the future.\n\n## GPTAssistantAgent\n\nIn `v0.2`, `GPTAssistantAgent` is a special agent class that is backed by the OpenAI Assistant API.\n\nIn `v0.4`, the equivalent is the {py:class}`~autogen_ext.agents.openai.OpenAIAssistantAgent` class.\nIt supports the same set of features as the `GPTAssistantAgent` in `v0.2` with\nmore such as customizable threads and file uploads.\nSee {py:class}`~autogen_ext.agents.openai.OpenAIAssistantAgent` for more details.\n\n## Long Context Handling\n\nIn `v0.2`, long context that overflows the model's context window can be handled\nby using the `transforms` capability that is added to an `ConversableAgent`\nafter which is contructed.\n\nThe feedbacks from our community has led us to believe this feature is essential\nand should be a built-in component of {py:class}`~autogen_agentchat.agents.AssistantAgent`, and can be used for\nevery custom agent.\n\nIn `v0.4`, we introduce the {py:class}`~autogen_core.model_context.ChatCompletionContext` base class that manages\nmessage history and provides a virtual view of the history. Applications can use\nbuilt-in implementations such as {py:class}`~autogen_core.model_context.BufferedChatCompletionContext` to\nlimit the message history sent to the model, or provide their own implementations\nthat creates different virtual views.\n\nTo use {py:class}`~autogen_core.model_context.BufferedChatCompletionContext` in an {py:class}`~autogen_agentchat.agents.AssistantAgent` in a chatbot scenario.\n\n```python\nimport asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n        model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    \n    await model_client.close()\n\nasyncio.run(main())\n```\n\nIn this example, the chatbot can only read the last 10 messages in the history.\n\n## Observability and Control\n\nIn `v0.4` AgentChat, you can observe the agents by using the `on_messages_stream` method\nwhich returns an async generator to stream the inner thoughts and actions of the agent.\nFor teams, you can use the `run_stream` method to stream the inner conversation among the agents in the team.\nYour application can use these streams to observe the agents and teams in real-time.\n\nBoth the `on_messages_stream` and `run_stream` methods takes a {py:class}`~autogen_core.CancellationToken` as a parameter\nwhich can be used to cancel the output stream asynchronously and stop the agent or team.\nFor teams, you can also use termination conditions to stop the team when a certain condition is met.\nSee [Termination Condition Tutorial](./tutorial/termination.ipynb)\nfor more details.\n\nUnlike the `v0.2` which comes with a special logging module, the `v0.4` API\nsimply uses Python's `logging` module to log events such as model client calls.\nSee [Logging](../core-user-guide/framework/logging.md)\nin the Core API documentation for more details.\n\n## Code Executors\n\nThe code executors in `v0.2` and `v0.4` are nearly identical except\nthe `v0.4` executors support async API. You can also use\n{py:class}`~autogen_core.CancellationToken` to cancel a code execution if it takes too long.\nSee [Command Line Code Executors Tutorial](../core-user-guide/components/command-line-code-executors.ipynb)\nin the Core API documentation.\n\nWe also added `ACADynamicSessionsCodeExecutor` that can use Azure Container Apps (ACA)\ndynamic sessions for code execution.\nSee [ACA Dynamic Sessions Code Executor Docs](../extensions-user-guide/azure-container-code-executor.ipynb).",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/migration-guide.md"
  },
  {
    "code": false,
    "content": "# Quickstart\n\nThis documentation provides a quick introduction to building applications with AgentChat, utilizing preset agents. In this section, we'll walk through the steps to create a simple agent capable of interacting with tools, specifically a weather tool.\n\n## Installation\n\nBefore diving into the code, the first step is to install the required packages for AgentChat and its extensions. This includes the core library and the necessary models for functionality.\n\n```python\npip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\"\n```\n\nThis installation command ensures you have the latest version of the AgentChat package alongside the extensions for OpenAI and Azure. If you wish to use models other than OpenAI's, you can update the `model_client` configuration as described in the **Models** section of the documentation. For detailed instructions on utilizing Azure OpenAI models, refer to the guide [here](./tutorial/models.ipynb#azure-openai).\n\n## Importing Libraries\n\nAfter installing the packages, the next step involves importing the necessary classes and functions to set up your agent and tools.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nIn this block, we import:\n- `AssistantAgent`: The core class for creating an agent.\n- `Console`: The user interface for displaying outputs.\n- `OpenAIChatCompletionClient`: The client to interact with OpenAI's models.\n\nThese components serve as the foundation for building your agent that can respond to queries.\n\n## Defining the Model Client\n\nNext, you need to configure the model client that will be used by your agent. Here, we will use an OpenAI model as an example.\n\n```python\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n```\n\nIn this snippet, we create an instance of `OpenAIChatCompletionClient`, specifying the desired model (\"gpt-4o\" in this case). You may need to include your OpenAI API key in the commented line for authentication. This setup prepares your agent to generate responses based on user input.\n\n## Creating a Tool\n\nTo enhance the functionality of your agent, you can define tools that the agent can use for various tasks. Here, we create a simple weather tool.\n\n```python\nasync def get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n```\n\nThe `get_weather` function simulates fetching weather information for a specified city and returns a mock response. This tool can be called by the agent during interaction, allowing it to provide useful information to users.\n\n## Configuring the Assistant Agent\n\nNow, let\u2019s create the AssistantAgent, which combines the model client, tool, and system instructions for the agent.\n\n```python\nagent = AssistantAgent(\n    name=\"weather_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    system_message=\"You are a helpful assistant.\",\n    reflect_on_tool_use=True,\n    model_client_stream=True,  # Enable streaming tokens from the model client.\n)\n```\n\nHere, we configure the agent with the following properties:\n- `name`: The identifier for the agent.\n- `model_client`: The model client we created earlier.\n- `tools`: A list containing the tools available to the agent, like our `get_weather` function.\n- `system_message`: An instruction setting the context for the agent's behavior.\n- `reflect_on_tool_use`: Enabling the agent to consider its tool usage.\n- `model_client_stream`: Allows streaming responses from the model, enhancing real-time interactions.\n\nThis setup effectively prepares the agent to process tasks and respond dynamically.\n\n## Running the Agent\n\nTo execute the agent and interact with it through the console, we need to define the main execution function.\n\n```python\nasync def main() -> None:\n    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n    # Close the connection to the model client.\n    await model_client.close()\n```\n\nIn this code block:\n- The `main` function calls the `run_stream` method on the agent, passing in a sample task: \"What is the weather in New York?\".\n- The results are displayed in the console using the `Console` class.\n- After execution, we ensure to close the connection to the model client, which is a best practice to avoid resource leaks.\n\nTo run this asynchronously in a standalone Python script, you would use `asyncio.run(main())`. For environments that support `await`, you can directly call `await main()` as shown.\n\n## Conclusion\n\nNow you've set up a basic agent capable of using tools to provide information. This foundational example equips you with the necessary understanding to build more complex interactions using AgentChat. \n\n## What's Next?\n\nWith this quickstart guide, you have a basic understanding of creating a single agent. To explore more features, workflows, and capabilities, consider following the comprehensive tutorial available [here](./tutorial/index.md). This will expand your knowledge on creating advanced agents and utilizing various tools effectively.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/quickstart.ipynb"
  },
  {
    "code": false,
    "content": "# Selector Group Chat\n\nThe `SelectorGroupChat` class from `autogen_agentchat.teams` provides a structure for dynamic team collaboration. Participants take turns broadcasting messages, while a generative model (like a Large Language Model, LLM) selects the next speaker based on the current context, allowing for rich, context-aware interaction.\n\n## Key Features\n\n- **Model-based Speaker Selection**: Chooses the next speaker using an intelligent model.\n- **Customizable Roles**: Define the roles and descriptions of team members.\n- **Turn Management**: (Optional) Prevent consecutive turns by the same speaker.\n- **Customized Prompting**: Tailor how the model selects the next speaker.\n- **Flexible Selection Logic**: Override default selection with custom logic.\n- **Candidate Filtering**: Narrow down agent choices for selection.\n\n```{note}\nFor more authoritative control over your chat logic, explore the [Group Chat Pattern](../core-user-guide/design-patterns/group-chat.ipynb) in the Core API documentation.\n```\n\n## How Does it Work?\n\nThe `SelectorGroupChat` operates similarly to `RoundRobinGroupChat`, but utilizes a model-driven selection mechanism for determining who speaks next. The flow of operation when a task is initiated involves the following steps:\n\n1. **Context Analysis**: The team analyzes the conversation's context, which includes the dialogue history and agent attributes (name and description), to select the next speaker via a model. By default, repeated speaking by the same agent is disallowed unless it's the only agent available. This behavior can be modified by setting `allow_repeated_speaker=True` or by providing a custom selection function.\n   \n2. **Response Generation**: The selected agent is prompted for a response, which gets broadcasted to all members of the group.\n\n3. **Continuation Check**: After each turn, the team evaluates the termination condition to see if the conversation should continue or end. If it continues, control loops back to step 1.\n\n4. **Task Completion**: When the conversation concludes, the system returns a `TaskResult` that encompasses the entire dialogue history.\n\nMoreover, after a task concludes, the conversation context remains accessible, allowing future tasks to build upon past discussions. You can reset the context by using the `reset` method.\n\nIn this documentation, we illustrate the usage of `SelectorGroupChat` with a practical example centering on a web search and data analysis task.\n\n## Example: Web Search and Analysis\n\n### Setup \n\nBegin by importing necessary libraries and defining your agent classes:\n\n```python\nfrom typing import List, Sequence\n\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n### Agent Roles\n\n![Selector Group Chat](selector-group-chat.svg)\n\nThis setup involves three specialized agents:\n\n- **Planning Agent**: The strategist that deconstructs complex tasks into manageable subtasks.\n- **Web Search Agent**: Responsible for information retrieval, leveraging the `search_web_tool`.\n- **Data Analyst Agent**: Focused on calculation tasks, utilizing the `percentage_change_tool`.\n\nBelow are mock implementations for the external tools:\n\n```python\n# Mock tools for demonstration purposes\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n```\n\n### Defining the Agents\n\nNext, instantiate the agents using `AssistantAgent`, ensuring each has meaningful names and descriptions for effective context-driven model selections:\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nplanning_agent = AssistantAgent(\n    \"PlanningAgent\",\n    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n    model_client=model_client,\n    system_message=\"\"\"\n    You are a planning agent.\n    Your job is to break down complex tasks into smaller, manageable subtasks.\n    Your team members are:\n        WebSearchAgent: Searches for information\n        DataAnalystAgent: Performs calculations\n\n    You only plan and delegate tasks - you do not execute them yourself.\n\n    When assigning tasks, use this format:\n    1. <agent> : <task>\n\n    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n    \"\"\",\n)\n\nweb_search_agent = AssistantAgent(\n    \"WebSearchAgent\",\n    description=\"An agent for searching information on the web.\",\n    tools=[search_web_tool],\n    model_client=model_client,\n    system_message=\"\"\"\n    You are a web search agent.\n    Your only tool is search_tool - use it to find information.\n    You make only one search call at a time.\n    Once you have the results, you never do calculations based on them.\n    \"\"\",\n)\n\ndata_analyst_agent = AssistantAgent(\n    \"DataAnalystAgent\",\n    description=\"An agent for performing calculations.\",\n    model_client=model_client,\n    tools=[percentage_change_tool],\n    system_message=\"\"\"\n    You are a data analyst.\n    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n    If you have not seen the data, ask for it.\n    \"\"\",\n)\n```\n\n```{note}\nBy default, `AssistantAgent` returns tool output directly as the response. If your tool outputs raw data, consider implementing a reflective step with `reflect_on_tool_use=True`.\n```\n\n### Workflow Execution\n\nThe workflow for task execution follows a structured approach:\n\n1. **Task Initiation**: The `SelectorGroupChat` receives the task and identifies the most suitable agent to initialize the process (usually the Planning Agent).\n\n2. **Task Breakdown**: The Planning Agent segments the main task into actionable subtasks, systematically assigning each to the appropriate agents.\n\n3. **Dynamic Agent Selection**: The chat manager selects agents based on their assigned tasks and conversation context.\n\n4. **Information Gathering**: The Web Search Agent executes searches sequentially, documenting results within the shared conversation log.\n\n5. **Data Analysis**: The Data Analyst agent processes the information using specified calculation tools.\n\n6. **Processing Continuation**: This cycle persists until the Planning Agent identifies all subtasks as complete and sends \"TERMINATE,\" or a pre-defined maximum message limit is reached.\n\nIt's crucial to include descriptive attributes (`description`) for agents, as these inform the selection algorithm about which agent to engage next.\n\n### Setting Termination Conditions\n\nTo control when the conversation ends, we employ two termination conditions:\n- `TextMentionTermination`: Concludes the chat upon receiving \"TERMINATE.\"\n- `MaxMessageTermination`: Limits the interaction to 25 messages to prevent loops.\n\n```python\ntext_mention_termination = TextMentionTermination(\"TERMINATE\")\nmax_messages_termination = MaxMessageTermination(max_messages=25)\ntermination = text_mention_termination | max_messages_termination\n```\n\n### Custom Selector Prompt\n\nThe model that drives agent selection relies on prompts to operate effectively. Below is a customizable selector prompt tailored to our workflow:\n\n```python\nselector_prompt = \"\"\"Select an agent to perform task.\n\n{roles}\n\nCurrent conversation context:\n{history}\n\nRead the above conversation, then select an agent from {participants} to perform the next task.\nMake sure the planner agent has assigned tasks before other agents start working.\nOnly select one agent.\n\"\"\"\n```\n\nThe prompt elements include:\n- `{participants}`: A list of candidates in the format `[\"<name1>\", \"<name2>\", ...]`.\n- `{roles}`: A detailed explanation of the agent roles.\n- `{history}`: The conversation history, formatted appropriately.\n\n```{tip}\nKeep prompts concise. If too complex, it may overwhelm the model. Simpler is often better.\n```\n\n### Running the Team\n\nAssemble the team with agents, termination conditions, and a custom selector prompt:\n\n```python\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,  # Permit agents to speak consecutively.\n)\n```\n\nRun the team with a specified task regarding an NBA player:\n\n```python\ntask = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n```\n\n```python\n# If running this in a script, ensure to use asyncio.run(...).\nawait Console(team.run_stream(task=task))\n```\n\nThe expected output indicates that Dwayne Wade led in points during the 2006-2007 season, and the percentage change in his rebounds across the subsequent season is calculated at 85.98%.\n\n## Custom Selector Function\n\nTo grant detailed control over speaker selection, utilize the `selector_func` argument, permitting the implementation of advanced logic for selection processes.\n\nFor instance, you may want to ensure the Planning Agent follows after any specialized agent to monitor progress:\n\n```{note}\nReturning `None` from the custom selector function invokes the default model-based selection.\n``` \n\n```{note}\nCustom selector functions remain un-serialized when invoking `.dump_component()` on the `SelectorGroupChat` team. To serialize configurations with custom selectors, consider developing unique workflows.\n```\n\n### Custom Selector Function Example\n\n```python\ndef selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n    if messages[-1].source != planning_agent.name:\n        return planning_agent.name\n    return None\n\n# Reset and re-run using the selector function.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,\n    selector_func=selector_func,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n### Custom Candidate Function\n\nYou might also need a mechanism to automatically filter potential speakers by using a `candidate_func` parameter to decide which agents may be selected for speaking turns. This can limit the choices after a designated agent.\n\n```{note}\nThe `candidate_func` is only applicable if no `selector_func` is present. Returning `None` or an empty list will yield a `ValueError`.\n```\n\n### Custom Candidate Function Example\n\n```python\ndef candidate_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str]:\n    if messages[-1].source == \"user\":\n        return [planning_agent.name]\n\n    last_message = messages[-1]\n    if last_message.source == planning_agent.name:\n        participants = []\n        if web_search_agent.name in last_message.to_text():\n            participants.append(web_search_agent.name)\n        if data_analyst_agent.name in last_message.to_text():\n            participants.append(data_analyst_agent.name)\n        if participants:\n            return participants  # Only return specific agents for selection.\n\n    previous_agents = {message.source for message in messages}\n    if web_search_agent.name in previous_agents and data_analyst_agent.name in previous_agents:\n        return [planning_agent.name]\n\n    return [planning_agent.name, web_search_agent.name, data_analyst_agent.name]\n\n# Reset and re-run with the candidate function.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    candidate_func=candidate_func,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n## User Feedback Integration\n\nTo enhance interactivity, incorporate a `UserProxyAgent` to provide user feedback during execution, guided by the user's input on task approvals.\n\n### User Feedback Implementation\n\n```python\nuser_proxy_agent = UserProxyAgent(\"UserProxyAgent\", description=\"A proxy for the user to approve or disapprove tasks.\")\n\ndef selector_func_with_user_proxy(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n    if messages[-1].source != planning_agent.name and messages[-1].source != user_proxy_agent.name:\n        return planning_agent.name\n    if messages[-1].source == planning_agent.name:\n        if messages[-2].source == user_proxy_agent.name and \"APPROVE\" in messages[-1].content.upper():\n            return None\n        return user_proxy_agent.name\n    if messages[-1].source == user_proxy_agent.name:\n        if \"APPROVE\" not in messages[-1].content.upper():\n            return planning_agent.name\n    return None\n\n# Reset and run again with user feedback incorporated.\nawait team.reset()\nteam = SelectorGroupChat(\n    [planning_agent, web_search_agent, data_analyst_agent, user_proxy_agent],\n    model_client=model_client,\n    termination_condition=termination,\n    selector_prompt=selector_prompt,\n    selector_func=selector_func_with_user_proxy,\n    allow_repeated_speaker=True,\n)\n\nawait Console(team.run_stream(task=task))\n```\n\n## Using Reasoning Models\n\nIn previous sections, we utilized a `gpt-4o` model. This model provides sufficient capability for handling complex dialogue flows. However, if employing reasoning models like `o3-mini`, adopt a simplified approach to selector prompts and system messages.\n\n### Example: Using Reasoning Model\n\nHere is how to set up agents and the team using the reasoning model `o3-mini` while forgoing the Planning Agent:\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"o3-mini\")\n\nweb_search_agent = AssistantAgent(\n    \"WebSearchAgent\",\n    description=\"An agent for searching information on the web.\",\n    tools=[search_web_tool],\n    model_client=model_client,\n    system_message=\"\"\"Use web search tool to find information.\"\"\",\n)\n\ndata_analyst_agent = AssistantAgent(\n    \"DataAnalystAgent\",\n    description=\"An agent for performing calculations.\",\n    model_client=model_client,\n    tools=[percentage_change_tool],\n    system_message=\"\"\"Use tool to perform calculation. If you have not seen the data, ask for it.\"\"\",\n)\n\nuser_proxy_agent = UserProxyAgent(\n    \"UserProxyAgent\",\n    description=\"A user to approve or disapprove tasks.\",\n)\n\nselector_prompt = \"\"\"Select an agent to perform task.\n\n{roles}\n\nCurrent conversation context:\n{history}\n\nRead the above conversation, then select an agent from {participants} to perform the next task.\nWhen the task is complete, let the user approve or disapprove the task.\n\"\"\"\n\nteam = SelectorGroupChat(\n    [web_search_agent, data_analyst_agent, user_proxy_agent],\n    model_client=model_client,\n    termination_condition=termination,  # Maintain the same termination condition.\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,\n)\n```\n\n```python\nawait Console(team.run_stream(task=task))\n```\n\n```{tip}\nFor deeper insight on optimizing prompts for reasoning models, refer to the Azure AI Services Blog on [Prompt Engineering for OpenAI's O1 and O3-mini Reasoning Models](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010).\n```",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/selector-group-chat.ipynb"
  },
  {
    "code": false,
    "content": "# Serializing Components \n\nAutoGen provides a {py:class}`~autogen_core.Component` configuration class that facilitates the serialization and deserialization of components into declarative specifications. This can be achieved using the `.dump_component()` method for serialization and `.load_component()` for deserialization. These features are valuable for debugging, visualization, and sharing your work with others. In this documentation, we will illustrate how to serialize multiple components to a JSON specification and load them back.\n\n> **Warning**\n>\n> ONLY LOAD COMPONENTS FROM TRUSTED SOURCES.\n> \n> Serialized components contain logic for their serialization and deserialization, meaning that loading a component may involve executing code. Be cautious and ensure you only load components from trusted sources.\n\n> **Note**\n>\n> The `selector_func` is not serializable and will be ignored during the serialization and deserialization process.\n\n## Termination Condition Example \n\nIn this section, we demonstrate how to define termination conditions, a crucial part of an agent team. We will serialize these conditions to a dictionary or JSON format and show how to load them back.\n\n```python\nfrom autogen_agentchat.conditions import MaxMessageTermination, StopMessageTermination\n\n# Define termination conditions\nmax_termination = MaxMessageTermination(5)  # Maximum 5 messages\nstop_termination = StopMessageTermination()  # Stop termination condition\n\n# Combine termination conditions with logical OR\nor_termination = max_termination | stop_termination\n\n# Serialize to a JSON-like structure\nor_term_config = or_termination.dump_component()\nprint(\"Config: \", or_term_config.model_dump_json())\n\n# Load the termination condition back from the JSON structure\nnew_or_termination = or_termination.load_component(or_term_config)\n```\nIn the code above, we first import two different termination conditions. We define a maximum message limit and a stop condition, combine them using logical OR, and serialize the combined condition to JSON format. Finally, we demonstrate how to load the serialized termination condition back into an object.\n\n## Agent Example \n\nNext, we create an agent using AutoGen and illustrate the serialization process for this component. The agent utilizes an OpenAI GPT-4o model.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an agent with the OpenAI GPT-4o model\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    # tools=[], # Note: serializing tools is not yet supported\n    system_message=\"Use tools to solve tasks.\",\n)\nuser_proxy = UserProxyAgent(name=\"user\")\n```\nIn this snippet, we define the `AssistantAgent` and a `UserProxyAgent`. The agent is configured with a model client and various parameters that influence its behavior, such as handoffs and system messages.\n\n```python\n# Serialize the user proxy agent\nuser_proxy_config = user_proxy.dump_component()  \nprint(user_proxy_config.model_dump_json())  # Print the serialized configuration\n\n# Load the user proxy agent back from the configuration\nup_new = user_proxy.load_component(user_proxy_config)  \n```\nHere, we serialize the `UserProxyAgent` to a JSON object and demonstrate how to load it back into an agent object.\n\n```python\n# Serialize the assistant agent\nagent_config = agent.dump_component()  \nprint(agent_config.model_dump_json())  # Print the serialized configuration\n\n# Load the assistant agent back from the configuration\nagent_new = agent.load_component(agent_config)  \n```\nSimilarly, we serialize and load the `AssistantAgent`, showing the process for this component as well.\n\nA similar approach can be employed for the `MultiModalWebSurfer` agent.\n\n```python\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\n# Create a multimodal web surfer agent\nagent = MultimodalWebSurfer(\n    name=\"web_surfer\",\n    model_client=model_client,\n    headless=False,\n)\n\n# Serialize the web surfer agent\nweb_surfer_config = agent.dump_component()  \nprint(web_surfer_config.model_dump_json())  # Print the serialized configuration\n```\nThis snippet demonstrates how to create and serialize a `MultimodalWebSurfer` agent. The process remains consistent: define the agent, serialize it, and visualize the JSON output.\n\n## Team Example\n\nIn this section, we will assemble a team of agents and demonstrate how to serialize the complete team configuration.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI GPT-4o model client\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    # tools=[], # Note: serializing tools is not yet supported\n    system_message=\"Use tools to solve tasks.\",\n)\n\n# Define a team with a round-robin chat format and a termination condition\nteam = RoundRobinGroupChat(participants=[agent], termination_condition=MaxMessageTermination(2))\n\n# Serialize the team configuration\nteam_config = team.dump_component()  \nprint(team_config.model_dump_json())  # Print the serialized configuration\n\nawait model_client.close()  # Close the model client when done\n```\nIn this example, we create a team composed of an assistant agent. The team utilizes a round-robin chat structure with a specified termination condition, serializing the entire team into JSON format. Finally, we ensure to close the model client to free up resources.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/serialize-components.ipynb"
  },
  {
    "code": false,
    "content": "# Documentation for Swarm\n\n## Overview of the Swarm Class\n\nThe `{py:class}`~autogen_agentchat.teams.Swarm` class implements a team architecture that enables agents to transfer tasks among themselves based on their individual capabilities. This multi-agent pattern, introduced by OpenAI in their [Swarm repository](https://github.com/openai/swarm), optimizes task allocation by allowing agents to delegate work, resulting in a more efficient and dynamic interaction model compared to traditional central orchestrators like `{py:class}`~autogen_agentchat.teams.SelectorGroupChat`.\n\nThe distinctive feature of the Swarm pattern is the shared message context, which allows agents to make decisions locally. This use of localized decision-making differentiates Swarm from other multi-agent framework implementations.\n\n```{note}\nThe `{py:class}`~autogen_agentchat.teams.Swarm` serves as a high-level API. For more nuanced control and custom functionality that isn't captured within this API, consider exploring the [Handoff Pattern](../core-user-guide/design-patterns/handoffs.ipynb) in the Core API documentation to implement your version of the Swarm pattern.\n```\n\n## Mechanism of Operation\n\nThe `{py:class}`~autogen_agentchat.teams.Swarm` functions fundamentally as a group chat where agents collaborate on tackling tasks collectively. Unlike other versions like `{py:class}`~autogen_agentchat.teams.SelectorGroupChat` and `{py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`, Swarm selects the speaking agent based on the most recent `{py:class}`~autogen_agentchat.messages.HandoffMessage` in the context.\n\nEach agent is responsible for generating `{py:class}`~autogen_agentchat.messages.HandoffMessage` messages to indicate when tasks are handed off to another agent. When instantiating `{py:class}`~autogen_agentchat.agents.AssistantAgent`, its `handoffs` argument can be configured to define which agents it is authorized to delegate tasks to. \n\nThe overall task flow within a Swarm setup comprises several key steps:\n\n1. Each agent generates `{py:class}`~autogen_agentchat.messages.HandoffMessage` to indicate appropriate handoff targets.\n2. Upon initiating a task, agent collaboration ensues as they decide locally whether to delegate tasks.\n3. When one agent sends a `{py:class}`~autogen_agentchat.messages.HandoffMessage`, the recipient agent assumes responsibility for the task, maintaining the shared message context.\n4. This process continues iteratively until a designated termination condition occurs.\n\n```{note}\nThe `{py:class}`~autogen_agentchat.agents.AssistantAgent` delegate tasks through the model\u2019s tool calling capability. Ensure that the model supports this feature, and consider disabling parallel tool calls in the configuration to avoid unexpected behavior. For instance, when using `{py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient`, you can manage the configuration with `parallel_tool_calls=False`.\n```\n\nIn the upcoming sections, we provide examples illustrating the implementation of the `{py:class}`~autogen_agentchat.teams.Swarm` pattern within two distinct workflows:\n\n1. A customer support use case with human-in-the-loop handoffs.\n2. An autonomous team constructing market research reports.\n\n## Example 1: Customer Support Application\n\n![Customer Support](swarm_customer_support.svg)\n\nThis implementation simulates a flight refund scenario involving two agents:\n\n- **Travel Agent**: Manages general travel inquiries and coordinates refund processes.\n- **Flights Refunder**: Specializes in handling flight refund operations utilizing the `refund_flight` tool.\n\nThe design also allows user interactions when agents hand off tasks to `\"user\"`.\n\n### Workflow Steps\n1. The **Travel Agent** begins the conversation, assessing the user's request.\n2. Depending on the context:\n   - If refund-related, the Travel Agent forwards the query to the **Flights Refunder**.\n   - For additional information from the customer, either agent may hand off to the `\"user\"`.\n3. The **Flights Refunder** utilizes the `refund_flight` tool for applicable refund processes.\n4. When handed off to the `\"user\"`, the operation halts to await user feedback.\n5. User responses are relayed back into the team as a `{py:class}`~autogen_agentchat.messages.HandoffMessage`, directed to the agent that initially requested the input.\n6. The cycle repeats until the Travel Agent concludes that the task is complete.\n\n### Implementation Code\n\n```python\nfrom typing import Any, Dict, List\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\nfrom autogen_agentchat.messages import HandoffMessage\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nIn the code snippet above, we import necessary classes and functions to set up our agents and team structure.\n\n### Tool Definition\n\n```python\ndef refund_flight(flight_id: str) -> str:\n    \"\"\"Refund a flight.\"\"\"\n    return f\"Flight {flight_id} refunded\"\n```\n\nHere, we define a simple function that simulates the process of refunding a flight, returning a success message when executed.\n\n### Agent Configuration\n\n```python\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\ntravel_agent = AssistantAgent(\n    \"travel_agent\",\n    model_client=model_client,\n    handoffs=[\"flights_refunder\", \"user\"],\n    system_message=\"\"\"You are a travel agent.\n    The flights_refunder is in charge of refunding flights.\n    If you need information from the user, you must first send your message, then handoff to the user.\n    Use TERMINATE when the travel planning is complete.\"\"\",\n)\n\nflights_refunder = AssistantAgent(\n    \"flights_refunder\",\n    model_client=model_client,\n    handoffs=[\"travel_agent\", \"user\"],\n    tools=[refund_flight],\n    system_message=\"\"\"You are an agent specialized in refunding flights.\n    You only need flight reference numbers to refund a flight.\n    You have the ability to refund a flight using the refund_flight tool.\n    If you need information from the user, you must first send your message, then handoff to the user.\n    When the transaction is complete, handoff to the travel agent to finalize.\"\"\",\n)\n```\n\nIn this block, we configure the `OpenAIChatCompletionClient` and instantiate the two agents\u2014Travel Agent and Flights Refunder\u2014with their corresponding system messages and handoff capabilities.\n\n### Termination Condition\n\n```python\ntermination = HandoffTermination(target=\"user\") | TextMentionTermination(\"TERMINATE\")\nteam = Swarm([travel_agent, flights_refunder], termination_condition=termination)\n```\n\nWe establish the termination conditions and create an instance of the Swarm with both agents. The task will end when it meets these criteria.\n\n### Running the Team\n\n```python\ntask = \"I need to refund my flight.\"\n\n\nasync def run_team_stream() -> None:\n    task_result = await Console(team.run_stream(task=task))\n    last_message = task_result.messages[-1]\n\n    while isinstance(last_message, HandoffMessage) and last_message.target == \"user\":\n        user_message = input(\"User: \")\n\n        task_result = await Console(\n            team.run_stream(task=HandoffMessage(source=\"user\", target=last_message.source, content=user_message))\n        )\n        last_message = task_result.messages[-1]\n\n# Use asyncio.run(...) if you are running this in a script.\nawait run_team_stream()\nawait model_client.close()\n```\n\nIn the final portion, we outline an asynchronous function to run the team stream. The workflow captures user input when the last message is a handoff to the user, ensuring dynamic interaction until task completion.\n\n## Example 2: Stock Research Application\n\n![Stock Research](swarm_stock_research.svg)\n\nThe second example showcases a stock research application with four specialized agents:\n\n- **Planner**: Oversees coordination and task delegation among specialized agents.\n- **Financial Analyst**: Analyzes stocks using tools like `get_stock_data`.\n- **News Analyst**: Gathers and summarizes relevant news articles about stocks.\n- **Writer**: Compiles findings into a cohesive report.\n\n### Workflow Steps\n1. The **Planner** initiates the research process by allocating tasks to specialized agents systematically.\n2. Each agent operates independently, contributing to and referring to the shared message history.\n3. Once an agent completes its task, control is returned to the Planner.\n4. This cycle persists until the Planner decides that all tasks are complete.\n\n### Tool Definitions\n\n```python\nasync def get_stock_data(symbol: str) -> Dict[str, Any]:\n    \"\"\"Get stock market data for a given stock symbol.\"\"\"\n    return {\"price\": 180.25, \"volume\": 1000000, \"pe_ratio\": 65.4, \"market_cap\": \"700B\"}\n\n\nasync def get_news(query: str) -> List[Dict[str, str]]:\n    \"\"\"Retrieve recent news articles relevant to a company.\"\"\"\n    return [\n        {\n            \"title\": \"Tesla Expands Cybertruck Production\",\n            \"date\": \"2024-03-20\",\n            \"summary\": \"Tesla ramps up Cybertruck manufacturing capacity at Gigafactory Texas, aiming to meet strong demand.\",\n        },\n        {\n            \"title\": \"Tesla FSD Beta Shows Promise\",\n            \"date\": \"2024-03-19\",\n            \"summary\": \"Latest Full Self-Driving beta demonstrates significant improvements in urban navigation and safety features.\",\n        },\n        {\n            \"title\": \"Model Y Dominates Global EV Sales\",\n            \"date\": \"2024-03-18\",\n            \"summary\": \"Tesla's Model Y becomes best-selling electric vehicle worldwide, capturing significant market share.\",\n        },\n    ]\n```\n\nThese functions are defined to represent the tools used by the agents for acquiring stock data and relevant news information, respectively.\n\n### Agent Configuration\n\n```python\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\nplanner = AssistantAgent(\n    \"planner\",\n    model_client=model_client,\n    handoffs=[\"financial_analyst\", \"news_analyst\", \"writer\"],\n    system_message=\"\"\"You are a research planning coordinator.\n    Coordinate market research by delegating to specialized agents:\n    - Financial Analyst: For stock data analysis\n    - News Analyst: For news gathering and analysis\n    - Writer: For compiling final report\n    Always send your plan first, then handoff to appropriate agent.\n    Always handoff to a single agent at a time.\n    Use TERMINATE when research is complete.\"\"\",\n)\n\nfinancial_analyst = AssistantAgent(\n    \"financial_analyst\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    tools=[get_stock_data],\n    system_message=\"\"\"You are a financial analyst.\n    Analyze stock market data using the get_stock_data tool.\n    Provide insights on financial metrics.\n    Always handoff back to planner when analysis is complete.\"\"\",\n)\n\nnews_analyst = AssistantAgent(\n    \"news_analyst\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    tools=[get_news],\n    system_message=\"\"\"You are a news analyst.\n    Gather and analyze relevant news using the get_news tool.\n    Summarize key market insights from news.\n    Always handoff back to planner when analysis is complete.\"\"\",\n)\n\nwriter = AssistantAgent(\n    \"writer\",\n    model_client=model_client,\n    handoffs=[\"planner\"],\n    system_message=\"\"\"You are a financial report writer.\n    Compile research findings into clear, concise reports.\n    Always handoff back to planner when writing is complete.\"\"\",\n)\n```\n\nIn this block, we create and configure the agents, defining their communication protocols and available tools.\n\n### Defining the Termination Condition\n\n```python\n# Define termination condition\ntext_termination = TextMentionTermination(\"TERMINATE\")\ntermination = text_termination\n\nresearch_team = Swarm(\n    participants=[planner, financial_analyst, news_analyst, writer], termination_condition=termination\n)\n```\n\nThis portion establishes the termination condition and constructs the research team instance, ensuring orderly operation.\n\n### Executing the Research Team\n\n```python\ntask = \"Conduct market research for TSLA stock\"\nawait Console(research_team.run_stream(task=task))\nawait model_client.close()\n```\n\nIn this final step, we spawn the research task into motion utilizing asynchronous execution, facilitating interaction with the agents up until task completion.\n\n--- \n\nThe examples provided offer a foundational understanding of how to implement the `{py:class}`~autogen_agentchat.teams.Swarm` pattern in practical scenarios, illustrating the flexibility and adaptability of this architecture for various applications.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/swarm.ipynb"
  },
  {
    "code": false,
    "content": "# Tracing and Observability\n\nAutoGen provides robust support for tracing and observability, allowing developers to collect detailed records of application execution. This capability is essential for tasks such as debugging, performance analysis, and gaining insights into application flow.\n\nLeverage the power of the [OpenTelemetry](https://opentelemetry.io/) library, which is utilized by AutoGen for tracing. OpenTelemetry-compatible backends can be employed to collect and analyze trace data effectively. AutoGen adheres to the [OpenTelemetry Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/) for tracing, as well as the [Semantic Conventions for GenAI Systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/) which is currently under development.\n\n## Setup\n\nTo start using tracing in AutoGen, the OpenTelemetry Python package must be installed. This can be accomplished with pip by executing the following command:\n\n```bash\npip install opentelemetry-sdk opentelemetry-exporter-otlp-proto-grpc opentelemetry-instrumentation-openai\n```\n\nAfter installation, you can set up tracing by following these steps:\n1. Configure an OpenTelemetry tracer provider.\n2. Set up an exporter to transmit traces to your selected backend.\n3. Connect the tracer provider to the AutoGen runtime.\n\nBy following this setup, you will enable tracing for your application, allowing you to capture trace data as it executes.\n\n## Telemetry Backend\n\nSetting up a telemetry backend is crucial for collecting and visualizing traces. Several open-source telemetry options, such as Jaeger and Zipkin, offer various features for trace analysis. In this guide, we will use Jaeger as our telemetry backend.\n\nFor a quick setup, Jaeger can be run locally using Docker with the following command:\n\n```bash\ndocker run -d --name jaeger \\\n  -e COLLECTOR_OTLP_ENABLED=true \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:latest\n```\n\nExecuting this command initiates a Jaeger instance that is accessible through the Jaeger user interface (UI) at `http://localhost:16686`. Additionally, the OpenTelemetry collector listens on port 4317.\n\n## Tracing an AgentChat Team\n\nIn this section, we will explore how to enable tracing with an AutoGen GroupChat team. The AutoGen runtime already integrates OpenTelemetry for automatic logging of message metadata, which simplifies the tracing setup. We will create a tracing service to instrument the AutoGen runtime.\n\nHere's how you can set up the tracing service:\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.openai import OpenAIInstrumentor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Set up a telemetry span exporter.\notel_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\nspan_processor = BatchSpanProcessor(otel_exporter)\n\n# Configure a telemetry trace provider.\ntracer_provider = TracerProvider(resource=Resource({\"service.name\": \"autogen-test-agentchat\"}))\ntracer_provider.add_span_processor(span_processor)\ntrace.set_tracer_provider(tracer_provider)\n\n# Instrument the OpenAI Python library\nOpenAIInstrumentor().instrument()\n\n# Reference the tracer using its service name\n# tracer = trace.get_tracer(\"autogen-test-agentchat\")\n```\n\nIn this code block, we establish a telemetry span exporter that connects to the OpenTelemetry collector. We also configure a trace provider to associate our AutoGen service with the tracer. Finally, we instrument the OpenAI library to ensure that OpenAI interactions are traced.\n\n### Note on AgentChat Teams\nAgentChat teams operate using the AutoGen Core's agent runtime. This runtime is already instrumented to log telemetry data (see the [Core Telemetry Guide](../core-user-guide/framework/telemetry.md)). If you desire to disable the agent runtime telemetry, you can do so by setting the `trace_provider` to `opentelemetry.trace.NoOpTracerProvider` in the runtime constructor. Additionally, use the environment variable `AUTOGEN_DISABLE_RUNTIME_TRACING` to toggle off the telemetry without access to the runtime constructor.\n\n## Creating AgentChat Components\n\nWith tracing set up, we can now create the AgentChat team. Below is a code snippet that constructs the necessary agents and the runtime for our application:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Acquire a tracer using the default tracer provider.\n    tracer = trace.get_tracer(\"tracing-autogen-agentchat\")\n\n    # Create a span for the main function's execution.\n    with tracer.start_as_current_span(\"run_team\"):\n        planning_agent = AssistantAgent(\n            \"PlanningAgent\",\n            description=\"A planning agent responsible for breaking down tasks.\",\n            model_client=model_client,\n            system_message=\"\"\"\n            You are a planning agent.\n            Your job is to break down complex tasks into manageable subtasks.\n            Your team includes:\n                WebSearchAgent: Searches for information\n                DataAnalystAgent: Performs calculations\n            Only plan and delegate tasks, do not execute them.\n            Format for task assignments:\n            1. <agent> : <task>\n            After all tasks are complete, summarize findings and conclude with \"TERMINATE\".\n            \"\"\",\n        )\n\n        web_search_agent = AssistantAgent(\n            \"WebSearchAgent\",\n            description=\"An agent for web searches.\",\n            tools=[search_web_tool],\n            model_client=model_client,\n            system_message=\"\"\"\n            You are a web search agent.\n            Use search_tool to find information, making one search at a time.\n            Do not perform calculations with the results.\n            \"\"\",\n        )\n\n        data_analyst_agent = AssistantAgent(\n            \"DataAnalystAgent\",\n            description=\"An agent for data analysis.\",\n            model_client=model_client,\n            tools=[percentage_change_tool],\n            system_message=\"\"\"\n            You are a data analyst.\n            Analyze any assigned tasks using the tools available. \n            If data is missing, request it.\n            \"\"\",\n        )\n\n        text_mention_termination = TextMentionTermination(\"TERMINATE\")\n        max_messages_termination = MaxMessageTermination(max_messages=25)\n        termination = text_mention_termination | max_messages_termination\n\n        selector_prompt = \"\"\"Select an agent for task execution.\n\n        {roles}\n\n        Current conversation context:\n        {history}\n\n        Review the conversation above, then select an agent from {participants} to proceed.\n        Ensure the planner agent has assigned tasks first.\n        Only select one agent.\n        \"\"\"\n\n        task = \"Identify the Miami Heat player with the most points in the 2006-2007 season, and determine the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons.\"\n\n        runtime = SingleThreadedAgentRuntime(\n            tracer_provider=trace.NoOpTracerProvider(),  # Disable telemetry for the runtime.\n        )\n        runtime.start()\n\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,\n            runtime=runtime,\n        )\n        await Console(team.run_stream(task=task))\n\n        await runtime.stop()\n\n    await model_client.close()\n\n\n# asyncio.run(main())\n```\n\nIn this segment, we define the agents within the chat team and outline their responsibilities. The `search_web_tool` and `percentage_change_tool` functions serve as tools for the web search and analysis agents, respectively. The main function orchestrates the execution of these agents, enabling a seamless interaction that is captured with tracing.\n\n## Viewing Traces with Jaeger\n\nOnce the setup is complete and the agents have run, you can utilize the Jaeger UI to visualize the collected traces. This provides valuable insights into the workflow of your application and facilitates analysis of performance metrics.\n\n```python\nawait main()\n```\n\nAfter executing the main function, you can navigate to the Jaeger UI at `http://localhost:16686` to explore the generated traces and gain insights into your application's performance dynamics.\n\n![Jaeger UI](jaeger.png)\n\nThis comprehensive tracing setup provides a foundation for debugging and performance monitoring in your applications using AutoGen.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tracing.ipynb"
  },
  {
    "code": false,
    "content": "# Agents\n\nAutoGen AgentChat provides a collection of preset Agents, each designed to respond to messages in unique ways. All agents share a common set of attributes and methods that enable their functionality.\n\n## Core Attributes and Methods\n\nEach agent in AutoGen AgentChat includes the following key attributes and methods:\n\n- **Name**: The unique identifier for the agent (`BaseChatAgent.name`).\n- **Description**: A text description providing insight into the agent's purpose (`BaseChatAgent.description`).\n- **Run Method**: The primary method used to execute the agent's functionality, taking a task as input and outputting a result (`BaseChatAgent.run`). Note that this method is designed to be stateful, meaning that it is called with new messages rather than the full history.\n- **Run Stream Method**: Similar to the `run` method, but returns an asynchronous iterator of messages, providing a more granular view of the agent's processing (`BaseChatAgent.run_stream`).\n\nFor more details on AgentChat message types, consult the `autogen_agentchat.messages` module.\n\n## Assistant Agent Overview\n\nThe `AssistantAgent` is a built-in agent that utilizes a language model and has the capability to utilize additional tools.\n\n```{warning}\nThe `AssistantAgent` serves as a comprehensive agent for prototyping and educational purposes due to its broad functionality. It is recommended to review the documentation and implementation to grasp the design choices thoroughly. After assessing this, you may choose to develop a custom agent. See [Custom Agent](../custom-agents.ipynb) for guidance.\n```\n\n### Initialization Example\n\nHere\u2019s how to import and initialize the `AssistantAgent` along with a mock web search tool:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import StructuredMessage\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Define a tool that searches the web for information.\nasync def web_search(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4.1-nano\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)\n```\n\nIn this example, we import the necessary classes, define a mock web search function, and create an instance of the `AssistantAgent` using a specific OpenAI model.\n\n## Obtaining Results\n\nTo execute a task with the agent, utilize the `run` method. This method returns a `TaskResult` containing the agent's response and \"thought process.\"\n\n```python\n# Use asyncio.run(agent.run(...)) when running in a script.\nresult = await agent.run(task=\"Find information on AutoGen\")\nprint(result.messages)\n```\n\nThe output from the `run` method is captured in the `TaskResult` object, especially noting the `messages` attribute that logs the conversations leading up to the response.\n\n### Important Considerations\n\n```{note}\nCalling the `run` method will modify the agent's internal state by adding the task's messages to its history. You can also invoke `run` without a task to generate responses based solely on the agent's current state.\n```\n\n```{note}\nIn contrast to previous versions of AgentChat, the execution of tools now occurs directly within the same call to the `run` method. The agent typically returns the result of the tool call as its final output.\n```\n\n## Multi-Modal Input Handling\n\nThe `AssistantAgent` can also process multi-modal input through the `MultiModalMessage` class, thereby allowing it to engage with various types of data.\n\n```python\nfrom io import BytesIO\nimport PIL\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image\n\n# Create a multi-modal message with random image and text.\npil_image = PIL.Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = Image(pil_image)\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"user\")\nimg\n```\n\nIn this code block, we download a random image from the web and encapsulate it within a multi-modal message alongside a text prompt.\n\n### Execution of Multi-Modal Tasks\n\nTo obtain results from the `AssistantAgent` using a multi-modal input, simply run the `run` method with the constructed `MultiModalMessage`:\n\n```python\n# Use asyncio.run(...) when running in a script.\nresult = await agent.run(task=multi_modal_message)\nprint(result.messages[-1].content)  # type: ignore\n```\n\n## Streaming Messages\n\nThe `run_stream` method allows for streaming generated messages from the agent in real-time. This can enhance user engagement by presenting responses as they develop.\n\n```python\nasync def assistant_run_stream() -> None:\n    # Option 1: read each message from the stream.\n    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n    #     print(message)\n\n    # Option 2: use Console to print all messages as they appear.\n    await Console(\n        agent.run_stream(task=\"Find information on AutoGen\"),\n        output_stats=True,  # Enable stats printing.\n    )\n\n# Use asyncio.run(assistant_run_stream()) when running in a script.\nawait assistant_run_stream()\n```\n\nUsing streaming capabilities, we can view each message in real-time, offering better interactivity than traditional response structures.\n\n### Insights from the Streamed Messages\n\nThe `run_stream` method yields an asynchronous generator that provides a series of messages, culminating in a `TaskResult` object at the end.\n\n## Leveraging External Tools\n\nAutoGen Agents can enhance their capabilities by using external tools, such as fetching data from APIs or executing specific actions. This is essential for tackling complex tasks that require more than simple text generation.\n\nThe `AssistantAgent` can execute predefined tools, such as `web_search`, to gather information autonomously.\n\n### Tool Calling\n\nThis feature of modern LLMs allows agents to generate messages that include tool calls, facilitating a more interactive approach to problem-solving. Refer to the documentation from [OpenAI](https://platform.openai.com/docs/guides/function-calling) and [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/tool-use) for more on tool calling.\n\n### Built-in Tools and Workbench\n\nThe AutoGen extension provides a variety of built-in tools available for use, enhancing the functionality of the Assistant Agent. Check the [API documentation](../../../reference/index.md) for a full list of tools, such as:\n\n- `~autogen_ext.tools.graphrag`: Tools related to GraphRAG indexing.\n- `~autogen_ext.tools.http`: Tools for making HTTP requests.\n- `~autogen_ext.tools.langchain`: Adaptor for LangChain tools.\n- `~autogen_ext.tools.mcp`: Tools related to Model Chat Protocol (MCP) servers.\n\n## Function Tools\n\nA Python function can be easily transformed into a usable tool for the Assistant Agent. The function's schema is auto-generated based on its signature and documentation.\n\n```python\nfrom autogen_core.tools import FunctionTool\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n# Automatically converts the function to a tool.\nweb_search_function_tool = FunctionTool(web_search_func, description=\"Find information on the web\")\n```\n\n## Model Context Protocol Workbench\n\nThe `AssistantAgent` can integrate with external tool servers through the MCP server mechanism, allowing for more complex interactions:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n# Get the fetch tool from mcp-server-fetch.\nfetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n\n# Create an MCP workbench which provides a session to the MCP server.\nasync with McpWorkbench(fetch_mcp_server) as workbench:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    fetch_agent = AssistantAgent(\n        name=\"fetcher\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True\n    )\n\n    result = await fetch_agent.run(task=\"Summarize the content of https://en.wikipedia.org/wiki/Seattle\")\n    assert isinstance(result.messages[-1], TextMessage)\n    print(result.messages[-1].content)\n\n    await model_client.close()\n```\n\n## Agent as a Tool\n\nAny agent can be utilized as a tool by wrapping it in an `AgentTool`, facilitating a dynamic environment where agents can leverage one another to complete tasks effectively.\n\n### Parallel Tool Calls\n\nMany models support calling multiple tools concurrently, enhancing efficiency for tasks requiring simultaneous operations. However, caution is advised when tools may interfere with each other's operations.\n\n```{important}\nWhen using `AgentTool` or `TeamTool`, you **must** disable parallel tool calls to prevent conflicts due to shared state.\n```\n\n### Tool Iterations\n\nThe `AssistantAgent` manages the execution of tool iterations, i.e., one model call followed by a tool call. It can be configured to allow multiple iterations until a certain condition is met, like ceasing tool calls.\n\n```python\nagent_loop = AssistantAgent(\n    name=\"assistant_loop\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    max_tool_iterations=10,  # Maximum iterations for tool calls.\n)\n```\n\n## Structured Output Integration\n\nStructured output enables agents to return organized JSON responses validated by predefined schemas using Pydantic models:\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\n\n# Define Pydantic base model for output.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n# Create an agent to return structured output.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\nagent = AssistantAgent(\n    \"assistant\",\n    model_client=model_client,\n    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n    output_content_type=AgentResponse,\n)\n\nresult = await Console(agent.run_stream(task=\"I am happy.\"))\n\n# Validate and print the structured response.\nassert isinstance(result.messages[-1], StructuredMessage)\nassert isinstance(result.messages[-1].content, AgentResponse)\nprint(\"Thought: \", result.messages[-1].content.thoughts)\nprint(\"Response: \", result.messages[-1].content.response)\nawait model_client.close()\n```\n\n## Streaming Tokens from the Model Client\n\nTo enable token streaming capabilities, set `model_client_stream=True`. This allows the agent to yield messages as the model generates them:\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nstreaming_assistant = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    system_message=\"You are a helpful assistant.\",\n    model_client_stream=True,  # Enable streaming tokens.\n)\n\n# For use within an async context.\nasync for message in streaming_assistant.run_stream(task=\"Name two cities in South America\"):\n    print(message)\n```\n\nYou\u2019ll observe messages appearing in real-time as they are generated by the model, improving interactivity.\n\n## Utilizing Model Context\n\nThe `AssistantAgent` includes a `model_context` parameter for limiting the context sent to the model. This helps manage the size of the conversation context:\n\n```python\nfrom autogen_core.model_context import BufferedChatCompletionContext\n\n# Create an agent restricted to the last 5 messages for context.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    model_context=BufferedChatCompletionContext(buffer_size=5),  # Last 5 messages only.\n)\n```\n\n## Additional Preset Agents\n\nAutoGen comes with additional preset agents beyond the `AssistantAgent`, including:\n\n- **UserProxyAgent**: Handles user input directly.\n- **CodeExecutorAgent**: Executes code snippets.\n- **OpenAIAssistantAgent**: Utilizes OpenAI technology with custom tool support.\n- **MultimodalWebSurfer**: Searches the web and retrieves multi-modal information.\n- **FileSurfer**: Searches local files for information.\n- **VideoSurfer**: Analyzes video content for insights.\n\n## Next Steps\n\nHaving explored the capabilities of the `AssistantAgent`, the next section introduces the teams feature in AgentChat, expanding the versatility of multi-agent interactions.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/agents.ipynb"
  },
  {
    "code": false,
    "content": "# Human-in-the-Loop Documentation\n\nIn the previous section [Teams](./teams.ipynb), we explored how to create, observe, and control a team of agents. In this section, we focus on how to interact with the team from your application and provide human feedback to the team.\n\n## Interaction Methods\n\nThere are two primary methods to provide feedback during a team session:\n\n1. **During a Team's Run**: Employ the `UserProxyAgent` to provide real-time feedback while executing the methods like `BaseGroupChat.run` or `BaseGroupChat.run_stream`.\n2. **Post Run Feedback**: After a run completes, feedback can be supplied through input to subsequent calls to `BaseGroupChat.run` or `BaseGroupChat.run_stream`.\n\nFor immediate code integration examples with web frameworks, consider visiting:\n- [AgentChat + FastAPI](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_fastapi)\n- [AgentChat + ChainLit](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_chainlit)\n- [AgentChat + Streamlit](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_streamlit)\n\n## Providing Feedback During a Run\n\nThe `UserProxyAgent` serves as a proxy agent for users to automatically relay feedback. To effectively integrate the `UserProxyAgent`, instantiate it and embed it into the team prior to running the execution. The team will determine the ideal moments to solicit feedback.\n\n### Feedback Mechanics\n\nIn a `RoundRobinGroupChat`, the `UserProxyAgent` will be activated in sequence based on its input position. Conversely, in a `SelectorGroupChat`, the control mechanism\u2014either the selector prompt or function\u2014will dictate when the user agent is consulted. \n\nThe following diagram illustrates the interaction flow during a team's execution:\n\n![human-in-the-loop-user-proxy](./human-in-the-loop-user-proxy.svg)\n\nThe bold arrows indicate the feedback control flow: the team defers execution until the user provides feedback, effectively pausing its progress.\n\n```{note}\nWhen the `UserProxyAgent` is invoked during a run, it blocks the team's execution until feedback is offered or an error occurs, leading to a temporary halt that may leave the team in an unstable state.\n```\n\nGiven this blocking behavior, it is advisable to limit its use to scenarios requiring brief user interactions, such as immediate approvals or urgent alerts.\n\n### Example Code: Poetry Generation Task\n\nHere\u2019s an illustration of utilizing the `UserProxyAgent` in a `RoundRobinGroupChat` for generating poetry:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the agents.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nassistant = AssistantAgent(\"assistant\", model_client=model_client)\nuser_proxy = UserProxyAgent(\"user_proxy\", input_func=input)\n\n# Termination condition for when the user approves.\ntermination = TextMentionTermination(\"APPROVE\")\n\n# Create the team.\nteam = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n\n# Execute the conversation and stream output.\nstream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\nawait Console(stream)\nawait model_client.close()\n```\n\nIn the console output, feedback will be required from the user through the `user_proxy` to approve the generated poem.\n\n### Custom Input Functions\n\nYou can customize the feedback process by supplying your own input functions. For instance, when working with a web service, you might want to utilize a custom function to capture input via a web socket. Here's how you could implement this using the FastAPI framework:\n\n```python\n@app.websocket(\"/ws/chat\")\nasync def chat(websocket: WebSocket):\n    await websocket.accept()\n\n    async def _user_input(prompt: str, cancellation_token: CancellationToken | None) -> str:\n        data = await websocket.receive_json()\n        message = TextMessage.model_validate(data)\n        return message.content\n```\n\nCheck out the [AgentChat FastAPI sample](https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_fastapi) for a complete implementation.\n\nFor further examples of using the `UserProxyAgent` within the ChainLit framework, refer to the [AgentChat ChainLit sample](https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit).\n\n## Providing Feedback for Future Runs\n\nIt's commonplace for an application or user to interact with the team of agents in an ongoing cycle. The standard process involves:\n\n1. The team runs until a termination condition is met.\n2. The user or application supplies feedback.\n3. The team then runs again incorporating this feedback.\n\nThis method is ideal for situations that require state persistence. After the team completes a run, its state can be saved to persistent storage and resumed later upon receiving user feedback.\n\n```{note}\nRefer to [Managing State](./state.ipynb) for instructions on saving and loading team states; this section will concentrate on feedback mechanisms.\n```\n\n### Control Flow Illustration\n\nThe following diagram illustrates how control flows within this interactive process:\n\n![human-in-the-loop-termination](./human-in-the-loop-termination.svg)\n\n## Implementing Feedback Mechanism\n\nThere are two main strategies to facilitate user feedback for future runs:\n\n1. **Maximum Turns**: Set a predefined maximum number of turns for the team to ensure it pauses for user input periodically.\n2. **Termination Conditions**: Use conditions such as `TextMentionTermination` or `HandoffTermination` which allow the team to autonomously determine when to pause and exchange control.\n\n### Example: Using Maximum Turns\n\nTo achieve structured pauses for user feedback, set the `max_turns` parameter in the `RoundRobinGroupChat` to limit the number of turns to 1. This is particularly impactful for interactive applications like chatbots.\n\n```python\nteam = RoundRobinGroupChat([...], max_turns=1)\n```\n\nUpon stopping, the turn counter resets, but essential team states are preserved. The conversation can continue from where it left off, maintaining the historical context.\n\n```{note}\nThe `max_turn` feature is specific to certain team classes, currently supported by `RoundRobinGroupChat`, `SelectorGroupChat`, and `Swarm`. If termination conditions are employed, the team will pause when either condition triggers.\n```\n\n### Example: Implementing Maximum Turns\n\nHere\u2019s how to set `max_turns` in a `RoundRobinGroupChat` for a poetry task with a limit of 1 turn:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the agents.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nassistant = AssistantAgent(\"assistant\", model_client=model_client)\n\n# Create a team with limited turns.\nteam = RoundRobinGroupChat([assistant], max_turns=1)\n\ntask = \"Write a 4-line poem about the ocean.\"\nwhile True:\n    stream = team.run_stream(task=task)\n    await Console(stream)\n    task = input(\"Enter your feedback (type 'exit' to leave): \")\n    if task.lower().strip() == \"exit\":\n        break\nawait model_client.close()\n```\n\nIn this scenario, the team halts directly after the first agent's response, inviting immediate user feedback.\n\n## Utilizing Termination Conditions\n\nPreviously, we've discussed termination conditions, focusing now on `HandoffTermination`, which halts the team when an agent issues a `HandoffMessage`.\n\n### Example: Implementing Handoff Termination\n\nConsider establishing a team with a single `AssistantAgent` configured to handoff, ideally utilized for requests needing further input from the user:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import Handoff\nfrom autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create an OpenAI model client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n# Assuming the assistant will hand off when required.\nlazy_agent = AssistantAgent(\n    \"lazy_assistant\",\n    model_client=model_client,\n    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n    system_message=\"If unable to complete the task, transfer to user. Finish with 'TERMINATE'.\",\n)\n\n# Setup termination conditions for handoff and specific text.\nhandoff_termination = HandoffTermination(target=\"user\")\ntext_termination = TextMentionTermination(\"TERMINATE\")\n\n# Assemble the team with the lazy agent and termination settings.\nlazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)\n\n# Run and stream task output.\ntask = \"What is the weather in New York?\"\nawait Console(lazy_agent_team.run_stream(task=task), output_stats=True)\n```\n\nYou can observe the team stopping due to the detection of a handoff message. To facilitate the continuation, the user could provide the necessary information:\n\n```python\nawait Console(lazy_agent_team.run_stream(task=\"The weather in New York is sunny.\"))\n```\n\nThis process allows the team to proceed once user input is received, showcasing the dynamics of user-agent interaction effectively.\n\n```{note}\nWhen deploying the `Swarm` team utilizing `HandoffTermination` targeting a user, remember to set the `task` as a `HandoffMessage` indicating which agent should resume. Refer to the [Swarm Documentation](../swarm.ipynb) for further guidance.\n``` \n\nThis comprehensive documentation should serve as a solid foundation for working with human feedback in the AgentChat framework.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.ipynb"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "Tutorial for AgentChat, a high-level API for AutoGen\n"
      }
    },
    "content": "# Introduction\n\nThis tutorial provides a step-by-step guide to using AgentChat.\nMake sure you have first followed the [installation instructions](../installation.md)\nto prepare your environment.\n\nAt any point you are stuck, feel free to ask for help on\n[GitHub Discussions](https://github.com/microsoft/autogen/discussions)\nor [Discord](https://aka.ms/autogen-discord).\n\n```{note}\nIf you are coming from AutoGen v0.2, please read the [migration guide](../migration-guide.md).\n```\n\n::::{grid} 2 2 2 2\n:gutter: 3\n\n:::{grid-item-card} {fas}`brain;pst-color-primary` Models\n:link: ./models.html\n:link-alt: Models: How to use LLM model clients\n\nHow to use LLM model clients\n:::\n\n:::{grid-item-card} {fas}`envelope;pst-color-primary` Messages\n:link: ./messages.html\n:link-alt: Messages: Understand the message types\n\nUnderstand the message types\n:::\n\n:::{grid-item-card} {fas}`robot;pst-color-primary` Agents\n:link: ./agents.html\n:link-alt: Agents: Work with AgentChat agents and get started with autogen_agentchat.agents.AssistantAgent\n\nWork with AgentChat agents and get started with {py:class}`~autogen_agentchat.agents.AssistantAgent`\n:::\n\n:::{grid-item-card} {fas}`sitemap;pst-color-primary` Teams\n:link: ./teams.html\n:link-alt: Teams: Work with teams of agents and get started with autogen_agentchat.teams.RoundRobinGroupChat.\n\nWork with teams of agents and get started with {py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`.\n:::\n\n:::{grid-item-card} {fas}`person-chalkboard;pst-color-primary` Human-in-the-Loop\n:link: ./human-in-the-loop.html\n:link-alt: Human-in-the-Loop: Best practices for providing feedback to a team\n\nBest practices for providing feedback to a team\n:::\n\n:::{grid-item-card} {fas}`circle-stop;pst-color-primary` Termination\n:link: ./termination.html\n:link-alt: Termination: Control a team using termination conditions\n\nControl a team using termination conditions\n:::\n\n:::{grid-item-card} {fas}`code;pst-color-primary` Custom Agents\n:link: ./custom-agents.html\n:link-alt: Custom Agents: Create your own agents\n\nCreate your own agents\n:::\n\n:::{grid-item-card} {fas}`database;pst-color-primary` Managing State\n:link: ./state.html\n:link-alt: Managing State: Save and load agents and teams for persistent sessions\n\nSave and load agents and teams for persistent sessions\n:::\n::::",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/index.md"
  },
  {
    "code": false,
    "content": "# Messages in AutoGen AgentChat\n\nIn AutoGen AgentChat, _messages_ are essential for facilitating communication and information exchange among agents, orchestrators, and applications. The AgentChat framework supports multiple message types, each tailored for specific tasks. This document elucidates the various types of messages, their purposes, and how to implement them effectively.\n\n## Types of Messages\n\nMessages in AgentChat can broadly be categorized into two types: **agent-agent messages** and **internal events**. The following sections delve into each type, providing examples and insights into their usage.\n\n### Agent-Agent Messages\n\nAgent-agent messages are designed for communication between different agents within the AgentChat ecosystem. These messages are subclasses of the base class `BaseChatMessage`. The concrete subclasses facilitate a range of communication forms, including simple text messages and more complex multimodal messages.\n\n#### Text Messages\n\nText messages allow for straightforward exchange of textual information. To create a text message, the `TextMessage` class is utilized, which requires a string as the content and a source identifier. Here\u2019s a simple demonstration:\n\n```python\nfrom autogen_agentchat.messages import TextMessage\n\n# Creating a text message\ntext_message = TextMessage(content=\"Hello, world!\", source=\"User\")\n```\n\nThis snippet creates a text message that can be sent to other agents. It encapsulates a simple greeting, indicating effective agent-to-agent communication.\n\n#### MultiModal Messages\n\nIn situations where messages need to incorporate various types of content such as text and images, the `MultiModalMessage` class is employed. This class can accept a list that includes strings or `Image` objects from the `autogen_core` module. The next snippet illustrates how to create a multimodal message:\n\n```python\nfrom io import BytesIO\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image as AGImage\nfrom PIL import Image\n\n# Fetching and processing an image\npil_image = Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = AGImage(pil_image)\n\n# Creating a multimodal message\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"User\")\nimg\n```\n\nIn this example, an image is fetched from the web and converted into a format suitable for the `MultiModalMessage`. The use of multimodal messages enriches the interaction, allowing agents to communicate with a broader set of data.\n\n### Utilizing Messages\n\nThe messages created using either `TextMessage` or `MultiModalMessage` can be sent directly to agents through the `ChatAgent.on_messages` method. Alternatively, they can be used as tasks within a group context via the `BaseGroupChat.run` method. Additionally, messages play a critical role in the responses generated by agents, which will be further discussed in the [Agents](./agents.ipynb) and [Teams](./teams.ipynb) documents.\n\n### Internal Events\n\nIn addition to agent-agent messages, AgentChat supports internal events. These messages are pivotal for communicating various actions that occur within an agent. They belong to subclasses of the `BaseAgentEvent` class and typically include events that indicate tool calls or execution results.\n\n#### Tool Call Events\n\nExamples of internal event messages include `ToolCallRequestEvent`, which signals that a tool has been requested, and `ToolCallExecutionEvent`, which communicates the results of such calls. Events are generally created by the agent itself and stored in the `Response.inner_messages` attribute of the `Response` object returned from the `ChatAgent.on_messages` method.\n\n```python\n# Example of how an event might be utilized\nevent = ToolCallRequestEvent(tool_id=\"example_tool\", parameters={\"param1\": \"value1\"})\n```\n\nIn this instance, an event is created to request a tool. This functionality enables agents to react dynamically to actions and report back necessary information to the user interface or other agents.\n\n### Custom Agent Events\n\nIf you are developing a customized agent and need to communicate internal events to external entities (like a UI), you can include these events in the `Response.inner_messages` field. This flexibility allows for sophisticated interactions and is essential for building responsive applications. For practical examples, refer to the [Custom Agents](../custom-agents.ipynb) documentation.\n\n## Custom Message Types\n\nOne of the strengths of AgentChat is the ability to create custom message types by subclassing the `BaseChatMessage` or `BaseAgentEvent` classes. This capability allows you to define specific message formats and behaviors tailored to your application's requirements. Custom message types facilitate writing bespoke agents that can address unique tasks more efficiently.\n\n### Creating a Custom Message\n\nTo create a custom message, subclass one of the base classes and implement your specific logic. Below is a simplified example:\n\n```python\nfrom autogen_agentchat.messages import BaseChatMessage\n\nclass CustomMessage(BaseChatMessage):\n    def __init__(self, custom_data, source):\n        super().__init__(source=source)\n        self.custom_data = custom_data\n```\n\nIn this snippet, a new message type called `CustomMessage` is defined. It can encapsulate any required data along with the source of the message, providing a foundational structure for developers to expand upon.\n\nBy leveraging these capabilities, you can tailor your messages to fit exactly what your application needs, enhancing the overall functionality and user experience of your agents. \n\n---\n\nThis documentation provides a concise, structured overview of messages in AutoGen AgentChat, along with practical examples to guide you in implementing effective messaging strategies. For a deeper dive, continue exploring related documentation on Agents and Teams.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/messages.ipynb"
  },
  {
    "code": false,
    "content": "# Models Overview\n\nIn the field of AI, agents often require access to Large Language Models (LLMs) offered by various providers such as OpenAI, Azure OpenAI, or local models. Due to the diversity in APIs across different service providers, the `autogen-core` library implements a protocol for model clients, while `autogen-ext` provides a suite of model clients for popular services. These model clients enable the `AgentChat` framework to effectively interact with these model services. \n\nThis documentation outlines the available model clients and provides high-level examples of how to use them within your applications. For detailed instructions, please refer to the [Model Clients guide](../../core-user-guide/components/model-clients.ipynb) in the Core API documentation.\n\n## Logging Model Calls\n\nEffective logging is crucial when integrating with model services. The `AutoGen` framework employs Python's built-in logging module to track events related to model calls and their responses. The logger is identified by the name `{py:attr}`autogen_core.EVENT_LOGGER_NAME`, and the relevant event type is `LLMCall`.\n\nTo configure logging, you can use the following code snippet:\n\n```python\nimport logging\n\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\nThis setup initializes the logging system to output warning messages and above to the console, providing visibility into crucial interactions with the model APIs.\n\n## OpenAI Model Client\n\nTo utilize OpenAI models, you first need to install the `openai` extension, which allows access to the `{py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient`.\n\n### Installation\n\n```python\npip install \"autogen-ext[openai]\"\n```\n\nYou will also need an [API key](https://platform.openai.com/account/api-keys) from OpenAI to authenticate your requests.\n\n### Client Initialization\n\nAfter obtaining your API key, initialize the OpenAI model client as shown below:\n\n```python\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nopenai_model_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-2024-08-06\",\n    # api_key=\"sk-...\",  # Optional if OPENAI_API_KEY environment variable is set.\n)\n```\n\n### Testing the Model Client\n\nTo verify that the client works as expected, you can send a simple prompt and await its response:\n\n```python\nfrom autogen_core.models import UserMessage\n\nresult = await openai_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait openai_model_client.close()\n```\n\nThis code constructs a user message asking for the capital of France and prints the model's response. Note that this client can also handle models hosted on OpenAI-compatible endpoints, although thorough testing is advised.\n\n## Azure OpenAI Model Client\n\nFor accessing Azure OpenAI models, you will need to install both the `azure` and `openai` extensions, enabling you to use the `{py:class}`~autogen_ext.models.openai.AzureOpenAIChatCompletionClient.\n\n### Installation\n\n```python\npip install \"autogen-ext[openai,azure]\"\n```\n\n### Client Configuration\n\nThe Azure model client requires specific credentials, including the deployment ID, Azure Cognitive Services endpoint, API version, and model capabilities. You can authenticate using either an API key or an Azure Active Directory (AAD) token credential.\n\nHere is an example of using AAD authentication:\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.auth.azure import AzureTokenProvider\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom azure.identity import DefaultAzureCredential\n\n# Create the token provider\ntoken_provider = AzureTokenProvider(\n    DefaultAzureCredential(),\n    \"https://cognitiveservices.azure.com/.default\"\n)\n\naz_model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"{model-name, such as gpt-4o}\",\n    api_version=\"2024-06-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    azure_ad_token_provider=token_provider,  # Optional if using key-based authentication.\n    # api_key=\"sk-...\",  # Alternative for key-based authentication.\n)\n```\n\n### Sending a Request\n\nTo send a test request to the Azure model client, use the following code:\n\n```python\nresult = await az_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait az_model_client.close()\n```\n\nThis snippet sends a user message to retrieve information, similar to the OpenAI example. For more details on using the Azure client, refer to [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity#chat-completions).\n\n## Azure AI Foundry Client\n\nAzure AI Foundry (formerly Azure AI Studio) hosts various models on Azure. To access these models, utilize the `{py:class}`~autogen_ext.models.azure.AzureAIChatCompletionClient.\n\n### Installation\n\n```python\npip install \"autogen-ext[azure]\"\n```\n\n### Client Setup\n\nHere is an example of how to set up the Azure AI Foundry client using the Phi-4 model from GitHub Marketplace:\n\n```python\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = AzureAIChatCompletionClient(\n    model=\"Phi-4\",\n    endpoint=\"https://models.github.ai/inference\",\n    credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),  # Personal Access Token required.\n    model_info={\n        \"json_output\": False,\n        \"function_calling\": False,\n        \"vision\": False,\n        \"family\": \"unknown\",\n        \"structured_output\": False,\n    },\n)\n\nresult = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait client.close()\n```\n\nThis code initializes the Azure AI Foundry client with the required credentials and settings, then sends a user message to retrieve the capital of France.\n\n## Anthropic Model Client (Experimental)\n\nYou can also utilize the `{py:class}`~autogen_ext.models.anthropic.AnthropicChatCompletionClient` for accessing Anthropic models. \n\n### Installation\n\n```python\n# !pip install -U \"autogen-ext[anthropic]\"\n```\n\n### Example Usage\n\nThe following snippet demonstrates how to initialize the Anthropic model client and send a request:\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\n\nanthropic_client = AnthropicChatCompletionClient(model=\"claude-3-7-sonnet-20250219\")\nresult = await anthropic_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\nawait anthropic_client.close()\n```\n\nThis example illustrates the process of sending a user message and retrieving the model's response.\n\n## Ollama Model Client (Experimental)\n\nOllama provides a local model server where you can run AI models. \n\n### Note on Local Models\n\nIt's important to note that small local models often do not perform as well as larger cloud models. Output may vary significantly based on the task.\n\n### Installation \n\nTo set up the Ollama client, install the necessary extension:\n\n```python\npip install -U \"autogen-ext[ollama]\"\n```\n\n### Client Setup\n\nOnce the client is installed, you can use it as follows:\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\n\n# Ensure your Ollama server is running locally on port 11434.\nollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n\nresponse = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait ollama_model_client.close()\n```\n\nThis code snippet shows how to initialize the Ollama model client and retrieve information.\n\n## Gemini Model Client (Experimental)\n\nGemini offers an OpenAI-compatible API for its services. You can utilize the `{py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient with Gemini's API.\n\n### Example Usage\n\nHere\u2019s how to initialize the client for Gemini models:\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gemini-1.5-flash-8b\",\n    # api_key=\"GEMINI_API_KEY\"\n)\n\nresponse = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait model_client.close()\n```\n\nFor newer models, you may need to specify additional capabilities through `model_info`.\n\n### Handling New Models\n\nWhen using models like `gemini-2.0-flash-lite`, define their capabilities as follows:\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import ModelInfo\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gemini-2.0-flash-lite\",\n    model_info=ModelInfo(vision=True, function_calling=True, json_output=True, family=\"unknown\", structured_output=True)\n    # api_key=\"GEMINI_API_KEY\",\n)\n\nresponse = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(response)\nawait model_client.close()\n```\n\nThis outlines how to work with both basic and advanced model capabilities.\n\n## Llama API Client (Experimental)\n\nThe Llama API from Meta provides an OpenAI-compatible endpoint. Similar to previous examples, you can utilize `{py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient with Llama API.\n\n### Example Usage\n\nBelow is an example that showcases both text and image capabilities:\n\n```python\nfrom pathlib import Path\nfrom autogen_core import Image\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Text Completion\nmodel_client = OpenAIChatCompletionClient(\n    model=\"Llama-4-Scout-17B-16E-Instruct-FP8\",\n    # api_key=\"LLAMA_API_KEY\"\n)\n\nresponse = await model_client.create([UserMessage(content=\"Write me a poem\", source=\"user\")])\nprint(response)\nawait model_client.close()\n\n# Image Analysis\nmodel_client = OpenAIChatCompletionClient(\n    model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    # api_key=\"LLAMA_API_KEY\"\n)\nimage = Image.from_file(Path(\"test.png\"))\n\nresponse = await model_client.create([UserMessage(content=[\"What is in this image\", image], source=\"user\")])\nprint(response)\nawait model_client.close()\n```\n\nThis snippet demonstrates how to generate responses for both text and image inputs.\n\n## Semantic Kernel Adapter\n\nThe `{py:class}`~autogen_ext.models.semantic_kernel.SKChatCompletionAdapter enables the use of Semantic Kernel model clients as `{py:class}`~autogen_core.models.ChatCompletionClient.\n\n### Installation\n\nYou need to install specific extras for the different providers:\n\n```plaintext\n- `semantic-kernel-anthropic`: For Anthropic models.\n- `semantic-kernel-google`: For Google Gemini models.\n- `semantic-kernel-ollama`: For Ollama models.\n- `semantic-kernel-mistralai`: For Mistral models.\n- `semantic-kernel-aws`: For AWS models.\n- `semantic-kernel-hugging-face`: For Hugging Face models.\n```\n\nFor example, to use Anthropic models, install:\n\n```python\n# pip install \"autogen-ext[semantic-kernel-anthropic]\"\n```\n\n### Example Setup\n\nTo set up an Anthropic model client using the Semantic Kernel adapter, follow this example:\n\n```python\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.semantic_kernel import SKChatCompletionAdapter\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings\nfrom semantic_kernel.memory.null_memory import NullMemory\n\nsk_client = AnthropicChatCompletion(\n    ai_model_id=\"claude-3-5-sonnet-20241022\",\n    api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n    service_id=\"my-service-id\",  # Optional; for targeting specific services within Semantic Kernel\n)\nsettings = AnthropicChatPromptExecutionSettings(\n    temperature=0.2,\n)\n\nanthropic_model_client = SKChatCompletionAdapter(\n    sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=settings\n)\n\n# Call the model directly.\nmodel_result = await anthropic_model_client.create(\n    messages=[UserMessage(content=\"What is the capital of France?\", source=\"User\")]\n)\nprint(model_result)\nawait anthropic_model_client.close()\n```\n\nThis example showcases how to utilize the Semantic Kernel adapter effectively. For further exploration, refer to the [Semantic Kernel Adapter documentation](../../../reference/python/autogen_ext.models.semantic_kernel.rst).",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/models.ipynb"
  },
  {
    "code": false,
    "content": "# Managing State\n\nIn multi-agent applications, maintaining the state of various components such as agents, teams, and termination conditions is crucial. This document outlines how to save and load this state effectively, particularly in a web application context where stateless endpoints operate on persistent storage.\n\n## Saving and Loading Agents\n\nTo manage the state of an agent, we use the `save_state` method from the `AssistantAgent` class. This functionality allows you to capture the current state of an agent, which can later be reloaded using the `load_state` method.\n\nFirst, we initiate an instance of the `AssistantAgent`, set up a model client, and communicate with the agent:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\n\n# Use asyncio.run(...) when running in a script.\nresponse = await assistant_agent.on_messages(\n    [TextMessage(content=\"Write a 3 line poem on lake tangayika\", source=\"user\")], CancellationToken()\n)\nprint(response.chat_message)\nawait model_client.close()\n```\n\nIn this block, we create an `AssistantAgent`, send a message requesting a poem, and print the response. The response is generated asynchronously utilizing the model client.\n\nNext, we save the state of the agent:\n\n```python\nagent_state = await assistant_agent.save_state()\nprint(agent_state)\n```\n\nHere, the `save_state` method is called to preserve the current state of the agent, and the result is printed for verification.\n\nSubsequently, we can create a new instance of `AssistantAgent` and load the previously saved state:\n\n```python\nnew_assistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\nawait new_assistant_agent.load_state(agent_state)\n\n# Use asyncio.run(...) when running in a script.\nresponse = await new_assistant_agent.on_messages(\n    [TextMessage(content=\"What was the last line of the previous poem you wrote\", source=\"user\")], CancellationToken()\n)\nprint(response.chat_message)\nawait model_client.close()\n```\n\nIn this step, we initialize a new agent and load the saved state. By querying the last line of the previously generated poem, we ensure that loading the state operates as intended.\n\n```{note}\nFor the `AssistantAgent`, its state consists of the model context. If you develop a custom agent, it's advisable to override the `save_state` and `load_state` methods from `BaseChatAgent` to tailor the saving and loading behaviors, as the defaults save and load an empty state.\n```\n\n## Saving and Loading Teams \n\nSimilar to agents, teams can also be serialized and deserialized. When invoking the `save_state` method on a team, the state of all included agents will be preserved, allowing for a seamless restoration of context.\n\nHere's an example where we create a `RoundRobinGroupChat` team featuring a single agent and request a poem:\n\n```python\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\n# Define a team.\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    system_message=\"You are a helpful assistant\",\n    model_client=model_client,\n)\nagent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\n\n# Run the team and stream messages to the console.\nstream = agent_team.run_stream(task=\"Write a beautiful poem 3-line about lake tangayika\")\n\n# Use asyncio.run(...) when running in a script.\nawait Console(stream)\n\n# Save the state of the agent team.\nteam_state = await agent_team.save_state()\n```\n\nIn this segment, we define a new team with the assistant agent and direct it to compose a poem. The team state is then saved for later use.\n\nTo illustrate the importance of state management, we can reset the team and attempt to query the last line of the previous poem:\n\n```python\nawait agent_team.reset()\nstream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\n```\n\nThis attempt showcases that without restoring state, the team cannot reference any prior interactions.\n\nNext, we load the team\u2019s state and re-attempt the question about the poem:\n\n```python\nprint(team_state)\n\n# Load team state.\nawait agent_team.load_state(team_state)\nstream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\n```\n\nHere, after reloading the saved state, the team is capable of correctly responding to the inquiry, demonstrating the effectiveness of state management.\n\n## Persisting State (File or Database)\n\nFor durability, it is often necessary to persist team states to disk or a database. The agent state can be serialized into a format like JSON, allowing for easy storage and retrieval.\n\nWe begin by saving the state to a file:\n\n```python\nimport json\n\n# Save state to disk\nwith open(\"coding/team_state.json\", \"w\") as f:\n    json.dump(team_state, f)\n```\n\nIn this code block, we utilize the `json` library to write the team state to a JSON file, ensuring the state is safely stored.\n\nNext, we load the state from the same file:\n\n```python\n# Load state from disk\nwith open(\"coding/team_state.json\", \"r\") as f:\n    team_state = json.load(f)\n\nnew_agent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\nawait new_agent_team.load_state(team_state)\nstream = new_agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\nawait Console(stream)\nawait model_client.close()\n```\n\nUpon loading the state, we instantiate a new team and load the serialized data, confirming that state persistence mechanisms work as intended. This process demonstrates how we can maintain continuity in agent interactions over time.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/state.ipynb"
  },
  {
    "code": false,
    "content": "# Teams Documentation\n\nThis documentation provides a comprehensive guide to creating and managing multi-agent teams using AutoGen. A team consists of multiple agents that collaborate to achieve a common objective, making it suitable for complex tasks requiring diverse expertise.\n\n## Overview of Team Types\n\nAutoGen supports various team presets that cater to different collaborative needs:\n\n- **RoundRobinGroupChat**: Agents respond in a round-robin order while maintaining a shared context.\n- **SelectorGroupChat**: The next speaker is determined by a ChatCompletion model after each message.\n- **MagenticOneGroupChat**: A versatile multi-agent system that addresses open-ended tasks across multiple domains.\n- **Swarm**: Agents transition smoothly between roles using specific messages to signal these transitions.\n\n```{note}\n**When should you use a team?**\n\nTeams are beneficial for complex tasks requiring collaboration. However, they involve additional complexity compared to single agents. Therefore, it\u2019s advisable to optimize your single agent first before transitioning to a team setup.\n```\n\n## Creating a Team\n\nIn this section, we focus on creating a team using the `RoundRobinGroupChat` class. This configuration allows agents to share context and respond in a sequential manner, facilitating coherence in their conversation. \n\nWe will construct a simple team consisting of two `AssistantAgent` instances and a termination condition that stops the agents when a specific word appears in their responses. This setup demonstrates the reflection pattern, where a critic agent provides feedback on a primary agent's performance.\n\n```python\nimport asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.conditions import ExternalTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize the OpenAI model client\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-2024-08-06\",\n    # Optional: Set API key if not provided as an environment variable.\n)\n\n# Set up the primary agent\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Set up the critic agent\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\n# Termination condition to stop the team when the critic approves.\ntext_termination = TextMentionTermination(\"APPROVE\")\n\n# Create the team with the primary and critic agents.\nteam = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)\n```\n\nIn this implementation, the `primary_agent` acts as the main contributor, while the `critic_agent` evaluates and provides feedback. The termination condition checks for the word \"APPROVE\" to determine when to stop the team.\n\n## Running a Team\n\nTo initiate the team, we will call the `run` method. This method engages the agents in a round-robin fashion until the termination condition is satisfied.\n\n```python\n# Use `asyncio.run(...)` when executing this in a script.\nresult = await team.run(task=\"Write a short poem about the fall season.\")\nprint(result)\n```\n\nThe agents collaborate by sharing their responses with the team. The session continues until the critic agent approves the primary agent's output. Once the task concludes, a `TaskResult` object containing all generated messages is returned.\n\n## Observing a Team's Behavior\n\nTo monitor the team's output in real time, use the `run_stream` method. This function streams messages produced by the agents as they generate them.\n\n```python\n# Use async main function in scripts.\nawait team.reset()  # Prepare the team for a new task.\nasync for message in team.run_stream(task=\"Write a short poem about the fall season.\"):\n    if isinstance(message, TaskResult):\n        print(\"Stop Reason:\", message.stop_reason)\n    else:\n        print(message)\n```\n\nBy assessing the `stop_reason` attribute, you can gain insights into why the team concluded its session. The `Console` utility allows you to format and display messages more neatly.\n\n```python\nawait team.reset()  # Reset for next task.\nawait Console(team.run_stream(task=\"Write a short poem about the fall season.\"))\n```\n\n## Resetting a Team\n\nTo prepare the team for a new task, use the `reset` method. This clears the team's internal state, allowing your agents to start fresh.\n\n```python\nawait team.reset()  # Prepare the team for subsequent runs.\n```\n\nIt is advisable to reset the team if the next task differs significantly from the previous task. If the new task is related, you may continue without resetting.\n\n## External Termination\n\nBesides built-in termination conditions, teams can also be halted externally using `ExternalTermination`. This allows agents to complete their current turn before stopping.\n\n```python\n# Create an External Termination condition.\nexternal_termination = ExternalTermination()\nteam = RoundRobinGroupChat(\n    [primary_agent, critic_agent],\n    termination_condition=external_termination | text_termination,\n)\n\n# Run the team as a background task.\nrun = asyncio.create_task(Console(team.run_stream(task=\"Write a short poem about the fall season.\")))\n\n# Allow for some processing time.\nawait asyncio.sleep(0.1)\n\n# Initiate external termination.\nexternal_termination.set()\n\n# Wait for the team to finish.\nawait run\n```\n\nThe example illustrates how the team concludes upon meeting the external termination condition yet allows the agent to finish its current message.\n\n## Resuming a Team\n\nTeams maintain their state between runs, enabling you to continue conversations seamlessly. You can resume a team by calling the `run` or `run_stream` methods without a new task.\n\n```python\nawait Console(team.run_stream())  # Resume the team to continue.\n```\n\nThis will cause the next agent to speak from where the previous session left off. You can also set a new task while maintaining context.\n\n```python\n# Set a new task to translate the poem into Chinese Tang-style poetry.\nawait Console(team.run_stream(task=\"\u5c06\u8fd9\u9996\u8bd7\u7528\u4e2d\u6587\u5510\u8bd7\u98ce\u683c\u5199\u4e00\u904d\u3002\"))\n```\n\n## Aborting a Team\n\nIf you need to halt a team\u2019s execution, use a `CancellationToken`. This will immediately stop the ongoing operation, raising an exception.\n\n```{note}\nThe caller will receive an `asyncio.CancelledError` exception if the team is aborted.\n```\n\n```python\n# Initialize a cancellation token.\ncancellation_token = CancellationToken()\n\n# Run the team in another coroutine.\nrun = asyncio.create_task(\n    team.run(\n        task=\"Translate the poem to Spanish.\",\n        cancellation_token=cancellation_token,\n    )\n)\n\n# Trigger cancellation.\ncancellation_token.cancel()\n\ntry:\n    result = await run  # This will raise a CancelledError.\nexcept asyncio.CancelledError:\n    print(\"Task was cancelled.\")\n```\n\n## Single-Agent Team Configuration\n\nStarting from version 0.6.2, you can run a single `AssistantAgent` in a team-like setup with multiple tool iterations, eliminating the need for a full team configuration.\n\nThis is particularly useful for tasks where continuous engagement from a single agent is essential. Below is an example where an agent increments a number until it reaches a specified limit using a tool.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    # API key setting options.\n    parallel_tool_calls=False,  # Disable parallel tool execution for this example.\n)\n\n# Define a function to increment a number.\ndef increment_number(number: int) -> int:\n    \"\"\"Increment a number by 1.\"\"\"\n    return number + 1\n\n# Instantiate a tool agent utilizing the increment function.\nlooped_assistant = AssistantAgent(\n    \"looped_assistant\",\n    model_client=model_client,\n    tools=[increment_number],  # Register the incrementing tool.\n    system_message=\"You are a helpful AI assistant, use the tool to increment the number.\",\n)\n\n# Termination condition when the agent responds with a text message.\ntermination_condition = TextMessageTermination(\"looped_assistant\")\n\n# Create a team for the tool agent.\nteam = RoundRobinGroupChat(\n    [looped_assistant],\n    termination_condition=termination_condition,\n)\n\n# Execute the team with a task to increment a number.\nasync for message in team.run_stream(task=\"Increment the number 5 to 10.\"):\n    print(type(message).__name__, message)\n\nawait model_client.close()\n```\n\nIn this program, the key focus is on the termination condition which enables the team to continue until the agent generates a result message. Other termination conditions can also be utilized as per your requirements. For further details, please refer to the section on [Termination Conditions](./termination.ipynb).",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/teams.ipynb"
  },
  {
    "code": false,
    "content": "# Termination\n\nIn the previous section, we explored how to define agents and organize them into teams that can solve tasks. However, an agent's run could potentially continue indefinitely. Therefore, it's essential to implement a mechanism that determines when these runs should stop. This is the role of the termination condition.\n\nAgentChat provides a robust infrastructure to support various termination conditions through a base class, `{py:class}`~autogen_agentchat.base.TerminationCondition`, with multiple implementations that inherit from it. \n\nA termination condition is essentially a callable entity that accepts a sequence of either `{py:class}`~autogen_agentchat.messages.BaseAgentEvent or `{py:class}`~autogen_agentchat.messages.BaseChatMessage objects **since the last invocation of the condition**. It returns a `{py:class}`~autogen_agentchat.messages.StopMessage` when the conversation should terminate or `None` when it should continue. Once a termination condition is triggered, it must be reset by invoking `{py:meth}`~autogen_agentchat.base.TerminationCondition.reset` before it can be reused.\n\n## Key Features of Termination Conditions\n\n- **Statefulness**: Termination conditions maintain their state throughout a run but reset automatically when the run is completed. This auto-reset feature applies to methods like `{py:meth}`~autogen_agentchat.base.TaskRunner.run` or `{py:meth}`~autogen_agentchat.base.TaskRunner.run_stream`.\n- **Combination via Operators**: You can create complex termination logic by combining different termination conditions using AND (`&`) and OR (`|`) operators.\n\n```{note}\nFor teams engaging in group chats (like `{py:class}`~autogen_agentchat.teams.RoundRobinGroupChat`, `{py:class}`~autogen_agentchat.teams.SelectorGroupChat`, and `{py:class}`~autogen_agentchat.teams.Swarm`), the termination condition is invoked after each agent's response. Notably, while a single response may involve multiple inner messages, the team only calls the termination condition once for the accumulated messages since the last invocation. This results in what's known as the \"delta sequence\" of messages.\n```\n\n## Built-In Termination Conditions\n\nAgentChat includes several built-in termination conditions:\n\n1. **{py:class}`~autogen_agentchat.conditions.MaxMessageTermination`**: Stops the conversation after a specified number of messages have been generated.\n2. **{py:class}`~autogen_agentchat.conditions.TextMentionTermination`**: Terminates when a specific text, such as \"TERMINATE\", is mentioned.\n3. **{py:class}`~autogen_agentchat.conditions.TokenUsageTermination`**: Triggers termination when a set number of prompt or completion tokens is used.\n4. **{py:class}`~autogen_agentchat.conditions.TimeoutTermination`**: Ends the conversation after a predetermined time period.\n5. **{py:class}`~autogen_agentchat.conditions.HandoffTermination`**: Stops when a handoff to a specified target is requested.\n6. **{py:class}`~autogen_agentchat.conditions.SourceMatchTermination`**: Terminates the conversation upon a specific agent's response.\n7. **{py:class}`~autogen_agentchat.conditions.ExternalTermination`**: Allows for external programmatic control over the termination, ideal for UI interfaces.\n8. **{py:class}`~autogen_agentchat.conditions.StopMessageTermination`**: Ends when a `{py:class}`~autogen_agentchat.messages.StopMessage` is produced.\n9. **{py:class}`~autogen_agentchat.conditions.TextMessageTermination`**: Stops upon receiving a `{py:class}`~autogen_agentchat.messages.TextMessage` from an agent.\n10. **{py:class}`~autogen_agentchat.conditions.FunctionCallTermination`**: Ends when a matching function call result is received.\n11. **{py:class}`~autogen_agentchat.conditions.FunctionalTermination`**: Stops when a custom function expression evaluates to `True` based on the latest message delta.\n\n## Basic Usage\n\nTo illustrate the use of termination conditions, we'll create a team consisting of two agents: a primary agent responsible for generating text and a critic agent that reviews and provides feedback on the generated output.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize the OpenAIChatCompletion client with specified parameters.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    temperature=1,\n)\n\n# Create the primary agent tasked with text generation.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create the critic agent that provides feedback.\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    system_message=\"Provide constructive feedback for every message. Respond with 'APPROVE' when your feedback is addressed.\",\n)\n```\n\nIn the setup above, we have initialized two agents\u2014one for generating text and the other for critique. This forms the basis of our conversation structure.\n\n### Demonstrating Termination Reset\n\nNow, let's see how termination conditions automatically reset after each run or run_stream call, allowing the conversation to pick up from where it left off.\n\n```python\nmax_msg_termination = MaxMessageTermination(max_messages=3)\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)\n\n# Execute the team conversation. (Use asyncio.run(...) if running standalone)\nawait Console(round_robin_team.run_stream(task=\"Write a unique Haiku about the weather in Paris\"))\n```\n\nHere, the conversation halts after reaching the maximum message limit of 3. Since the primary agent didn't respond to the critic's input, we can continue the conversation:\n\n```python\n# Continue the conversation from the last state of the round_robin_team.\nawait Console(round_robin_team.run_stream())\n```\n\nBy running the code above, the team resumes the conversation, allowing the primary agent to respond to feedback.\n\n## Combining Termination Conditions\n\nTermination conditions can be combined using logical operators to create more complex termination environments. In this example, we\u2019ll configure a team to stop when either 10 messages are produced or when the critic agent approves a message.\n\n```python\nmax_msg_termination = MaxMessageTermination(max_messages=10)\ntext_termination = TextMentionTermination(\"APPROVE\")\ncombined_termination = max_msg_termination | text_termination\n\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=combined_termination)\n\n# Execute the team conversation. (Use asyncio.run(...) if running standalone)\nawait Console(round_robin_team.run_stream(task=\"Write a unique Haiku about the weather in Paris\"))\n```\n\nIn this setup, the conversation will stop once the critic agent approves the message or when the maximum limit of 10 messages is reached.\n\nIf we want to implement a scenario where the conversation stops only when both conditions are fulfilled, we can utilize the AND (`&`) operator:\n\n```python\ncombined_termination = max_msg_termination & text_termination\n```\n\nThis setup signifies that both conditions need to be satisfied for termination to occur.\n\n## Custom Termination Condition\n\nWhile the built-in termination conditions cater to many scenarios, you may need to implement a custom termination condition. This involves subclassing the `{py:class}`~autogen_agentchat.base.TerminationCondition class.\n\nIn this example, we will create a custom termination condition that triggers when a specific function call is executed.\n\n```python\nfrom typing import Sequence\nfrom autogen_agentchat.base import TerminatedException, TerminationCondition\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage, ToolCallExecutionEvent\nfrom autogen_core import Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\n\nclass FunctionCallTerminationConfig(BaseModel):\n    \"\"\"Configuration for the termination condition, enabling serialization\n    and deserialization of the component.\n    \"\"\"\n    function_name: str\n\n\nclass FunctionCallTermination(TerminationCondition, Component[FunctionCallTerminationConfig]):\n    \"\"\"Terminate when a specific FunctionExecutionResult is received.\"\"\"\n\n    component_config_schema = FunctionCallTerminationConfig\n    component_provider_override = \"autogen_agentchat.conditions.FunctionCallTermination\"\n\n    def __init__(self, function_name: str) -> None:\n        self._terminated = False\n        self._function_name = function_name\n\n    @property\n    def terminated(self) -> bool:\n        return self._terminated\n\n    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:\n        if self._terminated:\n            raise TerminatedException(\"Termination condition has already been triggered.\")\n        for message in messages:\n            if isinstance(message, ToolCallExecutionEvent):\n                for execution in message.content:\n                    if execution.name == self._function_name:\n                        self._terminated = True\n                        return StopMessage(\n                            content=f\"Function '{self._function_name}' was executed.\",\n                            source=\"FunctionCallTermination\",\n                        )\n        return None\n\n    async def reset(self) -> None:\n        self._terminated = False\n\n    def _to_config(self) -> FunctionCallTerminationConfig:\n        return FunctionCallTerminationConfig(\n            function_name=self._function_name,\n        )\n\n    @classmethod\n    def _from_config(cls, config: FunctionCallTerminationConfig) -> Self:\n        return cls(\n            function_name=config.function_name,\n        )\n```\n\n### Implementing the Custom Termination Condition\n\nNext, we'll employ this new termination condition to stop the conversation when an approve action is executed by the critic agent.\n\nFirst, let\u2019s define the approve function:\n\n```python\ndef approve() -> None:\n    \"\"\"Approve the message when all feedback has been incorporated.\"\"\"\n    pass\n```\n\nNow, we create our agents and equip the critic agent with the `approve` tool:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize the OpenAIChatCompletion client.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    temperature=1,\n)\n\n# Create the primary agent responsible for generation.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create the critic agent equipped with the approve tool.\ncritic_agent = AssistantAgent(\n    \"critic\",\n    model_client=model_client,\n    tools=[approve],  # Register the approve tool.\n    system_message=\"Provide constructive feedback. Use the approve tool to signal message approval.\",\n)\n```\n\n### Running the Conversation\n\nFinally, we will establish the team and execute the conversation task as follows:\n\n```python\nfunction_call_termination = FunctionCallTermination(function_name=\"approve\")\nround_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=function_call_termination)\n\n# Execute the team conversation. (Use asyncio.run(...) if running standalone)\nawait Console(round_robin_team.run_stream(task=\"Write a unique Haiku about the weather in Paris\"))\nawait model_client.close()\n```\n\nYou will observe that the conversation concludes when the critic agent utilizes the `approve` function call to signify message approval.\n\nThis concludes our exploration of the termination conditions within AgentChat, showcasing both built-in and custom implementations. Through these mechanisms, we can effectively manage the lifecycle of agent interactions and ensure they conclude appropriately when required.",
    "filename": "python/docs/src/user-guide/agentchat-user-guide/tutorial/termination.ipynb"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "FAQ for AutoGen Studio - A low code tool for building and debugging multi-agent systems\n"
      }
    },
    "content": "# Experimental Features\n\n## Authentication\n\nAutoGen Studio offers an experimental authentication feature to enable personalized experiences (multiple users). Currently, only GitHub authentication is supported. You can extend the base authentication class to add support for other authentication methods.\n\nBy default authenticatio is disabled and only enabled when you pass in the `--auth-config` argument when running the application.\n\n### Enable GitHub Authentication\n\nTo enable GitHub authentication, create a `auth.yaml` file in your app directory:\n\n```yaml\ntype: github\njwt_secret: \"your-secret-key\" # keep secure!\ntoken_expiry_minutes: 60\ngithub:\n  client_id: \"your-github-client-id\"\n  client_secret: \"your-github-client-secret\"\n  callback_url: \"http://localhost:8081/api/auth/callback\"\n  scopes: [\"user:email\"]\n```\n\n```{note}\n\n**JWT Secret**\n\n\n- Generate a strong, unique JWT secret (at least 32 random bytes). You can run `openssl rand -hex 32` to generate a secure random key.\n- Never commit your JWT secret to version control\n- In production, store secrets in environment variables or secure secret management services\n- Regularly rotate your JWT secret to limit the impact of potential breaches\n\n**Callback URL**\n\n- The callback URL is the URL that GitHub will redirect to after the user has authenticated. It should match the URL you set in your GitHub OAuth application settings.\n- Ensure that the callback URL is accessible from the internet if you are running AutoGen Studio on a remote server.\n\n```\n\nPlease see the documentation on [GitHub OAuth](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authenticating-to-the-rest-api-with-an-oauth-app) for more details on obtaining the `client_id` and `client_secret`.\n\nTo pass in this configuration you can use the `--auth-config` argument when running the application:\n\n```bash\nautogenstudio ui --auth-config /path/to/auth.yaml\n```\n\nOr set the environment variable:\n\n```bash\nexport AUTOGENSTUDIO_AUTH_CONFIG=\"/path/to/auth.yaml\"\n```\n\n```{note}\n- Authentication is currently experimental and may change in future releases\n- User data is stored in your configured database\n- When enabled, all API endpoints require authentication except for the authentication endpoints\n- WebSocket connections require the token to be passed as a query parameter (`?token=your-jwt-token`)\n\n```",
    "filename": "python/docs/src/user-guide/autogenstudio-user-guide/experimental.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "FAQ for AutoGen Studio - A low code tool for building and debugging multi-agent systems\n"
      }
    },
    "content": "# FAQ\n\n## Q: How do I specify the directory where files(e.g. database) are stored?\n\nA: You can specify the directory where files are stored by setting the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database (default) and other files in the specified directory e.g. `/path/to/folder/database.sqlite`.\n\n## Q: Can I use other models with AutoGen Studio?\n\nYes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint.\n\nAutoGen Studio is based on declaritive specifications which applies to models as well. Agents can include a model_client field which specifies the model endpoint details including `model`, `api_key`, `base_url`, `model type`. Note, you can define your [model client](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/components/model-clients.html) in python and dump it to a json file for use in AutoGen Studio.\n\nIn the following sample, we will define an OpenAI, AzureOpenAI and a local model client in python and dump them to a json file.\n\n```python\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\nfrom autogen_core.models import ModelInfo\n\nmodel_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-mini\",\n        )\nprint(model_client.dump_component().model_dump_json())\n\n\naz_model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"gpt-4o\",\n    api_version=\"2024-06-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    api_key=\"sk-...\",\n)\nprint(az_model_client.dump_component().model_dump_json())\n\nanthropic_client = AnthropicChatCompletionClient(\n        model=\"claude-3-sonnet-20240229\",\n        api_key=\"your-api-key\",  # Optional if ANTHROPIC_API_KEY is set in environment\n    )\nprint(anthropic_client.dump_component().model_dump_json())\n\nmistral_vllm_model = OpenAIChatCompletionClient(\n        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        base_url=\"http://localhost:1234/v1\",\n        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family=\"unknown\", structured_output=True),\n    )\nprint(mistral_vllm_model.dump_component().model_dump_json())\n```\n\nOpenAI\n\n```json\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for OpenAI hosted models.\",\n  \"label\": \"OpenAIChatCompletionClient\",\n  \"config\": { \"model\": \"gpt-4o-mini\" }\n}\n```\n\nAzure OpenAI\n\n```json\n{\n  \"provider\": \"autogen_ext.models.openai.AzureOpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for Azure OpenAI hosted models.\",\n  \"label\": \"AzureOpenAIChatCompletionClient\",\n  \"config\": {\n    \"model\": \"gpt-4o\",\n    \"api_key\": \"sk-...\",\n    \"azure_endpoint\": \"https://{your-custom-endpoint}.openai.azure.com/\",\n    \"azure_deployment\": \"{your-azure-deployment}\",\n    \"api_version\": \"2024-06-01\"\n  }\n}\n```\n\nAnthropic\n\n```json\n{\n  \"provider\": \"autogen_ext.models.anthropic.AnthropicChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for Anthropic's Claude models.\",\n  \"label\": \"AnthropicChatCompletionClient\",\n  \"config\": {\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"max_tokens\": 4096,\n    \"temperature\": 1.0,\n    \"api_key\": \"your-api-key\"\n  }\n}\n```\n\nHave a local model server like Ollama, vLLM or LMStudio that provide an OpenAI compliant endpoint? You can use that as well.\n\n```json\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for OpenAI hosted models.\",\n  \"label\": \"OpenAIChatCompletionClient\",\n  \"config\": {\n    \"model\": \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    \"model_info\": {\n      \"vision\": false,\n      \"function_calling\": true,\n      \"json_output\": false,\n      \"family\": \"unknown\",\n      \"structured_output\": true\n    },\n    \"base_url\": \"http://localhost:1234/v1\"\n  }\n}\n```\n\n```{caution}\nIt is important that you add the `model_info` field to the model client specification for custom models. This is used by the framework instantiate and use the model correctly. Also, the `AssistantAgent` and many other agents in AgentChat require the model to have the `function_calling` capability.\n```\n\n## Q: The server starts but I can't access the UI\n\nA: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correctly), you may need to specify the host address. By default, the host address is set to `localhost`. You can specify the host address using the `--host <host>` argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:\n\n```bash\nautogenstudio ui --port 8081 --host 0.0.0.0\n```\n\n## Q: How do I use AutoGen Studio with a different database?\n\nA: By default, AutoGen Studio uses SQLite as the database. However, it uses the SQLModel library, which supports multiple database backends. You can use any database supported by SQLModel, such as PostgreSQL or MySQL. To use a different database, you need to specify the connection string for the database using the `--database-uri` argument when running the application. Example connection strings include:\n\n- SQLite: `sqlite:///database.sqlite`\n- PostgreSQL: `postgresql+psycopg://user:password@localhost/dbname`\n- MySQL: `mysql+pymysql://user:password@localhost/dbname`\n- AzureSQL: `mssql+pyodbc:///?odbc_connect=DRIVER%3D%7BODBC+Driver+17+for+SQL+Server%7D%3BSERVER%3Dtcp%3Aservername.database.windows.net%2C1433%3BDATABASE%3Ddatabasename%3BUID%3Dusername%3BPWD%3Dpassword123%3BEncrypt%3Dyes%3BTrustServerCertificate%3Dno%3BConnection+Timeout%3D30%3B`\n\nYou can then run the application with the specified database URI. For example, to use PostgreSQL, you can run the following command:\n\n```bash\nautogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname\n```\n\n> **Note:** Make sure to install the appropriate database drivers for your chosen database:\n>\n> - PostgreSQL: `pip install psycopg2` or `pip install psycopg2-binary`\n> - MySQL: `pip install pymysql`\n> - SQL Server/Azure SQL: `pip install pyodbc`\n> - Oracle: `pip install cx_oracle`\n\n## Q: Can I export my agent workflows for use in a python app?\n\nYes. In the Team Builder view, you select a team and download its specification. This file can be imported in a python application using the `TeamManager` class. For example:\n\n```python\n\nfrom autogenstudio.teammanager import TeamManager\n\ntm = TeamManager()\nresult_stream =  tm.run(task=\"What is the weather in New York?\", team_config=\"team.json\") # or wm.run_stream(..)\n\n```\n\nYou can also load the team specification as an AgentChat object using the `load_component` method.\n\n```python\n\nimport json\nfrom autogen_agentchat.teams import BaseGroupChat\nteam_config = json.load(open(\"team.json\"))\nteam = BaseGroupChat.load_component(team_config)\n\n```\n\n## Q: Can I run AutoGen Studio in a Docker container?\n\nA: Yes, you can run AutoGen Studio in a Docker container. You can build the Docker image using the provided [Dockerfile](https://github.com/microsoft/autogen/blob/autogenstudio/samples/apps/autogen-studio/Dockerfile) and run the container using the following commands:\n\n```bash\nFROM python:3.10-slim\n\nWORKDIR /code\n\nRUN pip install -U gunicorn autogenstudio\n\nRUN useradd -m -u 1000 user\nUSER user\nENV HOME=/home/user \\\n    PATH=/home/user/.local/bin:$PATH \\\n    AUTOGENSTUDIO_APPDIR=/home/user/app\n\nWORKDIR $HOME/app\n\nCOPY --chown=user . $HOME/app\n\nCMD gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind \"0.0.0.0:8081\"\n```\n\nUsing Gunicorn as the application server for improved performance is recommended. To run AutoGen Studio with Gunicorn, you can use the following command:\n\n```bash\ngunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind\n```",
    "filename": "python/docs/src/user-guide/autogenstudio-user-guide/faq.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AutoGen Studio - A low code tool for building and debugging multi-agent systems\n"
      }
    },
    "content": "# AutoGen Studio\n\n[![PyPI version](https://badge.fury.io/py/autogenstudio.svg)](https://badge.fury.io/py/autogenstudio)\n[![Downloads](https://static.pepy.tech/badge/autogenstudio/week)](https://pepy.tech/project/autogenstudio)\n\nAutoGen Studio is a low-code interface built to help you rapidly prototype AI agents, enhance them with tools, compose them into teams and interact with them to accomplish tasks. It is built on [AutoGen AgentChat](https://microsoft.github.io/autogen) - a high-level API for building multi-agent applications.\n\n> See a video tutorial on AutoGen Studio v0.4 (02/25) - [https://youtu.be/oum6EI7wohM](https://youtu.be/oum6EI7wohM)\n\n[![A Friendly Introduction to AutoGen Studio v0.4](https://img.youtube.com/vi/oum6EI7wohM/maxresdefault.jpg)](https://www.youtube.com/watch?v=oum6EI7wohM)\n\nCode for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio)\n\n```{caution}\nAutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app. Developers are encouraged to use the AutoGen framework to build their own applications, implementing authentication, security and other features required for deployed applications.\n```\n\n## Capabilities - What Can You Do with AutoGen Studio?\n\nAutoGen Studio offers four main interfaces to help you build and manage multi-agent systems:\n\n1. **Team Builder**\n\n   - A visual interface for creating agent teams through declarative specification (JSON) or drag-and-drop\n   - Supports configuration of all core components: teams, agents, tools, models, and termination conditions\n   - Fully compatible with AgentChat's component definitions\n\n2. **Playground**\n\n   - Interactive environment for testing and running agent teams\n   - Features include:\n     - Live message streaming between agents\n     - Visual representation of message flow through a control transition graph\n     - Interactive sessions with teams using UserProxyAgent\n     - Full run control with the ability to pause or stop execution\n\n3. **Gallery**\n\n   - Central hub for discovering and importing community-created components\n   - Enables easy integration of third-party components\n\n4. **Deployment**\n   - Export and run teams in python code\n   - Setup and test endpoints based on a team configuration\n   - Run teams in a docker container\n\n### Roadmap\n\nReview project roadmap and issues [here](https://github.com/microsoft/autogen/issues/4006) .\n\n## Contribution Guide\n\nWe welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:\n\n- Review the overall AutoGen project [contribution guide](https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md)\n- Please review the AutoGen Studio [roadmap](https://github.com/microsoft/autogen/issues/4006) to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with `help-wanted`\n- Please use the tag [`proj-studio`](https://github.com/microsoft/autogen/issues?q=is%3Aissue%20state%3Aopen%20label%3Aproj-studio) tag for any issues, questions, and PRs related to Studio\n- Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.\n- Submit a pull request with your contribution!\n- If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in `.devcontainer/README.md` to use it\n\n## A Note on Security\n\nAutoGen Studio is a research prototype and is **not meant to be used** in a production environment. Some baseline practices are encouraged e.g., using Docker code execution environment for your agents.\n\nHowever, other considerations such as rigorous tests related to jailbreaking, ensuring LLMs only have access to the right keys of data given the end user's permissions, and other security features are not implemented in AutoGen Studio.\n\nIf you are building a production application, please use the AutoGen framework and implement the necessary security features.\n\n## Acknowledgements and Citation\n\nAutoGen Studio is based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Victor Dibia, Gagan Bansal, Adam Fourney, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).\n\nIf you use AutoGen Studio in your research, please cite the following paper:\n\n```\n@inproceedings{autogenstudio,\n  title={AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},\n  author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  pages={72--79},\n  year={2024}\n}\n```\n\n## Next Steps\n\nTo begin, follow the [installation instructions](installation.md) to install AutoGen Studio.\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n\ninstallation\nusage\nexperimental\nfaq\n```",
    "filename": "python/docs/src/user-guide/autogenstudio-user-guide/index.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AutoGen Studio - A low code tool for building and debugging multi-agent systems\n"
      }
    },
    "content": "# Installation\n\nThere are two ways to install AutoGen Studio - from PyPi or from source. We **recommend installing from PyPi** unless you plan to modify the source code.\n\n## Create a Virtual Environment (Recommended)\n\nWe recommend using a virtual environment as this will ensure that the dependencies for AutoGen Studio are isolated from the rest of your system.\n\n``````{tab-set}\n\n`````{tab-item} venv\n\nCreate and activate:\n\nLinux/Mac:\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nWindows command-line:\n```batch\npython3 -m venv .venv\n.venv\\Scripts\\activate.bat\n```\n\nTo deactivate later, run:\n\n```bash\ndeactivate\n```\n\n`````\n\n`````{tab-item} conda\n\n[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.\n\n\nCreate and activate:\n\n```bash\nconda create -n autogen python=3.10\nconda activate autogen\n```\n\nTo deactivate later, run:\n\n```bash\nconda deactivate\n```\n\n\n`````\n\n\n\n``````\n\n## Install from PyPi (Recommended)\n\nYou can install AutoGen Studio using pip, the Python package manager.\n\n```bash\npip install -U autogenstudio\n```\n\n## Install from source\n\n_Note: This approach requires some familiarity with building interfaces in React._\n\nYou have two options for installing from source: manually or using a dev container.\n\n### A) Install from source manually\n\n1. Ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed.\n2. Clone the AutoGen Studio repository.\n3. Navigate to the `python/packages/autogen-studio` and install its Python dependencies using `pip install -e .`\n4. Navigate to the `python/packages/autogen-studio/frontend` directory, install the dependencies, and build the UI:\n\n```bash\nnpm install -g gatsby-cli\nnpm install --global yarn\ncd frontend\nyarn install\nyarn build\n# Windows users may need alternative commands to build the frontend:\ngatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui\n```\n\n### B) Install from source using a dev container\n\n1. Follow the [Dev Containers tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial) to install VS Code, Docker and relevant extensions.\n2. Clone the AutoGen Studio repository.\n3. Open `python/packages/autogen-studio/`in VS Code. Click the blue button in bottom the corner or press F1 and select _\"Dev Containers: Reopen in Container\"_.\n4. Build the UI:\n\n```bash\ncd frontend\nyarn build\n```\n\n## Running the Application\n\nOnce installed, run the web UI by entering the following in your terminal:\n\n```bash\nautogenstudio ui --port 8081\n```\n\nThis command will start the application on the specified port. Open your web browser and go to <http://localhost:8081/> to use AutoGen Studio.\n\nAutoGen Studio also takes several parameters to customize the application:\n\n- `--host <host>` argument to specify the host address. By default, it is set to `localhost`.\n- `--appdir <appdir>` argument to specify the directory where the app files (e.g., database and generated user files) are stored. By default, it is set to the `.autogenstudio` directory in the user's home directory.\n- `--port <port>` argument to specify the port number. By default, it is set to `8080`.\n- `--reload` argument to enable auto-reloading of the server when changes are made to the code. By default, it is set to `False`.\n- `--database-uri` argument to specify the database URI. Example values include `sqlite:///database.sqlite` for SQLite and `postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL. If this is not specified, the database URL defaults to a `database.sqlite` file in the `--appdir` directory.\n- `--upgrade-database` argument to upgrade the database schema to the latest version. By default, it is set to `False`.\n\nNow that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
    "filename": "python/docs/src/user-guide/autogenstudio-user-guide/installation.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "Usage for AutoGen Studio - A low code tool for building and debugging multi-agent systems\n"
      }
    },
    "content": "# Usage\n\nAutoGen Studio (AGS) provides a Team Builder interface where developers can define multiple components and behaviors. Users can create teams, add agents to teams, attach tools and models to agents, and define team termination conditions.\nAfter defining a team, users can test directly in the team builder view or attach it to a session for use in the Playground view.\n\n> See a video tutorial on AutoGen Studio v0.4 (02/25) - [https://youtu.be/oum6EI7wohM](https://youtu.be/oum6EI7wohM)\n\n[![A Friendly Introduction to AutoGen Studio v0.4](https://img.youtube.com/vi/oum6EI7wohM/maxresdefault.jpg)](https://www.youtube.com/watch?v=oum6EI7wohM)\n\n## Setting Up an API Key\n\nMost of your agents will require an API key. You can set up an environment variable `OPENAI_API_KEY` (assuming you are using OpenAI models) and AutoGen will automatically use this for any OpenAI model clients you specify for your agents or teams. Alternatively you can specify the api key as part of the team or agent configuration.\n\nSee the section below on how to build an agent team either using the visual builder or by directly editing the JSON configuration.\n\n## Building an Agent Team\n\n<br/>\n\nAutoGen Studio integrates closely with all component abstractions provided by AutoGen AgentChat, including {py:class}`~autogen_agentchat.teams`, {py:class}`~autogen_agentchat.agents`, {py:class}`~autogen_core.models`, {py:class}`~autogen_core.tools`, and termination {py:class}`~autogen_agentchat.conditions`.\n\nThe Team Builder view in AGS provides a visual team builder that allows users to define components through either drag-and-drop functionality or by editing a JSON configuration of the team directly.\n\n### Using the Visual Builder\n\nThe visual builder is enabled by default and allows users to drag-and-drop components from the provided Component library to the Team Builder canvas. The team builder canvas represents a team and consists of a main team node and a set of a connected agent nodes. It includes a Component Library that has a selection of components that can be added to the team or agent nodes in the canvas.\n\n![Team Builder](teambuilder.jpg)\n\nThe core supported behaviours include:\n\n- Create a new team. This can be done by clicking on the \"New Team\" button in the Team Builder view or by selecting any of the existing default teams that ship with the default AGS Gallery. Once you do this, a new team node and agent node(s) will be created in the canvas.\n- Drag and drop components from the library to the team or agent nodes in the canvas.\n  - Teams: drag in agents and termination conditions to the team node (there are specific drop zones for these components)\n  - Agents: drag in models and tools to the agent node (there are specific drop zones for these components)\n- Editing Team/Agent Nodes: Click on the edit icon (top right) of the node to view and edit its properties. This pops up a panel that allows you to edit the fields of the node. In some cases you will need to scroll down and click into specific sections e.g., for an agent with a model client, you will need to click into the model client section to edit the model client properties. Once done with editing, click on the save button to save the changes.\n\n### Using the JSON Editor\n\n![JSON Editor](jsoneditor.jpg)\n\nAGS also lets you directly modify the JSON configuration of the team. This can be done by toggling the visual builder mode off. Once you do this, you will see the JSON configuration of the team. You can then edit the JSON configuration directly.\n\n> Did you know that you define your agents in Python, export them to JSON and then paste them in the JSON editor? The section below shows how to accomplish this.\n\n## Declarative Specification of Components\n\nAutoGen Studio is built on the declarative specification behaviors of AutoGen AgentChat. This allows users to define teams, agents, models, tools, and termination conditions in Python and then dump them into a JSON file for use in AutoGen Studio.\n\nHere's an example of an agent team and how it is converted to a JSON file:\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import  TextMentionTermination\n\nagent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=OpenAIChatCompletionClient(\n            model=\"gpt-4o-mini\",\n        ),\n    )\n\nagent_team = RoundRobinGroupChat([agent], termination_condition=TextMentionTermination(\"TERMINATE\"))\nconfig = agent_team.dump_component()\nprint(config.model_dump_json())\n```\n\n```json\n{\n  \"provider\": \"autogen_agentchat.teams.RoundRobinGroupChat\",\n  \"component_type\": \"team\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"A team that runs a group chat with participants taking turns in a round-robin fashion\\n    to publish a message to all.\",\n  \"label\": \"RoundRobinGroupChat\",\n  \"config\": {\n    \"participants\": [\n      {\n        \"provider\": \"autogen_agentchat.agents.AssistantAgent\",\n        \"component_type\": \"agent\",\n        \"version\": 1,\n        \"component_version\": 1,\n        \"description\": \"An agent that provides assistance with tool use.\",\n        \"label\": \"AssistantAgent\",\n        \"config\": {\n          \"name\": \"weather_agent\",\n          \"model_client\": {\n            \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n            \"component_type\": \"model\",\n            \"version\": 1,\n            \"component_version\": 1,\n            \"description\": \"Chat completion client for OpenAI hosted models.\",\n            \"label\": \"OpenAIChatCompletionClient\",\n            \"config\": { \"model\": \"gpt-4o-mini\" }\n          },\n          \"tools\": [],\n          \"handoffs\": [],\n          \"model_context\": {\n            \"provider\": \"autogen_core.model_context.UnboundedChatCompletionContext\",\n            \"component_type\": \"chat_completion_context\",\n            \"version\": 1,\n            \"component_version\": 1,\n            \"description\": \"An unbounded chat completion context that keeps a view of the all the messages.\",\n            \"label\": \"UnboundedChatCompletionContext\",\n            \"config\": {}\n          },\n          \"description\": \"An agent that provides assistance with ability to use tools.\",\n          \"system_message\": \"You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.\",\n          \"model_client_stream\": false,\n          \"reflect_on_tool_use\": false,\n          \"tool_call_summary_format\": \"{result}\"\n        }\n      }\n    ],\n    \"termination_condition\": {\n      \"provider\": \"autogen_agentchat.conditions.TextMentionTermination\",\n      \"component_type\": \"termination\",\n      \"version\": 1,\n      \"component_version\": 1,\n      \"description\": \"Terminate the conversation if a specific text is mentioned.\",\n      \"label\": \"TextMentionTermination\",\n      \"config\": { \"text\": \"TERMINATE\" }\n    }\n  }\n}\n```\n\nThis example shows a team with a single agent, using the `RoundRobinGroupChat` type and a `TextMentionTermination` condition. You will also notice that the model client is an `OpenAIChatCompletionClient` model client where only the model name is specified. In this case, the API key is assumed to be set as an environment variable `OPENAI_API_KEY`. You can also specify the API key as part of the model client configuration.\n\nTo understand the full configuration of an model clients, you can refer to the [AutoGen Model Clients documentation](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/components/model-clients.html).\n\nNote that you can similarly define your model client in Python and call `dump_component()` on it to get the JSON configuration and use it to update the model client section of your team or agent configuration.\n\nFinally, you can use the `load_component()` method to load a team configuration from a JSON file:\n\n```python\n\nimport json\nfrom autogen_agentchat.teams import BaseGroupChat\nteam_config = json.load(open(\"team.json\"))\nteam = BaseGroupChat.load_component(team_config)\n\n```\n\n## Gallery - Sharing and Reusing Components\n\nAGS provides a Gallery view, where a gallery is a collection of components - teams, agents, models, tools, and terminations - that can be shared and reused across projects.\n\nUsers can create a local gallery or import a gallery (from a URL, a JSON file import or simply by copying and pasting the JSON). At any given time, users can select any of the current Gallery items as a **default gallery**. This **default gallery** will be used to populate the Team Builder sidebar with components.\n\n- Create new galleries via Gallery -> New Gallery\n- Edit gallery JSON as needed\n- Set a **default** gallery (click pin icon in sidebar) to make components available in Team Builder.\n\n## Interactively Running Teams\n\nThe AutoGen Studio Playground enables users to:\n\n- Test teams on specific tasks\n- Review generated artifacts (images, code, text)\n- Monitor team \"inner monologue\" during task execution\n- View performance metrics (turn count, token usage)\n- Track agent actions (tool usage, code execution results)\n\n## Importing and Reusing Team Configurations\n\nAutoGen Studio's Gallery view offers a default component collection and supports importing external configurations:\n\n- Create/Import galleries through Gallery -> New Gallery -> Import\n- Set default galleries via sidebar pin icon\n- Access components in Team Builder through Sidebar -> From Gallery\n\n### Python Integration\n\nTeam configurations can be integrated into Python applications using the `TeamManager` class:\n\n```python\nfrom autogenstudio.teammanager import TeamManager\n\ntm = TeamManager()\nresult_stream = tm.run(task=\"What is the weather in New York?\", team_config=\"team.json\") # or tm.run_stream(..)\n```\n\nTo export team configurations, use the export button in Team Builder to generate a JSON file for Python application use.",
    "filename": "python/docs/src/user-guide/autogenstudio-user-guide/usage.md"
  },
  {
    "code": false,
    "content": "# Command Line Code Executors\n\nCommand line code execution provides a straightforward method for running code blocks. The code execution is achieved by saving each block to a file and executing it in a new process. There are two primary types of code executors available:\n\n- **Docker Command Executor**: Executes commands within a Docker container.\n- **Local Command Executor**: Executes commands on the host machine.\n\n## Docker Command Line Code Executor\n\nUsing the **Docker Command Line Code Executor** allows you to run commands isolated in a standardized environment. \n\n```{note}\nTo utilize the `DockerCommandLineCodeExecutor`, ensure the `autogen-ext[docker]` package is installed. Consult the [Packages Documentation](https://microsoft.github.io/autogen/dev/packages/index.html) for installation details.\n```\n\nThe `DockerCommandLineCodeExecutor` runs commands inside a Docker container. By default, it uses the `python:3-slim` image, although this can be configured via the `image` parameter when creating the executor. If the specified image isn't found locally, the executor will attempt to pull it, making it essential to have either the base image or an appropriate custom image pre-built. The only requirements for compatibility are having `sh` and `python` installed in the image, allowing for easy customization of dependencies.\n\n### Container Management with Context Manager\n\nUsing the executor as a context manager ensures that the Docker container is properly cleaned up post-execution. If you don't use it as a context manager, the `atexit` module would handle stopping the container upon program exit.\n\n### Persisting Containers for Inspection\n\nIf you want to retain the container after execution\u2014perhaps for inspection\u2014you can set the `auto_remove` parameter to `False` when creating the executor. Additionally, the `stop_container` parameter can also be set to `False` to prevent the container from being stopped after the execution ends.\n\n### Example Usage of Docker Executor\n\nThe following example demonstrates how to use the `DockerCommandLineCodeExecutor` to execute a simple Python command:\n\n```python\nfrom pathlib import Path\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n\n# Set up working directory\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\n# Execute code block within Docker\nasync with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:\n    print(\n        await executor.execute_code_blocks(\n            code_blocks=[\n                CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n            ],\n            cancellation_token=CancellationToken(),\n        )\n    )\n```\n\nThe code above creates a working directory named \"coding\" and then executes a Python code block within a Docker container. The output is printed directly.\n\n### Combining Docker Applications and Executors\n\nTo execute code within a containerized application while allowing it to spawn other containers, the \"Docker out of Docker\" approach is recommended. This strategy involves mounting the Docker socket from the main AutoGen container into the application's container. \n\nTo achieve this, you can modify the `docker run` command as follows:\n\n```bash\n-v /var/run/docker.sock:/var/run/docker.sock\n```\n\nThis command provides the application container the necessary permissions to manage sibling containers. If you also need to bind a working directory from the host to your application\u2019s container, utilize the `bind_dir` parameter. By default, it refers back to `work_dir`.\n\n## Local Command Line Code Executor\n\n```{attention}\nThe Local Command Line Code Executor runs code directly on your local system. Use it with caution to avoid unintentional side effects.\n```\n\nFor executing code directly on the host machine, you can use the `LocalCommandLineCodeExecutor`. This executor runs code blocks in the local environment, making it essential to ensure that the code being executed is safe and intended for the local context.\n\n### Example Usage of Local Executor\n\nThe following example illustrates the use of the `LocalCommandLineCodeExecutor` for executing code:\n\n```python\nfrom pathlib import Path\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\n# Set up working directory\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\n# Execute code block locally\nlocal_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\nprint(\n    await local_executor.execute_code_blocks(\n        code_blocks=[\n            CodeBlock(language=\"python\", code=\"print('Hello, World!')\"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n)\n```\n\nThe code shown above follows similar steps to the Docker executor but runs directly on the local machine. After creating the working directory, it executes a Python code block.\n\n## Running Code within a Virtual Environment\n\nIf you'd prefer to run the code in an isolated environment, you can create a virtual environment during your application setup. This provides a dedicated context for dependencies, making your executions cleaner and preventing global environment conflicts.\n\n### Example Setting Up a Virtual Environment\n\nThe following example demonstrates how to set up and use a virtual environment with the `LocalCommandLineCodeExecutor`:\n\n```python\nimport venv\nfrom pathlib import Path\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\n# Set up working directory\nwork_dir = Path(\"coding\")\nwork_dir.mkdir(exist_ok=True)\n\n# Create virtual environment\nvenv_dir = work_dir / \".venv\"\nvenv_builder = venv.EnvBuilder(with_pip=True)\nvenv_builder.create(venv_dir)\nvenv_context = venv_builder.ensure_directories(venv_dir)\n\n# Execute code block within virtual environment\nlocal_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)\nawait local_executor.execute_code_blocks(\n    code_blocks=[\n        CodeBlock(language=\"bash\", code=\"pip install matplotlib\"),\n    ],\n    cancellation_token=CancellationToken(),\n)\n```\n\nIn the above example, a virtual environment is created within the \"coding\" directory. The `LocalCommandLineCodeExecutor` then utilizes this environment to install the `matplotlib` package, ensuring that the dependency is isolated from the global Python environment. This offers a controlled and reproducible setup for your code executions.",
    "filename": "python/docs/src/user-guide/core-user-guide/components/command-line-code-executors.ipynb"
  },
  {
    "code": false,
    "content": "# Model Clients\n\nAutoGen provides a suite of built-in model clients for using the ChatCompletion API. Each model client implements the `ChatCompletionClient` protocol class from the `autogen_core` library, allowing for seamless interactions with various AI models.\n\n## Available Model Clients\n\nCurrently, AutoGen supports the following built-in model clients:\n\n- **OpenAIChatCompletionClient**: For OpenAI models and other models compatible with the OpenAI API (e.g., Gemini).\n- **AzureOpenAIChatCompletionClient**: Specifically designed for Azure OpenAI models.\n- **AzureAIChatCompletionClient**: For GitHub models and those hosted on Azure.\n- **OllamaChatCompletionClient** (Experimental): For local models hosted on Ollama.\n- **AnthropicChatCompletionClient** (Experimental): For models hosted by Anthropic.\n- **SKChatCompletionAdapter**: An adapter for Semantic Kernel AI connectors.\n\nRefer to the documentation for each client to understand their specific usage and capabilities.\n\n## Logging Model Calls\n\nAutoGen leverages the standard Python logging module to track events such as model calls and responses. The logger is accessible via `EVENT_LOGGER_NAME`, and the specific event type being logged is `LLMCall`.\n\nHere's how to set up the logging configuration:\n\n```python\nimport logging\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\nIn the above code, the logging is configured to display warning level messages and above. You can change the logging level to `DEBUG` for more detailed output during development.\n\n## Calling a Model Client\n\nTo interact with a model client, use the `create` method from the `ChatCompletionClient`. The following example demonstrates how to utilize the `OpenAIChatCompletionClient` to ask a question about geography.\n\n```python\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4\", temperature=0.3\n)  # Assuming OPENAI_API_KEY is set in the environment.\n\nresult = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\nprint(result)\n```\n\nIn this example, a model client is instantiated for the GPT-4 model, and a user message is sent to it. The expected output is the model's response detailing the capital of France, which is printed to the console.\n\n## Streaming Tokens\n\nThe `create_stream` method allows you to receive token chunks of a chat completion request in real time. This can be particularly useful for applications requiring instantaneous feedback.\n\n```python\nfrom autogen_core.models import CreateResult, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")  # Assuming OPENAI_API_KEY is set in the environment.\n\nmessages = [\n    UserMessage(content=\"Write a very short story about a dragon.\", source=\"user\"),\n]\n\n# Create a stream.\nstream = model_client.create_stream(messages=messages)\n\n# Iterate over the stream and print the responses.\nprint(\"Streamed responses:\")\nasync for chunk in stream:  # type: ignore\n    if isinstance(chunk, str):\n        # The chunk is a string.\n        print(chunk, flush=True, end=\"\")\n    else:\n        # The final chunk is a CreateResult object.\n        assert isinstance(chunk, CreateResult) and isinstance(chunk.content, str)\n        print(\"\\n\\n------------\\n\")\n        print(\"The complete response:\", flush=True)\n        print(chunk.content, flush=True)\n```\n\nThis code snippet demonstrates how to set up a streaming call to the model. As messages are processed, chunks are printed directly to the console, allowing users to see the response building in real-time.\n\n```{note}\nThe last response in the streaming output is always of the type `CreateResult` and contains the complete model message.\n```\n\n```{note}\nThe default response behavior may return zero values. To enable usage, see the method `create_stream` in the `BaseOpenAIChatCompletionClient` for more details.\n```\n\n## Structured Output\n\nYou can enhance the output from certain models by enabling structured output. This requires setting the `response_format` attribute in respective model classes to a Pydantic `BaseModel`.\n\n```{note}\nStructured output is only available for specific models that support it, namely the OpenAI and Azure OpenAI model clients.\n```\n\nHere\u2019s how to define and use structured output:\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\n\n# Define a response format for the agent.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n# Create an agent using the OpenAI GPT-4o model with structured output.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    response_format=AgentResponse,  # type: ignore\n)\n\n# Send a message and await the response.\nmessages = [\n    UserMessage(content=\"I am happy.\", source=\"user\"),\n]\nresponse = await model_client.create(messages=messages)\nassert isinstance(response.content, str)\nparsed_response = AgentResponse.model_validate_json(response.content)\nprint(parsed_response.thoughts)\nprint(parsed_response.response)\n\n# Close the connection to the model client.\nawait model_client.close()\n```\n\nIn this example, the expected output from the model will be automatically validated and structured according to the `AgentResponse` model, making it easy to access and manipulate the information.\n\n## Caching Model Responses\n\nAutoGen provides a caching mechanism through the `ChatCompletionCache` class found in the `autogen_ext.models.cache` package. This allows you to avoid token costs when querying the same prompt multiple times.\n\nThe `ChatCompletionCache` utilizes the `CacheStore` protocol, offering various storage backends such as `DiskCacheStore` and `RedisStore`. Here's how you can set up local caching with `diskcache`:\n\n```python\n# pip install -U \"autogen-ext[openai, diskcache]\"\n```\n\n```python\nimport asyncio\nimport tempfile\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom autogen_ext.models.cache import CHAT_CACHE_VALUE_TYPE, ChatCompletionCache\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom diskcache import Cache\n\nasync def main() -> None:\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n        \n        # Initialize the CacheStore using diskcache.\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should output response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n\n        await openai_model_client.close()\n        await cache_client.close()\n\nasyncio.run(main())\n```\n\nThis example demonstrates how to set up a simple caching mechanism. After the first call, subsequent calls with the same input will return the cached response if the arguments have not changed.\n\n## Building an Agent with a Model Client\n\nYou can create an AI agent that communicates through the ChatCompletion API using the example provided below. \n\n```python\nfrom dataclasses import dataclass\nfrom autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n@dataclass\nclass Message:\n    content: str\n\nclass SimpleAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A simple agent\")\n        self._system_messages = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Prepare input for the chat completion model.\n        user_message = UserMessage(content=message.content, source=\"user\")\n        response = await self._model_client.create(\n            self._system_messages + [user_message], cancellation_token=ctx.cancellation_token\n        )\n        # Return the model's response.\n        assert isinstance(response.content, str)\n        return Message(content=response.content)\n```\n\nIn this example, the `SimpleAgent` class is derived from `RoutedAgent`. It handles user messages by sending them to the `ChatCompletionClient` and returning the responses. The agent maintains a contextual conversation by using system messages.\n\n```{note}\nThe `cancellation_token`, a part of the `CancellationToken` class, allows for interruptions of ongoing asynchronous operations linked to message handlers.\n```\n\nTo utilize the agent, create a runtime and register it:\n\n```python\n# Create the runtime and register the agent.\nfrom autogen_core import AgentId\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY set in the environment.\n)\n\nruntime = SingleThreadedAgentRuntime()\nawait SimpleAgent.register(\n    runtime,\n    \"simple_agent\",\n    lambda: SimpleAgent(model_client=model_client),\n)\n# Start processing messages.\nruntime.start()\n# Send a message to the agent and get the response.\nmessage = Message(\"Hello, what are some fun things to do in Seattle?\")\nresponse = await runtime.send_message(message, AgentId(\"simple_agent\", \"default\"))\nprint(response.content)\n# Stop processing messages.\nawait runtime.stop()\nawait model_client.close()\n```\n\nThe `SimpleAgent` is designed to operate with a fresh context for each interaction while the runtime manages message processing. This modular design facilitates future enhancements, such as integrating memory or context from past conversations.\n\n## API Keys from Environment Variables\n\nIn the previous examples, the API key can be provided as an argument; however, for convenience and security, both OpenAI and Azure OpenAI clients can automatically read API keys from environment variables:\n\n- For OpenAI, set the `OPENAI_API_KEY` environment variable.\n- For Azure OpenAI, use the `AZURE_OPENAI_API_KEY` environment variable.\n- For Gemini (Beta), define the `GEMINI_API_KEY` environment variable.\n\nUsing environment variables is a recommended practice for keeping sensitive information out of your source code.\n``` \n\nThis documentation aims to clarify the capabilities and features associated with using Model Clients in the AutoGen ecosystem while maintaining readability and ease of understanding for new users.",
    "filename": "python/docs/src/user-guide/core-user-guide/components/model-clients.ipynb"
  },
  {
    "code": false,
    "content": "# Documentation for Chat Completion Model Context\n\n## Overview of Model Context\n\nThe model context enables the storage and retrieval of Chat Completion messages in an efficient manner. This functionality is essential for generating responses from language models (LLMs). When used in conjunction with a model client, such as `OpenAIChatCompletionClient`, it allows for more manageable interactions by controlling the flow and storage of conversation history.\n\nOne implementation of this model context is the `BufferedChatCompletionContext`, a most-recently-used (MRU) context that maintains a specified number of recent messages, helping prevent issues related to context overflow, which can affect the performance of many LLMs.\n\nThis documentation provides an example that illustrates how to utilize the `BufferedChatCompletionContext` within a simple agent framework.\n\n## Setting Up the Environment\n\nTo begin, necessary modules are imported from the `autogen_core` library. The `BufferedChatCompletionContext`, along with other supporting classes like `MessageContext`, `RoutedAgent`, and message types (UserMessage, AssistantMessage, and SystemMessage), are essential for constructing the agent.\n\nHere's the code to import these modules:\n\n```python\nfrom dataclasses import dataclass\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nThis code sets up the foundational imports required to build a chat agent that can interact with LLMs.\n\n## Defining the Message Structure\n\nA simple message structure is defined next, using Python's `@dataclass` decorator. This allows for the easy creation of message instances with content.\n\nHere\u2019s the definition for the `Message` data class:\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nThe `Message` class encapsulates a single piece of text, enabling a streamlined way to handle user messages and the AI's responses.\n\n## Creating a Simple Agent\n\nThe core functionality of the agent is implemented in the class `SimpleAgentWithContext`, which inherits from `RoutedAgent`. This class is responsible for handling user messages and generating responses.\n\nThe constructor initializes the agent, the system messages, the model client, and the buffer context:\n\n```python\nclass SimpleAgentWithContext(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A simple agent\")\n        self._system_messages = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n```\n\nIn this snippet, the agent is equipped with a static system message that provides context for the conversation. The buffer size is set to five messages, allowing the agent to retain a recent history for context.\n\n## Handling User Messages\n\nThe agent's ability to handle incoming messages is defined in the `handle_user_message` method, decorated with `@message_handler`. Here, incoming messages are processed, added to the context, and a response is generated.\n\nThe message-handling process is as follows:\n\n```python\n@message_handler\nasync def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n    user_message = UserMessage(content=message.content, source=\"user\")\n    await self._model_context.add_message(user_message)\n    response = await self._model_client.create(\n        self._system_messages + (await self._model_context.get_messages()),\n        cancellation_token=ctx.cancellation_token,\n    )\n    assert isinstance(response.content, str)\n    await self._model_context.add_message(AssistantMessage(content=response.content, source=self.metadata[\"type\"]))\n    return Message(content=response.content)\n```\n\n1. **User Input Preparation**: The incoming message is transformed into a `UserMessage` type.\n2. **Context Management**: The user message is added to the model context for future reference.\n3. **Response Generation**: The model client generates a response based on the system message and the recent context.\n4. **Response Handling**: The response is validated, added to the context, and returned as a `Message` instance.\n\n## Running the Agent in a Runtime\n\nOnce the agent is structured, it can be executed within a runtime. An instance of `OpenAIChatCompletionClient` is created, and the agent is registered for processing messages.\n\nHere's a step-by-step outline of the setup:\n\n```python\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n)\n\nruntime = SingleThreadedAgentRuntime()\nawait SimpleAgentWithContext.register(\n    runtime,\n    \"simple_agent_context\",\n    lambda: SimpleAgentWithContext(model_client=model_client),\n)\nruntime.start()\nagent_id = AgentId(\"simple_agent_context\", \"default\")\n```\n\nThis code initializes the model client and runtime environment, allowing the agent to begin responding to user inputs.\n\n## Interacting with the Agent\n\nWith the agent registered, you can now send messages to the agent and observe its responses. Here\u2019s an example of sending two consecutive messages:\n\n```python\n# First question.\nmessage = Message(\"Hello, what are some fun things to do in Seattle?\")\nprint(f\"Question: {message.content}\")\nresponse = await runtime.send_message(message, agent_id)\nprint(f\"Response: {response.content}\")\nprint(\"-----\")\n\n# Second question.\nmessage = Message(\"What was the first thing you mentioned?\")\nprint(f\"Question: {message.content}\")\nresponse = await runtime.send_message(message, agent_id)\nprint(f\"Response: {response.content}\")\n```\n\n1. **First Interaction**: A user asks about activities in Seattle, and the agent provides a response.\n2. **Follow-Up Question**: The user inquires about the agent's previous response, showcasing the context retention capability of the agent.\n\n## Stopping the Runtime\n\nFinally, once interactions are complete, you should properly stop the runtime and close the model client connection:\n\n```python\nawait runtime.stop()\nawait model_client.close()\n```\n\nThis ensures that all resources are released appropriately, maintaining good performance and reliability of the application.\n\nThe combined functionalities demonstrated in this documentation allow developers to create interactive agents that can effectively retain conversation history, making for a more coherent and integrated user experience.",
    "filename": "python/docs/src/user-guide/core-user-guide/components/model-context.ipynb"
  },
  {
    "code": false,
    "content": "# Tools\n\n## Introduction to Tools\n\nTools are executable pieces of code utilized by agents to perform a variety of actions. These can include simple functions, like a calculator, or API calls to third-party services, such as obtaining stock prices or weather forecasts. In the realm of AI agents, tools facilitate actions in response to model-generated function calls.\n\nThe AutoGen framework provides access to the `autogen_core.tools` module, which includes a variety of built-in tools and utilities that simplify creating and using custom tools.\n\n## Built-in Tools\n\nAmong the built-in tools available is the `PythonCodeExecutionTool`, which enables agents to execute snippets of Python code. This tool is particularly useful for dynamically evaluating code, making it a versatile asset in agent operations.\n\n### Example of Python Code Execution Tool\n\nHere's an example demonstrating how to create and utilize the `PythonCodeExecutionTool`:\n\n```python\nfrom autogen_core import CancellationToken\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\n# Create the tool.\ncode_executor = DockerCommandLineCodeExecutor()\nawait code_executor.start()\ncode_execution_tool = PythonCodeExecutionTool(code_executor)\ncancellation_token = CancellationToken()\n\n# Execute a sample code directly without an agent.\ncode = \"print('Hello, world!')\"\nresult = await code_execution_tool.run_json({\"code\": code}, cancellation_token)\nprint(code_execution_tool.return_value_as_string(result))\n```\n\nIn this code:\n- A `DockerCommandLineCodeExecutor` instance is created, which runs Python snippets within a Docker container.\n- This executor is wrapped by the `PythonCodeExecutionTool`, facilitating easy execution of Python code.\n- The example runs a simple print statement and outputs the result.\n\n### Additional Built-in Tools\n\nIn addition to the Python code executor, AutoGen provides several other built-in tools:\n- `LocalSearchTool` and `GlobalSearchTool` for GraphRAG functionality.\n- `mcp_server_tools` for interacting with Model Context Protocol (MCP) servers.\n- `HttpTool` for making HTTP requests to REST APIs.\n- `LangChainToolAdapter` for integrating with LangChain tools.\n\n## Custom Function Tools\n\nCustom tools can be easily created by defining standard Python functions and wrapping them with the `FunctionTool` class. This flexibility allows developers to build tailored solutions for specific tasks.\n\n### Example of Creating a Custom Function Tool\n\nHere\u2019s an example illustrating how to create a custom function to retrieve stock prices:\n\n```python\nimport random\nfrom autogen_core import CancellationToken\nfrom autogen_core.tools import FunctionTool\nfrom typing_extensions import Annotated\n\nasync def get_stock_price(ticker: str, date: Annotated[str, \"Date in YYYY/MM/DD\"]) -> float:\n    # Returns a random stock price for demonstration purposes.\n    return random.uniform(10, 200)\n\n# Create a function tool.\nstock_price_tool = FunctionTool(get_stock_price, description=\"Get the stock price.\")\n\n# Run the tool.\ncancellation_token = CancellationToken()\nresult = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n\n# Print the result.\nprint(stock_price_tool.return_value_as_string(result))\n```\n\nIn this example:\n- The function `get_stock_price` generates a random stock price for the specified ticker and date.\n- This function is wrapped in a `FunctionTool`, allowing it to be called programmatically.\n- The tool is invoked with parameters for the stock ticker and date, outputting the result.\n\n## Calling Tools with Model Clients\n\nAll tools in AutoGen derive from the `BaseTool` class, which automatically generates a JSON schema for the specific tool. This schema is used by model clients to structure calls to the tools.\n\n### Example of Obtaining Tool Schema\n\nAn example of retrieving the JSON schema for our `stock_price_tool` is as follows:\n\n```python\nstock_price_tool.schema\n```\n\n### Using Tools with Model Clients\n\nModel clients function alongside tools by utilizing the JSON schema to facilitate tool calls. Here\u2019s how to use the `FunctionTool` in conjunction with the `OpenAIChatCompletionClient`:\n\n```python\nimport json\nfrom autogen_core.models import AssistantMessage, FunctionExecutionResult, FunctionExecutionResultMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the OpenAI chat completion client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create a user message.\nuser_message = UserMessage(content=\"What is the stock price of AAPL on 2021/01/01?\", source=\"user\")\n\n# Execute the chat completion with the stock_price_tool.\ncancellation_token = CancellationToken()\ncreate_result = await model_client.create(\n    messages=[user_message], tools=[stock_price_tool], cancellation_token=cancellation_token\n)\ncreate_result.content\n```\n\nIn this segment:\n- An `OpenAIChatCompletionClient` is initialized to interact with the OpenAI model.\n- A user message queries stock prices, prompting a tool call via the model's response.\n\nAs part of its operations, the model client generates a structured JSON request based on the tool schema and accompanying messages to retrieve a result. The model is trained to output tool calls that follow the defined schema, allowing AutoGen to parse the response effectively.\n\n## Executing Tool Results and Reflection\n\nUpon receiving a response, the resultant tool call can be processed and reflected upon. The result is encapsulated within a `FunctionExecutionResult` object, which includes both the output of the execution and the ID of the invoked tool.\n\n### Example Reflection on Tool Execution\n\nHere\u2019s how we can parse and utilize the execution result:\n\n```python\nassert isinstance(create_result.content, list)\narguments = json.loads(create_result.content[0].arguments)  # type: ignore\ntool_result = await stock_price_tool.run_json(arguments, cancellation_token)\ntool_result_str = stock_price_tool.return_value_as_string(tool_result)\ntool_result_str\n```\n\nIn this snippet:\n- The JSON string from the model response is converted into a Python dictionary.\n- The corresponding tool is executed with the parsed arguments, and the string representation of the result is generated.\n\nAfter obtaining the result, we can make another client call to have the model provide a reflection based on the tool's output:\n\n```python\n# Create a function execution result\nexec_result = FunctionExecutionResult(\n    call_id=create_result.content[0].id,  # type: ignore\n    content=tool_result_str,\n    is_error=False,\n    name=stock_price_tool.name,\n)\n\n# Prepare and execute another chat completion with the reflected message.\nmessages = [\n    user_message,\n    AssistantMessage(content=create_result.content, source=\"assistant\"),\n    FunctionExecutionResultMessage(content=[exec_result]),\n]\ncreate_result = await model_client.create(messages=messages, cancellation_token=cancellation_token)  # type: ignore\nprint(create_result.content)\nawait model_client.close()\n```\n\nHere:\n- A new chat completion captures the interaction history and includes the results of the function execution for reflection.\n- The agent can now return an enriched response containing insights from the previously executed tool.\n\n## Tool-Equipped Agent\n\nBy integrating model clients and tools, you can assemble a fully functional, tool-equipped agent capable of performing actions and reflecting on outcomes\u2014enhancing user interaction and when dealing with complex queries.\n\n### Example of Tool-Equipped Agent Implementation\n\nHere\u2019s an example demonstrating how to establish a tool-equipped agent:\n\n```python\nimport asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    AgentId,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n@dataclass\nclass Message:\n    content: str\n\nclass ToolUseAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[Tool]) -> None:\n        super().__init__(\"An agent with tools\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._tools = tool_schema\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        session: List[LLMMessage] = self._system_messages + [UserMessage(content=message.content, source=\"user\")]\n        create_result = await self._model_client.create(\n            messages=session,\n            tools=self._tools,\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        if isinstance(create_result.content, str):\n            return Message(content=create_result.content)\n        assert isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        )\n\n        session.append(AssistantMessage(content=create_result.content, source=\"assistant\"))\n        results = await asyncio.gather(\n            *[self._execute_tool_call(call, ctx.cancellation_token) for call in create_result.content]\n        )\n        session.append(FunctionExecutionResultMessage(content=results))\n\n        create_result = await self._model_client.create(\n            messages=session,\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(create_result.content, str)\n\n        return Message(content=create_result.content)\n\n    async def _execute_tool_call(\n        self, call: FunctionCall, cancellation_token: CancellationToken\n    ) -> FunctionExecutionResult:\n        tool = next((tool for tool in self._tools if tool.name == call.name), None)\n        assert tool is not None\n        try:\n            arguments = json.loads(call.arguments)\n            result = await tool.run_json(arguments, cancellation_token)\n            return FunctionExecutionResult(\n                call_id=call.id, content=tool.return_value_as_string(result), is_error=False, name=tool.name\n            )\n        except Exception as e:\n            return FunctionExecutionResult(call_id=call.id, content=str(e), is_error=True, name=tool.name)\n```\n\nIn this implementation:\n- The `ToolUseAgent` class acts as a bridge between user messages and the tools.\n- It generates tool calls based on user queries and reflects on the results, ultimately returning a coherent response.\n\n### Running the Tool-Equipped Agent\n\nLastly, we can set up and run the agent's runtime:\n\n```python\n# Create the model client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create a runtime.\nruntime = SingleThreadedAgentRuntime()\n\n# Create the tools.\ntools: List[Tool] = [FunctionTool(get_stock_price, description=\"Get the stock price.\")]\n\n# Register the agent.\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_use_agent\",\n    lambda: ToolUseAgent(\n        model_client=model_client,\n        tool_schema=tools,\n    ),\n)\n\n# Start processing messages.\nruntime.start()\ntool_use_agent = AgentId(\"tool_use_agent\", \"default\")\nresponse = await runtime.send_message(Message(\"What is the stock price of NVDA on 2024/06/01?\"), tool_use_agent)\nprint(response.content)\n\n# Stop processing messages.\nawait runtime.stop()\nawait model_client.close()\n```\n\nIn this code:\n- An instance of `OpenAIChatCompletionClient` is created and registered with the agent runtime.\n- The agent can process user queries and return relevant information about stock prices.\n\nThis module illustrates the functionality of tool integration within AI agents, allowing the execution of actions and engagement with users in a streamlined manner.",
    "filename": "python/docs/src/user-guide/core-user-guide/components/tools.ipynb"
  },
  {
    "code": false,
    "content": "# Workbench (and MCP)\n\nA `Workbench` serves as a collection of tools that maintains shared state and resources, differing from the `Tool` class, which interfaces with individual tools. This coordination allows various tools to be used in a cohesive manner, providing seamless access and integration.\n\n## Using Workbench\n\nTo create an agent leveraging the `Workbench`, follow this example. This agent will handle user messages and interact with various tools through the workbench.\n\n### Import Required Libraries\n\nFirst, we need to import the essential libraries and classes from `autogen_core` to set up our agent.\n\n```python\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    message_handler,\n)\nfrom autogen_core.model_context import ChatCompletionContext\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    FunctionExecutionResult,\n    FunctionExecutionResultMessage,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import ToolResult, Workbench\n```\n\nIn this code block, we import necessary modules that will facilitate message handling, context management, and the use of tools through the workbench.\n\n### Define the Message Class\n\nNext, define a simple `Message` class for structuring the messages exchanged between the user and the agent.\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nThe `Message` class holds a single attribute `content`, which stores the text of the message.\n\n### Create the Workbench Agent\n\nNow, we can define our workbench agent. This agent will handle user messages, call tools using the workbench, and maintain the conversation context.\n\n```python\nclass WorkbenchAgent(RoutedAgent):\n    def __init__(\n        self, model_client: ChatCompletionClient, model_context: ChatCompletionContext, workbench: Workbench\n    ) -> None:\n        super().__init__(\"An agent with a workbench\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._model_context = model_context\n        self._workbench = workbench\n```\n\nIn the `WorkbenchAgent` class's constructor, we initialize various required components such as the model client, the context for managing messages, and the workbench that manages the tools.\n\n### Handle User Messages\n\nWe now implement a method to handle incoming user messages, allowing the agent to respond intelligently.\n\n```python\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n        print(\"---------User Message-----------\")\n        print(message.content)\n```\n\nIn the `handle_user_message` method, we add the user message to the model context and print it for debugging purposes.\n\n### Using Tools in a Loop\n\nOnce the agent has processed the user message, it runs a function call loop, making use of the tools provided by the workbench.\n\n```python\n        create_result = await self._model_client.create(\n            messages=self._system_messages + (await self._model_context.get_messages()),\n            tools=(await self._workbench.list_tools()),\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        while isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        ):\n            print(\"---------Function Calls-----------\")\n            for call in create_result.content:\n                print(call)\n```\n\nHere, we call the model client to process messages and tools. If the result contains a list of function calls, we iterate over them and log the calls.\n\n### Executing Functions and Updating Context\n\nNext, we execute the functions using the workbench and update the model context with the results.\n\n```python\n            await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n            print(\"---------Function Call Results-----------\")\n            results: List[ToolResult] = []\n            for call in create_result.content:\n                result = await self._workbench.call_tool(\n                    call.name, arguments=json.loads(call.arguments), cancellation_token=ctx.cancellation_token\n                )\n                results.append(result)\n                print(result)\n\n            await self._model_context.add_message(\n                FunctionExecutionResultMessage(\n                    content=[\n                        FunctionExecutionResult(\n                            call_id=call.id,\n                            content=result.to_text(),\n                            is_error=result.is_error,\n                            name=result.name,\n                        )\n                        for call, result in zip(create_result.content, results, strict=False)\n                    ]\n                )\n            )\n```\n\nIn this block, we call each tool and capture the results, adding them to the model context to maintain a history of interactions. Each execution result is logged to facilitate debugging.\n\n### Finalizing the Response\n\nAfter processing the function calls, we need to update the chat completion, ensuring that the agent reflects on the results of the tool usage in its response.\n\n```python\n            create_result = await self._model_client.create(\n                messages=self._system_messages + (await self._model_context.get_messages()),\n                tools=(await self._workbench.list_tools()),\n                cancellation_token=ctx.cancellation_token,\n            )\n\n        assert isinstance(create_result.content, str)\n\n        print(\"---------Final Response-----------\")\n        print(create_result.content)\n\n        await self._model_context.add_message(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n        return Message(content=create_result.content)\n```\n\nThe final response is logged and stored in the model context. The function returns the final message back to the user, completing the interaction.\n\n## MCP Workbench\n\nThe Model Context Protocol (MCP) provides a standardized approach for integrating tools and resources with language models. An MCP server manages the state of tools, while an MCP client, like the `McpWorkbench`, connects to the server, enabling the language model to access necessary tools effectively.\n\nIn AutoGen, we provide the `McpWorkbench` class, which acts as an MCP client, allowing agents to interact with tools hosted on MCP servers.\n\n## Web Browsing Agent using Playwright MCP\n\nLet's explore how to create a web browsing agent with the `WorkbenchAgent` and the Playwright MCP server. First, ensure that you have the necessary browser dependencies for Playwright installed.\n\n```bash\n# npx playwright install chrome\n```\n\nNext, start the Playwright MCP server by executing the following command in your terminal:\n\n```bash\n# npx @playwright/mcp@latest --port 8931\n```\n\nThe server should be running and ready to accept requests.\n\n### Creating the Web Browsing Agent\n\nFinally, we can set up our agent using the `WorkbenchAgent` and the `McpWorkbench`, connecting to our running Playwright MCP server.\n\n```python\nfrom autogen_core import AgentId, SingleThreadedAgentRuntime\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, SseServerParams\n\nplaywright_server_params = SseServerParams(\n    url=\"http://localhost:8931/sse\",\n)\n\nasync with McpWorkbench(playwright_server_params) as workbench:\n    runtime = SingleThreadedAgentRuntime()\n\n    await WorkbenchAgent.register(\n        runtime=runtime,\n        type=\"WebAgent\",\n        factory=lambda: WorkbenchAgent(\n            model_client=OpenAIChatCompletionClient(model=\"gpt-4.1-nano\"),\n            model_context=BufferedChatCompletionContext(buffer_size=10),\n            workbench=workbench,\n        ),\n    )\n\n    runtime.start()\n\n    await runtime.send_message(\n        Message(content=\"Use Bing to find out the address of Microsoft Building 99\"),\n        recipient=AgentId(\"WebAgent\", \"default\"),\n    )\n\n    await runtime.stop()\n```\n\nIn this block:\n\n1. We establish connection parameters for the MCP server.\n2. Inside a context manager, we create an instance of `McpWorkbench`.\n3. The agent runtime is initialized, and the `WorkbenchAgent` is registered.\n4. After starting the runtime, we send a message to the agent, triggering a web browsing task.\n5. Finally, we stop the runtime once the interaction is complete.\n\nThis example demonstrates how to leverage the MCP protocol to create a practical web browsing agent, utilizing the tools provided by the Playwright MCP server.",
    "filename": "python/docs/src/user-guide/core-user-guide/components/workbench.ipynb"
  },
  {
    "content": "# Azure OpenAI with AAD Auth\n\nThis guide will show you how to use the Azure OpenAI client with Azure Active Directory (AAD) authentication.\n\nThe identity used must be assigned the [**Cognitive Services OpenAI User**](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control#cognitive-services-openai-user) role.\n\n## Install Azure Identity client\n\nThe Azure identity client is used to authenticate with Azure Active Directory.\n\n```sh\npip install azure-identity\n```\n\n## Using the Model Client\n\n```python\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n# Create the token provider\ntoken_provider = get_bearer_token_provider(\n    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n)\n\nclient = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"{your-azure-deployment}\",\n    model=\"{model-name, such as gpt-4o}\",\n    api_version=\"2024-02-01\",\n    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n    azure_ad_token_provider=token_provider,\n)\n```\n\n```{note}\nSee [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity#chat-completions) for how to use the Azure client directly or for more info.\n```",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.md"
  },
  {
    "code": false,
    "content": "# Extracting Results with an Agent\n\nIn multi-agent systems, extracting results efficiently post-execution can be crucial. This documentation outlines a method for achieving this by utilizing an agent that publishes the final results to an accessible location. Given that agent instances are not directly accessible from external contexts, it is essential to create an intermediary that can relay these results.\n\n## Overview of the Approach\n\nTo facilitate the extraction of results, we will model our system to publish a specific type, `FinalResult`. An agent will subscribe to these results and make them available externally. This can be accomplished with a minimal amount of boilerplate code by using the `ClosureAgent` class from the `autogen_core` library. The `ClosureAgent` allows us to define a function that serves as the agent's message handler.\n\nIn this guiding example, we will set up a shared queue to handle the results between the agent and external code. This enables us to effectively manage the flow of information and ensures that results are easily retrievable once the system has finished processing.\n\n## Importing Required Libraries\n\nWe begin by importing the necessary libraries and modules. The `autogen_core` library provides essential functionalities that make creating and managing agents efficient.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    ClosureAgent,\n    ClosureContext,\n    DefaultSubscription,\n    DefaultTopicId,\n    MessageContext,\n    SingleThreadedAgentRuntime,\n)\n```\n\nThis code block imports modules needed for asynchronous programming (`asyncio`), along with data structures (`dataclass`) and core components from the `autogen_core` library for agent management.\n\n## Defining the Data Structure\n\nNext, we define a simple data structure to represent the final result. This encapsulation allows us to manage result attributes easily. \n\n```python\n@dataclass\nclass FinalResult:\n    value: str\n```\n\nHere, we create a `FinalResult` class using the `dataclass` decorator. This class contains a single attribute, `value`, which holds the result as a string. By using a dataclass, we can automatically generate methods like `__init__` and `__repr__`, keeping our code clean and readable.\n\n## Setting Up the Queue\n\nTo safely and asynchronously communicate the results from the agent to the external code, we establish a queue.\n\n```python\nqueue = asyncio.Queue[FinalResult]()\n```\n\nIn this code, we create an instance of `asyncio.Queue` specifically designated for holding `FinalResult` objects. This queue will act as a bridge, facilitating the transfer of results once they are available from the agent.\n\n## Implementing the Message Handler\n\nWe then define an asynchronous function that will handle messages received by the agent. This function must adhere to a specific signature as outlined in the documentation.\n\n```python\nasync def output_result(_agent: ClosureContext, message: FinalResult, ctx: MessageContext) -> None:\n    await queue.put(message)\n```\n\nIn this `output_result` function, we are using `_agent` and `ctx` as context parameters, although we will focus primarily on `message`, which contains the `FinalResult` instance. The function places the received message onto the queue for external access.\n\n## Registering the Agent\n\nWith our message handler defined, we can now create an agent and register it within an agent runtime. \n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait ClosureAgent.register_closure(\n    runtime, \"output_result\", output_result, subscriptions=lambda: [DefaultSubscription()]\n)\n```\n\nHere, we initialize a `SingleThreadedAgentRuntime` and register our `ClosureAgent`. The agent is set up to utilize the `output_result` function as its message handler, subscribing to default topics through a lambda function. This setup allows our agent to listen for incoming messages of the type `FinalResult`.\n\n## Simulating Result Collection\n\nTo illustrate how results can be generated and published, we simulate the process by publishing messages directly to the runtime.\n\n```python\nruntime.start()\nawait runtime.publish_message(FinalResult(\"Result 1\"), DefaultTopicId())\nawait runtime.publish_message(FinalResult(\"Result 2\"), DefaultTopicId())\nawait runtime.stop_when_idle()\n```\n\nIn this block, we first start the runtime. Next, we publish two `FinalResult` messages to the default topic. Finally, we instruct the runtime to stop once it becomes idle, indicating that no further messages are expected.\n\n## Retrieving the Results\n\nOnce results have been published, we can access them from the queue. This allows us to fetch and display the results processed by our agent.\n\n```python\nwhile not queue.empty():\n    print((result := await queue.get()).value)\n```\n\nThis snippet iterates over the queue and prints the results until it is empty. Each result is accessed using the `await` keyword, ensuring that we handle the asynchronous nature of the queue effectively. \n\nIn conclusion, this guide demonstrated how to extract results from a multi-agent system using a closure agent and a shared queue. By following these steps, one can manage result publication and retrieval efficiently, facilitating better control and access to outcomes generated within the agent framework.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.ipynb"
  },
  {
    "content": "# Cookbook\n\nThis section contains a collection of recipes that demonstrate how to use the Core API features.\n\n## List of recipes\n\n```{toctree}\n:maxdepth: 1\n\nazure-openai-with-aad-auth\ntermination-with-intervention\ntool-use-with-intervention\nextracting-results-with-an-agent\nopenai-assistant-agent\nlanggraph-agent\nllamaindex-agent\nlocal-llms-ollama-litellm\ninstrumenting\ntopic-subscription-scenarios\nstructured-output-agent\nllm-usage-logger\n```",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/index.md"
  },
  {
    "content": "# Instrumentating your code locally\n\nAutoGen supports instrumenting your code using [OpenTelemetry](https://opentelemetry.io). This allows you to collect traces and logs from your code and send them to a backend of your choice.\n\nWhile debugging, you can use a local backend such as [Aspire](https://aspiredashboard.com/) or [Jaeger](https://www.jaegertracing.io/). In this guide we will use Aspire as an example.\n\n## Setting up Aspire\n\nFollow the instructions [here](https://learn.microsoft.com/en-us/dotnet/aspire/fundamentals/dashboard/overview?tabs=bash#standalone-mode) to set up Aspire in standalone mode. This will require Docker to be installed on your machine.\n\n## Instrumenting your code\n\nOnce you have a dashboard set up, now it's a matter of sending traces and logs to it. You can follow the steps in the [Telemetry Guide](../framework/telemetry.md) to set up the opentelemetry sdk and exporter.\n\nAfter instrumenting your code with the Aspire Dashboard running, you should see traces and logs appear in the dashboard as your code runs.\n\n## Observing LLM calls using Open AI\n\nIf you are using the Open AI package, you can observe the LLM calls by setting up the opentelemetry for that library. We use [opentelemetry-instrumentation-openai](https://pypi.org/project/opentelemetry-instrumentation-openai/) in this example.\n\nInstall the package:\n```bash\npip install opentelemetry-instrumentation-openai\n```\n\nEnable the instrumentation:\n```python\nfrom opentelemetry.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument()\n```\n\nNow running your code will send traces including the LLM calls to your telemetry backend (Aspire in our case).\n\n![Open AI Telemetry logs](../../../images/open-ai-telemetry-example.png)",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/instrumenting.md"
  },
  {
    "code": false,
    "content": "# Using LangGraph-Backed Agent\n\nThis document outlines how to create an AI agent utilizing LangGraph, based on examples from the LangGraph documentation: [LangGraph Documentation](https://langchain-ai.github.io/langgraph/).\n\n## Prerequisites\n\nBefore getting started, you need to install the necessary dependencies. Make sure to run the following command in your terminal:\n\n```bash\n# pip install langgraph langchain-openai azure-identity\n```\n\n## Importing Required Modules\n\nThe next step involves importing the necessary modules which will be used throughout the agent's implementation. Here\u2019s the import block:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Literal\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.tools import tool  # pyright: ignore\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\nfrom langgraph.graph import END, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n```\n\nIn this block, we import various components from libraries such as `langchain`, `autogen_core`, and `azure.identity`. These modules will allow us to define messaging structures, create chat tools, and integrate Azure for our agent's runtime. \n\n## Defining the Message Structure\n\nTo facilitate communication between the user and the agent, we define a simple `Message` data class:\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nThis class has one attribute, `content`, which will store the text of messages exchanged with the agent. \n\n## Tool Definition for Weather Queries\n\nNext, we define a tool that the agent will use to provide weather information. Although this is a placeholder function, it serves as an example of how tools can interact with the agent:\n\n```python\n@tool  # pyright: ignore\ndef get_weather(location: str) -> str:\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in location.lower() or \"san francisco\" in location.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n```\n\nThe `get_weather` function checks if the location provided matches San Francisco ('SF') and returns a corresponding weather response. This serves as a basic demonstration of how the agent can utilize tool functions to generate responses.\n\n## Creating the LangGraph Agent\n\nNow we implement the agent class utilizing LangGraph's API:\n\n```python\nclass LangGraphToolUseAgent(RoutedAgent):\n    def __init__(self, description: str, model: ChatOpenAI, tools: List[Callable[..., Any]]) -> None:  # pyright: ignore\n        super().__init__(description)\n        self._model = model.bind_tools(tools)  # pyright: ignore\n\n        # Define the function that determines whether to continue or not\n        def should_continue(state: MessagesState) -> Literal[\"tools\", END]:  # type: ignore\n            messages = state[\"messages\"]\n            last_message = messages[-1]\n            if last_message.tool_calls:  # type: ignore\n                return \"tools\"\n            return END\n\n        # Define the function that calls the model\n        async def call_model(state: MessagesState):  # type: ignore\n            messages = state[\"messages\"]\n            response = await self._model.ainvoke(messages)\n            return {\"messages\": [response]}\n\n        tool_node = ToolNode(tools)  # pyright: ignore\n        self._workflow = StateGraph(MessagesState)\n\n        self._workflow.add_node(\"agent\", call_model)  # pyright: ignore\n        self._workflow.add_node(\"tools\", tool_node)  # pyright: ignore\n        self._workflow.set_entry_point(\"agent\")\n\n        self._workflow.add_conditional_edges(\"agent\", should_continue)  # type: ignore\n        self._workflow.add_edge(\"tools\", \"agent\")\n        self._app = self._workflow.compile()\n```\n\nThe `LangGraphToolUseAgent` class inherits from `RoutedAgent` and sets up an internal workflow for handling messages. It defines functions to call the model and to determine whether it needs to use a tool or respond directly. The workflow includes conditional and normal edges to cycle through the agent and tool nodes seamlessly.\n\n## Handling User Messages\n\nThe following method enables the agent to handle messages from users:\n\n```python\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Use the Runnable\n        final_state = await self._app.ainvoke(\n            {\n                \"messages\": [\n                    SystemMessage(\n                        content=\"You are a helpful AI assistant. You can use tools to help answer questions.\"\n                    ),\n                    HumanMessage(content=message.content),\n                ]\n            },\n            config={\"configurable\": {\"thread_id\": 42}},\n        )\n        response = Message(content=final_state[\"messages\"][-1].content)\n        return response\n```\n\nIn this method, the agent processes the user\u2019s message by sending it to the constructed workflow. It generates a response based on the model's predictions, which can include tool utilization where appropriate. The response is then wrapped in a `Message` object before being returned to the caller.\n\n## Testing the Agent\n\nTo test the agent, we must first create a runtime context and register the agent with it. The following code block demonstrates this process:\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait LangGraphToolUseAgent.register(\n    runtime,\n    \"langgraph_tool_use_agent\",\n    lambda: LangGraphToolUseAgent(\n        \"Tool use agent\",\n        ChatOpenAI(\n            model=\"gpt-4o\",\n            # api_key=os.getenv(\"OPENAI_API_KEY\"),\n        ),\n        [get_weather],\n    ),\n)\nagent = AgentId(\"langgraph_tool_use_agent\", key=\"default\")\n```\n\nIn this block, an instance of `SingleThreadedAgentRuntime` is created. The agent is registered with the runtime by providing a factory function that constructs an instance of `LangGraphToolUseAgent`. You can utilize either local OpenAI or Azure models, depending on your configuration.\n\n## Running the Agent\n\nThe next step is starting the agent runtime which enables it to process incoming messages:\n\n```python\nruntime.start()\n```\n\nThis will initiate the agent, allowing it to listen for user messages and produce responses.\n\n## Interacting with the Agent\n\nTo interact with the agent, you can send a direct message as shown below:\n\n```python\nresponse = await runtime.send_message(Message(\"What's the weather in SF?\"), agent)\nprint(response.content)\n```\n\nIn this block, a message is sent to the agent asking for the weather in San Francisco. The agent processes the message using its workflow, and the response is printed to the console.\n\n## Stopping the Agent Runtime\n\nFinally, it is important to gracefully stop the agent when it is no longer needed:\n\n```python\nawait runtime.stop()\n```\n\nThis ensures that resources are released and that the agent shuts down cleanly.\n\nIn summary, this documentation provides a guided walkthrough on setting up a LangGraph-backed agent, illustrating each key component along the way\u2014from installation to interaction.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/langgraph-agent.ipynb"
  },
  {
    "code": false,
    "content": "# Using LlamaIndex-Backed Agent\n\nThis document provides a comprehensive guide on how to create an AI agent using the LlamaIndex framework. The example covers installation, module imports, agent definition, message handling, and runtime operations. \n\n## Installation of Dependencies\n\nBefore diving into the code, ensure you have all the necessary dependencies installed. These libraries provide functionalities for embedding models, chat responses, and interfacing with Azure OpenAI.\n\n```python\n# Install required packages\n# pip install \"llama-index-readers-web\" \"llama-index-readers-wikipedia\" \"llama-index-tools-wikipedia\" \"llama-index-embeddings-azure-openai\" \"llama-index-llms-azure-openai\" \"llama-index\" \"azure-identity\"\n```\n\n## Importing Modules\n\nImport the required modules from various libraries to set up the functionality for your agent. This includes imports from `autogen_core`, `llama_index`, and `azure.identity`, among others.\n\n```python\nimport os\nfrom typing import List, Optional\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom llama_index.core import Settings\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.core.agent.runner.base import AgentRunner\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    MessageRole,\n)\nfrom llama_index.core.chat_engine.types import AgentChatResponse\nfrom llama_index.core.memory import ChatSummaryMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.tools.wikipedia import WikipediaToolSpec\nfrom pydantic import BaseModel\n```\n\n## Defining Message Structures\n\nThis section defines the structures that will be used to communicate between the user and the agent. The `Resource` class represents content retrieved by the agent, while the `Message` class encapsulates user inquiries and sources.\n\n```python\nclass Resource(BaseModel):\n    content: str\n    node_id: str\n    score: Optional[float] = None\n\nclass Message(BaseModel):\n    content: str\n    sources: Optional[List[Resource]] = None\n```\n\n## Creating the LlamaIndex Agent\n\nIn this section, we define the `LlamaIndexAgent` class, which is a routed agent that handles user messages. The agent retrieves history from memory, processes incoming messages, and interacts with an underlying LlamaIndex agent.\n\n```python\nclass LlamaIndexAgent(RoutedAgent):\n    def __init__(self, description: str, llama_index_agent: AgentRunner, memory: BaseMemory | None = None) -> None:\n        super().__init__(description)\n\n        self._llama_index_agent = llama_index_agent\n        self._memory = memory\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Retrieve history messages from memory\n        history_messages: List[ChatMessage] = []\n\n        response: AgentChatResponse  # pyright: ignore\n        if self._memory is not None:\n            history_messages = self._memory.get(input=message.content)\n\n            response = await self._llama_index_agent.achat(message=message.content, history_messages=history_messages)  # pyright: ignore\n        else:\n            response = await self._llama_index_agent.achat(message=message.content)  # pyright: ignore\n\n        if isinstance(response, AgentChatResponse):\n            if self._memory is not None:\n                self._memory.put(ChatMessage(role=MessageRole.USER, content=message.content))\n                self._memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response.response))\n\n            assert isinstance(response.response, str)\n\n            resources: List[Resource] = [\n                Resource(content=source_node.get_text(), score=source_node.score, node_id=source_node.id_)\n                for source_node in response.source_nodes\n            ]\n\n            tools: List[Resource] = [\n                Resource(content=source.content, node_id=source.tool_name) for source in response.sources\n            ]\n\n            resources.extend(tools)\n            return Message(content=response.response, sources=resources)\n        else:\n            return Message(content=\"I'm sorry, I don't have an answer for you.\")\n```\n\n## Setting Up LlamaIndex\n\nNow that we have our agent defined, we can configure LlamaIndex with the necessary machine learning models. Here, you'll set up the language model (LLM) and embedding model.\n\n```python\n# llm = AzureOpenAI(\n#     deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n#     temperature=0.0,\n#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),\n#     # api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n# )\nllm = OpenAI(\n    model=\"gpt-4o\",\n    temperature=0.0,\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# embed_model = AzureOpenAIEmbedding(\n#     deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n#     temperature=0.0,\n#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),\n#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n# )\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-ada-002\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n## Creating Tools for the Agent\n\nIn this step, we configure any additional tools the agent might leverage. Here, we use the Wikipedia tool spec to prepare a tool for retrieving information from Wikipedia.\n\n```python\nwiki_spec = WikipediaToolSpec()\nwikipedia_tool = wiki_spec.to_tool_list()[1]\n```\n\n## Running the Agent\n\nNext, we set up the agent runtime. This includes registering the agent by its name and defining a factory function to create the agent instance.\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait LlamaIndexAgent.register(\n    runtime,\n    \"chat_agent\",\n    lambda: LlamaIndexAgent(\n        description=\"Llama Index Agent\",\n        llama_index_agent=ReActAgent.from_tools(\n            tools=[wikipedia_tool],\n            llm=llm,\n            max_iterations=8,\n            memory=ChatSummaryMemoryBuffer(llm=llm, token_limit=16000),\n            verbose=True,\n        ),\n    ),\n)\nagent = AgentId(\"chat_agent\", \"default\")\n```\n\n## Starting and Interacting with the Agent\n\nHere, we start the agent runtime and send a test message to the agent. The response is printed for user reference.\n\n```python\nruntime.start()\n\nmessage = Message(content=\"What are the best movies from studio Ghibli?\")\nresponse = await runtime.send_message(message, agent)\nassert isinstance(response, Message)\nprint(response.content)\n\nif response.sources is not None:\n    for source in response.sources:\n        print(source.content)\n```\n\n## Stopping the Agent Runtime\n\nAfter completing interactions, it's essential to stop the agent runtime to clean up resources properly.\n\n```python\nawait runtime.stop()\n```\n\nThis documentation provides a structured overview of how to create and use a LlamaIndex-backed AI agent, guiding users through the setup, implementation, and operational phases.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb"
  },
  {
    "code": false,
    "content": "# Tracking LLM Usage with a Logger\n\n## Overview\n\nThis guide demonstrates how to implement a logging mechanism to track the usage of Large Language Models (LLMs) through structured events. The provided code leverages built-in logging capabilities in Python to monitor the number of tokens used during prompts and completions. By incorporating this logging mechanism, developers can gain insights into the model's usage patterns and performance.\n\nThe events relevant for tracking LLM usage are logged with the name defined as `autogen_core.EVENT_LOGGER_NAME`. This ensures that the events can be retrieved and utilized effectively.\n\n## Implementing the LLM Usage Tracker\n\nThe first step in tracking LLM usage is to create a custom logging handler, `LLMUsageTracker`, which extends the base `logging.Handler`. This handler captures token usage metrics for both prompts and completions. Here is the implementation:\n\n```python\nimport logging\nfrom autogen_core.logging import LLMCallEvent\n\nclass LLMUsageTracker(logging.Handler):\n    def __init__(self) -> None:\n        \"\"\"Logging handler that tracks the number of tokens used in the prompt and completion.\"\"\"\n        super().__init__()\n        self._prompt_tokens = 0\n        self._completion_tokens = 0\n\n    @property\n    def tokens(self) -> int:\n        return self._prompt_tokens + self._completion_tokens\n\n    @property\n    def prompt_tokens(self) -> int:\n        return self._prompt_tokens\n\n    @property\n    def completion_tokens(self) -> int:\n        return self._completion_tokens\n\n    def reset(self) -> None:\n        self._prompt_tokens = 0\n        self._completion_tokens = 0\n\n    def emit(self, record: logging.LogRecord) -> None:\n        \"\"\"Emit the log record. To be used by the logging module.\"\"\"\n        try:\n            # Use the StructuredMessage if the message is an instance of it\n            if isinstance(record.msg, LLMCallEvent):\n                event = record.msg\n                self._prompt_tokens += event.prompt_tokens\n                self._completion_tokens += event.completion_tokens\n        except Exception:\n            self.handleError(record)\n```\n\n### Code Explanation\n\n1. **Initialization**: The `LLMUsageTracker` class initializes two private attributes, `_prompt_tokens` and `_completion_tokens`, to zero. These will store the total counts of tokens used in prompts and completions, respectively.\n\n2. **Properties**: The class includes properties to retrieve the total token count (`tokens`), prompt tokens (`prompt_tokens`), and completion tokens (`completion_tokens`). These properties make it easy to access the token statistics.\n\n3. **Reset Method**: The `reset()` method resets both token counts to zero, allowing for fresh tracking in subsequent uses.\n\n4. **Emit Method**: The core functionality of the logger is in the `emit()` method, where it checks if the log record's message is an instance of `LLMCallEvent`. If so, it updates the counts of prompt and completion tokens based on the event data.\n\n## Setting Up the Logger\n\nOnce the `LLMUsageTracker` has been implemented, it can be attached to a logger just like any other standard Python logger. The following code illustrates how to configure the logger to utilize the custom handler:\n\n```python\nfrom autogen_core import EVENT_LOGGER_NAME\n\n# Set up the logging configuration to use the custom handler\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nllm_usage = LLMUsageTracker()\nlogger.handlers = [llm_usage]\n\n# client.create(...)\n```\n\n### Code Explanation\n\n1. **Logger Initialization**: The logger is initialized using the `EVENT_LOGGER_NAME`. This ensures that all events related to LLM are captured under the specified name.\n\n2. **Setting the Log Level**: The log level is set to `INFO`, allowing for the emission of informational messages. This can be adjusted based on the desired verbosity of logging.\n\n3. **Adding the Custom Handler**: An instance of `LLMUsageTracker` is created and set as the handler for the logger. This means that all log messages will be processed through the custom handler, allowing for token tracking.\n\n4. **Model Interaction**: You can now call the model\u2019s methods (e.g., `client.create(...)`) where the logging events will be triggered as needed.\n\n## Accessing Token Data\n\nAfter executing your model commands, you can retrieve the tracked data from the `LLMUsageTracker` instance, thereby gaining insights into the usage patterns. Here\u2019s how to access the token counts:\n\n```python\nprint(llm_usage.prompt_tokens)\nprint(llm_usage.completion_tokens)\n```\n\n### Code Explanation\n\n1. **Printing Token Counts**: After your interactions with the model, you simply print the number of prompt and completion tokens captured by `llm_usage`. This allows you to review how many tokens were utilized during the model's operations.\n\n## Conclusion\n\nIn summary, this guide provides a comprehensive overview of setting up a logging mechanism to track LLM usage effectively. By implementing the `LLMUsageTracker` and integrating it with the Python logging framework, developers can monitor model interactions and optimize performance based on usage insights. This tracking is crucial for understanding how models are utilized and for making data-driven decisions in future implementations.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/llm-usage-logger.ipynb"
  },
  {
    "code": false,
    "content": "# Local LLMs with LiteLLM & Ollama\n\nIn this documentation, we will create two agents, Joe and Cathy, who enjoy exchanging jokes using locally running Large Language Models (LLMs). This guide will provide step-by-step instructions to set up your environment, import necessary classes, and implement the agents. \n\nFollow the guide at [AutoGen Documentation](https://microsoft.github.io/autogen/docs/topics/non-openai-models/local-litellm-ollama/) for complete installation instructions. However, if you're in a hurry and using Linux, you can quickly set up LiteLLM and Ollama by running the following commands:\n\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\nollama pull llama3.2:1b\npip install 'litellm[proxy]'\nlitellm --model ollama/llama3.2:1b\n```\n\nThis will start the proxy server, making it available at `http://0.0.0.0:4000/`.\n\n## Importing Required Classes\n\nTo kick off our implementation, we need to import necessary classes that will help us manage the agents and messages effectively.\n\n```python\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    AgentId,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    default_subscription,\n    message_handler,\n)\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n### Explanation:\nThis section imports relevant classes from the `autogen_core` and `autogen_ext` libraries. Notably, `RoutedAgent` will help in creating agents, while `ChatCompletionClient` is crucial for interacting with the model. The `dataclass` decorator will allow us to easily define structured data types.\n\n## Setting Up the Local LLM Client\n\nNext, we set up the local LLM model client with the following function:\n\n```python\ndef get_model_client() -> OpenAIChatCompletionClient:  # type: ignore\n    \"Mimic OpenAI API using Local LLM Server.\"\n    return OpenAIChatCompletionClient(\n        model=\"llama3.2:1b\",\n        api_key=\"NotRequiredSinceWeAreLocal\",\n        base_url=\"http://0.0.0.0:4000\",\n        model_capabilities={\n            \"json_output\": False,\n            \"vision\": False,\n            \"function_calling\": True,\n        },\n    )\n```\n\n### Explanation:\nThis function initializes and returns an instance of `OpenAIChatCompletionClient`, configured to use the locally running model. It specifies the model name, a dummy API key since we are operating locally, and the server's base URL along with capabilities of the model.\n\n## Defining a Message Class\n\nNow, let's create a simple `Message` class to encapsulate the joke content.\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\n### Explanation:\nThe `Message` class is defined as a data structure to hold the content of messages exchanged between agents. Using the `@dataclass` decorator simplifies the boilerplate code needed for initializing and representing the class.\n\n## Implementing the Agent\n\nWe will now define the `Assistant` agent, which will act as the body of our joke-telling functionality.\n\n```python\n@default_subscription\nclass Assistant(RoutedAgent):\n    def __init__(self, name: str, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"An assistant agent.\")\n        self._model_client = model_client\n        self.name = name\n        self.count = 0\n        self._system_messages = [\n            SystemMessage(\n                content=f\"Your name is {name} and you are a part of a duo of comedians.\"\n                \"You laugh when you find the joke funny, else reply 'I need to go now'.\",\n            )\n        ]\n        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        self.count += 1\n        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n        result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n\n        print(f\"\\n{self.name}: {message.content}\")\n\n        if \"I need to go\".lower() in message.content.lower() or self.count > 2:\n            return\n\n        await self._model_context.add_message(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n```\n\n### Explanation:\nThe `Assistant` class extends `RoutedAgent` and represents each agent (Cathy and Joe). It initializes with a name and a model client. The `handle_message` method processes incoming messages, increases the joke count, adds messages to the context, and generates responses based on the content. The agent will respond with a joke, or, after a certain condition, signal that it needs to \"go\".\n\n## Registering the Agents\n\nNext, we will set up our agents with the runtime environment.\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = get_model_client()\n\ncathy = await Assistant.register(\n    runtime,\n    \"cathy\",\n    lambda: Assistant(name=\"Cathy\", model_client=model_client),\n)\n\njoe = await Assistant.register(\n    runtime,\n    \"joe\",\n    lambda: Assistant(name=\"Joe\", model_client=model_client),\n)\n```\n\n### Explanation:\nThis code initializes a single-threaded agent runtime and registers the two agents, Cathy and Joe. The agents are instantiated and linked with the local LLM model client, making them ready for interaction.\n\n## Running the Agents\n\nFinally, we can run our agents and initiate the conversation.\n\n```python\nruntime.start()\nawait runtime.send_message(\n    Message(\"Joe, tell me a joke.\"),\n    recipient=AgentId(joe, \"default\"),\n    sender=AgentId(cathy, \"default\"),\n)\nawait runtime.stop_when_idle()\n\n# Close the connections to the model clients.\nawait model_client.close()\n```\n\n### Explanation:\nThe runtime is started, and an initial message is sent from Cathy to Joe, prompting him to tell a joke. The process continues until all agents are idle, after which the connections to the model clients are closed.\n\nThis concludes the implementation of locally running agents for joke-telling using LiteLLM and Ollama. Enjoy experimenting with your agents!",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.ipynb"
  },
  {
    "code": false,
    "content": "# OpenAI Assistant Agent Documentation\n\nThis documentation provides an overview of building an AI agent using the OpenAI Assistant API. It illustrates the structure, functionalities, and workflow of the agent, allowing it to perform code execution and question-answering over documents.\n\n## Message Protocol\n\nThe first step in building the OpenAI Assistant Agent involves defining the message protocol. The message protocol establishes the structure for messages exchanged between the agent and other components interfacing with it. In this implementation, we define four message types: `TextMessage`, `Reset`, `UploadForCodeInterpreter`, and `UploadForFileSearch`.\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n@dataclass\nclass Reset:\n    pass\n\n@dataclass\nclass UploadForCodeInterpreter:\n    file_path: str\n\n@dataclass\nclass UploadForFileSearch:\n    file_path: str\n    vector_store_id: str\n```\n\nThe `TextMessage` class is used for day-to-day interactions, encapsulating message content and the sender's source. The `Reset` message is a control signal used to clear the agent's memory, enabling the initiation of a fresh conversation. The `UploadForCodeInterpreter` and `UploadForFileSearch` classes facilitate file uploads\u2014one for code interpretation tasks and another for searching documents, respectively\u2014both containing a `file_path` for the file to be uploaded.\n\n## Defining the Agent\n\nWith the message protocol in place, we proceed to define the core `OpenAIAssistantAgent` class. The constructor of this class accepts several parameters necessary for its operation: `description`, `client`, `assistant_id`, `thread_id`, and a factory function for the event handler. The `client` parameter is an instance of the asynchronous OpenAI client, while the `assistant_event_handler_factory` manages event handling within the assistant.\n\nThe agent class is equipped with various message handlers that process the different message types defined earlier, namely:\n\n- `handle_message`: Processes `TextMessage`, sending the assistant's response.\n- `handle_reset`: Manages `Reset` messages, clearing the agent's memory.\n- `handle_upload_for_code_interpreter`: Handles uploads of files for code interpretation.\n- `handle_upload_for_file_search`: Handles uploads of files for searching documents.\n\nThe agent's memory is maintained in a thread on the server, as referenced by the `thread_id`.\n\n```python\nimport asyncio\nimport os\nfrom typing import Any, Callable, List\n\nimport aiofiles\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.thread import ToolResources, ToolResourcesFileSearch\n\nclass OpenAIAssistantAgent(RoutedAgent):\n    \"\"\"An agent implementation that uses the OpenAI Assistant API to generate responses.\"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        client: AsyncClient,\n        assistant_id: str,\n        thread_id: str,\n        assistant_event_handler_factory: Callable[[], AsyncAssistantEventHandler],\n    ) -> None:\n        super().__init__(description)\n        self._client = client\n        self._assistant_id = assistant_id\n        self._thread_id = thread_id\n        self._assistant_event_handler_factory = assistant_event_handler_factory\n\n    @message_handler\n    async def handle_message(self, message: TextMessage, ctx: MessageContext) -> TextMessage:\n        \"\"\"Handle a message and return the assistant's response.\"\"\"\n        # Save the message to the thread.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.messages.create(\n                    thread_id=self._thread_id,\n                    content=message.content,\n                    role=\"user\",\n                    metadata={\"sender\": message.source},\n                )\n            )\n        )\n        # Generate a response.\n        async with self._client.beta.threads.runs.stream(\n            thread_id=self._thread_id,\n            assistant_id=self._assistant_id,\n            event_handler=self._assistant_event_handler_factory(),\n        ) as stream:\n            await ctx.cancellation_token.link_future(asyncio.ensure_future(stream.until_done()))\n\n        # Retrieve and return the last message.\n        messages = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, order=\"desc\", limit=1))\n        )\n        last_message_content = messages.data[0].content\n        text_content = [content for content in last_message_content if content.type == \"text\"]\n        if not text_content:\n            raise ValueError(f\"Expected text content in the last message: {last_message_content}\")\n\n        return TextMessage(content=text_content[0].text.value, source=self.metadata[\"type\"])\n\n    @message_handler()\n    async def on_reset(self, message: Reset, ctx: MessageContext) -> None:\n        \"\"\"Reset the agent's memory by deleting all messages in the thread.\"\"\"\n        all_msgs: List[str] = []\n        while True:\n            msgs = await ctx.cancellation_token.link_future(\n                asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, after=all_msgs[-1] if all_msgs else None))\n            )\n            all_msgs.extend([msg.id for msg in msgs.data])\n            if not msgs.has_next_page():\n                break\n        # Delete all messages.\n        for msg_id in all_msgs:\n            status = await ctx.cancellation_token.link_future(\n                asyncio.ensure_future(\n                    self._client.beta.threads.messages.delete(message_id=msg_id, thread_id=self._thread_id)\n                )\n            )\n            assert status.deleted is True\n\n    @message_handler()\n    async def on_upload_for_code_interpreter(self, message: UploadForCodeInterpreter, ctx: MessageContext) -> None:\n        \"\"\"Upload a file for the code interpreter and update the thread.\"\"\"\n        async with aiofiles.open(message.file_path, mode=\"rb\") as f:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(f.read()))\n        file_name = os.path.basename(message.file_path)\n        file = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.files.create(file=(file_name, file_content), purpose=\"assistants\"))\n        )\n        thread = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.retrieve(thread_id=self._thread_id))\n        )\n        tool_resources: ToolResources = thread.tool_resources if thread.tool_resources else ToolResources()\n\n        if tool_resources.code_interpreter and tool_resources.code_interpreter.file_ids:\n            file_ids = tool_resources.code_interpreter.file_ids\n        else:\n            file_ids = [file.id]\n\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.update(\n                    thread_id=self._thread_id,\n                    tool_resources={\"code_interpreter\": {\"file_ids\": file_ids}},\n                )\n            )\n        )\n\n    @message_handler()\n    async def on_upload_for_file_search(self, message: UploadForFileSearch, ctx: MessageContext) -> None:\n        \"\"\"Upload a file for file searching and update the vector store.\"\"\"\n        async with aiofiles.open(message.file_path, mode=\"rb\") as file:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(file.read()))\n        file_name = os.path.basename(message.file_path)\n\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.vector_stores.file_batches.upload_and_poll(\n                    vector_store_id=message.vector_store_id,\n                    files=[(file_name, file_content)],\n                )\n            )\n        )\n```\n\nThe `OpenAIAssistantAgent` class serves as an interface to the OpenAI Assistant API, allowing for enhancements like multi-modal message handling. This setup lays the foundation for building an interactive assistant capable of processing various types of requests and operations.\n\n## Assistant Event Handler\n\nThe assistant event handler facilitates callbacks that are triggered by events from the Assistant API. This functionality allows for better management of streaming outputs and can improve user interface integration.\n\n```python\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.threads import Message, Text, TextDelta\nfrom openai.types.beta.threads.runs import RunStep, RunStepDelta\nfrom typing_extensions import override\n\nclass EventHandler(AsyncAssistantEventHandler):\n    @override\n    async def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:\n        print(delta.value, end=\"\", flush=True)\n\n    @override\n    async def on_run_step_created(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\nGenerating code to interpret:\\n\\n```python\")\n\n    @override\n    async def on_run_step_done(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\n```\\nExecuting code...\")\n\n    @override\n    async def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:\n        details = delta.step_details\n        if details and details.type == \"tool_calls\":\n            for tool in details.tool_calls or []:\n                if tool.type == \"code_interpreter\" and tool.code_interpreter and tool.code_interpreter.input:\n                    print(tool.code_interpreter.input, end=\"\", flush=True)\n\n    @override\n    async def on_message_created(self, message: Message) -> None:\n        print(f\"{'-'*80}\\nAssistant:\\n\")\n\n    @override\n    async def on_message_done(self, message: Message) -> None:\n        if not message.content:\n            return\n        content = message.content[0]\n        if content.type != \"text\":\n            return\n        text_content = content.text\n        annotations = text_content.annotations\n        citations = []\n        for index, annotation in enumerate(annotations):\n            text_content.value = text_content.value.replace(annotation.text, f\"[{index}]\")\n            if file_citation := getattr(annotation, \"file_citation\", None):\n                client = AsyncClient()\n                cited_file = await client.files.retrieve(file_citation.file_id)\n                citations.append(f\"[{index}] {cited_file.filename}\")\n        if citations:\n            print(\"\\n\".join(citations))\n```\n\nThis event handler captures various assistant events such as text deltas and message lifecycle events, allowing dynamic responses during interaction. By handling events like code generation and execution, the user experience during code interpretation becomes smoother.\n\n## Using the Agent\n\nTo use this agent, we need to create an OpenAI client that can instantiate an assistant, manage threads, and set up a vector store for document searches.\n\n```python\nimport openai\n\n# Create an assistant with code interpreter and file search tools.\noai_assistant = openai.beta.assistants.create(\n    model=\"gpt-4o-mini\",\n    description=\"An AI assistant that helps with everyday tasks.\",\n    instructions=\"Help the user with their task.\",\n    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n)\n\n# Create a vector store to be used for file search.\nvector_store = openai.vector_stores.create()\n\n# Create a thread which is used as the memory for the assistant.\nthread = openai.beta.threads.create(\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n```\n\nIn the above code, we establish an assistant equipped with tools for code interpretation and document searching. Additionally, we create a vector store to facilitate searches and a thread to maintain conversation state.\n\nAfter setting up essential components, we can proceed to register the `OpenAIAssistantAgent` within a runtime. \n\n```python\nfrom autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait OpenAIAssistantAgent.register(\n    runtime,\n    \"assistant\",\n    lambda: OpenAIAssistantAgent(\n        description=\"OpenAI Assistant Agent\",\n        client=openai.AsyncClient(),\n        assistant_id=oai_assistant.id,\n        thread_id=thread.id,\n        assistant_event_handler_factory=lambda: EventHandler(),\n    ),\n)\nagent = AgentId(\"assistant\", \"default\")\n```\n\nThis registration process binds the agent to a single-threaded runtime, allowing it to handle asynchronous operations effectively.\n\nTo monitor the runtime's operations, we can turn on logging:\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)\n```\n\nThis logging configuration helps trace the actions and events occurring within the agent.\n\nNext, let's send a greeting message to the agent and observe its response:\n\n```python\nruntime.start()\nawait runtime.send_message(TextMessage(content=\"Hello, how are you today!\", source=\"user\"), agent)\nawait runtime.stop_when_idle()\n```\n\n## Assistant with Code Interpreter\n\nTo demonstrate the use of the code interpreter, we'll ask the agent a simple math question.\n\n```python\nruntime.start()\nawait runtime.send_message(TextMessage(content=\"What is 1332322 x 123212?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()\n```\n\nThis showcases the agent's capability to execute code and return accurate results.\n\nTo take it a step further, we can fetch data from an external source such as the City of Seattle Wage Data:\n\n```python\nimport requests\n\nresponse = requests.get(\"https://data.seattle.gov/resource/2khk-5ukd.csv\")\nwith open(\"seattle_city_wages.csv\", \"wb\") as file:\n    file.write(response.content)\n```\n\nOnce we have the data downloaded, we can send it to the agent using the `UploadForCodeInterpreter` message:\n\n```python\nruntime.start()\nawait runtime.send_message(UploadForCodeInterpreter(file_path=\"seattle_city_wages.csv\"), agent)\nawait runtime.stop_when_idle()\n```\n\nFollowing the upload, we can engage the agent with queries about the data:\n\n```python\nruntime.start()\nawait runtime.send_message(TextMessage(content=\"Take a look at the uploaded CSV file.\", source=\"user\"), agent)\nawait runtime.stop_when_idle()\n\nruntime.start()\nawait runtime.send_message(TextMessage(content=\"What are the top-10 salaries?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()\n```\n\n## Assistant with File Search\n\nWe can also implement the Q&A functionality over a document by sending content retrieved from the web. For example, let's download a Wikipedia page on the Third Anglo-Afghan War:\n\n```python\nresponse = requests.get(\"https://en.wikipedia.org/wiki/Third_Anglo-Afghan_War\")\nwith open(\"third_anglo_afghan_war.html\", \"wb\") as file:\n    file.write(response.content)\n```\n\nAfter saving the document, we upload it to the agent using the `UploadForFileSearch` message:\n\n```python\nruntime.start()\nawait runtime.send_message(\n    UploadForFileSearch(file_path=\"third_anglo_afghan_war.html\", vector_store_id=vector_store.id), agent\n)\nawait runtime.stop_when_idle()\n```\n\nWe can then engage the agent with questions about the document. To begin a new conversation fresh, we reset the agent's memory:\n\n```python\nruntime.start()\nawait runtime.send_message(Reset(), agent)\nawait runtime.send_message(\n    TextMessage(\n        content=\"When and where was the treaty of Rawalpindi signed? Answer using the document provided.\", source=\"user\"\n    ),\n    agent,\n)\nawait runtime.stop_when_idle()\n```\n\nWith this setup, we've crafted an intelligent agent powered by the OpenAI Assistant, capable of handling various tasks from code execution to document-based queries.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/openai-assistant-agent.ipynb"
  },
  {
    "code": false,
    "content": "# Structured Output Using GPT-4o Models\n\nThis documentation illustrates how to obtain structured output by utilizing GPT-4o models, focusing on the integration with the OpenAI beta client SDK. This SDK provides a parsing helper that allows developers to leverage their own Pydantic models, streamlining the process of handling structured data without needing to define a JSON schema.\n\n## Supported GPT-4o Models\n\nCurrently, the structured output feature is compatible with the following models:\n\n- **gpt-4o-mini** on OpenAI\n- **gpt-4o-2024-08-06** on OpenAI\n- **gpt-4o-2024-08-06** on Azure\n\nThis setup enables you to utilize a consistent approach across different environments while utilizing the same foundational logic.\n\n## Defining a Pydantic Model\n\nTo manage the structured output effectively, we start by defining a simple Pydantic model, which encapsulates a mathematical reasoning process. The model `MathReasoning` includes a list of steps, each containing an explanation and output, along with the final answer.\n\n```python\nfrom pydantic import BaseModel\n\nclass MathReasoning(BaseModel):\n    class Step(BaseModel):\n        explanation: str\n        output: str\n\n    steps: list[Step]\n    final_answer: str\n```\n\nIn this code block, the `MathReasoning` model comprises:\n1. A nested `Step` model that holds an explanation and the corresponding output.\n2. A list of `steps` and a `final_answer` field, which captures the result of the mathematical operation.\n\n## Setting Environment Variables\n\nNext, we need to set the necessary environment variables that configure the connection to the Azure OpenAI service. These include endpoints, API keys, and deployment names.\n\n```python\nimport os\n\n# Set the environment variable\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://YOUR_ENDPOINT_DETAILS.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\nos.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o-2024-08-06\"\nos.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n```\n\nIn this section, remember to replace `YOUR_ENDPOINT_DETAILS` and `YOUR_API_KEY` with actual values. This configuration establishes a secure connection to the Azure OpenAI service, ensuring that your requests are authenticated and directed to the appropriate model deployment.\n\n## Creating the OpenAI Client\n\nThe next step involves creating an instance of the Azure OpenAI client. This process includes error-checking to ensure all environment variables are set correctly.\n\n```python\nimport json\nimport os\nfrom typing import Optional\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\n# Function to get environment variable and ensure it is not None\ndef get_env_variable(name: str) -> str:\n    value = os.getenv(name)\n    if value is None:\n        raise ValueError(f\"Environment variable {name} is not set\")\n    return value\n\n# Create the client with type-checked environment variables\nclient = AzureOpenAIChatCompletionClient(\n    azure_deployment=get_env_variable(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n    model=get_env_variable(\"AZURE_OPENAI_MODEL\"),\n    api_version=get_env_variable(\"AZURE_OPENAI_API_VERSION\"),\n    azure_endpoint=get_env_variable(\"AZURE_OPENAI_ENDPOINT\"),\n    api_key=get_env_variable(\"AZURE_OPENAI_API_KEY\"),\n)\n```\n\nHere\u2019s what happens in this code block:\n- A function `get_env_variable` checks the existence of environment variables and raises an error if any are missing.\n- An instance of `AzureOpenAIChatCompletionClient` is created using validated environment variables, ensuring type safety and reducing potential errors in configuration.\n\n## Sending Messages and Retrieving Responses\n\nOnce the client is properly set up, you can now define a user message that will be sent to the GPT-4o model. In this example, we will ask a simple arithmetic question.\n\n```python\n# Define the user message\nmessages = [\n    UserMessage(content=\"What is 16 + 32?\", source=\"user\"),\n]\n\n# Call the create method on the client, passing the messages and additional arguments\n# The extra_create_args dictionary includes the response format as MathReasoning model we defined above\n# Providing the response format and pydantic model will use the new parse method from beta SDK\nresponse = await client.create(messages=messages, extra_create_args={\"response_format\": MathReasoning})\n\n# Ensure the response content is a valid JSON string before loading it\nresponse_content: Optional[str] = response.content if isinstance(response.content, str) else None\nif response_content is None:\n    raise ValueError(\"Response content is not a valid JSON string\")\n\n# Print the response content after loading it as JSON\nprint(json.loads(response_content))\n\n# Validate the response content with the MathReasoning model\nMathReasoning.model_validate(json.loads(response_content))\n```\n\nIn this code segment:\n1. A user-generated question is defined.\n2. The client creates a response while specifying that it should conform to the `MathReasoning` model previously defined.\n3. The response is checked to ensure it is a valid JSON string before parsing it.\n4. Finally, the content is printed and validated against the `MathReasoning` model to confirm that the structure matches our expectations.\n\nThis approach promotes a clean, structured output that can be seamlessly processed and utilized in various applications or further computations.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/structured-output-agent.ipynb"
  },
  {
    "code": false,
    "content": "# Termination Using Intervention Handler\n\nThis documentation covers the method of handling termination in the `autogen_core` library using an `InterventionHandler`. This approach is particularly applicable in scenarios where the runtime needs to gracefully finalize when it's no longer required to execute.\n\n## Overview of Termination Handling\n\nWhen working with the `autogen_core` framework, it is essential to manage how and when the runtime finishes execution. Various strategies exist for detecting and responding to termination requests. One effective method involves utilizing the `InterventionHandler` class to capture termination messages and implement appropriate actions based on them.\n\n### Importing Necessary Modules\n\nBefore we can create our termination handling process, we need to import the necessary classes and functions from the `autogen_core` library. The following code shows how to do this:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom autogen_core import (\n    DefaultInterventionHandler,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    default_subscription,\n    message_handler,\n)\n```\n\nThis code imports essential components needed for our agent and the termination handler, including a message context and a routing mechanism.\n\n## Defining the Message Structures\n\nTo facilitate communication between the agent and the termination handler, we first need to define two dataclasses, one for regular messages and another to indicate termination reasons.\n\n```python\n@dataclass\nclass Message:\n    content: Any\n\n\n@dataclass\nclass Termination:\n    reason: str\n```\n\n- `Message`: This class represents a general message with a generic content type.\n- `Termination`: This class signifies a termination event, encapsulating a reason for the termination.\n\n### Implementing the Agent\n\nNext, we create an agent that will publish a termination message once it determines it should stop executing. The agent will track how many messages it has received and trigger termination after receiving a predefined number.\n\n```python\n@default_subscription\nclass AnAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"MyAgent\")\n        self.received = 0\n\n    @message_handler\n    async def on_new_message(self, message: Message, ctx: MessageContext) -> None:\n        self.received += 1\n        if self.received > 3:\n            await self.publish_message(Termination(reason=\"Reached maximum number of messages\"), DefaultTopicId())\n```\n\n- The agent, `AnAgent`, subscribes to messages and will increment a counter each time it processes a new message.\n- If the count exceeds three, it publishes a `Termination` message, indicating that it has hit its threshold for processing.\n\n## Creating the Termination Handler\n\nTo detect the termination message, we need to implement a custom `InterventionHandler`. This handler will listen for published messages and track the termination state.\n\n```python\nclass TerminationHandler(DefaultInterventionHandler):\n    def __init__(self) -> None:\n        self._termination_value: Termination | None = None\n\n    async def on_publish(self, message: Any, *, message_context: MessageContext) -> Any:\n        if isinstance(message, Termination):\n            self._termination_value = message\n        return message\n\n    @property\n    def termination_value(self) -> Termination | None:\n        return self._termination_value\n\n    @property\n    def has_terminated(self) -> bool:\n        return self._termination_value is not None\n```\n\n- The `TerminationHandler` class extends `DefaultInterventionHandler`.\n- It has an internal variable to hold the termination message if one is received.\n- The `on_publish` method checks for termination messages and updates its internal state accordingly.\n- The properties `termination_value` and `has_terminated` are used to check whether a termination has been requested.\n\n### Setting Up the Runtime\n\nAfter defining the handler, we can initialize the runtime and register the agent. The following code handles this part of the process:\n\n```python\ntermination_handler = TerminationHandler()\nruntime = SingleThreadedAgentRuntime(intervention_handlers=[termination_handler])\n\nawait AnAgent.register(runtime, \"my_agent\", AnAgent)\n\nruntime.start()\n\n# Publish more than 3 messages to trigger termination.\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\nawait runtime.publish_message(Message(\"hello\"), DefaultTopicId())\n\n# Wait for termination.\nawait runtime.stop_when(lambda: termination_handler.has_terminated)\n\nprint(termination_handler.termination_value)\n```\n\n- The `TerminationHandler` is instantiated and passed to the `SingleThreadedAgentRuntime`.\n- The agent is registered and started, allowing it to receive and process messages.\n- Messages are published until the threshold is met, triggering the termination process.\n- The runtime will wait until a termination message is detected, at which point it will gracefully stop.\n\nIn conclusion, this documentation outlines the steps to implement termination handling using the `InterventionHandler` class in the `autogen_core` framework. By following these steps, users can ensure a clean and controlled shutdown of their runtime environments.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/termination-with-intervention.ipynb"
  },
  {
    "code": false,
    "content": "# User Approval for Tool Execution using Intervention Handler\n\nThis documentation presents a guide to intercept tool execution using an intervention handler, prompting the user for permission before proceeding with the execution of tools.\n\n## Importing Required Libraries\n\nFirst, we need to import the necessary libraries and modules that will facilitate our tool execution and intervention handling capabilities. This includes various classes from `autogen_core` and `autogen_ext` for messaging, agent management, and tool execution.\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Any, List\n\nfrom autogen_core import (\n    AgentId,\n    AgentType,\n    DefaultInterventionHandler,\n    DropMessage,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tool_agent import ToolAgent, ToolException, tool_agent_caller_loop\nfrom autogen_core.tools import ToolSchema\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n```\n\nThis section establishes the required classes and functions necessary to manage agents, handle messages, and execute tools within a Docker environment.\n\n## Defining a Simple Message Type\n\nNext, let's create a simple message structure that will be used to convey content through our agents.\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nIn this snippet, we define a `Message` class using the `dataclass` decorator. This class has an attribute `content` which will hold the textual message sent to or received from the user.\n\n## Creating a Tool Use Agent\n\nNow, we'll implement a `ToolUseAgent`, an agent responsible for executing tools by relaying the execution task to a `ToolAgent`.\n\n```python\nclass ToolUseAgent(RoutedAgent):\n    \"\"\"An agent that uses tools to perform tasks. It executes the tools\n    by itself by sending the tool execution task to a ToolAgent.\"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        system_messages: List[SystemMessage],\n        model_client: ChatCompletionClient,\n        tool_schema: List[ToolSchema],\n        tool_agent_type: AgentType,\n    ) -> None:\n        super().__init__(description)\n        self._model_client = model_client\n        self._system_messages = system_messages\n        self._tool_schema = tool_schema\n        self._tool_agent_id = AgentId(type=tool_agent_type, key=self.id.key)\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        \"\"\"Handle a user message, execute the model and tools, and return the response.\"\"\"\n        session: List[LLMMessage] = [UserMessage(content=message.content, source=\"User\")]\n        output_messages = await tool_agent_caller_loop(\n            self,\n            tool_agent_id=self._tool_agent_id,\n            model_client=self._model_client,\n            input_messages=session,\n            tool_schema=self._tool_schema,\n            cancellation_token=ctx.cancellation_token,\n        )\n        final_response = output_messages[-1].content\n        assert isinstance(final_response, str)\n        return Message(content=final_response)\n```\n\nThe `ToolUseAgent` class extends `RoutedAgent` and is designed to handle user messages. It uses an internal method that calls a `ToolAgent` to process tools based on the user input, ultimately returning a response message.\n\n## Creating an Intervention Handler\n\nTo control tool execution, we need an intervention handler. This handler will intercept the execution requests sent from the `ToolUseAgent` and allow for user permission before execution.\n\n```python\nclass ToolInterventionHandler(DefaultInterventionHandler):\n    async def on_send(\n        self, message: Any, *, message_context: MessageContext, recipient: AgentId\n    ) -> Any | type[DropMessage]:\n        if isinstance(message, FunctionCall):\n            # Request user prompt for tool execution.\n            user_input = input(\n                f\"Function call: {message.name}\\nArguments: {message.arguments}\\nDo you want to execute the tool? (y/n): \"\n            )\n            if user_input.strip().lower() != \"y\":\n                raise ToolException(content=\"User denied tool execution.\", call_id=message.id, name=message.name)\n        return message\n```\n\nIn this code block, the `ToolInterventionHandler` inherits from `DefaultInterventionHandler`. The `on_send` method is overridden to prompt the user for permission to execute a function call, effectively putting a hold on the tool execution until the user responds.\n\n## Configuring the Runtime with the Intervention Handler\n\nNext, we initialize the runtime environment, registering the intervention handler we created previously. This runtime serves as the foundation for managing agent interactions and processing messages.\n\n```python\n# Create the runtime with the intervention handler.\nruntime = SingleThreadedAgentRuntime(intervention_handlers=[ToolInterventionHandler()])\n```\n\nThe `SingleThreadedAgentRuntime` is set up with the `ToolInterventionHandler`, enabling it to handle messages and user interactions as defined in the intervention logic.\n\n## Setting Up the Code Execution Tool\n\nTo demonstrate the functionality, we will implement a Python code execution tool that utilizes a Docker-based command-line executor.\n\n```python\n# Create the docker executor for the Python code execution tool.\ndocker_executor = DockerCommandLineCodeExecutor()\n\n# Create the Python code execution tool.\npython_tool = PythonCodeExecutionTool(executor=docker_executor)\n```\n\nIn this segment, we instantiate a `DockerCommandLineCodeExecutor`, which will run our Python code execution tool within a Docker container environment, ensuring robust execution isolating potential system conflicts.\n\n## Registering the Agents and Tools\n\nNow we will register our agents with their respective tools and schemas.\n\n```python\n# Register agents.\ntool_agent_type = await ToolAgent.register(\n    runtime,\n    \"tool_executor_agent\",\n    lambda: ToolAgent(\n        description=\"Tool Executor Agent\",\n        tools=[python_tool],\n    ),\n)\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_enabled_agent\",\n    lambda: ToolUseAgent(\n        description=\"Tool Use Agent\",\n        system_messages=[SystemMessage(content=\"You are a helpful AI Assistant. Use your tools to solve problems.\")],\n        model_client=model_client,\n        tool_schema=[python_tool.schema],\n        tool_agent_type=tool_agent_type,\n    ),\n)\n```\n\nThis code registers two agents: a `ToolExecutorAgent` that can handle tool executions and a `ToolUseAgent` that allows user interaction and utilizes the tooling capabilities. \n\n## Running the Agents and Handling User Input\n\nFinally, we will start the runtime, send a command to the `ToolUseAgent`, and observe the intervention handler in action as it prompts for user permission.\n\n```python\n# Start the runtime and the docker executor.\nawait docker_executor.start()\nruntime.start()\n\n# Send a task to the tool user.\nresponse = await runtime.send_message(\n    Message(\"Run the following Python code: print('Hello, World!')\"), AgentId(\"tool_enabled_agent\", \"default\")\n)\nprint(response.content)\n\n# Stop the runtime and the docker executor.\nawait runtime.stop()\nawait docker_executor.stop()\n\n# Close the connection to the model client.\nawait model_client.close()\n```\n\nIn this final block, we initiate the `docker_executor` and the runtime environment, then send a message which triggers the intervention handler to prompt for execution permission. After processing, the runtime is neatly shut down, ensuring that all resources are properly released.\n\nThis concludes the guide on implementing a user approval mechanism for tool execution through an intervention handler. With this setup, you can safely execute tools while ensuring user involvement in critical processes.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/tool-use-with-intervention.ipynb"
  },
  {
    "code": false,
    "content": "## Topic and Subscription Example Scenarios\n\n### Introduction\n\nIn this documentation, we delve into how broadcasting enables agent communication in AutoGen through four distinct scenarios. These scenarios exemplify various methods of managing and distributing messages among agents. Our consistent example revolves around a tax management company processing client requests, showcasing how each broadcasting approach impacts communication flow.\n\n### Scenario Overview\n\nImagine a tax management firm that provides diverse services to clients, including tax planning, dispute resolution, compliance, and preparation. The firm employs specialized tax agents, each adept in a specific domain, and a tax system manager who orchestrates operations.\n\nClients submit requests that need processing by the relevant specialists. Communication between clients, the tax system manager, and tax specialists is facilitated through broadcasting. This documentation analyzes how different broadcasting strategies influence messaging dynamics and provide optimal solutions for agent communication.\n\n---\n\n### Broadcasting Scenarios Overview\n\nWe will cover the following broadcasting scenarios:\n\n1. **Single-Tenant, Single Scope of Publishing**\n2. **Multi-Tenant, Single Scope of Publishing**\n3. **Single-Tenant, Multiple Scopes of Publishing**\n4. **Multi-Tenant, Multiple Scopes of Publishing**\n\nEach scenario provides alternative approaches to message distribution and agent interaction, equipping you to design effective communication strategies tailored to your application's requirements.\n\n### Code Setup\n\nTo facilitate our examples, we first import necessary modules and define essential classes and enums.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import List\n\nfrom autogen_core import (\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    message_handler,\n)\nfrom autogen_core._default_subscription import DefaultSubscription\nfrom autogen_core._default_topic import DefaultTopicId\nfrom autogen_core.models import SystemMessage\n```\n\nIn the code snippet above, we import the AutoGen core components necessary for creating agents, handling messages, and managing communication in our scenarios. \n\n```python\nclass TaxSpecialty(str, Enum):\n    PLANNING = \"planning\"\n    DISPUTE_RESOLUTION = \"dispute_resolution\"\n    COMPLIANCE = \"compliance\"\n    PREPARATION = \"preparation\"\n\n@dataclass\nclass ClientRequest:\n    content: str\n\n@dataclass\nclass RequestAssessment:\n    content: str\n\nclass TaxSpecialist(RoutedAgent):\n    def __init__(\n        self,\n        description: str,\n        specialty: TaxSpecialty,\n        system_messages: List[SystemMessage],\n    ) -> None:\n        super().__init__(description)\n        self.specialty = specialty\n        self._system_messages = system_messages\n        self._memory: List[ClientRequest] = []\n\n    @message_handler\n    async def handle_message(self, message: ClientRequest, ctx: MessageContext) -> None:\n        # Process the client request.\n        print(f\"\\n{'='*50}\\nTax specialist {self.id} with specialty {self.specialty}:\\n{message.content}\")\n        # Send a response back to the manager\n        if ctx.topic_id is None:\n            raise ValueError(\"Topic ID is required for broadcasting\")\n        await self.publish_message(\n            message=RequestAssessment(content=f\"I can handle this request in {self.specialty}.\"),\n            topic_id=ctx.topic_id,\n        )\n```\n\nIn this code, we define `TaxSpecialty`, `ClientRequest`, and `RequestAssessment` to manage various specialties and client messages. The `TaxSpecialist` class, derived from `RoutedAgent`, provides a mechanism to handle incoming messages and provides feedback related to clients' requests.\n\n### 1. Single-Tenant, Single Scope of Publishing\n\n#### Scenario Explanation\n\nIn this scenario, all agents operate within a single tenant, and messages are published to one shared topic that all agents subscribe to. This means every agent receives all messages published to that topic.\n\n#### Application in the Tax Specialist Company\n\nHere, all tax specialists receive every client request and communication from the tax system manager, fostering close collaboration and transparency. This approach suits scenarios where comprehensive awareness among agents is essential.\n\n#### How the Scenario Works\n\n- **Subscriptions**: All agents utilize the default subscription.\n- **Publishing**: Messages are sent to the default topic.\n- **Message Handling**: Agents process messages based on their content and handlers.\n\n#### Benefits\n\n- **Simplicity**: Easy to set up and understand.\n- **Collaboration**: Encourages transparency among agents.\n- **Flexibility**: Agents can dynamically choose which messages to process.\n\n#### Considerations\n\n- **Scalability**: May struggle with high message volumes and numerous agents.\n- **Efficiency**: Agents could receive irrelevant messages, leading to unnecessary processing.\n\n```python\nasync def run_single_tenant_single_scope() -> None:\n    # Create the runtime.\n    runtime = SingleThreadedAgentRuntime()\n\n    # Register TaxSpecialist agents for each specialty\n    specialist_agent_type_1 = \"TaxSpecialist_1\"\n    specialist_agent_type_2 = \"TaxSpecialist_2\"\n    await TaxSpecialist.register(\n        runtime=runtime,\n        type=specialist_agent_type_1,\n        factory=lambda: TaxSpecialist(\n            description=\"A tax specialist 1\",\n            specialty=TaxSpecialty.PLANNING,\n            system_messages=[SystemMessage(content=\"You are a tax specialist.\")],\n        ),\n    )\n\n    await TaxSpecialist.register(\n        runtime=runtime,\n        type=specialist_agent_type_2,\n        factory=lambda: TaxSpecialist(\n            description=\"A tax specialist 2\",\n            specialty=TaxSpecialty.DISPUTE_RESOLUTION,\n            system_messages=[SystemMessage(content=\"You are a tax specialist.\")],\n        ),\n    )\n\n    # Add default subscriptions for each agent type\n    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_1))\n    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_2))\n\n    # Start the runtime and send a message to agents on default topic\n    runtime.start()\n    await runtime.publish_message(ClientRequest(\"I need to have my tax for 2024 prepared.\"), topic_id=DefaultTopicId())\n    await runtime.stop_when_idle()\n\nawait run_single_tenant_single_scope()\n```\n\nThis code initializes a single-threaded runtime, registers two tax specialist agents, and sends a client request to the default topic, showcasing a simple but effective broadcasting strategy.\n\n### 2. Multi-Tenant, Single Scope of Publishing\n\n#### Scenario Explanation\n\nIn this situation, the system accommodates multiple tenants (clients), each with its own isolated topic. Agents within a tenant listen to messages specific to that tenant, ensuring that communications do not converge between different clients.\n\n#### Application in the Tax Specialist Company\n\nIn our example, the tax management company serves multiple clients concurrently, creating a separate set of agent instances for each client. This isolation ensures that all agents within a client only receive messages relevant to them.\n\n#### How the Scenario Works\n\n- **Subscriptions**: Agents subscribe to topics unique to their tenant.\n- **Publishing**: Messages are directed to tenant-specific topics.\n- **Message Handling**: Each agent processes messages according to their assigned topics.\n\n#### Benefits\n\n- **Tenant Isolation**: Protects data privacy across different clients.\n- **Collaboration within Tenant**: Encourages teamwork relevant to each client.\n\n#### Considerations\n\n- **Complexity**: Increased management requirements for multiple agents.\n- **Resource Usage**: Elevated resource consumption due to separate agent instances.\n\n```python\nasync def run_multi_tenant_single_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n\n    # List of clients (tenants)\n    tenants = [\"ClientABC\", \"ClientXYZ\"]\n\n    # Initialize sessions and map the topic type to each TaxSpecialist agent type\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        specialist_subscription = DefaultSubscription(agent_type=specialist_agent_type)\n        await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Publish client requests to their respective topics\n    for tenant in tenants:\n        topic_source = tenant  # The topic source is the client name\n        topic_id = DefaultTopicId(source=topic_source)\n        await runtime.publish_message(\n            ClientRequest(f\"{tenant} requires tax services.\"),\n            topic_id=topic_id,\n        )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\nawait run_multi_tenant_single_scope()\n```\n\nIn this example, we instantiate a single-threaded runtime, register tax specialists for each client, and publish requests to the respective tenant's topics, illustrating effective isolation and targeted communication.\n\n### 3. Single-Tenant, Multiple Scopes of Publishing\n\n#### Scenario Explanation\n\nThis scenario features a single tenant utilizing multiple topics, allowing different agents to subscribe to specific topics that correlate with their roles. Thus, targeted communication can be maintained within the tenant.\n\n#### Application in the Tax Management Company\n\nIn our context, the tax system manager could send messages tailored to specific specialists based on their respective fields. Agents only subscribe to the topics matching their expertise, ensuring efficient message handling.\n\n#### How the Scenario Works\n\n- **Subscriptions**: Agents subscribe based on their roles and areas of specialty.\n- **Publishing**: Messages are dispatched to specific topics catering to those roles.\n- **Message Handling**: Relevant agents receive targeted messages.\n\n#### Benefits\n\n- **Targeted Communication**: Reduces noise by ensuring messages reach only relevant agents.\n- **Efficiency**: Minimizes unnecessary message processing for agents.\n\n#### Considerations\n\n- **Setup Complexity**: Requires careful management of subscriptions and topics.\n- **Flexibility**: Changes in communication patterns may necessitate modifications in subscriptions.\n\n```python\nasync def run_single_tenant_multiple_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n    \n    # Register TaxSpecialist agents for each specialty and add subscriptions\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        specialist_subscription = TypeSubscription(topic_type=specialty.value, agent_type=specialist_agent_type)\n        await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Publish a ClientRequest to each specialist's topic\n    for specialty in TaxSpecialty:\n        topic_id = TopicId(type=specialty.value, source=\"default\")\n        await runtime.publish_message(\n            ClientRequest(f\"I need assistance with {specialty.value} taxes.\"),\n            topic_id=topic_id,\n        )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\nawait run_single_tenant_multiple_scope()\n```\n\nThis code creates a single-threaded runtime, registers specialists, and selectively publishes messages based on the topics, demonstrating effective communication strategies within a single tenant.\n\n### 4. Multi-Tenant, Multiple Scopes of Publishing\n\n#### Scenario Explanation\n\nThis final scenario merges features of multiple tenants and multiple topics, resulting in an intricate system where agents communicate across various tenants while still targeting their specialty. This flexible model provides enhanced control over message routing.\n\n#### Application in the Tax Management Company\n\nIn this case, the tax management firm can effectively manage numerous clients, ensuring that communication remains relevant and isolated. Each tenant agent can subscribe to multiple topic types, allowing for specialization.\n\n#### How the Scenario Works\n\n- **Subscriptions**: Agents subscribe based on their tenant identity and specialty.\n- **Publishing**: Messages are directed to their respective tenant and specialty-specific topics.\n- **Message Handling**: Only agents intended for the tenant and that particular topic will receive messages.\n\n#### Benefits\n\n- **Complete Isolation**: Guarantees message handling maintains tenant-specific boundaries while allowing specialized interactions.\n- **Granular Control**: Provides precise routing of messages tailored to intended agents.\n\n#### Considerations\n\n- **Complexity**: Requires meticulous management of communications across numerous tenants and topics.\n- **Resource Usage**: Could necessitate additional resources due to the larger number of agent instances and topics.\n\n```python\nasync def run_multi_tenant_multiple_scope() -> None:\n    # Create the runtime\n    runtime = SingleThreadedAgentRuntime()\n\n    # Define TypeSubscriptions for each specialty and tenant\n    tenants = [\"ClientABC\", \"ClientXYZ\"]\n\n    # Initialize agents for all specialties and add type subscriptions\n    for specialty in TaxSpecialty:\n        specialist_agent_type = f\"TaxSpecialist_{specialty.value}\"\n        await TaxSpecialist.register(\n            runtime=runtime,\n            type=specialist_agent_type,\n            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore\n                description=f\"A tax specialist in {specialty.value}.\",\n                specialty=specialty,\n                system_messages=[SystemMessage(content=f\"You are a tax specialist in {specialty.value}.\")],\n            ),\n        )\n        for tenant in tenants:\n            specialist_subscription = TypeSubscription(\n                topic_type=f\"{tenant}_{specialty.value}\", agent_type=specialist_agent_type\n            )\n            await runtime.add_subscription(specialist_subscription)\n\n    # Start the runtime\n    runtime.start()\n\n    # Send messages for each tenant to each specialty\n    for tenant in tenants:\n        for specialty in TaxSpecialty:\n            topic_id = TopicId(type=f\"{tenant}_{specialty.value}\", source=tenant)\n            await runtime.publish_message(\n                ClientRequest(f\"{tenant} needs assistance with {specialty.value} taxes.\"),\n                topic_id=topic_id,\n            )\n\n    # Allow time for message processing\n    await asyncio.sleep(1)\n\n    # Stop the runtime when idle\n    await runtime.stop_when_idle()\n\nawait run_multi_tenant_multiple_scope()\n```\n\nThis example highlights a sophisticated method for managing agents across various tenants while allowing them to communicate within specialized topics, effectively balancing complexity with functionality.\n``` \n\nThis structured documentation provides clear, comprehensive explanations of each broadcasting scenario and insights into their applications, ensuring it remains accessible for readers new to the topic.",
    "filename": "python/docs/src/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.ipynb"
  },
  {
    "content": "# Agent and Multi-Agent Applications\n\nAn **agent** is a software entity that communicates via messages, maintains its own state, and performs actions in response to received messages or changes in its state. These actions may modify the agent\u2019s state and produce external effects, such as updating message logs, sending new messages, executing code, or making API calls.\n\nMany software systems can be modeled as a collection of independent agents that interact with one another. Examples include:\n\n- Sensors on a factory floor\n- Distributed services powering web applications\n- Business workflows involving multiple stakeholders\n- AI agents, such as those powered by language models (e.g., GPT-4), which can write code, interface with external systems, and communicate with other agents.\n\nThese systems, composed of multiple interacting agents, are referred to as **multi-agent applications**.\n\n> **Note:**  \n> AI agents typically use language models as part of their software stack to interpret messages, perform reasoning, and execute actions.\n\n## Characteristics of Multi-Agent Applications\n\nIn multi-agent applications, agents may:\n\n- Run within the same process or on the same machine\n- Operate across different machines or organizational boundaries\n- Be implemented in diverse programming languages and make use of different AI models or instructions\n- Work together towards a shared goal, coordinating their actions through messaging\n\nEach agent is a self-contained unit that can be developed, tested, and deployed independently. This modular design allows agents to be reused across different scenarios and composed into more complex systems.\n\nAgents are inherently **composable**: simple agents can be combined to form complex, adaptable applications, where each agent contributes a specific function or service to the overall system.",
    "filename": "python/docs/src/user-guide/core-user-guide/core-concepts/agent-and-multi-agent-application.md"
  },
  {
    "content": "(agentid_and_lifecycle)=\n# Agent Identity and Lifecycle\n\nThe agent runtime manages agents' identities\nand lifecycles.\nApplication does not create agents directly, rather,\nit registers an agent type with a factory function for\nagent instances.\nIn this section, we explain how agents are identified\nand created by the runtime.\n\n## Agent ID\n\nAgent ID uniquely identifies an agent instance within\nan agent runtime -- including distributed runtime.\nIt is the \"address\" of the agent instance for receiving messages.\nIt has two components: agent type and agent key.\n\n```{note}\nAgent ID = (Agent Type, Agent Key)\n```\n\nThe agent type is not an agent class.\nIt associates an agent with a specific\nfactory function, which produces instances of agents\nof the same agent type.\nFor example, different factory functions can produce the same\nagent class but with different constructor parameters.\nThe agent key is an instance identifier\nfor the given agent type.\nAgent IDs can be converted to and from strings. the format of this string is:\n```{note}\nAgent_Type/Agent_Key\n```\nTypes and Keys are considered valid if they only contain alphanumeric letters (a-z) and (0-9), or underscores (_). A valid identifier cannot start with a number, or contain any spaces.\n\nIn a multi-agent application, agent types are\ntypically defined directly by the application, i.e., they\nare defined in the application code.\nOn the other hand, agent keys are typically generated given\nmessages delivered to the agents, i.e., they are defined\nby the application data.\n\nFor example, a runtime has registered the agent type `\"code_reviewer\"`\nwith a factory function producing agent instances that perform\ncode reviews. Each code review request has a unique ID `review_request_id`\nto mark a dedicated\nsession.\nIn this case, each request can be handled by a new instance\nwith an agent ID, `(\"code_reviewer\", review_request_id)`.\n\n## Agent Lifecycle\n\nWhen a runtime delivers a message to an agent instance given its ID,\nit either fetches the instance,\nor creates it if it does not exist.\n\n![Agent Lifecycle](agent-lifecycle.svg)\n\nThe runtime is also responsible for \"paging in\" or \"out\" agent instances\nto conserve resources and balance load across multiple machines.\nThis is not implemented yet.",
    "filename": "python/docs/src/user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle.md"
  },
  {
    "content": "# Application Stack\n\nAutoGen core is designed to be an unopinionated framework that can be used to build\na wide variety of multi-agent applications. It is not tied to any specific\nagent abstraction or multi-agent pattern.\n\nThe following diagram shows the application stack.\n\n![Application Stack](application-stack.svg)\n\nAt the bottom of the stack is the base messaging and routing facilities that\nenable agents to communicate with each other. These are managed by the\nagent runtime, and for most applications, developers only need to interact\nwith the high-level APIs provided by the runtime (see [Agent and Agent Runtime](../framework/agent-and-agent-runtime.ipynb)).\n\nAt the top of the stack, developers need to define the\ntypes of the messages that agents exchange. This set of message types\nforms a behavior contract that agents must adhere to, and the\nimplementation of the contracts determines how agents handle messages.\nThe behavior contract is also sometimes referred to as the message protocol.\nIt is the developer's responsibility to implement the behavior contract.\nMulti-agent patterns emerge from these behavior contracts\n(see [Multi-Agent Design Patterns](../design-patterns/intro.md)).\n\n## An Example Application\n\nConsider a concrete example of a multi-agent application for\ncode generation. The application consists of three agents:\nCoder Agent, Executor Agent, and Reviewer Agent.\nThe following diagram shows the data flow between the agents,\nand the message types exchanged between them.\n\n![Code Generation Example](code-gen-example.svg)\n\nIn this example, the behavior contract consists of the following:\n\n- `CodingTaskMsg` message from application to the Coder Agent\n- `CodeGenMsg` from Coder Agent to Executor Agent\n- `ExecutionResultMsg` from Executor Agent to Reviewer Agent\n- `ReviewMsg` from Reviewer Agent to Coder Agent\n- `CodingResultMsg` from the Reviewer Agent to the application\n\nThe behavior contract is implemented by the agents' handling of these messages. For example, the Reviewer Agent listens for `ExecutionResultMsg`\nand evaluates the code execution result to decide whether to approve or reject,\nif approved, it sends a `CodingResultMsg` to the application,\notherwise, it sends a `ReviewMsg` to the Coder Agent for another round of\ncode generation.\n\nThis behavior contract is a case of a multi-agent pattern called _reflection_,\nwhere a generation result is reviewed by another round of generation,\nto improve the overall quality.",
    "filename": "python/docs/src/user-guide/core-user-guide/core-concepts/application-stack.md"
  },
  {
    "content": "# Agent Runtime Environments\n\nAt the foundation level, the framework provides a _runtime environment_, which facilitates\ncommunication between agents, manages their identities and lifecycles,\nand enforce security and privacy boundaries.\n\nIt supports two types of runtime environment: _standalone_ and _distributed_.\nBoth types provide a common set of APIs for building multi-agent applications,\nso you can switch between them without changing your agent implementation.\nEach type can also have multiple implementations.\n\n## Standalone Agent Runtime\n\nStandalone runtime is suitable for single-process applications where all agents\nare implemented in the same programming language and running in the same process.\nIn the Python API, an example of standalone runtime is the {py:class}`~autogen_core.SingleThreadedAgentRuntime`.\n\nThe following diagram shows the standalone runtime in the framework.\n\n![Standalone Runtime](architecture-standalone.svg)\n\nHere, agents communicate via messages through the runtime, and the runtime manages\nthe _lifecycle_ of agents.\n\nDevelopers can build agents quickly by using the provided components including\n_routed agent_, AI model _clients_, tools for AI models, code execution sandboxes,\nmodel context stores, and more.\nThey can also implement their own agents from scratch, or use other libraries.\n\n## Distributed Agent Runtime\n\nDistributed runtime is suitable for multi-process applications where agents\nmay be implemented in different programming languages and running on different\nmachines.\n\n![Distributed Runtime](architecture-distributed.svg)\n\nA distributed runtime, as shown in the diagram above,\nconsists of a _host servicer_ and multiple _workers_.\nThe host servicer facilitates communication between agents across workers\nand maintains the states of connections.\nThe workers run agents and communicate with the host servicer via _gateways_.\nThey advertise to the host servicer the agents they run and manage the agents' lifecycles.\n\nAgents work the same way as in the standalone runtime so that developers can\nswitch between the two runtime types with no change to their agent implementation.",
    "filename": "python/docs/src/user-guide/core-user-guide/core-concepts/architecture.md"
  },
  {
    "content": "# Topic and Subscription\n\nThere are two ways for runtime to deliver messages,\ndirect messaging or broadcast. Direct messaging is one to one: the sender\nmust provide the recipient's agent ID. On the other hand,\nbroadcast is one to many and the sender does not provide recipients'\nagent IDs.\n\nMany scenarios are suitable for broadcast.\nFor example, in event-driven workflows, agents do not always know who\nwill handle their messages, and a workflow can be composed of agents\nwith no inter-dependencies.\nThis section focuses on the core concepts in broadcast: topic and subscription.\n\n(topic_and_subscription_topic)=\n## Topic\n\nA topic defines the scope of a broadcast message.\nIn essence, agent runtime implements a publish-subscribe model through\nits broadcast API: when publishing a message, the topic must be specified.\nIt is an indirection over agent IDs.\n\nA topic consists of two components: topic type and topic source.\n\n```{note}\nTopic = (Topic Type, Topic Source)\n```\n\nSimilar to [agent ID](./agent-identity-and-lifecycle.md#agent-id),\nwhich also has two components, topic type is usually defined by\napplication code to mark the type of messages the topic is for.\nFor example, a GitHub agent may use `\"GitHub_Issues\"` as the topic type\nwhen publishing messages about new issues.\n\nTopic source is the unique identifier for a topic within a topic type.\nIt is typically defined by application data.\nFor example, the GitHub agent may use `\"github.com/{repo_name}/issues/{issue_number}\"`\nas the topic source to uniquely identifies the topic.\nTopic source allows the publisher to limit the scope of messages and create\nsilos.\n\nTopic IDs can be converted to and from strings. the format of this string is:\n```{note}\nTopic_Type/Topic_Source\n```\nTypes are considered valid if they are in UTF8 and only contain alphanumeric letters (a-z) and (0-9), or underscores (_). A valid identifier cannot start with a number, or contain any spaces.\nSources are considered valid if they are in UTF8 and only contain characters between (inclusive) ascii 32 (space) and 126 (~).\n\n## Subscription\n\nA subscription maps topic to agent IDs.\n\n![Subscription](subscription.svg)\n\nThe diagram above shows the relationship between topic and subscription.\nAn agent runtime keeps track of the subscriptions and uses them to deliver\nmessages to agents.\n\nIf a topic has no subscription, messages published to this topic will\nnot be delivered to any agent.\nIf a topic has many subscriptions, messages will be delivered\nfollowing all the subscriptions to every recipient agent only once.\nApplications can add or remove subscriptions using agent runtime's API.\n\n## Type-based Subscription\n\nA type-based subscription maps a topic type to an agent type\n(see [agent ID](./agent-identity-and-lifecycle.md#agent-id)).\nIt declares an unbounded mapping from topics to agent IDs without knowing the\nexact topic sources and agent keys.\nThe mechanism is simple: any topic matching the type-based subscription's\ntopic type will be mapped to an agent ID with the subscription's agent type\nand the agent key assigned to the value of the topic source.\nFor Python API, use {py:class}`~autogen_core.components.TypeSubscription`.\n\n```{note}\nType-Based Subscription = Topic Type --> Agent Type\n```\n\nGenerally speaking, type-based subscription is the preferred way to declare\nsubscriptions. It is portable and data-independent:\ndevelopers do not need to write application code that depends on specific agent IDs.\n\n### Scenarios of Type-Based Subscription\n\nType-based subscriptions can be applied to many scenarios when the exact\ntopic or agent IDs are data-dependent.\nThe scenarios can be broken down by two considerations:\n(1) whether it is single-tenant or multi-tenant, and\n(2) whether it is a single topic or multiple topics per tenant.\nA tenant typically refers to a set of agents that handle a specific \nuser session or a specific request. \n\n#### Single-Tenant, Single Topic\n\nIn this scenario, there is only one tenant and one topic for the entire\napplication.\nIt is the simplest scenario and can be used in many cases\nlike a command line tool or a single-user application.\n\nTo apply type-based subscription for this scenario, create one type-based\nsubscription for each agent type, and use the same topic type for all\nthe type-based subscriptions.\nWhen you publish, always use the same topic, i.e., the same topic type and topic source.\n\nFor example, assuming there are three agent types: `\"triage_agent\"`,\n`\"coder_agent\"` and `\"reviewer_agent\"`, and the topic type is `\"default\"`,\ncreate the following type-based subscriptions:\n\n```python\n# Type-based Subscriptions for single-tenant, single topic scenario\nTypeSubscription(topic_type=\"default\", agent_type=\"triage_agent\")\nTypeSubscription(topic_type=\"default\", agent_type=\"coder_agent\")\nTypeSubscription(topic_type=\"default\", agent_type=\"reviewer_agent\")\n```\n\nWith the above type-based subscriptions, use the same topic source \n`\"default\"` for all messages. So the topic is always `(\"default\", \"default\")`.\nA message published to this topic will be delivered to all the agents of\nall above types. Specifically, the message will be sent to the following agent IDs:\n\n```python\n# The agent IDs created based on the topic source\nAgentID(\"triage_agent\", \"default\")\nAgentID(\"coder_agent\", \"default\")\nAgentID(\"reviewer_agent\", \"default\")\n```\n\nThe following figure shows how type-based subscription works in this example.\n\n![Type-Based Subscription Single-Tenant, Single Topic Scenario Example](type-subscription-single-tenant-single-topic.svg)\n\nIf the agent with the ID does not exist, the runtime will create it.\n\n\n#### Single-Tenant, Multiple Topics\n\nIn this scenario, there is only one tenant but you want to control\nwhich agent handles which topic. This is useful when you want to\ncreate silos and have different agents specialized in handling different topics.\n\nTo apply type-based subscription for this scenario, \ncreate one type-based subscription for each agent type but with different\ntopic types. You can map the same topic type to multiple agent types if\nyou want these agent types to share a same topic.\nFor topic source, still use the same value for all messages when you publish.\n\nContinuing the example above with same agent types, create the following\ntype-based subscriptions:\n\n```python\n# Type-based Subscriptions for single-tenant, multiple topics scenario\nTypeSubscription(topic_type=\"triage\", agent_type=\"triage_agent\")\nTypeSubscription(topic_type=\"coding\", agent_type=\"coder_agent\")\nTypeSubscription(topic_type=\"coding\", agent_type=\"reviewer_agent\")\n```\n\nWith the above type-based subscriptions, any message published to the topic\n`(\"triage\", \"default\")` will be delivered to the agent with type\n`\"triage_agent\"`, and any message published to the topic\n`(\"coding\", \"default\")` will be delivered to the agents with types\n`\"coder_agent\"` and `\"reviewer_agent\"`. \n\nThe following figure shows how type-based subscription works in this example.\n\n![Type-Based Subscription Single-Tenant, Multiple Topics Scenario Example](type-subscription-single-tenant-multiple-topics.svg)\n\n\n#### Multi-Tenant Scenarios\n\nIn single-tenant scenarios, the topic source is always the same (e.g., `\"default\"`)\n-- it is hard-coded in the application code.\nWhen moving to multi-tenant scenarios, the topic source becomes data-dependent.\n\n```{note}\nA good indication that you are in a multi-tenant scenario is that you need \nmultiple instances of the same agent type. For example, you may want to have\ndifferent agent instances to handle different user sessions to \nkeep private data isolated, or, you may want to distribute a heavy workload\nacross multiple instances of the same agent type and have them work on it concurrently.\n```\n\nContinuing the example above, if you want to have dedicated instances of agents\nto handle a specific GitHub issue, you need to set the topic source to be a\nunique identifier for the issue. \n\nFor example, let's say there is one type-based subscription for the agent type\n`\"triage_agent\"`:\n\n```python\nTypeSubscription(topic_type=\"github_issues\", agent_type=\"triage_agent\")\n```\n\nWhen a message is published to the topic\n`(\"github_issues\", \"github.com/microsoft/autogen/issues/1\")`,\nthe runtime will deliver the message to the agent with ID\n`(\"triage_agent\", \"github.com/microsoft/autogen/issues/1\")`.\nWhen a message is published to the topic\n`(\"github_issues\", \"github.com/microsoft/autogen/issues/9\")`,\nthe runtime will deliver the message to the agent with ID\n`(\"triage_agent\", \"github.com/microsoft/autogen/issues/9\")`.\n\nThe following figure shows how type-based subscription works in this example.\n\n![Type-Based Subscription Multi-Tenant Scenario Example](type-subscription-multi-tenant.svg)\n\nNote the agent ID is data-dependent, and the runtime will create a new instance\nof the agent\nif it does not exist.\n\nTo support multiple topics per tenant, you can use different topic types,\njust like the single-tenant, multiple topics scenario.",
    "filename": "python/docs/src/user-guide/core-user-guide/core-concepts/topic-and-subscription.md"
  },
  {
    "code": false,
    "content": "# Code Execution Documentation\n\nIn this section, we explore the creation of custom agents to handle code generation and execution using the AutoGen framework. This guide illustrates how to implement lightweight agents that can replace the built-in functionalities of the provided `AssistantAgent` and `CodeExecutorAgent` implementations. A practical example is demonstrated, where we create two agents to generate and plot stock returns for Tesla and Nvidia.\n\n## Agent Definitions\n\nWe first define two agent classes: `Assistant` and `Executor`. The `Assistant` agent is responsible for writing code, while the `Executor` agent executes the generated code. Additionally, we create a `Message` data class to define the structure of messages exchanged between these agents.\n\n```python\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\nfrom autogen_core.code_executor import CodeBlock, CodeExecutor\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\n\n@dataclass\nclass Message:\n    content: str\n```\n\n### Code Explanation:\n- We import necessary packages and modules.\n- A `Message` data class is defined, which will be used for structuring messages between agents.\n- Other components from the `autogen_core` are also imported, which are essential for agent functionalities.\n\n```python\n@default_subscription\nclass Assistant(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"An assistant agent.\")\n        self._model_client = model_client\n        self._chat_history: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"Write Python script in markdown block, and it will be executed.\nAlways save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.\"\"\",\n            )\n        ]\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        self._chat_history.append(UserMessage(content=message.content, source=\"user\"))\n        result = await self._model_client.create(self._chat_history)\n        print(f\"\\n{'-'*80}\\nAssistant:\\n{result.content}\")\n        self._chat_history.append(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n```\n\n### Code Explanation:\n- The `Assistant` class extends `RoutedAgent`. It initializes with a `ChatCompletionClient` and a predefined system message for generating Python code.\n- The `handle_message` method processes incoming messages, appending them to chat history and generating Python code as a response. This code is then published back as a message for further handling.\n\n## Code Block Extraction\n\nNext, we need to extract code blocks from markdown text. The code below defines an extractor function for this purpose.\n\n```python\ndef extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:\n    pattern = re.compile(r\"```(?:\\s*([\\w\\+\\-]+))?\\n([\\s\\S]*?)```\")\n    matches = pattern.findall(markdown_text)\n    code_blocks: List[CodeBlock] = []\n    for match in matches:\n        language = match[0].strip() if match[0] else \"\"\n        code_content = match[1]\n        code_blocks.append(CodeBlock(code=code_content, language=language))\n    return code_blocks\n```\n\n### Code Explanation:\n- The function uses regex to find and extract code blocks from markdown text.\n- It captures the programming language (if provided) and the code content, compiling them into a list of `CodeBlock` objects for further execution.\n\n## Executor Implementation\n\nThe `Executor` agent is responsible for executing the code generated by the `Assistant` agent.\n\n```python\n@default_subscription\nclass Executor(RoutedAgent):\n    def __init__(self, code_executor: CodeExecutor) -> None:\n        super().__init__(\"An executor agent.\")\n        self._code_executor = code_executor\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        code_blocks = extract_markdown_code_blocks(message.content)\n        if code_blocks:\n            result = await self._code_executor.execute_code_blocks(\n                code_blocks, cancellation_token=ctx.cancellation_token\n            )\n            print(f\"\\n{'-'*80}\\nExecutor:\\n{result.output}\")\n            await self.publish_message(Message(content=result.output), DefaultTopicId())\n```\n\n### Code Explanation:\n- The `Executor` class extends `RoutedAgent` and is initialized with a `CodeExecutor`.\n- The `handle_message` method extracts code blocks from the received messages and executes them using the code executor. The output is then published as a new message.\n\n## Agent Runtime\n\nThe Agents' communication and lifecycle management are facilitated by the Agent Runtime. Below is an example of how to set up and execute the defined agents using the `SingleThreadedAgentRuntime`.\n\n```python\nimport tempfile\n\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nwork_dir = tempfile.mkdtemp()\n\n# Create a local embedded runtime.\nruntime = SingleThreadedAgentRuntime()\n\nasync with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:  # type: ignore[syntax]\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key=\"YOUR_API_KEY\"\n    )\n    await Assistant.register(\n        runtime,\n        \"assistant\",\n        lambda: Assistant(model_client=model_client),\n    )\n    await Executor.register(runtime, \"executor\", lambda: Executor(executor))\n\n    # Start the runtime and publish a message to the assistant.\n    runtime.start()\n    await runtime.publish_message(\n        Message(\"Create a plot of NVIDA vs TSLA stock returns YTD from 2024-01-01.\"), DefaultTopicId()\n    )\n\n    # Wait for the runtime to stop when idle.\n    await runtime.stop_when_idle()\n    # Close the connection to the model client.\n    await model_client.close()\n```\n\n### Code Explanation:\n- A temporary working directory is created for Docker execution.\n- The `SingleThreadedAgentRuntime` initializes and registers the `Assistant` and `Executor` agents.\n- The runtime starts processing, and a message is published to generate the desired plot. The system waits until it is idle before closing connections.\n\n## Visualization of Results\n\nOnce the execution is complete, we can visualize the produced output, which is a plot of the stock returns for Tesla and Nvidia.\n\n```python\nfrom IPython.display import Image\n\nImage(filename=f\"{work_dir}/nvidia_vs_tesla_ytd_returns.png\")  # type: ignore\n```\n\n### Code Explanation:\n- The last code block demonstrates how to display the generated image file containing the plot. The output image file path maps back to the temporary working directory created earlier.\n\n## Conclusion\n\nThe AutoGen framework provides robust support for both local and distributed agent runtime environments. With decoupled logic for message handling and execution, custom agents can be easily defined and implemented. \n\nFor further details on message handling, subscriptions, and advanced runtime configurations, please continue with the subsequent sections of this documentation.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/code-execution-groupchat.ipynb"
  },
  {
    "code": false,
    "content": "# Concurrent Agents\n\nIn this section, we explore the use of multiple agents working concurrently. We cover three main patterns:\n\n1. **Single Message & Multiple Processors**  \n   Demonstrates how a single message can be processed by multiple agents subscribed to the same topic simultaneously.\n\n2. **Multiple Messages & Multiple Processors**  \n   Illustrates how specific message types can be routed to dedicated agents based on topics.\n\n3. **Direct Messaging**  \n   Focuses on sending messages between agents and from the runtime to agents.\n\n## Setting Up the Environment\n\nBefore diving into the agent patterns, we need to import essential modules and define our message structures.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom autogen_core import (\n    AgentId,\n    ClosureAgent,\n    ClosureContext,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    default_subscription,\n    message_handler,\n    type_subscription,\n)\n```\n\nIn the code above, we import necessary libraries and classes to work with agents and messaging. The `asyncio` module enables asynchronous programming, allowing our agents to process tasks concurrently. The `dataclass` is used to define our message structures, `Task` and `TaskResponse`, which we will use throughout this documentation.\n\n```python\n@dataclass\nclass Task:\n    task_id: str\n\n\n@dataclass\nclass TaskResponse:\n    task_id: str\n    result: str\n```\n\nIn the above code, we define two data classes, `Task` and `TaskResponse`, to structure our messages. The `Task` class contains a unique identifier for each task, while `TaskResponse` will encapsulate the results of the tasks processed by agents.\n\n## Single Message & Multiple Processors\n\nThe first pattern shows how a single message can be processed by multiple agents simultaneously:\n\n- Each `Processor` agent subscribes to the default topic using the `default_subscription` decorator.\n- When publishing a message to the default topic, all registered agents will process the message independently.\n\n```{note}\nBelow, we are subscribing `Processor` using the `default_subscription` decorator. There's an alternative way to subscribe an agent without using decorators altogether, as shown in [Subscribe and Publish to Topics](../framework/message-and-communication.ipynb#subscribe-and-publish-to-topics). This way, the same agent class can be subscribed to different topics.\n```\n\n```python\n@default_subscription\nclass Processor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"{self._description} starting task {message.task_id}\")\n        await asyncio.sleep(2)  # Simulate work\n        print(f\"{self._description} finished task {message.task_id}\")\n```\n\nIn the above code, we define a `Processor` agent that listens for `Task` messages. The `on_task` method simulates processing the task for 2 seconds and prints status updates. Each instance of `Processor` will independently handle incoming tasks.\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nawait Processor.register(runtime, \"agent_1\", lambda: Processor(\"Agent 1\"))\nawait Processor.register(runtime, \"agent_2\", lambda: Processor(\"Agent 2\"))\n\nruntime.start()\n\nawait runtime.publish_message(Task(task_id=\"task-1\"), topic_id=DefaultTopicId())\n\nawait runtime.stop_when_idle()\n```\n\nThis code initializes a `SingleThreadedAgentRuntime`, registers two instances of the `Processor`, and publishes a task. Upon executing, both agents independently handle the published task, showcasing parallel processing.\n\n## Multiple Messages & Multiple Processors\n\nThe second pattern demonstrates routing different types of messages to specific processors:\n- `UrgentProcessor` subscribes to the \"urgent\" topic.\n- `NormalProcessor` subscribes to the \"normal\" topic.\n\nWe achieve this by making agents subscribe to specific topic types using the `type_subscription` decorator.\n\n```python\nTASK_RESULTS_TOPIC_TYPE = \"task-results\"\ntask_results_topic_id = TopicId(type=TASK_RESULTS_TOPIC_TYPE, source=\"default\")\n\n\n@type_subscription(topic_type=\"urgent\")\nclass UrgentProcessor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"Urgent processor starting task {message.task_id}\")\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"Urgent processor finished task {message.task_id}\")\n\n        task_response = TaskResponse(task_id=message.task_id, result=\"Results by Urgent Processor\")\n        await self.publish_message(task_response, topic_id=task_results_topic_id)\n\n\n@type_subscription(topic_type=\"normal\")\nclass NormalProcessor(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> None:\n        print(f\"Normal processor starting task {message.task_id}\")\n        await asyncio.sleep(3)  # Simulate work\n        print(f\"Normal processor finished task {message.task_id}\")\n\n        task_response = TaskResponse(task_id=message.task_id, result=\"Results by Normal Processor\")\n        await self.publish_message(task_response, topic_id=task_results_topic_id)\n```\n\nHere, `UrgentProcessor` and `NormalProcessor` classes are defined to handle tasks based on their urgency level. Each processor simulates different processing times and returns results via `TaskResponse`. \n\nAfter registering the agents, we can publish messages to the \"urgent\" and \"normal\" topics:\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nawait UrgentProcessor.register(runtime, \"urgent_processor\", lambda: UrgentProcessor(\"Urgent Processor\"))\nawait NormalProcessor.register(runtime, \"normal_processor\", lambda: NormalProcessor(\"Normal Processor\"))\n\nruntime.start()\n\nawait runtime.publish_message(Task(task_id=\"normal-1\"), topic_id=TopicId(type=\"normal\", source=\"default\"))\nawait runtime.publish_message(Task(task_id=\"urgent-1\"), topic_id=TopicId(type=\"urgent\", source=\"default\"))\n\nawait runtime.stop_when_idle()\n```\n\nIn the above code, we register both processors and publish tasks to their respective topics. Each processor operates independently, processing tasks based on the defined urgency.\n\n#### Collecting Results\n\nIn the previous example, we relied on console printing to verify task completion. However, in real applications, we typically want to collect and process the results programmatically.\n\nTo collect these messages, we'll use a `ClosureAgent`. We've defined a dedicated topic `TASK_RESULTS_TOPIC_TYPE` where both `UrgentProcessor` and `NormalProcessor` publish their results. The `ClosureAgent` will then process messages from this topic.\n\n```python\nqueue = asyncio.Queue[TaskResponse]()\n\n\nasync def collect_result(_agent: ClosureContext, message: TaskResponse, ctx: MessageContext) -> None:\n    await queue.put(message)\n\n\nruntime.start()\n\nCLOSURE_AGENT_TYPE = \"collect_result_agent\"\nawait ClosureAgent.register_closure(\n    runtime,\n    CLOSURE_AGENT_TYPE,\n    collect_result,\n    subscriptions=lambda: [TypeSubscription(topic_type=TASK_RESULTS_TOPIC_TYPE, agent_type=CLOSURE_AGENT_TYPE)],\n)\n\nawait runtime.publish_message(Task(task_id=\"normal-1\"), topic_id=TopicId(type=\"normal\", source=\"default\"))\nawait runtime.publish_message(Task(task_id=\"urgent-1\"), topic_id=TopicId(type=\"urgent\", source=\"default\"))\n\nawait runtime.stop_when_idle()\n```\n\nIn the code above, we create a queue to store the results from the processors. The `collect_result` function captures and queues each `TaskResponse`. This allows us to collect and handle results programmatically.\n\n```python\nwhile not queue.empty():\n    print(await queue.get())\n```\n\nFinally, this snippet retrieves and prints all results collected in the queue, demonstrating how to process messages received from different processors effectively.\n\n## Direct Messages\n\nIn contrast to the previous patterns, this pattern focuses on direct messages. Here we demonstrate two ways to send them:\n\n- Direct messaging between agents  \n- Sending messages from the runtime to specific agents  \n\nThings to consider in the example below:\n\n- Messages are addressed using the `AgentId`.  \n- The sender can expect to receive a response from the target agent.  \n- We register the `WorkerAgent` class only once; however, we send tasks to two different workers.\n\n```python\nclass WorkerAgent(RoutedAgent):\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:\n        print(f\"{self.id} starting task {message.task_id}\")\n        await asyncio.sleep(2)  # Simulate work\n        print(f\"{self.id} finished task {message.task_id}\")\n        return TaskResponse(task_id=message.task_id, result=f\"Results by {self.id}\")\n\n\nclass DelegatorAgent(RoutedAgent):\n    def __init__(self, description: str, worker_type: str):\n        super().__init__(description)\n        self.worker_instances = [AgentId(worker_type, f\"{worker_type}-1\"), AgentId(worker_type, f\"{worker_type}-2\")]\n\n    @message_handler\n    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:\n        print(f\"Delegator received task {message.task_id}.\")\n\n        subtask1 = Task(task_id=\"task-part-1\")\n        subtask2 = Task(task_id=\"task-part-2\")\n\n        worker1_result, worker2_result = await asyncio.gather(\n            self.send_message(subtask1, self.worker_instances[0]), self.send_message(subtask2, self.worker_instances[1])\n        )\n\n        combined_result = f\"Part 1: {worker1_result.result}, \" f\"Part 2: {worker2_result.result}\"\n        task_response = TaskResponse(task_id=message.task_id, result=combined_result)\n        return task_response\n```\n\nIn this code, `WorkerAgent` processes incoming tasks and returns results directly. The `DelegatorAgent` coordinates tasks between two worker instances and combines their results. This showcases direct communication between agents, enhancing the flexibility of task management.\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nawait WorkerAgent.register(runtime, \"worker\", lambda: WorkerAgent(\"Worker Agent\"))\nawait DelegatorAgent.register(runtime, \"delegator\", lambda: DelegatorAgent(\"Delegator Agent\", \"worker\"))\n\nruntime.start()\n\ndelegator = AgentId(\"delegator\", \"default\")\nresponse = await runtime.send_message(Task(task_id=\"main-task\"), recipient=delegator)\n\nprint(f\"Final result: {response.result}\")\nawait runtime.stop_when_idle()\n```\n\nThe final code snippet sets up the runtime, registering both the `WorkerAgent` and `DelegatorAgent`. It sends a main task to the delegator and prints the aggregated result after processing, emphasizing the efficiency and responsiveness of direct messaging.\n\n## Additional Resources\n\nIf you're interested in more about concurrent processing, check out the [Mixture of Agents](./mixture-of-agents.ipynb) pattern, which relies heavily on concurrent agents.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/concurrent-agents.ipynb"
  },
  {
    "code": false,
    "content": "# Group Chat Documentation\n\nGroup chat is a design pattern where a set of agents communicates through a known topic, taking turns to publish and respond to messages. Each agent plays a distinct role such as writer, illustrator, or editor, allowing for effective collaboration on complex tasks. This documentation breaks down the implementation of the group chat pattern using AutoGen's Core API, especially focused on crafting a children's storybook.\n\n## Overview of Group Chat\n\nIn a group chat, agents sequentially publish messages, maintaining a clear order of conversation through a Group Chat Manager. Various selection algorithms dictate which agent speaks next, often utilizing a round-robin approach or advanced models such as LLMs (Large Language Models). This structure allows for specialized agents to tackle different facets of a project simultaneously and can even support nested group chats for more complex workflows.\n\nThis example employs AutoGen\u2019s Core API to demonstrate the group chat mechanism using event-driven agents. For foundational understanding, please review the documentation on [Topics and Subscriptions](../core-concepts/topic-and-subscription.md) and [Messages and Communication](../framework/message-and-communication.ipynb). The example provided will guide you through setting up a simple group chat involving an LLM-based selector for message management.\n\n> **Note**: While this serves as a foundational example, it is complex and intended as a starting point for your custom group chat systems. You can explore using the pre-built [AgentChat API](../../agentchat-user-guide/index.md) if you prefer not to interact with the Core API directly.\n\n## Required Libraries\n\nTo format displayed messages attractive, we utilize the `rich` library. Ensure it is installed:\n\n```bash\n# Install the required library\n! pip install rich\n```\n\n## Message Protocol\n\nThe message protocol defines how communication occurs within the group chat:\n\n1. An initial `GroupChatMessage` is published by either a user or an external agent to a shared topic.\n2. The Group Chat Manager then selects the next agent to engage, sending a `RequestToSpeak` message to them.\n3. Upon receiving the `RequestToSpeak`, the selected agent publishes their message back to the same topic.\n4. This cycle continues until a termination condition is met, at which point the Group Chat Manager halts further requests.\n\nThe following diagram outlines the flow of messages in the group chat protocol:\n\n![Group chat message protocol](groupchat.svg)\n\n```python\nclass GroupChatMessage(BaseModel):\n    body: UserMessage\n\n\nclass RequestToSpeak(BaseModel):\n    pass\n```\n\n## Base Group Chat Agent\n\nNext, we establish a base class for the agents involved in the group chat, which uses an LLM for text generation. This class serves as a foundation for our specialized agents like Writer, Editor, and Illustrator.\n\n```python\nclass BaseGroupChatAgent(RoutedAgent):\n    \"\"\"A group chat participant using an LLM.\"\"\"\n\n    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient, system_message: str) -> None:\n        super().__init__(description=description)\n        self._group_chat_topic_type = group_chat_topic_type\n        self._model_client = model_client\n        self._system_message = SystemMessage(content=system_message)\n        self._chat_history: List[LLMMessage] = []\n\n    @message_handler\n    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n        self._chat_history.extend([\n            UserMessage(content=f\"Transferred to {message.body.source}\", source=\"system\"),\n            message.body,\n        ])\n\n    @message_handler\n    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n        Console().print(Markdown(f\"### {self.id.type}: \"))\n        self._chat_history.append(\n            UserMessage(content=f\"Transferred to {self.id.type}, adopt the persona immediately.\", source=\"system\")\n        )\n        completion = await self._model_client.create([self._system_message] + self._chat_history)\n        assert isinstance(completion.content, str)\n        self._chat_history.append(AssistantMessage(content=completion.content, source=self.id.type))\n        Console().print(Markdown(completion.content))\n        await self.publish_message(\n            GroupChatMessage(body=UserMessage(content=completion.content, source=self.id.type)),\n            topic_id=DefaultTopicId(type=self._group_chat_topic_type),\n        )\n```\n\n### Explanation\n\nIn this code block, we define the `BaseGroupChatAgent` class. This class includes initialization for various parameters like description, chat topic type, and model client. The two message handlers handle messages and requests to speak, respectively, managing the chat history and generating responses using the LLM model.\n\n## Writer and Editor Agents\n\nUsing the `BaseGroupChatAgent`, we create specialized agents for writing and editing, each with tailored system messages to guide their behavior.\n\n```python\nclass WriterAgent(BaseGroupChatAgent):\n    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient) -> None:\n        super().__init__(\n            description=description,\n            group_chat_topic_type=group_chat_topic_type,\n            model_client=model_client,\n            system_message=\"You are a Writer. You produce good work.\",\n        )\n\n\nclass EditorAgent(BaseGroupChatAgent):\n    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient) -> None:\n        super().__init__(\n            description=description,\n            group_chat_topic_type=group_chat_topic_type,\n            model_client=model_client,\n            system_message=\"You are an Editor. Plan and guide the task given by the user. Provide critical feedback to the draft and illustration produced by Writer and Illustrator. \"  \n            \"Approve if the task is completed and meets user's requirements.\",\n        )\n```\n\n### Explanation\n\nThe `WriterAgent` focuses on content creation, tasked with generating compelling text. The `EditorAgent`, on the other hand, oversees the quality of the outputs, ensuring that generated content aligns with user requirements. Each agent initializes with a specific behavior defined in their system message.\n\n## Illustrator Agent with Image Generation\n\nNext, we implement the `IllustratorAgent`, which utilizes the DALL-E model for generating illustrations based on the messages received.\n\n```python\nclass IllustratorAgent(BaseGroupChatAgent):\n    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient, image_client: openai.AsyncClient) -> None:\n        super().__init__(\n            description=description,\n            group_chat_topic_type=group_chat_topic_type,\n            model_client=model_client,\n            system_message=\"You are an Illustrator. You use the generate_image tool to create images given user's requirement. \"\n            \"Make sure the images have consistent characters and style.\",\n        )\n        self._image_client = image_client\n        self._image_gen_tool = FunctionTool(\n            self._image_gen, name=\"generate_image\", description=\"Call this to generate an image.\"\n        )\n\n    async def _image_gen(self, character_appearence: str, style_attributes: str, worn_and_carried: str, scenario: str) -> str:\n        prompt = f\"Digital painting of a {character_appearence} character with {style_attributes}. Wearing {worn_and_carried}, {scenario}.\"\n        response = await self._image_client.images.generate(\n            prompt=prompt, model=\"dall-e-3\", response_format=\"b64_json\", size=\"1024x1024\"\n        )\n        return response.data[0].b64_json  # type: ignore\n\n    @message_handler\n    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:  # type: ignore\n        Console().print(Markdown(f\"### {self.id.type}: \"))\n        self._chat_history.append(\n            UserMessage(content=f\"Transferred to {self.id.type}, adopt the persona immediately.\", source=\"system\")\n        )\n        completion = await self._model_client.create(\n            [self._system_message] + self._chat_history,\n            tools=[self._image_gen_tool],\n            extra_create_args={\"tool_choice\": \"required\"},\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(completion.content, list) and all(\n            isinstance(item, FunctionCall) for item in completion.content\n        )\n        images: List[str | Image] = []\n        for tool_call in completion.content:\n            arguments = json.loads(tool_call.arguments)\n            Console().print(arguments)\n            result = await self._image_gen_tool.run_json(arguments, ctx.cancellation_token)\n            image = Image.from_base64(self._image_gen_tool.return_value_as_string(result))\n            image = Image.from_pil(image.image.resize((256, 256)))\n            display(image.image)  # type: ignore\n            images.append(image)\n        await self.publish_message(\n            GroupChatMessage(body=UserMessage(content=images, source=self.id.type)),\n            DefaultTopicId(type=self._group_chat_topic_type),\n        )\n```\n\n### Explanation\n\nIn this section, the `IllustratorAgent` is defined. Besides handling standard chat responses, it integrates an image generation function using DALL-E. This class listens for specific requests to illustrate and accordingly generates images which are then published back to the chat.\n\n## User Agent\n\nNow that our AI agents are set, we will define the `UserAgent`, representing the human participant in the conversation.\n\n```python\nclass UserAgent(RoutedAgent):\n    def __init__(self, description: str, group_chat_topic_type: str) -> None:\n        super().__init__(description=description)\n        self._group_chat_topic_type = group_chat_topic_type\n\n    @message_handler\n    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n        # When integrating with a frontend, this is where group chat message would be sent to the frontend.\n        pass\n\n    @message_handler\n    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n        user_input = input(\"Enter your message, type 'APPROVE' to conclude the task: \")\n        Console().print(Markdown(f\"### User: \\n{user_input}\"))\n        await self.publish_message(\n            GroupChatMessage(body=UserMessage(content=user_input, source=self.id.type)),\n            DefaultTopicId(type=self._group_chat_topic_type),\n        )\n```\n\n### Explanation\n\nThe `UserAgent` class serves to take user inputs, reflecting the human's involvement in the dialogue. It listens for requests and prompts the user to enter their responses, which are then published like any other message in the group chat.\n\n## Group Chat Manager\n\nThe `GroupChatManager` watches over the conversation and directs which agent speaks next based on the messages in the chat. \n\n```python\nclass GroupChatManager(RoutedAgent):\n    def __init__(self, participant_topic_types: List[str], model_client: ChatCompletionClient, participant_descriptions: List[str]) -> None:\n        super().__init__(\"Group chat manager\")\n        self._participant_topic_types = participant_topic_types\n        self._model_client = model_client\n        self._chat_history: List[UserMessage] = []\n        self._participant_descriptions = participant_descriptions\n        self._previous_participant_topic_type: str | None = None\n\n    @message_handler\n    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n        assert isinstance(message.body, UserMessage)\n        self._chat_history.append(message.body)\n        # If the message is an approval message from the user, stop the chat.\n        if message.body.source == \"User\":\n            assert isinstance(message.body.content, str)\n            if message.body.content.lower().strip(string.punctuation).endswith(\"approve\"):\n                return\n        # Format message history.\n        messages: List[str] = []\n        for msg in self._chat_history:\n            if isinstance(msg.content, str):\n                messages.append(f\"{msg.source}: {msg.content}\")\n            elif isinstance(msg.content, list):\n                line: List[str] = []\n                for item in msg.content:\n                    if isinstance(item, str):\n                        line.append(item)\n                    else:\n                        line.append(\"[Image]\")\n                messages.append(f\"{msg.source}: {', '.join(line)}\")\n        history = \"\\n\".join(messages)\n        # Format roles.\n        roles = \"\\n\".join([\n            f\"{topic_type}: {description}\".strip() for topic_type, description in zip(\n                self._participant_topic_types, self._participant_descriptions, strict=True\n            ) if topic_type != self._previous_participant_topic_type\n        ])\n        selector_prompt = \"\"\"You are in a role play game. The following roles are available:\n{roles}.\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\n\n{history}\n\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\n\"\"\"\n        system_message = SystemMessage(\n            content=selector_prompt.format(\n                roles=roles,\n                history=history,\n                participants=str([\n                    topic_type for topic_type in self._participant_topic_types\n                    if topic_type != self._previous_participant_topic_type\n                ]),\n            )\n        )\n        completion = await self._model_client.create([system_message], cancellation_token=ctx.cancellation_token)\n        assert isinstance(completion.content, str)\n        selected_topic_type: str\n        for topic_type in self._participant_topic_types:\n            if topic_type.lower() in completion.content.lower():\n                selected_topic_type = topic_type\n                self._previous_participant_topic_type = selected_topic_type\n                await self.publish_message(RequestToSpeak(), DefaultTopicId(type=selected_topic_type))\n                return\n        raise ValueError(f\"Invalid role selected: {completion.content}\")\n```\n\n### Explanation\n\nThe `GroupChatManager` oversees the chat and selects the next agent based on the conversation context. It checks if the editor approves the work to finalize the chat. The manager ensures roles are rotated appropriately, avoiding repetitive turns by flagging the last speaker.\n\n## Creating the Group Chat\n\nTo initialize our group chat, we set up a `SingleThreadedAgentRuntime`, where agents are registered, and their subscriptions are established.\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\neditor_topic_type = \"Editor\"\nwriter_topic_type = \"Writer\"\nillustrator_topic_type = \"Illustrator\"\nuser_topic_type = \"User\"\ngroup_chat_topic_type = \"group_chat\"\n\neditor_description = \"Editor for planning and reviewing the content.\"\nwriter_description = \"Writer for creating any text content.\"\nuser_description = \"User for providing final approval.\"\nillustrator_description = \"An illustrator for creating images.\"\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-2024-08-06\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\neditor_agent_type = await EditorAgent.register(\n    runtime,\n    editor_topic_type,  # Using topic type as the agent type.\n    lambda: EditorAgent(\n        description=editor_description,\n        group_chat_topic_type=group_chat_topic_type,\n        model_client=model_client,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=editor_topic_type, agent_type=editor_agent_type.type))\nawait runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=editor_agent_type.type))\n\nwriter_agent_type = await WriterAgent.register(\n    runtime,\n    writer_topic_type,  # Using topic type as the agent type.\n    lambda: WriterAgent(\n        description=writer_description,\n        group_chat_topic_type=group_chat_topic_type,\n        model_client=model_client,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=writer_topic_type, agent_type=writer_agent_type.type))\nawait runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=writer_agent_type.type))\n\nillustrator_agent_type = await IllustratorAgent.register(\n    runtime,\n    illustrator_topic_type,\n    lambda: IllustratorAgent(\n        description=illustrator_description,\n        group_chat_topic_type=group_chat_topic_type,\n        model_client=model_client,\n        image_client=openai.AsyncClient(\n            # api_key=\"YOUR_API_KEY\",\n        ),\n    ),\n)\nawait runtime.add_subscription(\n    TypeSubscription(topic_type=illustrator_topic_type, agent_type=illustrator_agent_type.type)\n)\nawait runtime.add_subscription(\n    TypeSubscription(topic_type=group_chat_topic_type, agent_type=illustrator_agent_type.type)\n)\n\nuser_agent_type = await UserAgent.register(\n    runtime,\n    user_topic_type,\n    lambda: UserAgent(description=user_description, group_chat_topic_type=group_chat_topic_type),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=user_topic_type, agent_type=user_agent_type.type))\nawait runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=user_agent_type.type))\n\ngroup_chat_manager_type = await GroupChatManager.register(\n    runtime,\n    \"group_chat_manager\",\n    lambda: GroupChatManager(\n        participant_topic_types=[writer_topic_type, illustrator_topic_type, editor_topic_type, user_topic_type],\n        model_client=model_client,\n        participant_descriptions=[writer_description, illustrator_description, editor_description, user_description],\n    ),\n)\nawait runtime.add_subscription(\n    TypeSubscription(topic_type=group_chat_topic_type, agent_type=group_chat_manager_type.type)\n)\n```\n\n### Explanation\n\nThe code sets up the runtime by initializing and registering each agent. Each agent subscribes to topics that enable them to receive messages relevant to their role, with the Group Chat Manager having oversight to coordinate the chat's turns.\n\n## Running the Group Chat\n\nTo commence the group chat, we start the runtime and publish an initial `GroupChatMessage` to kick-off the task.\n\n```python\nruntime.start()\nsession_id = str(uuid.uuid4())\nawait runtime.publish_message(\n    GroupChatMessage(\n        body=UserMessage(\n            content=\"Please write a short story about the gingerbread man with up to 3 photo-realistic illustrations.\",\n            source=\"User\",\n        )\n    ),\n    TopicId(type=group_chat_topic_type, source=session_id),\n)\nawait runtime.stop_when_idle()\nawait model_client.close()\n```\n\n### Explanation\n\nThe runtime begins executing, and a user message is published to initiate the group chat. Agents will now engage sequentially to collaboratively generate the content. The session continues until all tasks are approved or completed.\n\n## Next Steps\n\nWhile this example illustrates a simplistic group chat implementation, it's important to note that it is **not designed for production use.** Consider refining the agent-selection strategy or enhancing the approval process. For more comprehensive features and a higher-level API, refer to the [AgentChat API](../../agentchat-user-guide/index.md), which shares similar design principles while offering additional capabilities.\n\nThis documentation serves as a foundational guide to understanding and implementing group chat patterns with AutoGen\u2019s Core API, laying the groundwork for more complex and tailored implementations in the future.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/group-chat.ipynb"
  },
  {
    "code": false,
    "content": "# Handoffs\n\nThe Handoff pattern is a multi-agent design approach introduced by OpenAI, exemplified in an experimental project known as [Swarm](https://github.com/openai/swarm). The core principle of this pattern is to enable agents to delegate tasks to one another through a specialized tool call. This documentation explains how to implement this pattern using the AutoGen Core API, specifically employing event-driven agents.\n\nThe AutoGen framework (version 0.4+) offers several advantages over OpenAI's initial implementation and the previous iteration (version 0.2):\n\n1. Scalability: It accommodates distributed environments via a distributed agent runtime.\n2. Flexibility: Users can introduce their own agent implementations.\n3. Asynchronous capabilities: The natively async API provides smoother integration with UI elements and other systems.\n\nThis notebook serves as a demonstration for a straightforward implementation of the handoff pattern. For foundational knowledge on pub-sub and event-driven agents, refer to the guide on [Topics and Subscriptions](../core-concepts/topic-and-subscription.md).\n\n```{note}\nA high-level API for the handoff pattern is being developed within [AgentChat](../../agentchat-user-guide/index.md), aimed at facilitating quicker onboarding.\n```\n\n## Scenario Overview\n\nThis example is inspired by an [OpenAI scenario](https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb) featuring a customer service context. Here, a customer utilizes a chatbot to process a refund or purchase a new item. The chatbot comprises a multi-agent team, including:\n\n- **Triage Agent**: First point of contact, assessing customer inquiries and determining which agents to engage.\n- **Refund Agent**: Handles refund requests specifically.\n- **Sales Agent**: Assists with sales-related inquiries.\n- **Human Agent**: Takes over complex requests beyond the capabilities of AI agents.\n\nCustomer engagement occurs through a User Agent, as depicted in the interaction diagram below.\n\n![Handoffs](handoffs.svg)\n\n### Module Imports\n\nTo begin, we must import the essential modules required for the implementation.\n\n```python\nimport json\nimport uuid\nfrom typing import List, Tuple\n\nfrom autogen_core import (\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    message_handler,\n)\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    FunctionExecutionResult,\n    FunctionExecutionResultMessage,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom pydantic import BaseModel\n```\n\n### Message Protocol Definition\n\nNext, we need to establish the message protocol for agent communication. Utilizing an event-driven pub-sub architecture, we will define the essential message types, including:\n\n- **UserLogin**: Triggered by the runtime when a user initiates a new session.\n- **UserTask**: Contains the user\u2019s chat history, also utilized when an AI agent delegates a task to another agent.\n- **AgentResponse**: Published by the AI agents and the Human Agent, encompassing chat history and a topic type for user replies.\n\n```python\nclass UserLogin(BaseModel):\n    pass\n\nclass UserTask(BaseModel):\n    context: List[LLMMessage]\n\nclass AgentResponse(BaseModel):\n    reply_to_topic_type: str\n    context: List[LLMMessage]\n```\n\n## AI Agent Implementation\n\nWe commence with the `AIAgent` class, which serves as the foundation for all AI agents in the multi-agent chatbot (including Triage, Sales, and Issues & Repairs). Each `AIAgent` employs a `ChatCompletionClient` to produce responses and can utilize standard tools directly or delegate tasks via `delegate_tools`.\n\nEach agent subscribes to `agent_topic_type` to receive messages and communicates with the customer by publishing to `user_topic_type`. \n\nIn the `handle_task` method, the agent generates a response using the model. If the response signifies a handoff via a tool call, it publishes a `UserTask` message to the specified topic for the delegation. Regular tool calls are executed, and the agent continues to generate subsequent responses until it receives a non-tool call response.\n\nUpon finalizing the task, the agent publishes an `AgentResponse` message back to the customer.\n\n```python\nclass AIAgent(RoutedAgent):\n    def __init__(\n        self,\n        description: str,\n        system_message: SystemMessage,\n        model_client: ChatCompletionClient,\n        tools: List[Tool],\n        delegate_tools: List[Tool],\n        agent_topic_type: str,\n        user_topic_type: str,\n    ) -> None:\n        super().__init__(description)\n        self._system_message = system_message\n        self._model_client = model_client\n        self._tools = dict([(tool.name, tool) for tool in tools])\n        self._tool_schema = [tool.schema for tool in tools]\n        self._delegate_tools = dict([(tool.name, tool) for tool in delegate_tools])\n        self._delegate_tool_schema = [tool.schema for tool in delegate_tools]\n        self._agent_topic_type = agent_topic_type\n        self._user_topic_type = user_topic_type\n\n    @message_handler\n    async def handle_task(self, message: UserTask, ctx: MessageContext) -> None:\n        llm_result = await self._model_client.create(\n            messages=[self._system_message] + message.context,\n            tools=self._tool_schema + self._delegate_tool_schema,\n            cancellation_token=ctx.cancellation_token,\n        )\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{llm_result.content}\", flush=True)\n        \n        while isinstance(llm_result.content, list) and all(isinstance(m, FunctionCall) for m in llm_result.content):\n            tool_call_results: List[FunctionExecutionResult] = []\n            delegate_targets: List[Tuple[str, UserTask]] = []\n            for call in llm_result.content:\n                arguments = json.loads(call.arguments)\n                if call.name in self._tools:\n                    result = await self._tools[call.name].run_json(arguments, ctx.cancellation_token)\n                    result_as_str = self._tools[call.name].return_value_as_string(result)\n                    tool_call_results.append(\n                        FunctionExecutionResult(call_id=call.id, content=result_as_str, is_error=False, name=call.name)\n                    )\n                elif call.name in self._delegate_tools:\n                    result = await self._delegate_tools[call.name].run_json(arguments, ctx.cancellation_token)\n                    topic_type = self._delegate_tools[call.name].return_value_as_string(result)\n                    delegate_messages = list(message.context) + [\n                        AssistantMessage(content=[call], source=self.id.type),\n                        FunctionExecutionResultMessage(\n                            content=[\n                                FunctionExecutionResult(\n                                    call_id=call.id,\n                                    content=f\"Transferred to {topic_type}. Adopt persona immediately.\",\n                                    is_error=False,\n                                    name=call.name,\n                                )\n                            ]\n                        ),\n                    ]\n                    delegate_targets.append((topic_type, UserTask(context=delegate_messages)))\n                else:\n                    raise ValueError(f\"Unknown tool: {call.name}\")\n            if len(delegate_targets) > 0:\n                for topic_type, task in delegate_targets:\n                    print(f\"{'-'*80}\\n{self.id.type}:\\nDelegating to {topic_type}\", flush=True)\n                    await self.publish_message(task, topic_id=TopicId(topic_type, source=self.id.key))\n            if len(tool_call_results) > 0:\n                print(f\"{'-'*80}\\n{self.id.type}:\\n{tool_call_results}\", flush=True)\n                message.context.extend(\n                    [\n                        AssistantMessage(content=llm_result.content, source=self.id.type),\n                        FunctionExecutionResultMessage(content=tool_call_results),\n                    ]\n                )\n                llm_result = await self._model_client.create(\n                    messages=[self._system_message] + message.context,\n                    tools=self._tool_schema + self._delegate_tool_schema,\n                    cancellation_token=ctx.cancellation_token,\n                )\n                print(f\"{'-'*80}\\n{self.id.type}:\\n{llm_result.content}\", flush=True)\n            else:\n                return\n        assert isinstance(llm_result.content, str)\n        message.context.append(AssistantMessage(content=llm_result.content, source=self.id.type))\n        await self.publish_message(\n            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),\n            topic_id=TopicId(self._user_topic_type, source=self.id.key),\n        )\n```\n\n## Human Agent Implementation\n\nThe `HumanAgent` class functions as an interface for human operators within the chatbot system, tasked with handling inquiries beyond the AI agents' capabilities. This agent subscribes to `agent_topic_type` to receive messages, while publishing responses back to `user_topic_type`.\n\nFor demonstration purposes, this implementation uses the console for input. In a production scenario, improvements could include sending notifications through applications like Slack or Teams. The process could involve the chat platform publishing responses via the runtime.\n\n```python\nclass HumanAgent(RoutedAgent):\n    def __init__(self, description: str, agent_topic_type: str, user_topic_type: str) -> None:\n        super().__init__(description)\n        self._agent_topic_type = agent_topic_type\n        self._user_topic_type = user_topic_type\n\n    @message_handler\n    async def handle_user_task(self, message: UserTask, ctx: MessageContext) -> None:\n        human_input = input(\"Human agent input: \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{human_input}\", flush=True)\n        message.context.append(AssistantMessage(content=human_input, source=self.id.type))\n        await self.publish_message(\n            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),\n            topic_id=TopicId(self._user_topic_type, source=self.id.key),\n        )\n```\n\n## User Agent Implementation\n\nThe `UserAgent` class acts as the interface for the customer interacting with the chatbot. It processes two message types, `UserLogin` and `AgentResponse`. Upon receiving a `UserLogin` message, it initiates a session and publishes a `UserTask` to the AI agents subscribed to `agent_topic_type`. On receiving an `AgentResponse`, it relays the chatbot's reply to the user.\n\nSimilar to the `HumanAgent`, this implementation relies on console inputs, but can also be enhanced for better user interactions.\n\n```python\nclass UserAgent(RoutedAgent):\n    def __init__(self, description: str, user_topic_type: str, agent_topic_type: str) -> None:\n        super().__init__(description)\n        self._user_topic_type = user_topic_type\n        self._agent_topic_type = agent_topic_type\n\n    @message_handler\n    async def handle_user_login(self, message: UserLogin, ctx: MessageContext) -> None:\n        print(f\"{'-'*80}\\nUser login, session ID: {self.id.key}.\", flush=True)\n        user_input = input(\"User: \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{user_input}\")\n        await self.publish_message(\n            UserTask(context=[UserMessage(content=user_input, source=\"User\")]),\n            topic_id=TopicId(self._agent_topic_type, source=self.id.key),\n        )\n\n    @message_handler\n    async def handle_task_result(self, message: AgentResponse, ctx: MessageContext) -> None:\n        user_input = input(\"User (type 'exit' to close the session): \")\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{user_input}\", flush=True)\n        if user_input.strip().lower() == \"exit\":\n            print(f\"{'-'*80}\\nUser session ended, session ID: {self.id.key}.\")\n            return\n        message.context.append(UserMessage(content=user_input, source=\"User\"))\n        await self.publish_message(\n            UserTask(context=message.context), topic_id=TopicId(message.reply_to_topic_type, source=self.id.key)\n        )\n```\n\n## Tools for AI Agents\n\nAI agents can leverage standard tools to accomplish tasks that do not require delegation to other agents. Here, we define the tools as simple functions wrapped in `FunctionTool`.\n\n```python\ndef execute_order(product: str, price: int) -> str:\n    print(\"\\n\\n=== Order Summary ===\")\n    print(f\"Product: {product}\")\n    print(f\"Price: ${price}\")\n    print(\"=================\\n\")\n    confirm = input(\"Confirm order? y/n: \").strip().lower()\n    if confirm == \"y\":\n        print(\"Order execution successful!\")\n        return \"Success\"\n    else:\n        print(\"Order cancelled!\")\n        return \"User cancelled order.\"\n\ndef look_up_item(search_query: str) -> str:\n    item_id = \"item_132612938\"\n    print(\"Found item:\", item_id)\n    return item_id\n\ndef execute_refund(item_id: str, reason: str = \"not provided\") -> str:\n    print(\"\\n\\n=== Refund Summary ===\")\n    print(f\"Item ID: {item_id}\")\n    print(f\"Reason: {reason}\")\n    print(\"=================\\n\")\n    print(\"Refund execution successful!\")\n    return \"success\"\n\nexecute_order_tool = FunctionTool(execute_order, description=\"Price should be in USD.\")\nlook_up_item_tool = FunctionTool(\n    look_up_item, description=\"Use to find item ID.\\nSearch query can be a description or keywords.\"\n)\nexecute_refund_tool = FunctionTool(execute_refund, description=\"\")\n```\n\n## Topics for Agent Interaction\n\nWe define the various topic types that agents will subscribe to, providing a means for structured communication. Additional details on topic types can be found in the documentation on [Topics and Subscriptions](../core-concepts/topic-and-subscription.md).\n\n```python\nsales_agent_topic_type = \"SalesAgent\"\nissues_and_repairs_agent_topic_type = \"IssuesAndRepairsAgent\"\ntriage_agent_topic_type = \"TriageAgent\"\nhuman_agent_topic_type = \"HumanAgent\"\nuser_topic_type = \"User\"\n```\n\n## Delegate Tools for AI Agents\n\nIn addition to standard tools, AI agents can delegate tasks using delegate tools. Unlike regular tools, delegate tools facilitate a handoff to another agent rather than continue the response generation within the current agent.\n\n```python\ndef transfer_to_sales_agent() -> str:\n    return sales_agent_topic_type\n\ndef transfer_to_issues_and_repairs() -> str:\n    return issues_and_repairs_agent_topic_type\n\ndef transfer_back_to_triage() -> str:\n    return triage_agent_topic_type\n\ndef escalate_to_human() -> str:\n    return human_agent_topic_type\n\ntransfer_to_sales_agent_tool = FunctionTool(\n    transfer_to_sales_agent, description=\"Use for anything sales or buying related.\"\n)\ntransfer_to_issues_and_repairs_tool = FunctionTool(\n    transfer_to_issues_and_repairs, description=\"Use for issues, repairs, or refunds.\"\n)\ntransfer_back_to_triage_tool = FunctionTool(\n    transfer_back_to_triage,\n    description=\"Call this if the user brings up a topic outside of your purview,\\nincluding escalating to human.\",\n)\nescalate_to_human_tool = FunctionTool(escalate_to_human, description=\"Only call this if explicitly asked to.\")\n```\n\n## Team Creation\n\nWe have established the AI agents, the Human Agent, the User Agent, and the corresponding tools and topic types. It is now time to assemble the team of agents.\n\nUsing the `OpenAIChatCompletionClient` class, we will register the agents and define their respective tools and subscriptions. The runtime will manage the agent lifecycle, alleviating the need for manual instantiation. Detailed information on agent runtime can be found in the following documents: [Agent Runtime Environments](../core-concepts/architecture.md) and [Agent Identity and Lifecycle](../core-concepts/agent-identity-and-lifecycle.md).\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"YOUR_API_KEY\",\n)\n\n# Register the triage agent.\ntriage_agent_type = await AIAgent.register(\n    runtime,\n    type=triage_agent_topic_type,\n    factory=lambda: AIAgent(\n        description=\"A triage agent.\",\n        system_message=SystemMessage(\n            content=\"You are a customer service bot for ACME Inc. \"\n            \"Introduce yourself. Always be very brief. \"\n            \"Gather information to direct the customer to the right department. \"\n            \"But make your questions subtle and natural.\"\n        ),\n        model_client=model_client,\n        tools=[],\n        delegate_tools=[\n            transfer_to_issues_and_repairs_tool,\n            transfer_to_sales_agent_tool,\n            escalate_to_human_tool,\n        ],\n        agent_topic_type=triage_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=triage_agent_topic_type, agent_type=triage_agent_type.type))\n\n# Register the sales agent.\nsales_agent_type = await AIAgent.register(\n    runtime,\n    type=sales_agent_topic_type,\n    factory=lambda: AIAgent(\n        description=\"A sales agent.\",\n        system_message=SystemMessage(\n            content=\"You are a sales agent for ACME Inc.\"\n            \"Always answer in a sentence or less.\"\n            \"Follow the following routine with the user:\"\n            \"1. Ask them about any problems in their life related to catching roadrunners.\\n\"\n            \"2. Casually mention one of ACME's crazy made-up products can help.\\n\"\n            \" - Don't mention price.\\n\"\n            \"3. Once the user is bought in, drop a ridiculous price.\\n\"\n            \"4. Only after everything, and if the user says yes, \"\n            \"tell them a crazy caveat and execute their order.\\n\"\n            \"\"\n        ),\n        model_client=model_client,\n        tools=[execute_order_tool],\n        delegate_tools=[transfer_back_to_triage_tool],\n        agent_topic_type=sales_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=sales_agent_topic_type, agent_type=sales_agent_type.type))\n\n# Register the issues and repairs agent.\nissues_and_repairs_agent_type = await AIAgent.register(\n    runtime,\n    type=issues_and_repairs_agent_topic_type,\n    factory=lambda: AIAgent(\n        description=\"An issues and repairs agent.\",\n        system_message=SystemMessage(\n            content=\"You are a customer support agent for ACME Inc.\"\n            \"Always answer in a sentence or less.\"\n            \"Follow the following routine with the user:\"\n            \"1. First, ask probing questions and understand the user's problem deeper.\\n\"\n            \" - unless the user has already provided a reason.\\n\"\n            \"2. Propose a fix (make one up).\\n\"\n            \"3. ONLY if not satisfied, offer a refund.\\n\"\n            \"4. If accepted, search for the ID and then execute refund.\"\n        ),\n        model_client=model_client,\n        tools=[\n            execute_refund_tool,\n            look_up_item_tool,\n        ],\n        delegate_tools=[transfer_back_to_triage_tool],\n        agent_topic_type=issues_and_repairs_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\nawait runtime.add_subscription(\n    TypeSubscription(topic_type=issues_and_repairs_agent_topic_type, agent_type=issues_and_repairs_agent_type.type)\n)\n\n# Register the human agent.\nhuman_agent_type = await HumanAgent.register(\n    runtime,\n    type=human_agent_topic_type,\n    factory=lambda: HumanAgent(\n        description=\"A human agent.\",\n        agent_topic_type=human_agent_topic_type,\n        user_topic_type=user_topic_type,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=human_agent_topic_type, agent_type=human_agent_type.type))\n\n# Register the user agent.\nuser_agent_type = await UserAgent.register(\n    runtime,\n    type=user_topic_type,\n    factory=lambda: UserAgent(\n        description=\"A user agent.\",\n        user_topic_type=user_topic_type,\n        agent_topic_type=triage_agent_topic_type,\n    ),\n)\nawait runtime.add_subscription(TypeSubscription(topic_type=user_topic_type, agent_type=user_agent_type.type))\n```\n\n## Running the Team\n\nFinally, to simulate a user session, we will initiate the runtime and publish a `UserLogin` message. This message is sent to a topic ID defined by `user_topic_type`, and the source is set to a unique `session_id`. This `session_id` will serve as the identifier for all topic IDs and agent IDs during this user session. For more details on how topic ID and agent ID are created, consult [Agent Identity and Lifecycle](../core-concepts/agent-identity-and-lifecycle.md) and [Topics and Subscriptions](../core-concepts/topic-and-subscription.md).\n\n```python\n# Start the runtime.\nruntime.start()\n\n# Create a new session for the user.\nsession_id = str(uuid.uuid4())\nawait runtime.publish_message(UserLogin(), topic_id=TopicId(user_topic_type, source=session_id))\n\n# Run until completion.\nawait runtime.stop_when_idle()\nawait model_client.close()\n```\n\n## Next Steps\n\nThis documentation illustrates how to implement the Handoff pattern using AutoGen Core. Further enhancements to this design can include the incorporation of additional agents and tools or improving the user interfaces for the User Agent and Human Agent.\n\nEngagement with the community is encouraged \u2013 feel free to share your projects on our [community forum](https://github.com/microsoft/autogen/discussions).",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/handoffs.ipynb"
  },
  {
    "content": "# Intro\n\nAgents can work together in a variety of ways to solve problems.\nResearch works like [AutoGen](https://aka.ms/autogen-paper),\n[MetaGPT](https://arxiv.org/abs/2308.00352)\nand [ChatDev](https://arxiv.org/abs/2307.07924) have shown\nmulti-agent systems out-performing single agent systems at complex tasks\nlike software development.\n\nA multi-agent design pattern is a structure that emerges from message protocols:\nit describes how agents interact with each other to solve problems.\nFor example, the [tool-equipped agent](../components/tools.ipynb#tool-equipped-agent) in\nthe previous section employs a design pattern called ReAct,\nwhich involves an agent interacting with tools.\n\nYou can implement any multi-agent design pattern using AutoGen agents.\nIn the next two sections, we will discuss two common design patterns:\ngroup chat for task decomposition, and reflection for robustness.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/intro.md"
  },
  {
    "code": false,
    "content": "# Mixture of Agents\n\n[Mixture of Agents](https://arxiv.org/abs/2406.04692) is a multi-agent design pattern that emulates the structure of a feed-forward neural network. This system is composed of two main types of agents: worker agents and a singular orchestrator agent. The worker agents are layered, with each containing a predefined number of agents. The operational flow involves workers in one layer sending concatenated messages to those in the subsequent layer.\n\nThis documentation outlines the implementation of the Mixture of Agents pattern, leveraging a core library inspired by the [original implementation](https://github.com/togethercomputer/moa) of this multi-layer approach. Below is a step-by-step overview of how the pattern operates:\n\n1. The orchestrator agent receives a user-defined task and assigns it to worker agents in the initial layer.\n2. Worker agents within this layer execute the received task and report their results back to the orchestrator.\n3. The orchestrator synthesizes these results and sends an updated task to the worker agents in the next layer.\n4. This cycle continues until the final layer processes the task.\n5. In the last layer, the orchestrator compiles the results from all previous layers and delivers a conclusive result to the user.\n\nIn this implementation, we utilize the direct messaging API `{py:meth}`~autogen_core.BaseAgent.send_message`, which facilitates future enhancements, including worker task cancellation and error handling.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n## Message Protocol\n\nThe communication between agents occurs through defined message structures. These structures are responsible for encapsulating the tasks and the results of those tasks.\n\n```python\n@dataclass\nclass WorkerTask:\n    task: str\n    previous_results: List[str]\n\n\n@dataclass\nclass WorkerTaskResult:\n    result: str\n\n\n@dataclass\nclass UserTask:\n    task: str\n\n\n@dataclass\nclass FinalResult:\n    result: str\n```\n\n### Explanation\n- **WorkerTask**: Represents a task assigned to a worker, including previous results for context.\n- **WorkerTaskResult**: Holds the result generated by a worker agent.\n- **UserTask**: Captures a task submitted by the user.\n- **FinalResult**: Contains the final output produced by the orchestrator after processing.\n\n## Worker Agent\n\nThe worker agent is responsible for handling tasks dispatched by the orchestrator and returning results independently.\n\n```python\nclass WorkerAgent(RoutedAgent):\n    def __init__(\n        self,\n        model_client: ChatCompletionClient,\n    ) -> None:\n        super().__init__(description=\"Worker Agent\")\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:\n        if message.previous_results:\n            # Synthesize previous results into a cohesive response.\n            system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n            system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(message.previous_results)])\n            model_result = await self._model_client.create(\n                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n            )\n        else:\n            # Directly handle the user query when no previous results exist.\n            model_result = await self._model_client.create([UserMessage(content=message.task, source=\"user\")])\n        \n        assert isinstance(model_result.content, str)\n        print(f\"{'-'*80}\\nWorker-{self.id}:\\n{model_result.content}\")\n        return WorkerTaskResult(result=model_result.content)\n```\n\n### Explanation\n- The `WorkerAgent` class inherits from `RoutedAgent`. It initializes a model client that it utilizes to generate task results.\n- The `handle_task` method processes incoming worker tasks, synthesizing previous results if available to produce a refined output.\n- The synthesized response is then returned as a `WorkerTaskResult`.\n\n## Orchestrator Agent\n\nThe orchestrator agent serves as the central point of coordination, distributing tasks to worker agents and aggregating results.\n\n```python\nclass OrchestratorAgent(RoutedAgent):\n    def __init__(\n        self,\n        model_client: ChatCompletionClient,\n        worker_agent_types: List[str],\n        num_layers: int,\n    ) -> None:\n        super().__init__(description=\"Aggregator Agent\")\n        self._model_client = model_client\n        self._worker_agent_types = worker_agent_types\n        self._num_layers = num_layers\n\n    @message_handler\n    async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:\n        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived task: {message.task}\")\n        # Create task for the first layer.\n        worker_task = WorkerTask(task=message.task, previous_results=[])\n        \n        for i in range(self._num_layers - 1):\n            # Assign worker IDs for the current layer.\n            worker_ids = [\n                AgentId(worker_type, f\"{self.id.key}/layer_{i}/worker_{j}\")\n                for j, worker_type in enumerate(self._worker_agent_types)\n            ]\n            # Dispatch worker tasks.\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nDispatch to workers at layer {i}\")\n            results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])\n            print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nReceived results from workers at layer {i}\")\n            \n            # Prepare task for the next layer.\n            worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])\n        \n        # Aggregate final results.\n        print(f\"{'-'*80}\\nOrchestrator-{self.id}:\\nPerforming final aggregation\")\n        system_prompt = \"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\\n\\nResponses from models:\"\n        system_prompt += \"\\n\" + \"\\n\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(worker_task.previous_results)])\n        model_result = await self._model_client.create(\n            [SystemMessage(content=system_prompt), UserMessage(content=message.task, source=\"user\")]\n        )\n        \n        assert isinstance(model_result.content, str)\n        return FinalResult(result=model_result.content)\n```\n\n### Explanation\n- The `OrchestratorAgent` manages the workflow by receiving tasks from users and handing them off to multiple layers of worker agents.\n- It iterates over its defined number of layers, dispatching tasks and collecting results as it progresses.\n- The final results are aggregated using the same synthesis process employed by worker agents, ensuring a cohesive output is delivered back to the user.\n\n## Running Mixture of Agents\n\nTo see the mixture of agents in action, we can execute it with a defined mathematical task.\n\n```python\ntask = (\n    \"I have 432 cookies, and divide them 3:4:2 between Alice, Bob, and Charlie. How many cookies does each person get?\"\n)\n```\n\n### Explanation\n- This task asks how to distribute a specified number of cookies among three individuals according to given ratios.\n\nNext, we set up the runtime context to execute our mixture of agents with three layers of worker agents, each containing three agents.\n\n```python\nruntime = SingleThreadedAgentRuntime()\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nawait WorkerAgent.register(runtime, \"worker\", lambda: WorkerAgent(model_client=model_client))\nawait OrchestratorAgent.register(\n    runtime,\n    \"orchestrator\",\n    lambda: OrchestratorAgent(model_client=model_client, worker_agent_types=[\"worker\"] * 3, num_layers=3),\n)\n\nruntime.start()\nresult = await runtime.send_message(UserTask(task=task), AgentId(\"orchestrator\", \"default\"))\n\nawait runtime.stop_when_idle()\nawait model_client.close()\n\nprint(f\"{'-'*80}\\nFinal result:\\n{result.result}\")\n```\n\n### Explanation\n- The setup includes instantiating a runtime environment and registering the worker and orchestrator agents.\n- The orchestrator calls the worker agents to execute and return results for the provided task. \n- Finally, the result is printed, showcasing the final output after all processing layers.\n\nThis structure allows for flexible adjustments and scaling, enabling robust handling of user tasks through an orchestrated agent-based approach.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/mixture-of-agents.ipynb"
  },
  {
    "code": false,
    "content": "# Multi-Agent Debate Documentation\n\n## Overview\n\nThe **Multi-Agent Debate** is a design pattern that facilitates a multi-turn interaction among agents, allowing them to refine their responses based on peer feedback. This document outlines an implementation based on the GSM8K benchmark, which showcases the capabilities of this architecture to solve math problems.\n\nIn this model, there are two primary types of agents: **Solver Agents** and an **Aggregator Agent**. Solver Agents tackle math problems and exchange their solutions, while the Aggregator coordinates the process by assigning problems to Solver Agents, collecting their answers, and ultimately providing a final solution.\n\n### Operational Flow\n\nThe workflow of the Multi-Agent Debate involves the following steps:\n\n1. The user presents a math problem to the Aggregator Agent.\n2. The Aggregator distributes the problem to the Solver Agents.\n3. Solver Agents individually process the problem and share their responses with neighboring agents.\n4. Each Solver Agent refines its answer considering the feedback from its neighbors.\n5. This responsive interaction is reiterated for a predetermined number of rounds.\n6. In the final round, the Aggregator gathers the final responses, employs a majority voting mechanism, and presents the conclusive answer to the user.\n\nCommunication between agents utilizes a broadcast API, enabling them to publish messages on specific topics. For a comprehensive understanding, refer to the [Topics and Subscriptions documentation](../core-concepts/topic-and-subscription.md).\n\n### Importing Dependencies\n\nHere, we import the necessary modules and libraries for implementing the Multi-Agent Debate system. These include foundational classes and message-handling protocols that will allow us to define our agent behaviors.\n\n```python\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nfrom autogen_core import (\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TypeSubscription,\n    default_subscription,\n    message_handler,\n)\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\nThis code block establishes important imports for agent communication, data handling, and model interactions to solve math problems.\n\n## Message Protocol\n\nIn the Multi-Agent Debate, agents need to communicate effectively through standardized messages. We define the message structure that enables the interaction among solvers at different stages.\n\n```python\n@dataclass\nclass Question:\n    content: str\n\n\n@dataclass\nclass Answer:\n    content: str\n\n\n@dataclass\nclass SolverRequest:\n    content: str\n    question: str\n\n\n@dataclass\nclass IntermediateSolverResponse:\n    content: str\n    question: str\n    answer: str\n    round: int\n\n\n@dataclass\nclass FinalSolverResponse:\n    answer: str\n```\n\n### Explanation of Protocols\n\n1. **Question**: Represents the math problem being addressed.\n2. **Answer**: Holds the final resolved answer to be returned to the user.\n3. **SolverRequest**: Used for requesting solutions from solver agents, linking the agent's content and the related question.\n4. **IntermediateSolverResponse**: Contains the response from a solver at each round, detailing its answer and round number.\n5. **FinalSolverResponse**: Represents the final answer provided by the solver after multiple iterations.\n\nThese protocols will facilitate clear communication between agents as they process questions and responses throughout the debate.\n\n## Solver Agent\n\nThe **Solver Agent** is crucial for processing the math problem and refining its answer through peer interaction. Upon receiving a task, it uses a large language model (LLM) to generate answers and share responses.\n\n```python\n@default_subscription\nclass MathSolver(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int) -> None:\n        super().__init__(\"A debator.\")\n        self._topic_type = topic_type\n        self._model_client = model_client\n        self._num_neighbors = num_neighbors\n        self._history: List[LLMMessage] = []\n        self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}\n        self._system_messages = [\n            SystemMessage(\n                content=(\n                    \"You are a helpful assistant with expertise in mathematics and reasoning. \"\n                    \"Your task is to assist in solving a math reasoning problem by providing \"\n                    \"a clear and detailed solution. Limit your output within 100 words, \"\n                    \"and your final answer should be a single numerical number, \"\n                    \"in the form of {{answer}}, at the end of your response. \"\n                    \"For example, 'The answer is {{42}}.'\"\n                )\n            )\n        ]\n        self._round = 0\n        self._max_round = max_round\n\n    @message_handler\n    async def handle_request(self, message: SolverRequest, ctx: MessageContext) -> None:\n        ...\n        \n    @message_handler\n    async def handle_response(self, message: IntermediateSolverResponse, ctx: MessageContext) -> None:\n        ...\n```\n\n### Explanation of Class Responsibilities\n\n1. **Initialization**: The constructor sets the necessary parameters for the solver, such as the model client used for generating responses, the topic type for communication, the number of neighbors to connect with, and the maximum number of rounds for interaction.\n  \n2. **Handling Requests**: The `handle_request` method is triggered when a Solver Request is received. It appends user messages to the history, generates a response using the LLM, and publishes either an intermediate or final response.\n\n3. **Handling Responses**: The `handle_response` method collects responses from neighboring Solver Agents, checks if all neighbors have replied, and prepares to refine its answer accordingly.\n\nThis architecture enables a robust and iterative solution-finding process.\n\n## Aggregator Agent\n\nThe **Aggregator Agent** manages the overall operation and acts as a mediator between the user and the Solver Agents. It handles user queries, disseminates questions, and collates final answers.\n\n```python\n@default_subscription\nclass MathAggregator(RoutedAgent):\n    def __init__(self, num_solvers: int) -> None:\n        super().__init__(\"Math Aggregator\")\n        self._num_solvers = num_solvers\n        self._buffer: List[FinalSolverResponse] = []\n\n    @message_handler\n    async def handle_question(self, message: Question, ctx: MessageContext) -> None:\n        ...\n\n    @message_handler\n    async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:\n        ...\n```\n\n### Explanation of Class Responsibilities\n\n1. **Initialization**: This sets up the aggregator with the number of solver agents it will coordinate. It maintains a buffer to collect the final responses.\n\n2. **Handling Questions**: `handle_question` is triggered when the Aggregator receives a user query. It prepares the prompt for Solver Agents and broadcasts the `SolverRequest`.\n\n3. **Handling Final Responses**: `handle_final_solver_response` collects the final responses from all solvers, determines the majority response, and returns this as the answer to the user.\n\nThe Aggregator ensures the smooth execution of the debate and manages communication flow between agents.\n\n## Setting Up a Debate\n\nNow, we will establish a multi-agent debate setup featuring 4 Solver Agents and 1 Aggregator Agent, connected in a specific topology to facilitate interaction.\n\n```python\nruntime = SingleThreadedAgentRuntime()\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\nawait MathSolver.register(\n    runtime,\n    \"MathSolverA\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverA\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverB\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverB\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverC\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverC\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathSolver.register(\n    runtime,\n    \"MathSolverD\",\n    lambda: MathSolver(\n        model_client=model_client,\n        topic_type=\"MathSolverD\",\n        num_neighbors=2,\n        max_round=3,\n    ),\n)\nawait MathAggregator.register(runtime, \"MathAggregator\", lambda: MathAggregator(num_solvers=4))\n```\n\n### Explanation of Setup Process\n\n1. **Single Threaded Runtime**: Initializes the runtime environment where agents will operate.\n2. **Model Client**: Creates a connection to an LLM to facilitate the generation of responses.\n3. **Agent Registration**: Registers four Solver Agents and one Aggregator, defining their specific parameters such as topic types and the number of neighbors.\n\nThis configuration effectively establishes the multi-agent ecosystem necessary for the debate to take place.\n\n### Establishing Connections\n\nNext, we define how Solver Agents are interconnected through topic subscriptions, ensuring they can share responses with each other.\n\n```python\n# Subscriptions for topic published to by MathSolverA.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverA\", \"MathSolverD\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverA\", \"MathSolverB\"))\n\n# Subscriptions for topic published to by MathSolverB.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverB\", \"MathSolverA\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverB\", \"MathSolverC\"))\n\n# Subscriptions for topic published to by MathSolverC.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverC\", \"MathSolverB\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverC\", \"MathSolverD\"))\n\n# Subscriptions for topic published to by MathSolverD.\nawait runtime.add_subscription(TypeSubscription(\"MathSolverD\", \"MathSolverC\"))\nawait runtime.add_subscription(TypeSubscription(\"MathSolverD\", \"MathSolverA\"))\n\n# All solvers and the aggregator subscribe to the default topic.\n```\n\n### Explanation of Subscription Process\n\nThese subscriptions allow each Solver Agent to communicate with its designated neighbors according to the defined topology. This configuration supports the collaborative refinement of answers as each agent interacts with others in the network.\n\n## Solving Math Problems\n\nWe can now initiate the debate by sending a math question. The Aggregator will publish a `SolverRequest`, prompting the Solver Agents to begin their processing.\n\n```python\nquestion = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\nruntime.start()\nawait runtime.publish_message(Question(content=question), DefaultTopicId())\n# Wait for the runtime to stop when idle.\nawait runtime.stop_when_idle()\n# Close the connection to the model client.\nawait model_client.close()\n```\n\n### Explanation of Execution\n\n1. **Start the Runtime**: This activates the runtime, enabling agent interactions.\n2. **Publish the Question**: The Aggregator publishes a `Question`, initiating the debate.\n3. **Idle State Management**: The execution will wait until all operations are complete before shutting down.\n4. **Model Client Closure**: Finally, the connection to the language model is closed.\n\nThis sequence demonstrates the practical implementation of the Multi-Agent Debate pattern, showcasing the system\u2019s capability to collaboratively solve mathematical problems.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/multi-agent-debate.ipynb"
  },
  {
    "code": false,
    "content": "# Reflection Design Pattern in AutoGen Agents\n\nThe Reflection design pattern in the context of Large Language Models (LLMs) involves an iterative exchange between two or more agents, where the output of one serves as the input for the next. This process can enhance the quality of generated content through critiques or refinements. For illustration, we consider two agents: the Coder Agent, which generates code snippets, and the Reviewer Agent, which critiques those snippets.\n\nIn this document, we will explore the implementation of this design pattern using AutoGen agents. We will define a message protocol, create the agents, set up a logging mechanism, and run the design pattern in a test scenario.\n\n## Message Protocol\n\nBefore defining the agents, we need to establish the message protocol that will be used for communication between them. This protocol defines how tasks and results are communicated effectively.\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass CodeWritingTask:\n    task: str\n\n@dataclass\nclass CodeWritingResult:\n    task: str\n    code: str\n    review: str\n\n@dataclass\nclass CodeReviewTask:\n    session_id: str\n    code_writing_task: str\n    code_writing_scratchpad: str\n    code: str\n\n@dataclass\nclass CodeReviewResult:\n    review: str\n    session_id: str\n    approved: bool\n```\n\n### Explanation of the Protocol\n- **CodeWritingTask**: This message is sent by the application to initiate a coding task.\n- **CodeReviewTask**: After generating the code, the Coder Agent sends this message to the Reviewer Agent for critique.\n- **CodeReviewResult**: The Reviewer Agent sends this message back to the Coder Agent, containing feedback on the code.\n- **CodeWritingResult**: If the code is approved by the Reviewer Agent, this message is sent back to the application, containing the final code and review.\n\nThis protocol ensures a structured flow of information that allows the two agents to collaborate effectively.\n\n## Agents\n\nNext, we will define the two agents: CoderAgent and ReviewerAgent. These agents utilize a pub/sub architecture to communicate through the defined message protocol.\n\n### CoderAgent Implementation\n\n```python\nimport json\nimport re\nimport uuid\nfrom typing import Dict, List, Union\n\nfrom autogen_core import MessageContext, RoutedAgent, TopicId, default_subscription, message_handler\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\n\n@default_subscription\nclass CoderAgent(RoutedAgent):\n    \"\"\"An agent that performs code writing tasks.\"\"\"\n\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A code writing agent.\")\n        self._system_messages: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"You are a proficient coder. You write code to solve problems.\nWork with the reviewer to improve your code.\nAlways put all finished code in a single Markdown code block.\nFor example:\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\nRespond using the following format:\n\nThoughts: <Your comments>\nCode: <Your code>\n\"\"\",\n            )\n        ]\n        self._model_client = model_client\n        self._session_memory: Dict[str, List[CodeWritingTask | CodeReviewTask | CodeReviewResult]] = {}\n```\n\n### Explanation and Functionality\nThe `CoderAgent` is designed to handle code writing tasks. It:\n- Starts with a system message guiding it on how to write code.\n- Maintains a memory structure to track the state of different sessions based on the session ID.\n- Listens for incoming tasks and generates code accordingly. If the code is not acceptable, it will continue to loop until approved.\n\n### Handling Code Writing Tasks\n\n```python\n    @message_handler\n    async def handle_code_writing_task(self, message: CodeWritingTask, ctx: MessageContext) -> None:\n        session_id = str(uuid.uuid4())\n        self._session_memory.setdefault(session_id, []).append(message)\n\n        response = await self._model_client.create(\n            self._system_messages + [UserMessage(content=message.task, source=self.metadata[\"type\"])],\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(response.content, str)\n        code_block = self._extract_code_block(response.content)\n        if code_block is None:\n            raise ValueError(\"Code block not found.\")\n        code_review_task = CodeReviewTask(\n            session_id=session_id,\n            code_writing_task=message.task,\n            code_writing_scratchpad=response.content,\n            code=code_block,\n        )\n        self._session_memory[session_id].append(code_review_task)\n        await self.publish_message(code_review_task, topic_id=TopicId(\"default\", self.id.key))\n```\n\nThe `handle_code_writing_task` method performs the following:\n1. Generates a unique session ID to track the task.\n2. Uses the LLM to create a code snippet based on the received task.\n3. Extracts the code block and prepares a CodeReviewTask to send to the ReviewerAgent.\n\n### Handling Code Review Results\n\n```python\n    @message_handler\n    async def handle_code_review_result(self, message: CodeReviewResult, ctx: MessageContext) -> None:\n        self._session_memory[message.session_id].append(message)\n        review_request = next(\n            m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewTask)\n        )\n        assert review_request is not None\n        \n        if message.approved:\n            await self.publish_message(\n                CodeWritingResult(\n                    code=review_request.code,\n                    task=review_request.code_writing_task,\n                    review=message.review,\n                ),\n                topic_id=TopicId(\"default\", self.id.key),\n            )\n            print(\"Code Writing Result:\")\n            print(f\"Task:\\n{review_request.code_writing_task}\")\n            print(f\"Code:\\n{review_request.code}\")\n            print(f\"Review:\\n{message.review}\")\n        else:\n            ...\n```\n\nIn this handler:\n- The agent checks if the code has been approved.\n- If approved, it publishes a CodeWritingResult.\n- If not, the CoderAgent will generate a new CodeReviewTask based on the feedback received for re-evaluation.\n\n### ReviewerAgent Implementation\n\n```python\n@default_subscription\nclass ReviewerAgent(RoutedAgent):\n    \"\"\"An agent that performs code review tasks.\"\"\"\n\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A code reviewer agent.\")\n        self._system_messages: List[LLMMessage] = [\n            SystemMessage(\n                content=\"\"\"You are a code reviewer. You focus on correctness, efficiency and safety of the code.\nRespond using the following JSON format:\n{\n    \"correctness\": \"<Your comments>\",\n    \"efficiency\": \"<Your comments>\",\n    \"safety\": \"<Your comments>\",\n    \"approval\": \"<APPROVE or REVISE>\",\n    \"suggested_changes\": \"<Your comments>\"\n}\n\"\"\",\n            )\n        ]\n        self._session_memory: Dict[str, List[CodeReviewTask | CodeReviewResult]] = {}\n        self._model_client = model_client\n```\n\n### Explanation and Functionality of ReviewerAgent\nThe `ReviewerAgent` is responsible for reviewing the code:\n- Uses a different system message focused on evaluating the code for correctness, efficiency, and safety.\n- Stores previous feedback for context when reviewing new code.\n\n### Handling Code Review Tasks\n\n```python\n    @message_handler\n    async def handle_code_review_task(self, message: CodeReviewTask, ctx: MessageContext) -> None:\n        previous_feedback = \"\"\n        if message.session_id in self._session_memory:\n            previous_review = next(\n                (m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewResult)),\n                None,\n            )\n            if previous_review is not None:\n                previous_feedback = previous_review.review\n        \n        self._session_memory.setdefault(message.session_id, []).append(message)\n        prompt = f\"\"\"The problem statement is: {message.code_writing_task}\nThe code is:\n```\n{message.code}\n```\n\nPrevious feedback:\n{previous_feedback}\n\nPlease review the code. If previous feedback was provided, see if it was addressed.\n\"\"\"\n        response = await self._model_client.create(\n            self._system_messages + [UserMessage(content=prompt, source=self.metadata[\"type\"])],\n            cancellation_token=ctx.cancellation_token,\n            json_output=True,\n        )\n        assert isinstance(response.content, str)\n        review = json.loads(response.content)\n        review_text = \"Code review:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in review.items()])\n        approved = review[\"approval\"].lower().strip() == \"approve\"\n        result = CodeReviewResult(\n            review=review_text,\n            session_id=message.session_id,\n            approved=approved,\n        )\n        self._session_memory[message.session_id].append(result)\n        await self.publish_message(result, topic_id=TopicId(\"default\", self.id.key))\n```\n\nThe `handle_code_review_task` method:\n1. Gathers previous feedback for continuity in the review process.\n2. Generates a prompt to review the code and send it to the LLM.\n3. Receives the review results, constructs a CodeReviewResult, and publishes it back to the CoderAgent.\n\n## Logging\n\nIt's beneficial to have logging in place to track the communication between agents. Enable logging as follows:\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)\n```\n\nThis setup captures detailed logs from the `autogen_core` package, helping observe the interactions and data flow during execution.\n\n## Running the Design Pattern\n\nNow that we have our agents and logging set up, we can initiate a reflection process with a coding task. \n\n```python\nfrom autogen_core import DefaultTopicId, SingleThreadedAgentRuntime\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nruntime = SingleThreadedAgentRuntime()\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\nawait ReviewerAgent.register(runtime, \"ReviewerAgent\", lambda: ReviewerAgent(model_client=model_client))\nawait CoderAgent.register(runtime, \"CoderAgent\", lambda: CoderAgent(model_client=model_client))\nruntime.start()\nawait runtime.publish_message(\n    message=CodeWritingTask(task=\"Write a function to find the sum of all even numbers in a list.\"),\n    topic_id=DefaultTopicId(),\n)\n\n# Keep processing messages until idle.\nawait runtime.stop_when_idle()\n# Close the model client.\nawait model_client.close()\n```\n\n### Explanation of Execution\n- This block creates a single-threaded runtime and registers both agent types.\n- It publishes a `CodeWritingTask` to start the reflection process.\n- The runtime will handle message exchanges until all tasks are processed.\n\nLogging will provide insights into interactions between the coder and reviewer agents, allowing you to observe the final output of the code snippet and its critique.\n\nIn summary, this document outlines how to implement a Reflection design pattern utilizing AutoGen agents effectively. These agents enhance collaborative code generation through critique and iterative improvement, making them a powerful tool for developers looking to refine their code.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/reflection.ipynb"
  },
  {
    "code": false,
    "content": "# Sequential Workflow Documentation\n\n## Introduction\n\nA Sequential Workflow is a design pattern employing multiple agents that respond in a defined, deterministic order. This structure allows for organized task processing where each agent performs a specific role by receiving, processing, and forwarding messages. This workflow is especially beneficial for creating predictable processes involving collaborative efforts to achieve a final output.\n\nIn this documentation, we illustrate a sequential workflow that transforms a basic product description into a polished piece of marketing copy. The workflow consists of four distinct agent types, each performing key tasks:\n\n- **Concept Extractor Agent**: Identifies and extracts features, audience, and unique selling points (USPs) from the product description.\n- **Writer Agent**: Crafts engaging marketing copy based on the insights provided by the Concept Extractor.\n- **Format & Proof Agent**: Reviews and refines the draft for grammar, clarity, and tone, resulting in a polished final version.\n- **User Agent**: Presents the completed marketing copy to the user.\n\nThe sequential workflow is visualized in the following diagram:\n\n![Sequential Workflow](sequential-workflow.svg)\n\n## Implementation Overview\n\nThis Sequential Workflow will leverage publish-subscribe messaging to facilitate communication between agents. Each agent publishes a message containing its results to the topic subscribed to by the next agent. For a detailed understanding of the core concepts, refer to [Topic and Subscription](../core-concepts/topic-and-subscription.md) and [Broadcast Messaging](../framework/message-and-communication.ipynb#broadcast).\n\nIn this design, when the `ConceptExtractor` completes its analysis, it publishes its output to the `\"WriterAgent\"` topic, which the `WriterAgent` listens to. This collaborative approach continues through each step of the workflow until the final output is delivered.\n\n## Message Protocol\n\nThe messaging system within this workflow is structured around a simple text message protocol. Agents will transmit their work via a defined message format.\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nThis message class contains a single attribute, `content`, which holds the text that each agent will use to relay the information necessary for the next stage of processing.\n\n## Topics Definition\n\nIn the Sequential Workflow, each agent subscribes to a designated topic corresponding to its function. These topic types are named after the agents, enabling clear communication pathways throughout the workflow.\n\n```python\nconcept_extractor_topic_type = \"ConceptExtractorAgent\"\nwriter_topic_type = \"WriterAgent\"\nformat_proof_topic_type = \"FormatProofAgent\"\nuser_topic_type = \"User\"\n```\n\nWith these definitions in place, each agent can publish its results to the next agent's topic, ensuring clear and organized information flow.\n\n## Agent Classes\n\nEach agent class in this workflow is defined using the `@type_subscription` decorator from the framework, designating the topic type it subscribes to. As an alternative, agents can also be subscribed using the runtime's `add_subscription` method.\n\n### Concept Extractor Agent\n\nThe **Concept Extractor Agent** analyzes the initial product description to produce a structured output that highlights key features and target audience.\n\n```python\n@type_subscription(topic_type=concept_extractor_topic_type)\nclass ConceptExtractorAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A concept extractor agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are a marketing analyst. Given a product description, identify:\\n\"\n                \"- Key features\\n\"\n                \"- Target audience\\n\"\n                \"- Unique selling points\\n\\n\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_description(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Product description: {message.content}\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(writer_topic_type, source=self.id.key))\n```\n\nThe `handle_user_description` method takes the user's product description, processes it using a language model, and publishes its analysis to the Writer Agent's topic.\n\n### Writer Agent\n\nThe **Writer Agent** creates engaging marketing copy based on the analysis performed by the Concept Extractor.\n\n```python\n@type_subscription(topic_type=writer_topic_type)\nclass WriterAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A writer agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are a marketing copywriter. Given a block of text describing features, audience, and USPs, \"\n                \"compose a compelling marketing copy (like a newsletter section) that highlights these points. \"\n                \"Output should be short (around 150 words), output just the copy as a single text block.\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Below is the info about the product:\\n\\n{message.content}\"\n\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(format_proof_topic_type, source=self.id.key))\n```\n\nThis agent utilizes the insights from the Concept Extractor to generate a concise marketing copy, focusing on engagement and clarity, before passing it to the Format & Proof Agent for refinement.\n\n### Format & Proof Agent\n\nThe **Format & Proof Agent** ensures the final draft adheres to high standards of writing quality.\n\n```python\n@type_subscription(topic_type=format_proof_topic_type)\nclass FormatProofAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__(\"A format & proof agent.\")\n        self._system_message = SystemMessage(\n            content=(\n                \"You are an editor. Given the draft copy, correct grammar, improve clarity, ensure consistent tone, \"\n                \"give format and make it polished. Output the final improved copy as a single text block.\"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:\n        prompt = f\"Draft copy:\\n{message.content}.\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(user_topic_type, source=self.id.key))\n```\n\nThis agent takes the draft copy, applies editorial adjustments, and ensures the output is polished before sending it to the User Agent.\n\n### User Agent\n\nThe **User Agent** presents the final product to the end user or system.\n\n```python\n@type_subscription(topic_type=user_topic_type)\nclass UserAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"A user agent that outputs the final copy to the user.\")\n\n    @message_handler\n    async def handle_final_copy(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n{message.content}\")\n```\n\nIn this implementation, the User Agent simply prints the final marketing copy, but this functionality can be modified to perform additional actions like saving to a database or sending notifications.\n\n## Registering the Agents\n\nBefore executing the workflow, it is necessary to register the agents with the runtime environment. The agents are configured to automatically subscribe to their respective topics because of the use of the `@type_subscription` decorator.\n\n```python\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    # api_key=\"YOUR_API_KEY\"\n)\n\nruntime = SingleThreadedAgentRuntime()\n\nawait ConceptExtractorAgent.register(\n    runtime, type=concept_extractor_topic_type, factory=lambda: ConceptExtractorAgent(model_client=model_client)\n)\n\nawait WriterAgent.register(runtime, type=writer_topic_type, factory=lambda: WriterAgent(model_client=model_client))\n\nawait FormatProofAgent.register(\n    runtime, type=format_proof_topic_type, factory=lambda: FormatProofAgent(model_client=model_client)\n)\n\nawait UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())\n```\n\nThis code instantiates a client for the language model and sets up each agent in the runtime, linking them with their defined topics.\n\n## Executing the Workflow\n\nTo activate the sequential workflow, a message must be published to the first agent, starting the chain. \n\n```python\nruntime.start()\n\nawait runtime.publish_message(\n    Message(content=\"An eco-friendly stainless steel water bottle that keeps drinks cold for 24 hours\"),\n    topic_id=TopicId(concept_extractor_topic_type, source=\"default\"),\n)\n\nawait runtime.stop_when_idle()\nawait model_client.close()\n```\n\nIn this step, the workflow begins with the provision of a product description, which initiates processing through each agent in sequence and concludes with the presentation of the final marketing copy. The cleanup process ensures that all resources are appropriately released after execution.",
    "filename": "python/docs/src/user-guide/core-user-guide/design-patterns/sequential-workflow.ipynb"
  },
  {
    "content": "# FAQs\n\n## How do I get the underlying agent instance?\n\nAgents might be distributed across multiple machines, so the underlying agent instance is intentionally discouraged from being accessed. If the agent is definitely running on the same machine, you can access the agent instance by calling {py:meth}`autogen_core.AgentRuntime.try_get_underlying_agent_instance` on the `AgentRuntime`. If the agent is not available this will throw an exception.\n\n## How do I call call a function on an agent?\n\nSince the instance itself is not accessible, you can't call a function on an agent directly. Instead, you should create a type to represent the function call and its arguments, and then send that message to the agent. Then in the agent, create a handler for that message type and implement the required logic. This also supports returning a response to the caller.\n\nThis allows your agent to work in a distributed environment a well as a local one.\n\n## Why do I need to use a factory to register an agent?\n\nAn {py:class}`autogen_core.AgentId` is composed of a `type` and a `key`. The type corresponds to the factory that created the agent, and the key is a runtime, data dependent key for this instance.\n\nThe key can correspond to a user id, a session id, or could just be \"default\" if you don't need to differentiate between instances. Each unique key will create a new instance of the agent, based on the factory provided. This allows the system to automatically scale to different instances of the same agent, and to manage the lifecycle of each instance independently based on how you choose to handle keys in your application.\n\n## How do I increase the GRPC message size?\n\nIf you need to provide custom gRPC options, such as overriding the `max_send_message_length` and `max_receive_message_length`, you can define an `extra_grpc_config` variable and pass it to both the `GrpcWorkerAgentRuntimeHost` and `GrpcWorkerAgentRuntime` instances.\n\n```python\n# Define custom gRPC options\nextra_grpc_config = [\n    (\"grpc.max_send_message_length\", new_max_size),\n    (\"grpc.max_receive_message_length\", new_max_size),\n]\n\n# Create instances of GrpcWorkerAgentRuntimeHost and GrpcWorkerAgentRuntime with the custom gRPC options\n\nhost = GrpcWorkerAgentRuntimeHost(address=host_address, extra_grpc_config=extra_grpc_config)\nworker1 = GrpcWorkerAgentRuntime(host_address=host_address, extra_grpc_config=extra_grpc_config)\n```\n\n**Note**: When `GrpcWorkerAgentRuntime` creates a host connection for the clients, it uses `DEFAULT_GRPC_CONFIG` from `HostConnection` class as default set of values which will can be overriden if you pass parameters with the same name using `extra_grpc_config`.\n\n## What are model capabilities and how do I specify them?\n\nModel capabilites are additional capabilities an LLM may have beyond the standard natural language features. There are currently 3 additional capabilities that can be specified within Autogen\n\n- vision: The model is capable of processing and interpreting image data.\n- function_calling: The model has the capacity to accept function descriptions; such as the function name, purpose, input parameters, etc; and can respond with an appropriate function to call including any necessary parameters.\n- json_output: The model is capable of outputting responses to conform with a specified json format.\n\nModel capabilities can be passed into a model, which will override the default definitions. These capabilities will not affect what the underlying model is actually capable of, but will allow or disallow behaviors associated with them. This is particularly useful when [using local LLMs](cookbook/local-llms-ollama-litellm.ipynb).\n\n```python\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nclient = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    api_key=\"YourApiKey\",\n    model_capabilities={\n        \"vision\": True,\n        \"function_calling\": False,\n        \"json_output\": False,\n    }\n)\n```",
    "filename": "python/docs/src/user-guide/core-user-guide/faqs.md"
  },
  {
    "code": false,
    "content": "# Agent and Agent Runtime\n\nIn this section, we focus on the core concepts of AutoGen: agents, agent runtime, messages, and communication. These foundational building blocks are crucial for developing multi-agent applications.\n\n```{note}\nThe Core API is designed to be unopinionated and flexible. Thus, you may find it challenging at times. Continue if you are building an interactive, scalable, and distributed multi-agent system and want full control over workflows. If your goal is to get something running quickly, consider using the [AgentChat API](../../agentchat-user-guide/index.md).\n```\n\n## Understanding Agents\n\nAn **agent** in AutoGen is defined by the base interface `{py:class}`~autogen_core.Agent`. Each agent possesses a unique identifier of the type `{py:class}`~autogen_core.AgentId`, along with a metadata dictionary of type `{py:class}`~autogen_core.AgentMetadata`. \n\nTypically, developers can subclass their agents from the higher-level class `{py:class}`~autogen_core.RoutedAgent`. This allows for routing messages to corresponding message handlers specified by the `{py:meth}`~autogen_core.message_handler` decorator alongside a proper type hint for the `message` variable.\n\n## Defining an Agent Runtime\n\nAn **agent runtime** serves as the execution environment for agents within AutoGen. Similar to the runtime environments of programming languages, the agent runtime provides the necessary infrastructure to:\n\n- Facilitate communication between agents\n- Manage agent lifecycles\n- Enforce security boundaries\n- Support monitoring and debugging\n\nFor local development purposes, developers can employ `{py:class}`~autogen_core.SingleThreadedAgentRuntime`, which can be seamlessly integrated into a Python application.\n\n```{note}\nAgents are not instantiated and managed directly by application code. Instead, they are created by the runtime when necessary and maintained by the runtime itself.\n\nIf you are familiar with [AgentChat](../../agentchat-user-guide/index.md), note that its agents like `{py:class}`~autogen_agentchat.agents.AssistantAgent` are directly created by the application, hence not managed by the runtime. To incorporate an AgentChat agent within Core, you need to create a wrapper Core agent that delegates messages to the AgentChat agent, allowing the runtime to manage the wrapper agent.\n```\n\n## Implementing an Agent\n\nTo create an agent, developers must subclass the `{py:class}`~autogen_core.RoutedAgent` class and implement message handler methods for each message type the agent is expected to handle. This is achieved using the `{py:meth}`~autogen_core.message_handler` decorator. The following example illustrates an agent that handles a simple message type `MyMessageType` and prints the message it receives.\n\n```python\nfrom dataclasses import dataclass\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n\n@dataclass\nclass MyMessageType:\n    content: str\n\nclass MyAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"MyAgent\")\n\n    @message_handler\n    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:\n        print(f\"{self.id.type} received message: {message.content}\")\n```\n\nThis agent exclusively handles `MyMessageType`. Messages directed to this agent will be delivered to the `handle_my_message_type` method. Developers can add multiple message handlers for different message types by using the `{py:meth}`~autogen_core.message_handler` decorator and providing the appropriate type hint for the `message` variable in the handler function. They may also utilize [Python's typing union](https://docs.python.org/3/library/typing.html#typing.Union) for the `message` variable within a single message handler if it aligns with the agent's logic. For further details, refer to the next section on [message and communication](./message-and-communication.ipynb).\n\n## Using an AgentChat Agent\n\nIf you have an agent from [AgentChat](../../agentchat-user-guide/index.md) and wish to use it within the Core API, you can create a wrapper `{py:class}`~autogen_core.RoutedAgent` that delegates messages to the AgentChat agent. In the following example, we demonstrate how to create a wrapper agent for the `{py:class}`~autogen_agentchat.agents.AssistantAgent` in AgentChat.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nclass MyAssistant(RoutedAgent):\n    def __init__(self, name: str) -> None:\n        super().__init__(name)\n        model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n        self._delegate = AssistantAgent(name, model_client=model_client)\n\n    @message_handler\n    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:\n        print(f\"{self.id.type} received message: {message.content}\")\n        response = await self._delegate.on_messages(\n            [TextMessage(content=message.content, source=\"user\")], ctx.cancellation_token\n        )\n        print(f\"{self.id.type} responded: {response.chat_message}\")\n```\n\nThis implementation demonstrates how to delegate messages from a `MyAssistant` wrapper agent to the `AssistantAgent`. The model client, `OpenAIChatCompletionClient`, allows the agent to interact with the AssistantAgent effectively. For more details on using the model client, refer to the [Model Client](../components/model-clients.ipynb) section.\n\nThe flexibility of the Core API enables developers to not rely solely on the AgentChat API; they can create their own agents or integrate with other agent frameworks.\n\n## Registering Agent Types\n\nTo make agents available to the runtime, developers can utilize the `{py:meth}`~autogen_core.BaseAgent.register` class method. This process involves associating an agent type, uniquely identified by a string, with a factory function that creates an instance of the agent class.\n\nAn **agent type** (`{py:class}`~autogen_core.AgentType`) is distinct from the agent class. For instance, the agent type might be `AgentType(\"my_agent\")`, whereas the agent class refers to `MyAgent`. The factory function should return an instance of the agent class invoked by the `{py:meth}`~autogen_core.BaseAgent.register` method.\n\n```{note}\nDifferent agent types can be registered with factory functions returning the same agent class. For example, variations in constructor parameters can be utilized to create different instances of the same agent class.\n```\n\nTo register the agent types with the `{py:class}`~autogen_core.SingleThreadedAgentRuntime`, the code snippet below can be employed:\n\n```python\nfrom autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait MyAgent.register(runtime, \"my_agent\", lambda: MyAgent())\nawait MyAssistant.register(runtime, \"my_assistant\", lambda: MyAssistant(\"my_assistant\"))\n```\n\nOnce the agent types are registered, you can send messages directly to an agent instance using an `{py:class}`~autogen_core.AgentId`. The runtime will create the instance the first time it processes a message directed to this instance.\n\n```python\nruntime.start()  # Start processing messages in the background.\nawait runtime.send_message(MyMessageType(\"Hello, World!\"), AgentId(\"my_agent\", \"default\"))\nawait runtime.send_message(MyMessageType(\"Hello, World!\"), AgentId(\"my_assistant\", \"default\"))\nawait runtime.stop()  # Stop processing messages in the background.\n```\n\n```{note}\nSince the runtime manages agent lifecycles, an `{py:class}`~autogen_core.AgentId` is used solely for communication with the agent or to retrieve its metadata.\n```\n\n## Running the Single-Threaded Agent Runtime\n\nThe code snippet above utilizes the `{py:meth}`~autogen_core.SingleThreadedAgentRuntime.start` to commence a background task responsible for processing and delivering messages to the appropriate handlers.\n\nTo terminate the background task at any time, resort to the `{py:meth}`~autogen_core.SingleThreadedAgentRuntime.stop` method:\n\n```python\nruntime.start()\n# ... Send messages, publish messages, etc.\nawait runtime.stop()  # Stops the background task immediately but does not cancel any ongoing message handling.\n```\n\nYou can reactivate the background task by calling `{py:meth}`~autogen_core.SingleThreadedAgentRuntime.start` again.\n\nIn scenarios requiring batch processing, such as performance benchmarking, you may want the background task to halt automatically when no unprocessed messages are left, and no agent is currently handling messages. This can be achieved using the `{py:meth}`~autogen_core.SingleThreadedAgentRuntime.stop_when_idle` method:\n\n```python\nruntime.start()\n# ... Send messages, publish messages, etc.\nawait runtime.stop_when_idle()  # Blocks until the runtime is idle.\n```\n\nTo properly close the runtime and release resources, use the `{py:meth}`~autogen_core.SingleThreadedAgentRuntime.close` method:\n\n```python\nawait runtime.close()\n```\n\nNote that other runtime implementations will have their own specific methods for managing runtime operations.",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/agent-and-agent-runtime.ipynb"
  },
  {
    "code": false,
    "content": "# Component Configuration Documentation\n\nThe AutoGen framework allows for flexible and dynamic component configuration through a system known as \"component configuration\". This feature enables a variety of use cases, including configuration-based development environments such as AutoGen Studio, alongside traditional programming scenarios. \n\nThis documentation provides an overview of component configuration, its distinction from object state, usage examples, and guidelines for creating configurable components. \n\n## Overview of Component Configuration\n\nIn AutoGen, components can be constructed from configuration objects and vice versa. This capability facilitates an easy transition between code and configuration. For instance, you can define a component within your codebase and subsequently retrieve its configuration object, allowing for consistent and reproducible behaviors across instantiations.\n\nThis structure also supports external extensions, enabling components that are not inherently part of AutoGen to be configured in a similar manner, fostering extensibility and customization.\n\n## Distinction Between Component Configuration and State\n\nIt is crucial to distinguish between serializing object state and component configuration. When serializing an object, all data necessary to recreate that object, including variables like message history, must be included. Deserializing this serialized state should return the exact same object.\n\nIn contrast, component configuration acts as a blueprint. It outlines how to construct an object rather than holding the object's actual state. Multiple instances of the same object can be generated from this blueprint, providing greater flexibility and adaptability, allowing for varied behaviors without direct ties to specific internal states.\n\n## Using Component Configuration\n\nTo fetch the configuration for an existing component in Python, use the `dump_component` method from the `ComponentToConfig` class. The configuration object produced can later be passed into the `load_component` method from the `ComponentLoader` class to recreate the component.\n\n### Example: Loading a Component from Configuration\n\nHere's how to load a component from a configuration object. Use the `load_component` method to generate a component instance from a specified config:\n\n```python\nfrom autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"openai_chat_completion_client\",\n    \"config\": {\"model\": \"gpt-4o\"},\n}\n\nclient = ChatCompletionClient.load_component(config)\n```\n\nIn this example, a configuration dictionary is prepared to specify a component type (`openai_chat_completion_client`) and its associated configuration (`model: gpt-4o`). The `load_component` method then creates an instance of `ChatCompletionClient` based on this configuration.\n\n## Creating a Custom Component Class\n\nTo implement configurability in your class, follow these steps:\n\n1. Inherit from the `Component` class.\n2. Implement the required methods: `_to_config` and `_from_config`.\n\nThe following example demonstrates how to create a custom component class:\n\n```python\nfrom autogen_core import Component, ComponentBase\nfrom pydantic import BaseModel\n\n\nclass Config(BaseModel):\n    value: str\n\n\nclass MyComponent(ComponentBase[Config], Component[Config]):\n    component_type = \"custom\"\n    component_config_schema = Config\n\n    def __init__(self, value: str):\n        self.value = value\n\n    def _to_config(self) -> Config:\n        return Config(value=self.value)\n\n    @classmethod\n    def _from_config(cls, config: Config) -> \"MyComponent\":\n        return cls(value=config.value)\n```\n\nIn this example, `MyComponent` is defined with a type of `\"custom\"` and is backed by a Pydantic model called `Config`. The `_to_config` method converts the component's state into a configuration object, while the `_from_config` class method generates a new `MyComponent` instance from a given configuration.\n\n## Handling Secrets in Configuration\n\nWhen dealing with sensitive information in your configuration, it is essential to ensure such values are not inadvertently exposed. In Pydantic, this can be accomplished by using the `SecretStr` type, which prevents secret values from being dumped into the configuration object. \n\n### Example: Defining a Client Configuration with Secrets\n\nHere's how to define a configuration object that includes secrets:\n\n```python\nfrom pydantic import BaseModel, SecretStr\n\n\nclass ClientConfig(BaseModel):\n    endpoint: str\n    api_key: SecretStr\n```\n\nIn this example, `ClientConfig` includes an `api_key` field set as a `SecretStr`. This ensures that sensitive information remains hidden when converting the configuration to a serialized format, enhancing security and preserving confidentiality.\n\n## Conclusion\n\nComponent configuration within the AutoGen framework offers a robust and flexible way to define and manage components using configuration objects. Understanding the differences between configuration and state, as well as the importance of securely handling secrets, is crucial for effectively leveraging this system. By following the usage guidelines and implementation examples provided, you can create and manage configurable components that meet your application's needs.",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/component-config.ipynb"
  },
  {
    "code": false,
    "content": "# Distributed Agent Runtime Documentation\n\n```{attention}\nThe distributed agent runtime is an experimental feature. Expect breaking changes to the API.\n```\n\n## Overview\n\nA distributed agent runtime facilitates communication and agent lifecycle management across process boundaries. This architecture includes a host service and at least one worker runtime. \n\nThe **host service** maintains connections to all active worker runtimes, facilitates message delivery, and keeps sessions for all direct messages (Remote Procedure Calls, or RPCs). On the other hand, a **worker runtime** processes application code (agents) and connects to the host service. Additionally, it advertises the agents it supports, allowing the host service to route messages to the correct worker.\n\n````{note}\nTo utilize the distributed agent runtime, certain extra dependencies are required. Install them using:\n\n```bash\npip install \"autogen-ext[grpc]\"\n```\n````\n\n## Starting the Host Service\n\nWe can start a host service using the `GrpcWorkerAgentRuntimeHost` class from the `autogen_ext.runtimes.grpc` module. This host service will listen for incoming worker connections on a specified port.\n\n```python\nfrom autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n\nhost = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\nhost.start()  # Start a host service in the background.\n```\n\nIn the above code, we instantiate the host service on `localhost` at port `50051` and start it in the background to listen for worker connections effectively.\n\n## Defining the Agent\n\nBefore launching worker runtimes, we need to define our agent. This agent will handle incoming messages by publishing a message in response and tracking how many messages it has published. It will cease operations after publishing five messages.\n\n```python\nfrom dataclasses import dataclass\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\n\n@dataclass\nclass MyMessage:\n    content: str\n\n@default_subscription\nclass MyAgent(RoutedAgent):\n    def __init__(self, name: str) -> None:\n        super().__init__(\"My agent\")\n        self._name = name\n        self._counter = 0\n\n    @message_handler\n    async def my_message_handler(self, message: MyMessage, ctx: MessageContext) -> None:\n        self._counter += 1\n        if self._counter > 5:\n            return\n        content = f\"{self._name}: Hello x {self._counter}\"\n        print(content)\n        await self.publish_message(MyMessage(content=content), DefaultTopicId())\n```\n\nHere, `MyMessage` is a simple data structure for our messages, while `MyAgent` is a routed agent that subscribes to incoming messages. The `my_message_handler` method processes these messages, increments a counter, and generates a new message until a limit is reached.\n\n## Setting Up Worker Agent Runtimes\n\nWith the agent defined, we can set up the worker agent runtimes using the `GrpcWorkerAgentRuntime` class. In this example, we create two worker runtimes, each running its instance of the agent. Both agents will publish and subscribe to the default topic, enabling them to interact with one another.\n\n```python\nimport asyncio\nfrom autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime\n\nworker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\nawait worker1.start()\nawait MyAgent.register(worker1, \"worker1\", lambda: MyAgent(\"worker1\"))\n\nworker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\nawait worker2.start()\nawait MyAgent.register(worker2, \"worker2\", lambda: MyAgent(\"worker2\"))\n\nawait worker2.publish_message(MyMessage(content=\"Hello!\"), DefaultTopicId())\n\n# Let the agents run for a while.\nawait asyncio.sleep(5)\n```\n\nIn this code block, both `worker1` and `worker2` connect to the host service. After starting them, `worker2` sends an initial message, triggering the agents to publish their responses, which results in each agent publishing exactly five messages.\n\n## Stopping the Worker Runtimes\n\nShutting down the worker runtimes can be managed using the `stop` method available within the `GrpcWorkerAgentRuntime` class.\n\n```python\nawait worker1.stop()\nawait worker2.stop()\n\n# To keep the worker running until a termination signal is received (e.g., SIGTERM).\n# await worker1.stop_when_signal()\n```\n\nHere, we gracefully stop both worker runtimes. Optionally, we can keep a worker running until it receives a termination signal.\n\n## Stopping the Host Service\n\nStopping the host service is similarly straightforward. The `stop` method in `GrpcWorkerAgentRuntimeHost` will end the service.\n\n```python\nawait host.stop()\n\n# To keep the host service running until a termination signal (e.g., SIGTERM)\n# await host.stop_when_signal()\n```\n\nIn the above block, we call the `stop` method to terminate the host service once we\u2019ve completed our operations.\n\n## Cross-Language Runtimes\n\nThe described process applies to cross-language runtimes as well, but it's important to note that all message types must utilize shared protobuf schemas for seamless communication across different agents.\n\n## Next Steps\n\nTo explore complete examples of using the distributed runtime, review the following resources:\n\n- [Distributed Workers](https://github.com/microsoft/autogen/tree/main/python/samples/core_grpc_worker_runtime)  \n- [Distributed Semantic Router](https://github.com/microsoft/autogen/tree/main/python/samples/core_semantic_router)  \n- [Distributed Group Chat](https://github.com/microsoft/autogen/tree/main/python/samples/core_distributed-group-chat)  \n\nThese samples provide comprehensive insights into various implementations of the distributed agent runtime.",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/distributed-agent-runtime.ipynb"
  },
  {
    "content": "# Logging\n\nAutoGen uses Python's built-in [`logging`](https://docs.python.org/3/library/logging.html) module.\n\nThere are two kinds of logging:\n\n- **Trace logging**: This is used for debugging and is human readable messages to indicate what is going on. This is intended for a developer to understand what is happening in the code. The content and format of these logs should not be depended on by other systems.\n  - Name: {py:attr}`~autogen_core.TRACE_LOGGER_NAME`.\n- **Structured logging**: This logger emits structured events that can be consumed by other systems. The content and format of these logs can be depended on by other systems.\n  - Name: {py:attr}`~autogen_core.EVENT_LOGGER_NAME`.\n  - See the module {py:mod}`autogen_core.logging` to see the available events.\n- {py:attr}`~autogen_core.ROOT_LOGGER_NAME` can be used to enable or disable all logs.\n\n## Enabling logging output\n\nTo enable trace logging, you can use the following code:\n\n```python\nimport logging\n\nfrom autogen_core import TRACE_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(TRACE_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.DEBUG)\n```\n\nTo enable structured logging, you can use the following code:\n\n```python\nimport logging\n\nfrom autogen_core import EVENT_LOGGER_NAME\n\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n```\n\n### Structured logging\n\nStructured logging allows you to write handling logic that deals with the actual events including all fields rather than just a formatted string.\n\nFor example, if you had defined this custom event and were emitting it. Then you could write the following handler to receive it.\n\n```python\nimport logging\nfrom dataclasses import dataclass\n\n@dataclass\nclass MyEvent:\n    timestamp: str\n    message: str\n\nclass MyHandler(logging.Handler):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def emit(self, record: logging.LogRecord) -> None:\n        try:\n            # Use the StructuredMessage if the message is an instance of it\n            if isinstance(record.msg, MyEvent):\n                print(f\"Timestamp: {record.msg.timestamp}, Message: {record.msg.message}\")\n        except Exception:\n            self.handleError(record)\n```\n\nAnd this is how you could use it:\n\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nmy_handler = MyHandler()\nlogger.handlers = [my_handler]\n```\n\n## Emitting logs\n\nThese two names are the root loggers for these types. Code that emits logs should use a child logger of these loggers. For example, if you are writing a module `my_module` and you want to emit trace logs, you should use the logger named:\n\n```python\nimport logging\n\nfrom autogen_core import TRACE_LOGGER_NAME\nlogger = logging.getLogger(f\"{TRACE_LOGGER_NAME}.my_module\")\n```\n\n### Emitting structured logs\n\nIf your event is a dataclass, then it could be emitted in code like this:\n\n```python\nimport logging\nfrom dataclasses import dataclass\nfrom autogen_core import EVENT_LOGGER_NAME\n\n@dataclass\nclass MyEvent:\n    timestamp: str\n    message: str\n\nlogger = logging.getLogger(EVENT_LOGGER_NAME + \".my_module\")\nlogger.info(MyEvent(\"timestamp\", \"message\"))\n```",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/logging.md"
  },
  {
    "code": false,
    "content": "# Message and Communication\n\nIn AutoGen core, agents can send, publish, and respond to messages. Messages serve as the sole means of communication between agents, allowing them to convey information and commands effectively.\n\n## Messages\n\nMessages in AutoGen core are serializable data structures that can be defined using either:\n- A subclass of Pydantic's `pydantic.BaseModel`\n- A Python dataclass\n\nThe following examples illustrate how to create two distinct message types, `TextMessage` and `ImageMessage`, each holding relevant information:\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n@dataclass\nclass ImageMessage:\n    url: str\n    source: str\n```\n\n**Note:** Messages are designed to be purely data-oriented and should not contain any business logic.\n\n## Message Handlers\n\nWhen an agent receives a message, the runtime invokes the agent's message handler method (`autogen_core.Agent.on_message`). This method contains the logic for handling incoming messages. If an agent cannot process a message, it must raise a `autogen_core.exceptions.CantHandleException`. The base class, `autogen_core.BaseAgent`, does not provide message handling logic, so it is recommended for developers to extend the `autogen_core.RoutedAgent` class, which offers built-in message routing capabilities.\n\n### Routing Messages by Type\n\nThe `autogen_core.RoutedAgent` class allows developers to associate message types with specific handlers using the `autogen_core.components.message_handler` decorator. This eliminates the need to implement the `autogen_core.Agent.on_message` method directly. Below is an example of an agent that responds to both `TextMessage` and `ImageMessage` types:\n\n```python\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n\nclass MyAgent(RoutedAgent):\n    @message_handler\n    async def on_text_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you said {message.content}!\")\n\n    @message_handler\n    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you sent me {message.url}!\")\n```\n\nTo create and register the agent in the runtime, use the following code:\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait MyAgent.register(runtime, \"my_agent\", lambda: MyAgent(\"My Agent\"))\n```\n\nYou can then test the agent's response to `TextMessage` and `ImageMessage`:\n\n```python\nruntime.start()\nagent_id = AgentId(\"my_agent\", \"default\")\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"User\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"User\"), agent_id)\nawait runtime.stop_when_idle()\n```\n\nWhen the first message is delivered, the runtime automatically creates an instance of `MyAgent` with the agent ID `AgentId(\"my_agent\", \"default\")`.\n\n### Routing Messages of the Same Type\n\nIn certain scenarios, it may be necessary to route messages of the same type to different handlers based on sender information. This is accomplished using the `match` parameter of the `autogen_core.components.message_handler` decorator.\n\nThe `match` parameter takes a callable that checks the message and a `autogen_core.MessageContext`, returning a boolean to determine if the message should be handled by the decorated method. Here is an example where messages are routed based on sender:\n\n```python\nclass RoutedBySenderAgent(RoutedAgent):\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user1\"))  # type: ignore\n    async def on_user1_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello from user 1 handler, {message.source}, you said {message.content}!\")\n\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user2\"))  # type: ignore\n    async def on_user2_message(self, message: TextMessage, ctx: MessageContext) -> None:\n        print(f\"Hello from user 2 handler, {message.source}, you said {message.content}!\")\n\n    @message_handler(match=lambda msg, ctx: msg.source.startswith(\"user2\"))  # type: ignore\n    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:\n        print(f\"Hello, {message.source}, you sent me {message.url}!\")\n```\n\nThe `match` parameter for each handler utilizes the `source` field of the message to differentiate sender agents. The sender can also be addressed using the `sender` field within the `MessageContext`.\n\nTo test this agent:\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait RoutedBySenderAgent.register(runtime, \"my_agent\", lambda: RoutedBySenderAgent(\"Routed by sender agent\"))\nruntime.start()\nagent_id = AgentId(\"my_agent\", \"default\")\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"user1-test\"), agent_id)\nawait runtime.send_message(TextMessage(content=\"Hello, World!\", source=\"user2-test\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"user1-test\"), agent_id)\nawait runtime.send_message(ImageMessage(url=\"https://example.com/image.jpg\", source=\"user2-test\"), agent_id)\nawait runtime.stop_when_idle()\n```\n\nIn this example, the first `ImageMessage` is ignored since its `source` does not match any handler conditions.\n\n## Direct Messaging\n\nAutoGen core supports two primary communication types: \n- **Direct Messaging**: Sends a message directly to another agent.\n- **Broadcast**: Publishes a message to a designated topic.\n\nFocusing on direct messaging, to send a message to another agent, you can use the `autogen_core.BaseAgent.send_message` method from within a message handler or the `autogen_core.AgentRuntime.send_message` method from the runtime. The return value of these methods reflects the output of the receiving agent's message handler, and if a handler returns `None`, the call will also return `None`.\n\n**Note:** If the targeted agent raises an exception while the sender is awaiting the response, the exception is propagated back to the sender.\n\n### Request/Response\n\nDirect messaging can facilitate request/response interactions, where the sender expects a reply from the recipient. The recipient can respond by returning a value in its message handler. Below is an example involving two agents: `InnerAgent` and `OuterAgent`.\n\n```python\nfrom dataclasses import dataclass\nfrom autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\n\n@dataclass\nclass Message:\n    content: str\n\nclass InnerAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:\n        return Message(content=f\"Hello from inner, {message.content}\")\n\nclass OuterAgent(RoutedAgent):\n    def __init__(self, description: str, inner_agent_type: str):\n        super().__init__(description)\n        self.inner_agent_id = AgentId(inner_agent_type, self.id.key)\n\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"Received message: {message.content}\")\n        response = await self.send_message(Message(f\"Hello from outer, {message.content}\"), self.inner_agent_id)\n        print(f\"Received inner response: {response.content}\")\n```\n\nIn this case, when receiving a message, the `OuterAgent` sends a direct message to the `InnerAgent` and processes the reply.\n\nTo test the agents, you can run the following:\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait InnerAgent.register(runtime, \"inner_agent\", lambda: InnerAgent(\"InnerAgent\"))\nawait OuterAgent.register(runtime, \"outer_agent\", lambda: OuterAgent(\"OuterAgent\", \"inner_agent\"))\nruntime.start()\nouter_agent_id = AgentId(\"outer_agent\", \"default\")\nawait runtime.send_message(Message(content=\"Hello, World!\"), outer_agent_id)\nawait runtime.stop_when_idle()\n```\n\nBoth print statements originate from the `OuterAgent`'s handler; the second one depends on the response from the `InnerAgent`.\n\nIn summary, direct messaging is best suited for closely coupled scenarios where the sender is directly tied to a specific instance of the recipient, for example, when an agent needs to execute tool calls through a `autogen_core.tool_agent.ToolAgent`.\n\n## Broadcast\n\nBroadcasting employs a publish/subscribe model centered on topics and subscriptions. For detailed understanding, refer to the [Topic and Subscription](../core-concepts/topic-and-subscription.md) documentation. \n\nKey distinctions from direct messaging include that broadcast does not allow request/response interactions. A published message is one-way; thus, if a receiving agent returns a value, it will not be processed by the publisher.\n\n**Note:** Responses to published messages are discarded. Additionally, an agent will not receive messages it published to prevent infinite feedback loops.\n\n### Subscribe and Publish to Topics\n\nType-based subscriptions map published messages to agents of specific types. To have an agent subclassing `autogen_core.RoutedAgent` subscribe to a specific topic, use the `autogen_core.components.type_subscription` decorator. The example below demonstrates a `ReceivingAgent` that subscribes to messages of type `\"default\"`:\n\n```python\nfrom autogen_core import RoutedAgent, message_handler, type_subscription\n\n@type_subscription(topic_type=\"default\")\nclass ReceivingAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        print(f\"Received a message: {message.content}\")\n```\n\nTo publish a message, utilize the `autogen_core.BaseAgent.publish_message` method with a specified `autogen_core.TopicId`. This operation requires awaiting to ensure proper scheduling in the runtime. If an exception occurs during message handling, it will be logged but not relayed back to the publishing agent.\n\n```python\nfrom autogen_core import TopicId\n\nclass BroadcastingAgent(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        await self.publish_message(\n            Message(\"Publishing a message from broadcasting agent!\"),\n            topic_id=TopicId(type=\"default\", source=self.id.key),\n        )\n```\n\nTo register Agent with runtime, employ either the decorator or separate API methods as shown below:\n\n```python\nfrom autogen_core import TypeSubscription\n\nruntime = SingleThreadedAgentRuntime()\n\n# Option 1: with type_subscription decorator\nawait ReceivingAgent.register(runtime, \"receiving_agent\", lambda: ReceivingAgent(\"Receiving Agent\"))\n\n# Option 2: with TypeSubscription\nawait BroadcastingAgent.register(runtime, \"broadcasting_agent\", lambda: BroadcastingAgent(\"Broadcasting Agent\"))\nawait runtime.add_subscription(TypeSubscription(topic_type=\"default\", agent_type=\"broadcasting_agent\"))\n\n# Start runtime and publish a message\nruntime.start()\nawait runtime.publish_message(\n    Message(\"Hello, World! From the runtime!\"), topic_id=TopicId(type=\"default\", source=\"default\")\n)\nawait runtime.stop_when_idle()\n```\n\nYou can also publish directly through the runtime without requiring an agent instance, as demonstrated in the example. The output illustrates that the receiving agent captures both messages published via runtime and those relayed by the broadcasting agent.\n\n### Default Topic and Subscriptions\n\nWhile explicitly using `autogen_core.TopicId` and `autogen_core.components.TypeSubscription` addresses many scenarios effectively, there are instances where all agents engage with a single publishing scope. In such cases, leveraging the convenience classes `autogen_core.components.DefaultTopicId` and `autogen_core.components.default_subscription` can simplify implementation.\n\nThe `DefaultTopicId` creates a topic using `\"default\"` as its type and the publishing agent's key as its source. Conversely, `default_subscription` allows an agent to subscribe to this general topic. This simplification is illustrated in the following `BroadcastingAgent` definition:\n\n```python\nfrom autogen_core import DefaultTopicId, default_subscription\n\n@default_subscription\nclass BroadcastingAgentDefaultTopic(RoutedAgent):\n    @message_handler\n    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:\n        await self.publish_message(\n            Message(\"Publishing a message from broadcasting agent!\"),\n            topic_id=DefaultTopicId(),\n        )\n```\n\nUpon registering the agent type, a `autogen_core.components.TypeSubscription` is formed automatically. The following demonstrates how to register and test the agent using the simplified classes:\n\n```python\nruntime = SingleThreadedAgentRuntime()\nawait BroadcastingAgentDefaultTopic.register(\n    runtime, \"broadcasting_agent\", lambda: BroadcastingAgentDefaultTopic(\"Broadcasting Agent\")\n)\nawait ReceivingAgent.register(runtime, \"receiving_agent\", lambda: ReceivingAgent(\"Receiving Agent\"))\nruntime.start()\nawait runtime.publish_message(Message(\"Hello, World! From the runtime!\"), topic_id=DefaultTopicId())\nawait runtime.stop_when_idle()\n```\n\n**Note:** When your use case allows for a unified mechanism where all agents publish and subscribe to broadcast messages, prefer using `DefaultTopicId` and `default_subscription` for streamlined code and improved readability.",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/message-and-communication.ipynb"
  },
  {
    "content": "# Open Telemetry\n\nAutoGen has native support for [open telemetry](https://opentelemetry.io/). This allows you to collect telemetry data from your application and send it to a telemetry backend of your choosing.\n\nThese are the components that are currently instrumented:\n\n- Runtime ({py:class}`~autogen_core.SingleThreadedAgentRuntime` and {py:class}`~autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime`).\n- Tool ({py:class}`~autogen_core.tools.BaseTool`) with the `execute_tool` span in [GenAI semantic convention for tools](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#execute-tool-span).\n- AgentChat Agents ({py:class}`~autogen_agentchat.agents.BaseChatAgent`) with the `create_agent` and `invoke_agent` spans in [GenAI semantic convention for agents](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/#create-agent-span).\n\n```{note}\nTo disable the agent runtime telemetry, you can set the `trace_provider` to\n`opentelemetry.trace.NoOpTracerProvider` in the runtime constructor.\n\nAdditionally, you can set the environment variable `AUTOGEN_DISABLE_RUNTIME_TRACING` to `true` to disable the agent runtime telemetry if you don't have access to the runtime constructor. For example, if you are using `ComponentConfig`.\n```\n\n## Instrumenting your application\n\nTo instrument your application, you will need an sdk and an exporter. You may already have these if your application is already instrumented with open telemetry.\n\n## Clean instrumentation\n\nIf you do not have open telemetry set up in your application, you can follow these steps to instrument your application.\n\n```bash\npip install opentelemetry-sdk\n```\n\nDepending on your open telemetry collector, you can use grpc or http to export your telemetry.\n\n```bash\n# Pick one of the following\n\npip install opentelemetry-exporter-otlp-proto-http\npip install opentelemetry-exporter-otlp-proto-grpc\n```\n\nNext, we need to get a tracer provider:\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\ndef configure_oltp_tracing(endpoint: str = None) -> trace.TracerProvider:\n    # Configure Tracing\n    tracer_provider = TracerProvider(resource=Resource({\"service.name\": \"my-service\"}))\n    processor = BatchSpanProcessor(OTLPSpanExporter())\n    tracer_provider.add_span_processor(processor)\n    trace.set_tracer_provider(tracer_provider)\n\n    return tracer_provider\n```\n\nNow you can send the trace_provider when creating your runtime:\n\n```python\n# for single threaded runtime\nsingle_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n# or for worker runtime\nworker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)\n```\n\nAnd that's it! Your application is now instrumented with open telemetry. You can now view your telemetry data in your telemetry backend.\n\n### Existing instrumentation\n\nIf you have open telemetry already set up in your application, you can pass the tracer provider to the runtime when creating it:\n\n```python\nfrom opentelemetry import trace\n\n# Get the tracer provider from your application\ntracer_provider = trace.get_tracer_provider()\n\n# for single threaded runtime\nsingle_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n# or for worker runtime\nworker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)\n```\n\n### Examples\n\nSee [Tracing and Observability](../../agentchat-user-guide/tracing.ipynb)\nfor a complete example of how to set up open telemetry with AutoGen.",
    "filename": "python/docs/src/user-guide/core-user-guide/framework/telemetry.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AutoGen Core, a framework for building multi-agent applications with AI agents.\n"
      }
    },
    "content": "# Core\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n\ninstallation\nquickstart\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Core Concepts\n\ncore-concepts/agent-and-multi-agent-application\ncore-concepts/architecture\ncore-concepts/application-stack\ncore-concepts/agent-identity-and-lifecycle\ncore-concepts/topic-and-subscription\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Framework Guide\n\nframework/agent-and-agent-runtime\nframework/message-and-communication\nframework/logging\nframework/telemetry\nframework/distributed-agent-runtime\nframework/component-config\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Components Guide\n\ncomponents/model-clients\ncomponents/model-context\ncomponents/tools\ncomponents/workbench\ncomponents/command-line-code-executors\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: Multi-Agent Design Patterns\n\ndesign-patterns/intro\ndesign-patterns/concurrent-agents\ndesign-patterns/sequential-workflow\ndesign-patterns/group-chat\ndesign-patterns/handoffs\ndesign-patterns/mixture-of-agents\ndesign-patterns/multi-agent-debate\ndesign-patterns/reflection\ndesign-patterns/code-execution-groupchat\n```\n\n```{toctree}\n:maxdepth: 1\n:hidden:\n:caption: More\n\ncookbook/index\nfaqs\n```\n\nAutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.\n\nKey features of AutoGen core include:\n\n```{gallery-grid}\n:grid-columns: 1 2 2 3\n\n- header: \"{fas}`network-wired;pst-color-primary` Asynchronous Messaging\"\n  content: \"Agents communicate through asynchronous messages, enabling event-driven and request/response communication models.\"\n- header: \"{fas}`cube;pst-color-primary` Scalable & Distributed\"\n  content: \"Enable complex scenarios with networks of agents across organizational boundaries.\"\n- header: \"{fas}`code;pst-color-primary` Multi-Language Support\"\n  content: \"Python & Dotnet interoperating agents today, with more languages coming soon.\"\n- header: \"{fas}`globe;pst-color-primary` Modular & Extensible\"\n  content: \"Highly customizable with features like custom agents, memory as a service, tools registry, and model library.\"\n- header: \"{fas}`puzzle-piece;pst-color-primary` Observable & Debuggable\"\n  content: \"Easily trace and debug your agent systems.\"\n- header: \"{fas}`project-diagram;pst-color-primary` Event-Driven Architecture\"\n  content: \"Build event-driven, distributed, scalable, and resilient AI agent systems.\"\n```",
    "filename": "python/docs/src/user-guide/core-user-guide/index.md"
  },
  {
    "content": "# Installation\n\n## Create a Virtual Environment (optional)\n\nWhen installing AgentChat locally, we recommend using a virtual environment for the installation. This will ensure that the dependencies for AgentChat are isolated from the rest of your system.\n\n``````{tab-set}\n\n`````{tab-item} venv\n\nCreate and activate:\n\nLinux/Mac:\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nWindows command-line:\n```batch\npython3 -m venv .venv\n.venv\\Scripts\\activate.bat\n```\n\nTo deactivate later, run:\n\n```bash\ndeactivate\n```\n\n`````\n\n`````{tab-item} conda\n\n[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.\n\n\nCreate and activate:\n\n```bash\nconda create -n autogen python=3.12\nconda activate autogen\n```\n\nTo deactivate later, run:\n\n```bash\nconda deactivate\n```\n\n\n`````\n\n\n\n``````\n\n## Install using pip\n\nInstall the `autogen-core` package using pip:\n\n```bash\n\npip install \"autogen-core\"\n```\n\n```{note}\nPython 3.10 or later is required.\n```\n\n## Install OpenAI for Model Client\n\nTo use the OpenAI and Azure OpenAI models, you need to install the following\nextensions:\n\n```bash\npip install \"autogen-ext[openai]\"\n```\n\nIf you are using Azure OpenAI with AAD authentication, you need to install the following:\n\n```bash\npip install \"autogen-ext[azure]\"\n```\n\n## Install Docker for Code Execution (Optional)\n\nWe recommend using Docker to use {py:class}`~autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor` for execution of model-generated code.\nTo install Docker, follow the instructions for your operating system on the [Docker website](https://docs.docker.com/get-docker/).\n\nTo learn more code execution, see [Command Line Code Executors](./components/command-line-code-executors.ipynb)\nand [Code Execution](./design-patterns/code-execution-groupchat.ipynb).",
    "filename": "python/docs/src/user-guide/core-user-guide/installation.md"
  },
  {
    "code": false,
    "content": "# Quick Start\n\n:::{note}\nFor installation instructions, see [here](installation).\n:::\n\n## Overview\n\nThis guide provides a quick introduction to the core APIs of the AutoGen framework. We will walk through a simple example involving two agents, `Modifier` and `Checker`, that count down from 10 to 1. The purpose of this example is to illustrate how agents communicate and how to manage their lifecycle using the provided infrastructure.\n\n## Defining Agent Classes\n\nIn our example, we define two agent classes: `Modifier`, which modifies a number, and `Checker`, which checks the modified value against a certain condition. Additionally, we define a `Message` data class that represents the messages exchanged between the agents.\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\n\n@dataclass\nclass Message:\n    content: int\n```\n\nIn this code block, we establish the basic building blocks of our application. We create a `Message` class, which consists of a single attribute `content`, representing the integer value being modified or checked. This forms the backbone of the messages that circulate between our agents.\n\n## Creating the Modifier Agent\n\nNext, we implement the `Modifier` agent, which will handle incoming messages and modify their content.\n\n```python\n@default_subscription\nclass Modifier(RoutedAgent):\n    def __init__(self, modify_val: Callable[[int], int]) -> None:\n        super().__init__(\"A modifier agent.\")\n        self._modify_val = modify_val\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        val = self._modify_val(message.content)\n        print(f\"{'-'*80}\\nModifier:\\nModified {message.content} to {val}\")\n        await self.publish_message(Message(content=val), DefaultTopicId())  # type: ignore\n```\n\nThe `Modifier` class inherits from `RoutedAgent`. It initializes with a function that defines how to modify the message content. The `handle_message` method is the core part of the agent, where it receives a message, processes its content, and publishes the modified message. The decoupling of the message processing logic from the message delivery allows for flexibility in how agents operate.\n\n## Creating the Checker Agent\n\nThe `Checker` agent verifies whether the modified value meets certain conditions.\n\n```python\n@default_subscription\nclass Checker(RoutedAgent):\n    def __init__(self, run_until: Callable[[int], bool]) -> None:\n        super().__init__(\"A checker agent.\")\n        self._run_until = run_until\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        if not self._run_until(message.content):\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} passed the check, continue.\")\n            await self.publish_message(Message(content=message.content), DefaultTopicId())\n        else:\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} failed the check, stopping.\")\n```\n\nLike the `Modifier`, the `Checker` class extends `RoutedAgent`. It is initialized with a condition-checking function. In the `handle_message` method, it evaluates the content of the received message. Depending on the condition, it either publishes the message for further processing or halts, demonstrating how agents regulate each other's actions.\n\n## Understanding the Agent Runtime\n\nThe messages' delivery and the agents' lifecycle management are handled by the Agent Runtime\u2014an essential component of the framework. The runtime allows agents to operate independently of message handling, enabling simpler and cleaner code.\n\nFollowing the definitions of our agents, we will now see how to register and execute these agents using the `SingleThreadedAgentRuntime`.\n\n```python\n# Code to run the agents in the runtime\nfrom autogen_core import AgentId, SingleThreadedAgentRuntime\n\n# Create a local embedded runtime.\nruntime = SingleThreadedAgentRuntime()\n\n# Register the modifier and checker agents with the runtime.\nawait Modifier.register(\n    runtime,\n    \"modifier\",\n    lambda: Modifier(modify_val=lambda x: x - 1),\n)\n\nawait Checker.register(\n    runtime,\n    \"checker\",\n    lambda: Checker(run_until=lambda x: x <= 1),\n)\n\n# Start the runtime and send a direct message to the checker.\nruntime.start()\nawait runtime.send_message(Message(10), AgentId(\"checker\", \"default\"))\nawait runtime.stop_when_idle()\n```\n\nIn this code block, we create an instance of `SingleThreadedAgentRuntime`, register the `Modifier` and `Checker` agents, and start the runtime. The adjustments made by the `Modifier` agent are sent to the `Checker` agent, initiating the countdown sequence from 10 to 1. The output generated by the agents will illustrate this decrementing process.\n\n## Conclusion\n\nAutoGen not only supports embedded single-threaded runtime but also facilitates a distributed runtime that allows agents to run across different processes or machines. This architecture supports various identities, languages, and dependencies, making it flexible for various applications.\n\nFor further exploration of agent runtime, message handling, and subscriptions, please continue reading the subsequent sections.",
    "filename": "python/docs/src/user-guide/core-user-guide/quickstart.ipynb"
  },
  {
    "code": false,
    "content": "# ACA Dynamic Sessions Code Executor Documentation\n\nThis guide provides an overview of the Azure Container Apps dynamic sessions and demonstrates how to use the `ACADynamicSessionsCodeExecutor` class for executing Python code in an Azure environment. This service allows users to run Python scripts in a hosted environment, with capabilities for managing sessions, uploading and downloading files, and executing code.\n\n## Overview of Azure Container Apps Dynamic Sessions\n\nAzure Container Apps dynamic sessions is a component of the Azure Container Apps service that facilitates the execution of code within a remote cloud environment. Users can leverage this service to run Python code in a pre-configured Jupyter environment. The environment comes pre-loaded with many commonly used Python packages, enhancing the ease of coding and data analysis.\n\nUsers can create custom environments tailored to their specific application needs, offering flexibility in handling a variety of applications. Moreover, they can upload files to the session and download results or output files once operations are completed.\n\n## Creating a Container Apps Session Pool\n\nTo start using the dynamic session executor, you first need to create a `Container App Session Pool`. This can be done through the Azure Portal or via the Azure CLI. \n\n1. **Azure Portal:** Navigate to your Azure portal and create a new `Container App Session Pool` resource. Ensure that you set the pool type to `Python code interpreter`. After creation, take note of the `Pool management endpoint`. The structure of this endpoint should resemble:\n   ```\n   https://{region}.dynamicsessions.io/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/sessionPools/{session_pool_name}\n   ```\n\n2. **Azure CLI:** Alternatively, you can create a session pool using the command line. Refer to the [official documentation](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter#create-a-session-pool-with-azure-cli) for detailed steps.\n\n## ACADynamicSessionsCodeExecutor Class\n\nThe `ACADynamicSessionsCodeExecutor` class is designed for executing Python code within an Azure Container App session. Here's a breakdown of its usage, starting from initialization.\n\n### Initialization of the Executor\n\nTo initialize an instance of the `ACADynamicSessionsCodeExecutor`, you need to create a credentialing object that implements the `TokenProvider` interface. This allows the `executor` to authenticate and manage session states. A common choice for this is the `azure.identity.DefaultAzureCredential` class.\n\nFirst, ensure you have the required library installed:\n```bash\npip install azure.identity\n```\n\nNext, import the necessary modules as shown below:\n```python\nimport os\nimport tempfile\nfrom anyio import open_file\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.azure import ACADynamicSessionsCodeExecutor\nfrom azure.identity import DefaultAzureCredential\n```\n\nOnce the imports are done, you can create an executor instance and run a simple test code to verify functionality:\n```python\ncancellation_token = CancellationToken()\nPOOL_MANAGEMENT_ENDPOINT = \"...\"\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, \n        credential=DefaultAzureCredential(), \n        work_dir=temp_dir\n    )\n\n    code_blocks = [CodeBlock(code=\"import sys; print('hello world!')\", language=\"python\")]\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0 and \"hello world!\" in code_result.output\n```\nThis snippet initializes the executor and confirms that a basic Python script runs successfully.\n\n## File Uploading and Verification\n\nThe executor allows you to upload files into the serverless code interpreter environment. Uploaded files will reside in the `/mnt/data` directory, which also serves as the default working directory.\n\nHere's how to upload files and verify their presence:\n```python\nwith tempfile.TemporaryDirectory() as temp_dir:\n    test_file_1 = \"test_upload_1.txt\"\n    test_file_1_contents = \"test1 contents\"\n    test_file_2 = \"test_upload_2.txt\"\n    test_file_2_contents = \"test2 contents\"\n\n    async with await open_file(os.path.join(temp_dir, test_file_1), \"w\") as f:\n        await f.write(test_file_1_contents)\n    async with await open_file(os.path.join(temp_dir, test_file_2), \"w\") as f:\n        await f.write(test_file_2_contents)\n\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, \n        credential=DefaultAzureCredential(), \n        work_dir=temp_dir\n    )\n    await executor.upload_files([test_file_1, test_file_2], cancellation_token)\n\n    file_list = await executor.get_file_list(cancellation_token)\n    assert test_file_1 in file_list\n    assert test_file_2 in file_list\n```\nThis block of code demonstrates how to create two test files, upload them, and verify that they exist in the session.\n\nAdditionally, you can execute code that reads these files:\n```python\ncode_blocks = [\n    CodeBlock(\n        code=f\"\"\"\nwith open(\"{test_file_1}\") as f:\n  print(f.read())\nwith open(\"{test_file_2}\") as f:\n  print(f.read())\n\"\"\",\n        language=\"python\",\n    )\n]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code == 0\nassert test_file_1_contents in code_result.output\nassert test_file_2_contents in code_result.output\n```\nThis code reads the uploaded files and confirms their content matches expectations.\n\n## File Downloading\n\nThe process for downloading files is similar to uploading. First, ensure that the desired files are created within the session, and then initiate a download.\n\nHere's how it works:\n```python\nwith tempfile.TemporaryDirectory() as temp_dir:\n    test_file_1 = \"test_upload_1.txt\"\n    test_file_1_contents = \"test1 contents\"\n    test_file_2 = \"test_upload_2.txt\"\n    test_file_2_contents = \"test2 contents\"\n\n    executor = ACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, \n        credential=DefaultAzureCredential(), \n        work_dir=temp_dir\n    )\n\n    code_blocks = [\n        CodeBlock(\n            code=f\"\"\"\nwith open(\"{test_file_1}\", \"w\") as f:\n  f.write(\"{test_file_1_contents}\")\nwith open(\"{test_file_2}\", \"w\") as f:\n  f.write(\"{test_file_2_contents}\")\n\"\"\",\n            language=\"python\",\n        ),\n    ]\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0\n\n    await executor.download_files([test_file_1, test_file_2], cancellation_token)\n    \n    assert os.path.isfile(os.path.join(temp_dir, test_file_1))\n    async with await open_file(os.path.join(temp_dir, test_file_1), \"r\") as f:\n        content = await f.read()\n        assert test_file_1_contents in content\n```\nAfter downloading, this code snippet verifies the integrity of the downloaded files against the expected contents.\n\n## Managing Sessions\n\nEach instance of the `ACADynamicSessionsCodeExecutor` class operates in a unique session. Code executed within that instance shares the same context until the `restart` method is called, which resets the session.\n\nTo illustrate this, here\u2019s how to execute and restart a session:\n```python\nexecutor = ACADynamicSessionsCodeExecutor(\n    pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, \n    credential=DefaultAzureCredential()\n)\n\ncode_blocks = [CodeBlock(code=\"x = 'abcdefg'\", language=\"python\")]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code == 0\n\nawait executor.restart()\ncode_blocks = [CodeBlock(code=\"print(x)\", language=\"python\")]\ncode_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\nassert code_result.exit_code != 0 and \"NameError\" in code_result.output\n```\nIn this example, after the restart, any previously defined variables are no longer accessible.\n\n## Available Packages\n\nWhen utilizing the `ACADynamicSessionsCodeExecutor`, it is essential to be aware of the available libraries. While the environment comes with many pre-installed packages, you may wish to check which ones are available for your code execution needs.\n\nTo obtain a list of available packages, you can use:\n```python\nprint(await executor.get_available_packages(cancellation_token))\n```\nThis command retrieves the currently available packages in the execution environment.\n\n## Conclusion\n\nThe `ACADynamicSessionsCodeExecutor` facilitates seamless code execution in an Azure Container Apps environment. By following this documentation, you should be equipped to create session pools, execute code, manage file uploads/downloads, and maintain reliable sessions.\n\nFor further details, refer to the official [Microsoft Azure documentation](https://learn.microsoft.com/en-us/azure/container-apps/sessions).",
    "filename": "python/docs/src/user-guide/extensions-user-guide/azure-container-code-executor.ipynb"
  },
  {
    "code": false,
    "content": "# Azure AI Foundry Agent\n\nIn AutoGen, you have the capability to create and deploy agents powered by the [Azure AI Foundry Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview). This service simplifies the management of essential agent components such as the provisioned model, tools (e.g., code interpreter, Bing search grounding, file search), observability, and security. As a result, you can concentrate on developing your agent without being burdened by the underlying infrastructure.\n\nThis guide will walk you through creating an Azure AI Foundry Agent using the `AzureAIAgent` class to address tasks efficiently using the Azure Grounding with Bing Search tool.\n\n## Installation\n\nBefore diving into creating your agent, you need to ensure that the necessary packages are installed. You can install the required package using the following command:\n\n```python\n# pip install \"autogen-ext[azure]\"  # For Azure AI Foundry Agent Service\n```\n\nThis command installs the `autogen-ext` package along with Azure integrations, enabling the functionalities needed to work with Azure Foundry Agents.\n\n## Bing Search Grounding\n\nThe [Grounding with Bing Search](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding?tabs=python&pivots=overview#setup) feature allows your Azure AI Agents to incorporate real-time public web data when generating responses. You will first need to create a Grounding with Bing Search resource and connect it to your Azure AI Agents.\n\nWhen a user submits a query, the Azure AI Agent evaluates whether to utilize Bing Search. If the tool is implemented, it conducts a search over public web data and retrieves relevant sections. The agent then constructs a response based on these returned chunks.\n\n## Prerequisites\n\nBefore proceeding, ensure that you meet the following prerequisites:\n\n- An Azure subscription is required.\n- The Azure CLI must be installed and configured. You can log in using the command `az login` to enable default credentials.\n- Install the `autogen-ext[azure]` package.\n\nTo create a Grounding with Bing Search resource, you can do so through the [Azure portal](https://portal.azure.com/#create/Microsoft.BingGroundingSearch). Ensure you have the owner or contributor role in your subscription or resource group to create the resource. After the resource is created, you can pass it to the Azure Foundry Agent using its name.\n\n## Example: Creating an Azure Foundry Agent with Bing Search\n\nIn the following example, we will create a new Azure Foundry Agent that utilizes the Grounding with Bing Search resource. The example demonstrates how to initialize the agent and how it handles user messages.\n\n```python\nimport os\n\nimport dotenv\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure import AzureAIAgent\nfrom azure.ai.agents.models import BingGroundingTool\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\n\ndotenv.load_dotenv()\n```\n\nHere, we import necessary libraries, including Azure specific modules and load environment variables from a `.env` file.\n\n### Initializing the Azure Foundry Agent\n\nThe following code snippet demonstrates how to initialize the Azure Foundry Agent with Bing Search grounding.\n\n```python\nasync def bing_example() -> None:\n    async with DefaultAzureCredential() as credential:  # type: ignore\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\"))\n\n            bing_tool = BingGroundingTool(conn.id)\n            agent_with_bing_grounding = AzureAIAgent(\n                name=\"bing_agent\",\n                description=\"An AI assistant with Bing grounding\",\n                project_client=project_client,\n                deployment_name=\"gpt-4o\",\n                instructions=\"You are a helpful assistant.\",\n                tools=bing_tool.definitions,\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n```\n\nIn this code block, we utilize `DefaultAzureCredential` to authenticate and access the AI Project Client. The agent is initialized with a Bing grounding tool, which enables it to leverage Bing for real-time data retrieval.\n\n### Sending Messages to the Agent\n\nTo send a query to the agent, you can use the following code, which allows the agent to process user input and return a response.\n\n```python\n            result = await agent_with_bing_grounding.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"What is Microsoft's annual leave policy? Provide citations for your answers.\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n                message_limit=5,\n            )\n            print(result)\n```\n\nIn this segment, the agent receives a message asking for information about Microsoft's annual leave policy, with a request for citations. Upon processing, the agent will return a structured response based on the query and retrieved data.\n\n### Conclusion\n\n```python\nawait bing_example()\n```\n\nThe `await bing_example()` statement runs the defined asynchronous function to create and interact with the Azure AI Foundry Agent. \n\nYou can enhance your application by providing additional Azure-backed [tools](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/overview) as well as local client-side functions to the agent. For further details on using the `AzureAIAgent` class, please refer to the API documentation provided.",
    "filename": "python/docs/src/user-guide/extensions-user-guide/azure-foundry-agent.ipynb"
  },
  {
    "content": "# Creating your own extension\n\nWith the new package structure in 0.4, it is easier than ever to create and publish your own extension to the AutoGen ecosystem. This page details some best practices so that your extension package  integrates well with the AutoGen ecosystem.\n\n## Best practices\n\n### Naming\n\nThere is no requirement about naming. But prefixing the package name with `autogen-` makes it easier to find.\n\n### Common interfaces\n\nWhenever possible, extensions should implement the provided interfaces from the `autogen_core` package. This will allow for a more consistent experience for users.\n\n#### Dependency on AutoGen\n\nTo ensure that the extension works with the version of AutoGen that it was designed for, it is recommended to specify the version of AutoGen the dependency section of the `pyproject.toml` with adequate constraints.\n\n```toml\n[project]\n# ...\ndependencies = [\n    \"autogen-core>=0.4,<0.5\"\n]\n```\n\n### Usage of typing\n\nAutoGen embraces the use of type hints to provide a better development experience. Extensions should use type hints whenever possible.\n\n## Discovery\n\nTo make it easier for users to find your extension, sample, service or package, you can [add the topic](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/classifying-your-repository-with-topics) [`autogen`](https://github.com/topics/autogen) to the GitHub repo.\n\nMore specific topics are also available:\n\n- [`autogen-extension`](https://github.com/topics/autogen-extension) for extensions\n- [`autogen-sample`](https://github.com/topics/autogen-sample) for samples\n\n## Changes from 0.2\n\nIn AutoGen 0.2 it was common to merge 3rd party extensions and examples into the main repo. We are super appreciative of all of the users who have contributed to the ecosystem notebooks, modules and pages in 0.2. However, in general we are moving away from this model to allow for more flexibility and to reduce maintenance burden.\n\nThere is the `autogen-ext` package for 1st party supported extensions, but we want to be selective to manage maintenance load. If you would like to see if your extension makes sense to add into `autogen-ext`, please open an issue and let's discuss. Otherwise, we encourage you to publish your extension as a separate package and follow the guidance under [discovery](#discovery) to make it easy for users to find.",
    "filename": "python/docs/src/user-guide/extensions-user-guide/create-your-own.md"
  },
  {
    "content": "# Discover community projects\n\n::::{grid} 1 2 2 2\n:margin: 4 4 0 0\n:gutter: 1\n\n:::{grid-item-card} {fas}`globe;pst-color-primary` <br> Ecosystem\n:link: https://github.com/topics/autogen\n:link-alt: Ecosystem: Find samples, services and other things that work with AutoGen\n:class-item: api-card\n:columns: 12\n\nFind samples, services and other things that work with AutoGen\n\n:::\n\n:::{grid-item-card} {fas}`puzzle-piece;pst-color-primary` <br> Community Extensions\n:link: https://github.com/topics/autogen-extension\n:link-alt: Community Extensions: Find AutoGen extensions for 3rd party tools, components and services\n:class-item: api-card\n\nFind AutoGen extensions for 3rd party tools, components and services\n\n:::\n\n:::{grid-item-card} {fas}`vial;pst-color-primary` <br> Community Samples\n:link: https://github.com/topics/autogen-sample\n:link-alt: Community Samples: Find community samples and examples of how to use AutoGen\n:class-item: api-card\n\nFind community samples and examples of how to use AutoGen\n\n:::\n\n::::\n\n\n## List of community projects\n\n| Name | Package | Description |\n|---|---|---|\n| [autogen-watsonx-client](https://github.com/tsinggggg/autogen-watsonx-client)  | [PyPi](https://pypi.org/project/autogen-watsonx-client/) | Model client for [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) |\n| [autogen-openaiext-client](https://github.com/vballoli/autogen-openaiext-client)  | [PyPi](https://pypi.org/project/autogen-openaiext-client/) | Model client for other LLMs like Gemini, etc. through the OpenAI API |\n| [autogen-ext-mcp](https://github.com/richard-gyiko/autogen-ext-mcp) | [PyPi](https://pypi.org/project/autogen-ext-mcp/) | Tool adapter for Model Context Protocol server tools |\n| [autogen-ext-email](https://github.com/masquerlin/autogen-ext-email) | [PyPi](https://pypi.org/project/autogen-ext-email/) | A Email agent for generating email and sending |\n| [autogen-oaiapi](https://github.com/SongChiYoung/autogen-oaiapi)  | [PyPi](https://pypi.org/project/autogen-oaiapi/) | an OpenAI-style API server built on top of AutoGen |\n| [autogen-contextplus](https://github.com/SongChiYoung/autogen-contextplus)  | [PyPi](https://pypi.org/project/autogen-contextplus/) | Enhanced model_context implementations, with features such as automatic summarization and truncation of model context. |\n| [autogen-ext-yepcode](https://github.com/yepcode/autogen-ext-yepcode)  | [PyPi](https://pypi.org/project/autogen-ext-yepcode/) | Enables agents to securely execute code in isolated remote sandboxes using [YepCode](https://yepcode.io)\u2019s serverless runtime. |\n\n\n<!-- Example -->\n<!-- | [My Model Client](https://github.com/example)  | [PyPi](https://pypi.org/project/example) | Model client for my custom model service | -->\n<!-- - Name should link to the project page or repo\n- Package should link to the PyPi page\n- Description should be a brief description of the project. 1 short sentence is ideal. -->",
    "filename": "python/docs/src/user-guide/extensions-user-guide/discover.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AutoGen Extensions, a framework for building multi-agent applications with AI agents.\n"
      }
    },
    "content": "# Extensions\n\n```{toctree}\n:maxdepth: 3\n:hidden:\n\ninstallation\ndiscover\ncreate-your-own\n```\n\n```{toctree}\n:maxdepth: 3\n:hidden:\n:caption: Guides\n\nazure-container-code-executor\nazure-foundry-agent\n```\n\nAutoGen is designed to be extensible. The `autogen-ext` package contains the built-in component implementations maintained by the AutoGen project.\n\nExamples of components include:\n\n- `autogen_ext.agents.*` for agent implementations like {py:class}`~autogen_ext.agents.web_surfer.MultimodalWebSurfer`\n- `autogen_ext.models.*` for model clients like {py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient` and {py:class}`~autogen_ext.models.semantic_kernel.SKChatCompletionAdapter` for connecting to hosted and local models.\n- `autogen_ext.tools.*` for tools like GraphRAG {py:class}`~autogen_ext.tools.graphrag.LocalSearchTool` and {py:func}`~autogen_ext.tools.mcp.mcp_server_tools`.\n- `autogen_ext.executors.*` for executors like {py:class}`~autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor` and {py:class}`~autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor`\n- `autogen_ext.runtimes.*` for agent runtimes like {py:class}`~autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime`\n\nSee [API Reference](../../reference/index.md) for the full list of components and their APIs.\n\nWe strongly encourage developers to build their own components and publish them as part of the ecosytem.\n\n::::{grid} 2 2 2 2\n:gutter: 3\n\n:::{grid-item-card} {fas}`magnifying-glass;pst-color-primary` Discover\n:link: ./discover.html\n:link-alt: Discover: Discover community extensions and samples\n\nDiscover community extensions and samples\n:::\n\n:::{grid-item-card} {fas}`code;pst-color-primary` Create your own\n:link: ./create-your-own.html\n:link-alt: Create your own: Create your own extension\n\nCreate your own extension\n:::\n::::",
    "filename": "python/docs/src/user-guide/extensions-user-guide/index.md"
  },
  {
    "myst": {
      "html_meta": {
        "description lang=en": "User Guide for AutoGen Extensions, a framework for building multi-agent applications with AI agents.\n"
      }
    },
    "content": "# Installation\n\nFirst-part maintained extensions are available in the `autogen-ext` package.\n\n```sh\npip install \"autogen-ext\"\n```\n\nExtras:\n\n- `langchain` needed for {py:class}`~autogen_ext.tools.langchain.LangChainToolAdapter`\n- `azure` needed for {py:class}`~autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor`\n- `docker` needed for {py:class}`~autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor`\n- `openai` needed for {py:class}`~autogen_ext.models.openai.OpenAIChatCompletionClient`",
    "filename": "python/docs/src/user-guide/extensions-user-guide/installation.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script is designed to fix import statements in a set of Python files located in a specific directory relative to the script itself. The modification involves changing certain import lines to use relative imports. The script accomplishes this by reading each specified file, replacing the designated lines in the code, and then writing the modified content back to the same file.\n\n## Dependencies\n\nThe script utilizes the following modules:\n- `pathlib`: This module provides an object-oriented approach to working with filesystem paths.\n- `typing`: Specifically, it uses `Dict` from the `typing` module to denote a dictionary type for substitutions.\n\n## Path Configuration\n\nThe section of the code below defines the location of the current script:\n\n```python\nthis_file_dir = Path(__file__).parent\n```\n\nHere, `Path(__file__).parent` is used to determine the directory containing the script, allowing the subsequent file paths to be constructed relative to it.\n\n## Target Files\n\nNext, the script lists out a series of target files which are expected to contain the import statements that need fixing:\n\n```python\nfiles = [\n    this_file_dir / \"packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/agent_worker_pb2_grpc.py\",\n    ...\n]\n```\n\nThis list comprehensively details the full paths to Python files that require modification. Each file path is generated by appending the relevant filepath to `this_file_dir`.\n\n## Import Substitutions\n\nA dictionary named `substitutions` specifies how the old import statements should be replaced:\n\n```python\nsubstitutions: Dict[str, str] = {\n    \"\\nimport agent_worker_pb2 as agent__worker__pb2\\n\": \"\\nfrom . import agent_worker_pb2 as agent__worker__pb2\\n\",\n    ...\n}\n```\n\nKey-value pairs in this dictionary map the original import statement (the key) to the modified version using relative imports (the value). This allows for streamlined editing later in the script.\n\n## Main Function\n\nThe core functionality of the script is contained within the `main` function:\n\n```python\ndef main():\n    for file in files:\n        ...\n```\n\nThis function iterates over each file listed in the `files` array. The function is responsible for reading, modifying, and writing back to each file.\n\n### Step-by-Step Process\n\n1. **File Reading**: Each file is opened for reading, and its content is read into a string:\n   \n   ```python\n   with open(file, \"r\") as f:\n       content = f.read()\n   ```\n\n2. **Printing Status**: The script prints the name of the current file being processed to provide feedback on its progress:\n   \n   ```python\n   print(\"Fixing imports in file:\", file)\n   ```\n\n3. **Replacing Content**: The script performs the substitutions defined in the `substitutions` dictionary by iterating over each old key and its corresponding new value. It replaces occurrences of the old import with the new import in the content string:\n   \n   ```python\n   for old, new in substitutions.items():\n       content = content.replace(old, new)\n   ```\n\n4. **File Writing**: After making the necessary modifications, the script opens the file again, this time for writing, and it writes the updated content back to the file:\n   \n   ```python\n   with open(file, \"w\") as f:\n       f.write(content)\n   ```\n\n## Conclusion\n\nThe script automates the task of updating import statements in a series of Python files. This is especially useful when changing from absolute to relative imports in a package. By reading and overwriting the contents of the files, the script ensures that the changes are applied directly, facilitating smoother use of the modified files in larger projects. Ensure that this script is executed in a context where the file paths are correct and accessible.",
    "filename": "python/fixup_generated_files.py"
  },
  {
    "content": "# Contributing to AutoGenBench\n\nAs part of the broader AutoGen project, AutoGenBench welcomes community contributions. Contributions are subject to AutoGen's [contribution guidelines](https://microsoft.github.io/autogen/docs/Contribute), as well as a few additional AutoGenBench-specific requirements outlined here. You may also wish to develop your own private benchmark scenarios and the guidance in this document will help with such efforts as well. Below you will find the general requirements, followed by a detailed technical description.\n\n## General Contribution Requirements\nWe ask that all contributions to AutoGenBench adhere to the following:\n\n- Follow AutoGen's broader [contribution guidelines](https://microsoft.github.io/autogen/docs/Contribute)\n- All AutoGenBench benchmarks should live in a subfolder of `/benchmarks` alongside `HumanEval`, `GAIA`, etc.\n- Benchmark scenarios should include a detailed README.md, in the root of their folder, describing the benchmark and providing citations where warranted.\n- Benchmark data (tasks, ground truth, etc.) should be downloaded from their original sources rather than hosted in the AutoGen repository (unless the benchmark is original, and the repository *is* the original source)\n    - You can use the `Scripts/init_tasks.py` file to automate this download.\n- Basic scoring should be compatible with the `agbench tabulate` command (e.g., by outputting logs compatible with the default tabulation mechanism, or by providing a `Scripts/custom_tabulate.py` file)\n\nThese requirements are further detailed below, but if you simply copy the `HumanEval` folder, you will already be off to a great start.\n\n## Implementing and Running Benchmark Tasks\nAt the core of any benchmark is a set of tasks. To implement tasks that are runnable by AutoGenBench, you must adhere to AutoGenBench's templating and scenario expansion algorithms, as outlined below.\n\n### Task Definitions\n\nAll tasks are stored in JSONL files (in subdirectories under `./Tasks`). Each line of a tasks file is a JSON object with the following schema:\n\n```\n{\n   \"id\": string,\n   \"template\": dirname,\n   \"substitutions\" {\n       \"filename1\": {\n       \t   \"find_string1_1\": replace_string1_1,\n           \"find_string1_2\": replace_string1_2,\n           ...\n           \"find_string1_M\": replace_string1_N\n       }\n       \"filename2\": {\n       \t   \"find_string2_1\": replace_string2_1,\n           \"find_string2_2\": replace_string2_2,\n           ...\n           \"find_string2_N\": replace_string2_N\n       }\n   }\n}\n```\n\nFor example:\n\n```\n{\n    \"id\": \"two_agent_stocks_gpt4\",\n    \"template\": \"default_two_agents\",\n    \"substitutions\": {\n\t\"scenario.py\": {\n            \"__MODEL__\": \"gpt-4\",\n\t},\n\t\"prompt.txt\": {\n            \"__PROMPT__\": \"Plot and save to disk a chart of NVDA and TESLA stock price YTD.\"\n        }\n    }\n}\n```\n\nIn this example, the string `__MODEL__` will be replaced in the file `scenarios.py`, while the string `__PROMPT__` will be replaced in the `prompt.txt` file.\n\nThe `template` field can also take on a list value, but this usage is considered advanced and is not described here. See the `agbench/run_cmd.py` code, or the `GAIA` benchmark tasks files for additional information about this option.\n\n\n## Task Instance Expansion Algorithm\n\nOnce the tasks have been defined, as per above, they must be \"instantiated\" before they can be run. This instantiation happens automatically when the user issues the `agbench run` command and involves creating a local folder to share with Docker. Each instance and repetition gets its own folder along the path: `./results/[scenario]/[task_id]/[instance_id]`. For the sake of brevity we will refer to this folder as the `DEST_FOLDER`.\n\nThe algorithm for populating the `DEST_FOLDER` is as follows:\n\n1. Pre-populate DEST_FOLDER with all the basic starter files for running a scenario (found in `agbench/template`).\n2. Recursively copy the template folder specified in the JSONL line to DEST_FOLDER (if the JSON `template` attribute points to a folder) If the JSONs `template` attribute instead points to a file, copy the file, but rename it to `scenario.py`\n3. Apply any string replacements, as outlined in the prior section.\n4. Write a run.sh file to DEST_FOLDER that will be executed by Docker when it is loaded. The `run.sh` is described below.\n\n## Scenario Execution Algorithm\n\nOnce the task has been instantiated it is run (via run.sh). This script will execute the following steps:\n\n1. If a file named `global_init.sh` is present, run it.\n2. If a file named `scenario_init.sh` is present, run it.\n3. Install the requirements.txt file (if running in Docker)\n4. Run the task via `python scenario.py`\n5. If the scenario.py exited cleanly (exit code 0), then print \"SCENARIO.PY COMPLETE !#!#\"\n6. Clean up (delete cache, etc.)\n7. If a file named `scenario_finalize.sh` is present, run it.\n8. If a file named `global_finalize.sh` is present, run it.\n9. echo \"RUN COMPLETE !#!#\", signaling that all steps completed.\n\nNotably, this means that scenarios can add custom init and teardown logic by including `scenario_init.sh` and `scenario_finalize.sh` files.\n\nAt the time of this writing, the run.sh file is as follows:\n\n```sh\nexport AUTOGEN_TESTBED_SETTING=\"Docker\"\numask 000\n\n# Run the global init script if it exists\nif [ -f global_init.sh ] ; then\n    . ./global_init.sh\nfi\n\n# Run the scenario init script if it exists\nif [ -f scenario_init.sh ] ; then\n    . ./scenario_init.sh\nfi\n\n# Run the scenario\npip install -r requirements.txt\npython scenario.py\nEXIT_CODE=$?\nif [ $EXIT_CODE -ne 0 ]; then\n    echo SCENARIO.PY EXITED WITH CODE: $EXIT_CODE !#!#\nelse\n    echo SCENARIO.PY COMPLETE !#!#\nfi\n\n# Clean up\nif [ -d .cache ] ; then\n    rm -Rf .cache\nfi\n\n# Run the scenario finalize script if it exists\nif [ -f scenario_finalize.sh ] ; then\n    . ./scenario_finalize.sh\nfi\n\n# Run the global finalize script if it exists\nif [ -f global_finalize.sh ] ; then\n    . ./global_finalize.sh\nfi\n\necho RUN.SH COMPLETE !#!#\n```\n\nBe warned that this listing is provided here for illustration purposes, and may vary over time. The source of truth are the `run.sh` files found in the ``./results/[taskset]/[task_id]/[instance_id]`` folders.\n\n\n## Integrating with the `tabulate` \nThe above details are sufficient for defining and running tasks, but if you wish to support the `agbench tabulate`  commands, a few additional steps are required.\n\n### Tabulations\n\nIf you wish to leverage the default tabulation logic, it is as simple as arranging your `scenario.py` file to output the string \"ALL TESTS PASSED !#!#\" to the console in the event that a task was solved correctly.\n\nIf you wish to implement your own tabulation logic, simply create the file `Scripts/custom_tabulate.py` and include a `main(args)` method. Here, the `args` parameter will be provided by AutoGenBench, and is a drop-in replacement for `sys.argv`. In particular, `args[0]` will be the invocation command (similar to the executable or script name in `sys.argv`), and the remaining values (`args[1:]`) are the command line parameters.\n\nShould you provide a custom tabulation script, please implement `--help` and `-h` options for documenting your interface.\n\nThe `scenarios/GAIA/Scripts/custom_tabulate.py` is a great example of custom tabulation. It also shows how you can reuse some components of the default tabulator to speed up development.\n\n\n\n## Scripts/init_tasks.py\nFinally, you should provide an `Scripts/init_tasks.py` file, in your benchmark folder, and include a `main()` method therein. \n\nThis `init_tasks.py` script is a great place to download benchmarks from their original sources and convert them to the JSONL format required by AutoGenBench:\n- See `HumanEval/Scripts/init_tasks.py` for an example of how to expand a benchmark from an original GitHub repository.\n- See `GAIA/Scripts/init_tasks.py` for an example of how to expand a benchmark from `Hugging Face Hub`.",
    "filename": "python/packages/agbench/CONTRIBUTING.md"
  },
  {
    "content": "# AutoGenBench\n\nAutoGenBench (agbench) is a tool for repeatedly running a set of pre-defined AutoGen tasks in a setting with tightly-controlled initial conditions. With each run, AutoGenBench will start from a blank slate. The agents being evaluated will need to work out what code needs to be written, and what libraries or dependencies to install, to solve tasks. The results of each run are logged, and can be ingested by analysis or metrics scripts (such as `agbench tabulate`). By default, all runs are conducted in freshly-initialized docker containers, providing the recommended level of consistency and safety.\n\nAutoGenBench works with all AutoGen 0.1.*, and 0.2.* versions.\n\n## Technical Specifications\n\nIf you are already an AutoGenBench pro, and want the full technical specifications, please review the [contributor&#39;s guide](CONTRIBUTING.md).\n\n## Docker Requirement\n\nAutoGenBench also requires Docker (Desktop or Engine). **It will not run in GitHub codespaces**, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/).\n\nIf you are working in WSL, you can follow the instructions below to set up your environment:\n\n1. Install Docker Desktop. After installation, restart is needed, then open Docker Desktop, in Settings, Ressources, WSL Integration, Enable integration with additional distros \u2013 Ubuntu\n2. Clone autogen and export `AUTOGEN_REPO_BASE`. This environment variable enables the Docker containers to use the correct version agents.\n    ```bash\n    git clone git@github.com:microsoft/autogen.git\n    export AUTOGEN_REPO_BASE=<path_to_autogen>\n    ```\n\n## Installation and Setup\n\n[Deprecated currently] **To get the most out of AutoGenBench, the `agbench` package should be installed**. At present, the easiest way to do this is to install it via `pip`.\n\n\nIf you would prefer working from source code (e.g., for development, or to utilize an alternate branch), simply clone the [AutoGen](https://github.com/microsoft/autogen) repository, then install `agbench` via:\n\n```\npip install -e autogen/python/packages/agbench\n```\n\nAfter installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter described later.\n\nIf you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:\n\n```\nexport OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)\n```\n\nIf an OAI_CONFIG_LIST is *not* provided (by means of file or environment variable), AutoGenBench will use the OPENAI_API_KEY environment variable instead.\n\nFor some benchmark scenarios, additional keys may be required (e.g., keys for the Bing Search API). These can be added to an `ENV.json` file in the current working folder. An example `ENV.json` file is provided below:\n\n```\n{\n    \"BING_API_KEY\": \"xxxyyyzzz\"\n}\n```\n\n## A Typical Session\n\nOnce AutoGenBench and necessary keys are installed, a typical session will look as follows:\n\n\n\nNavigate to HumanEval\n\n```bash\ncd autogen/python/packages/agbench/benchmarks/HumanEval\n```\n**Note:** The following instructions are specific to the HumanEval benchmark. For other benchmarks, please refer to the README in the respective benchmark folder, e.g.,: [AssistantBench](benchmarks/AssistantBench/README.md).\n\n\nCreate a file called ENV.json with the following (required) contents (If you're using MagenticOne), if using Azure:\n\n```json\n{\n    \"CHAT_COMPLETION_KWARGS_JSON\": \"{}\",\n    \"CHAT_COMPLETION_PROVIDER\": \"azure\"\n}\n```\n\nYou can also use the openai client by replacing the last two entries in the ENV file by:\n\n- `CHAT_COMPLETION_PROVIDER='openai'`\n- `CHAT_COMPLETION_KWARGS_JSON` with the following JSON structure:\n\n```json\n{\n  \"api_key\": \"REPLACE_WITH_YOUR_API\",\n  \"model\": \"REPLACE_WITH_YOUR_MODEL\"\n}\n```\n\nNow initialize the tasks.\n\n```bash\npython Scripts/init_tasks.py\n```\n\nNote: This will attempt to download HumanEval\n\n\nOnce the script completes, you should now see a folder in your current directory called `Tasks` that contains one JSONL file per template in `Templates`.\n\nNow to run a specific subset of HumanEval use:\n\n```bash\nagbench run Tasks/human_eval_MagenticOne.jsonl\n```\n\nYou should see the command line print the raw logs that shows the agents in action To see a summary of the results (e.g., task completion rates), in a new terminal run the following:\n\n```bash\nagbench tabulate Results/human_eval_MagenticOne\n```\n\nWhere:\n\n- `agbench run Tasks/human_eval_MagenticOne.jsonl` runs the tasks defined in `Tasks/human_eval_MagenticOne.jsonl`\n- `agbench tablue results/human_eval_MagenticOne` tabulates the results of the run\n\nEach of these commands has extensive in-line help via:\n\n- `agbench --help`\n- `agbench run --help`\n- `agbench tabulate --help`\n- `agbench remove_missing --help`\n\n**NOTE:** If you are running `agbench` from within the repository, you need to navigate to the appropriate scenario folder (e.g., `scenarios/HumanEval`) and run the `Scripts/init_tasks.py` file.\n\nMore details of each command are provided in the sections that follow.\n\n\n## Running AutoGenBench\n\nTo run a benchmark (which executes the tasks, but does not compute metrics), simply execute:\n\n```\ncd [BENCHMARK]\nagbench run Tasks/*.jsonl\n```\n\nFor example,\n\n```\ncd HumanEval\nagbench run Tasks/human_eval_MagenticOne.jsonl\n```\n\nThe default is to run each task once. To run each scenario 10 times, use:\n\n```\nagbench run --repeat 10 Tasks/human_eval_MagenticOne.jsonl\n```\n\nThe `agbench` command-line tool allows a number of command-line arguments to control various parameters of execution. Type ``agbench -h`` to explore these options:\n\n```\n'agbench run' will run the specified autogen scenarios for a given number of repetitions and record all logs and trace information. When running in a Docker environment (default), each run will begin from a common, tightly controlled, environment. The resultant logs can then be further processed by other scripts to produce metrics.\n\npositional arguments:\n  scenario      The JSONL scenario file to run. If a directory is specified,\n                then all JSONL scenarios in the directory are run. (default:\n                ./scenarios)\n\noptions:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        The environment variable name or path to the OAI_CONFIG_LIST (default: OAI_CONFIG_LIST).\n  -r REPEAT, --repeat REPEAT\n                        The number of repetitions to run for each scenario (default: 1).\n  -s SUBSAMPLE, --subsample SUBSAMPLE\n                        Run on a subsample of the tasks in the JSONL file(s). If a decimal value is specified, then run on\n                        the given proportion of tasks in each file. For example \"0.7\" would run on 70% of tasks, and \"1.0\"\n                        would run on 100% of tasks. If an integer value is specified, then randomly select *that* number of\n                        tasks from each specified JSONL file. For example \"7\" would run tasks, while \"1\" would run only 1\n                        task from each specified JSONL file. (default: 1.0; which is 100%)\n  -m MODEL, --model MODEL\n                        Filters the config_list to include only models matching the provided model name (default: None, which\n                        is all models).\n  --requirements REQUIREMENTS\n                        The requirements file to pip install before running the scenario.\n  -d DOCKER_IMAGE, --docker-image DOCKER_IMAGE\n                        The Docker image to use when running scenarios. Can not be used together with --native. (default:\n                        'agbench:default', which will be created if not present)\n  --native              Run the scenarios natively rather than in docker. NOTE: This is not advisable, and should be done\n                        with great caution.\n```\n\n## Results\n\nBy default, the AutoGenBench stores results in a folder hierarchy with the following template:\n\n``./results/[scenario]/[task_id]/[instance_id]``\n\nFor example, consider the following folders:\n\n``./results/default_two_agents/two_agent_stocks/0``\n``./results/default_two_agents/two_agent_stocks/1``\n\n...\n\n``./results/default_two_agents/two_agent_stocks/9``\n\nThis folder holds the results for the ``two_agent_stocks`` task of the ``default_two_agents`` tasks file. The ``0`` folder contains the results of the first instance / run. The ``1`` folder contains the results of the second run, and so on. You can think of the _task_id_ as mapping to a prompt, or a unique set of parameters, while the _instance_id_ defines a specific attempt or run.\n\nWithin each folder, you will find the following files:\n\n- *timestamp.txt*: records the date and time of the run, along with the version of the autogen-agentchat library installed\n- *console_log.txt*: all console output produced by Docker when running AutoGen. Read this like you would a regular console.\n- *[agent]_messages.json*: for each Agent, a log of their messages dictionaries\n- *./coding*: A directory containing all code written by AutoGen, and all artifacts produced by that code.\n\n## Contributing or Defining New Tasks or Benchmarks\n\nIf you would like to develop -- or even contribute -- your own tasks or benchmarks, please review the [contributor&#39;s guide](CONTRIBUTING.md) for complete technical details.",
    "filename": "python/packages/agbench/README.md"
  },
  {
    "content": "# GAIA Benchmark\n\nThis scenario implements the [GAIA](https://arxiv.org/abs/2311.12983) agent benchmark. Before you begin, make sure you have followed instruction in `../README.md` to prepare your environment.\n\n### Setup Environment Variables for AgBench\n\nNavigate to GAIA\n\n```bash\ncd benchmarks/GAIA\n```\n\nUpdate `config.yaml` to point to your model host, as appropriate. The default configuration points to 'gpt-4o'.\n\nNow initialize the tasks.\n\n```bash\npython Scripts/init_tasks.py\n```\n\nNote: This will attempt to download GAIA from Hugginface, but this requires authentication.\n\nThe resulting folder structure should look like this:\n\n```\n.\n./Downloads\n./Downloads/GAIA\n./Downloads/GAIA/2023\n./Downloads/GAIA/2023/test\n./Downloads/GAIA/2023/validation\n./Scripts\n./Templates\n./Templates/TeamOne\n```\n\nThen run `Scripts/init_tasks.py` again.\n\nOnce the script completes, you should now see a folder in your current directory called `Tasks` that contains one JSONL file per template in `Templates`.\n\n### Running GAIA\n\nNow to run a specific subset of GAIA use:\n\n```bash\nagbench run Tasks/gaia_validation_level_1__MagenticOne.jsonl\n```\n\nYou should see the command line print the raw logs that shows the agents in action To see a summary of the results (e.g., task completion rates), in a new terminal run the following:\n\n```bash\nagbench tabulate Results/gaia_validation_level_1__MagenticOne/\n```\n\n## References\n\n**GAIA: a benchmark for General AI Assistants** `<br/>`\nGr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, Thomas Scialom `<br/>`\n[https://arxiv.org/abs/2311.12983](https://arxiv.org/abs/2311.12983)",
    "filename": "python/packages/agbench/benchmarks/GAIA/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script is designed to score the responses of a question-answering system by comparing its answers against expected ground truth answers. It employs two scoring methods: a custom scoring method (`in_house_question_scorer`) and an alternative method sourced from a public repository (`gaia_question_scorer`). The scoring is facilitated through reading and analyzing text files containing the expected answers and the output from the question-answering model.\n\n## Imports\n\nThe script imports several modules:\n\n- `os`, `sys`, `re`, `json`, `glob`, `string`: For handling file paths, system operations, regular expressions, JSON operations, file globbing, and string manipulations respectively.\n- `warnings`: For issuing warnings based on specific conditions during execution.\n- `numpy` and `pandas`: For numerical and data manipulation, although their usage is not explicitly visible in the provided code.\n- `agbench.tabulate_cmd`: A custom command module possibly related to formatting output.\n\n## Constants\n\n### EXCLUDE_DIR_NAMES\n\nThe list `EXCLUDE_DIR_NAMES` contains directories (like `__pycache__`) to be excluded from operations possibly related to file handling.\n\n## Normalization Functions\n\n### in_house_normalize_answer\n\nThis function:\n\n- Accepts a string `a` (the model's answer).\n- Normalizes it by:\n  - Converting to lowercase.\n  - Trimming leading and trailing spaces.\n  - Standardizing comma-separated values.\n  - Replacing multiple spaces with a single space.\n  - Removing trailing punctuation (`.`, `!`, `?`).\n\nThe normalized string is then returned.\n\n### in_house_question_scorer\n\nThis function:\n\n- Accepts two strings: the model's answer and the ground truth.\n- Uses `in_house_normalize_answer` to normalize both answers.\n- Compares them after normalization and returns `True` if they match, provided the ground truth is not empty.\n\n## GAIA Scoring Method\n\n### gaia_question_scorer\n\nThis is a more complex scoring function that differentiates between different types of ground truth formats (numeric, list, and string). \n\n1. **Helper Functions**:\n   - **normalize_number_str**: Converts a numeric string to a float after removing common characters like `$`, `%`, and `,`.\n   - **split_string**: Splits a string based on specified delimiter characters (like `,` and `;`).\n   - **normalize_str**: Normalizes a string by potentially removing punctuation, white spaces, and converting to lowercase.\n   - **is_float**: Checks if a string can be converted to a float.\n\n2. **Scoring Logic**:\n   - If the ground truth is a numeric value, it normalizes the model answer and checks equality.\n   - If the ground truth is a list (indicated by delimiters), it compares each element after normalizing and checking for length equivalences.\n   - If the ground truth is a string, it compares it directly with the normalized model answer.\n\nThe function returns `True` if the answers are considered equal based on the applicable conditions.\n\n## Scoring Function\n\n### scorer\n\nThe `scorer` function operates as follows:\n\n1. **File Handling**:\n   - It constructs paths for `expected_answer.txt` and `console_log.txt` in the specified `instance_dir`.\n   - Reads the expected answer from `expected_answer.txt`. If the file does not exist, it returns `None`.\n   - Reads the console log from `console_log.txt`, also returning `None` if the file is missing.\n\n2. **Final Answer Extraction**:\n   - Extracts the `FINAL ANSWER` from the console log using a regex search.\n   - If the final answer is not found, it returns `None`.\n\n3. **Scoring**:\n   - Calls either `in_house_question_scorer` or `gaia_question_scorer` to compare the final answer with the expected answer.\n\n## Main Functionality\n\n### main\n\nThis function serves as the entry point for the script when executed as a standalone module:\n\n- It processes command-line arguments and calls `default_tabulate`, passing the `scorer` function for evaluation.\n\n## Execution Context\n\nThe script includes a check for `__name__` and `__package__` to determine if it is being run directly, ensuring the `main` function is executed only in that context. This provides a level of modularity and prevents unintended execution during imports.\n\n---\n\nThis code serves as an evaluative tool to assess the accuracy of a model's generated answers against predefined correct responses, utilizing normalization and scoring strategies to ensure a flexible and robust comparison.",
    "filename": "python/packages/agbench/benchmarks/GAIA/Scripts/custom_tabulate.py"
  },
  {
    "code": false,
    "content": "# Documentation for Human Eval Dataset Download and Testbed Scenario Creation Script\n\n## Overview\n\nThis Python script automates the process of downloading the GAIA benchmark dataset from the Hugging Face Hub and creates corresponding JSON Lines (JSONL) files for validation and testing scenarios based on pre-defined templates. The final output consists of structured testbed scenarios that can be utilized in evaluating models, specifically geared towards coding tasks.\n\n## Importing Required Libraries\n\nThe script begins by importing necessary libraries:\n- `json`: For working with JSON data.\n- `os`: For interacting with the operating system to handle file paths and directories.\n- `re`: For regular expression operations, enhancing filename handling.\n- `sys`: For system-specific parameters and functions.\n- `snapshot_download` from `huggingface_hub`: For downloading datasets directly from Hugging Face.\n\n## Directory Setup\n\nThe script sets up several directory paths that are crucial for storing different components of the workflow:\n- `SCRIPT_PATH`: The absolute path to the script file.\n- `SCRIPT_NAME`: The name of the script file.\n- `SCRIPT_DIR`: The directory containing the script.\n- `SCENARIO_DIR`: The parent directory for scenarios.\n- `TEMPLATES_DIR`: Directory where template files are stored.\n- `TASKS_DIR`: Directory where task-specific files will be created.\n- `DOWNLOADS_DIR`: Directory designated for downloads from Hugging Face.\n- `REPO_DIR`: The exact path to where the GAIA dataset will be stored locally.\n\n## Function: `download_gaia`\n\nThe `download_gaia` function serves the purpose of downloading the GAIA benchmark dataset from the Hugging Face Hub. It checks if the `Downloads` directory exists; if not, it creates it. It then uses the `snapshot_download` function to download the dataset, ensuring that it's saved in the specified local directory (`REPO_DIR`). This function is integral for ensuring that the necessary files are obtained before creating JSONL files.\n\n## Function: `create_jsonl`\n\nThe `create_jsonl` function creates a JSON Lines (JSONL) file based on provided parameters. It takes four arguments: \n- `name`: The name of the output file.\n- `tasks`: A list of tasks to process and include in the output.\n- `files_dir`: Directory where task-related files can be found.\n- `template`: The template file to be used for generating the JSONL record.\n\nInside this function, a new JSONL file is opened, and records are constructed for each task using specified templates. Each record contains an identifier, the template paths, and necessary substitutions (like expected answers and prompts). The constructed data for each task is then written into the JSONL file. This function is pivotal for structuring data to be used later in model evaluation.\n\n## Main Function: `main`\n\nThe `main` function is where the script\u2019s workflow primarily resides. Here\u2019s a step-by-step breakdown of its operations:\n\n1. **Directory Checks**: It checks if the directories for validation and test files (within the downloaded GAIA dataset) exist. If they don't, it invokes the `download_gaia` function to download the dataset.\n   \n2. **Validation of Download**: Post-download, it verifies once again whether the directories are present. If not, it exits the script, indicating an error.\n\n3. **Loading GAIA Data**: \n   - It initializes lists to hold validation and test tasks categorized by levels.\n   - Reads a metadata JSONL file for validation tasks and organizes tasks into respective level lists.\n   - It processes a similar metadata file for test tasks, while skipping any placeholder data (task_id `0-0-0-0-0`).\n\n4. **Template Preparation**: It scans the `Templates` directory for available templates and gathers their paths into a dictionary. \n\n5. **Creating JSONL Files**: Using a loop, the script calls the `create_jsonl` function multiple times to generate combinations of test scenarios. It creates JSONL files for each model-template combination across three levels for both validation and test datasets.\n\n## Execution\n\nFinally, the script includes a standard Python invocation block (`if __name__ == \"__main__\"`) to ensure that the `main()` function runs when the script is executed directly. This ensures that the automated downloading and JSONL file creation occur without needing to call the functions explicitly in another script or environment.\n\n## Summary\n\nThe overall workflow of the script is designed for a seamless setup of the GAIA dataset and corresponding evaluation scenarios. It intelligently manages file paths, checks for necessary directories, downloads the required dataset, and structures the data into a usable format for further processing or modeling tasks. This script forms a crucial step in preparing data-driven experiments and evaluations in programming-related contexts.",
    "filename": "python/packages/agbench/benchmarks/GAIA/Scripts/init_tasks.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis Python script orchestrates various agents designed for processing a prompt, executing code, and interacting with web and file resources to ultimately provide a final output based on the prompt's requirements. It leverages asynchronous programming to manage different operations efficiently and employs external agents to carry out specific tasks that contribute to concluding the provided prompt.\n\n## Libraries and Dependencies\n\nThe script uses several libraries and modules to facilitate its operations:\n\n- **asyncio**: To manage asynchronous calls and enable concurrent execution of tasks.\n- **os**: For interacting with the operating system, particularly for file handling.\n- **yaml**: For reading configurations from a YAML file.\n- **warnings**: To suppress specific resource warnings related to unclosed sessions.\n- **autogen** packages: These custom modules appear to provide functionalities for agent interactions, code execution, and chat management.\n\nThe initialization of these imports indicates a structured approach to build an interaction between AI agents.\n\n## Asynchronous Main Function\n\nThe core operation of the script is encapsulated in the asynchronous `main()` function. Here\u2019s how it unfolds:\n\n1. **Configuration Loading**: The script begins by loading a configuration file named `config.yaml` which contains details for different AI components.\n   \n2. **Client Initialization**: Four `ChatCompletionClient` instances are created:\n   - `orchestrator_client`\n   - `coder_client`\n   - `web_surfer_client`\n   - `file_surfer_client`\n   \n   These clients are responsible for managing interactions with the respective AI models configured in the YAML file.\n\n3. **Prompt Reading**: The script reads a task prompt from a text file named `prompt.txt`. This input is essential as it determines the final output expected from the agents.\n\n4. **Agent Setup**: Several agents are instantiated:\n   - `MagenticOneCoderAgent`: Acts as a coding assistant using the designated `coder_client`.\n   - `CodeExecutorAgent`: Responsible for executing code using a local command line executor.\n   - `FileSurfer`: Designed to search and interact with files based on the AI model\u2019s instructions.\n   - `MultimodalWebSurfer`: Capable of navigating the web and obtaining multimodal content, configured to download files and save logs.\n\n5. **Group Chat Initialization**: A team of agents is compiled into a `MagenticOneGroupChat`, allowing them to communicate and collaborate on solving the prompt together. It\u2019s set to handle a maximum of 20 conversational turns.\n\n## Final Answer Formatting\n\nThe script includes a specific final answer prompt template to ensure that agents adhere to formatting requirements when providing the result. The detailed instructions include:\n\n- How to present the final answer based on the type of information requested (numerical, textual, or a list).\n- Adherence to formatting elements such as rounding, sequencing, and unit specification.\n- Clarification on avoiding unnecessary punctuation in the final output.\n\nThese parameters ensure that the output aligns precisely with user expectations.\n\n## Problem Context and Parameters Preparation\n\nIf a filename is provided, the script constructs an additional prompt context to inform the agents that the task relates to a specific file within the working directory. This contextual addition allows for more focused interactions with the file-related aspects of the prompt.\n\nThe final task string is created by concatenating the original prompt with any filename-related context, thereby equipping the group of agents with clear instructions.\n\n## Task Execution and Output Handling\n\nThe script proceeds to execute the task using the `run_stream` method on the `team` object, streaming the results of the agents' collaborative processing. This approach allows for real-time updates and interactions. \n\nLastly, the results are passed to a `Console`, presumably for display to the user, enabling them to observe the flow of information and the final outcome.\n\n## Execution Entry Point\n\nThe script includes a standard Python entry point check (`if __name__ == \"__main__\"`) to trigger the asynchronous `main()` function appropriately, using `asyncio.run(main())`. This approach ensures that the script can be executed as a standalone program. \n\nThe use of the asynchronous pattern indicates an emphasis on performance and responsiveness in dealing with potentially long-running tasks such as web searches or coding executions, allowing other tasks to proceed concurrently.\n\nOverall, this script serves as a robust framework for automating multi-agent interactions in response to user-provided prompts, effectively integrating file handling, web searching, and code execution.",
    "filename": "python/packages/agbench/benchmarks/GAIA/Templates/MagenticOne/scenario.py"
  },
  {
    "code": false,
    "content": "# Documentation for Async Task Aggregator\n\nThis document provides a high-level overview of the source code which orchestrates the execution of multiple teams processing a specified task using different agents. The code employs asynchronous programming practices with `asyncio` to handle concurrent operations, particularly in collecting results from separate teams and aggregating responses.\n\n## Overview\n\nThe primary intention of the code is to run a coordinated set of tasks using multiple artificial intelligence agents, each contributing a response to a shared query. The program collects outputs from these agents, performs logging, and generates a consolidated answer after evaluating the contributions from all the teams.\n\nThe script initializes various logging mechanisms to track events and captures responses from AI agents. It leverages context variables to maintain team-specific logging state, ensuring that each team's output can be distinctly recorded.\n\n## Import Dependencies\n\nThe code imports various modules:\n- **asyncio**: Provides support for asynchronous I/O operations.\n- **os**, **shutil**: Used for file handling and directory management.\n- **yaml**: To read configuration settings from a YAML file.\n- **logging**: For logging events and messages.\n- **json**: For handling JSON data formats in log outputs.\n- **datetime**: To manage datetime data for logging timestamps.\n- **collections**, **deque**: To efficiently manage message history for each team.\n- **contextvars**: For maintaining state across asynchronous calls.\n\nSeveral custom classes are imported to handle specific agent functionalities, model interactions, and logging.\n\n## Logging Configuration\n\nThe script sets up logging via a custom `LogHandler` class that extends Python's built-in `FileHandler`. \n\n### LogHandler Class\n\n- **Purpose**: Directly manages the logging of events, formatting them into a JSON lines format. It differentiates between various types of log messages based on the source of the logging call.\n- **emit() Method**: Responsible for constructing the log message format, handling both general logs and specific logging for language model calls.\n\n### Team-Specific Logging\n\nThe `tee_print` function overrides the built-in `print` function to route console outputs to specified log files, allowing all printed messages to be captured in real-time. The `team_specific_agentchat_event_logger_info` and `team_specific_core_event_logger_info` functions create team-specific loggers, ensuring that all logs are tagged with the corresponding team identifiers.\n\n## Asynchronous Team Execution\n\n### run_team() Function\n\n- **Purpose**: This asynchronous function executes a specific task associated with a team. It runs the team's protocol for the assigned task and logs the results.\n- **Parameters**: Accepts the team instance, team index, task string, cancellation token, and log file.\n- **Execution**: Using a `Console`, this function streams the task execution and captures the results, ensuring that context variables for logging are correctly managed.\n\n### aggregate_final_answer() Function\n\n- **Purpose**: Aggregates results from all teams' responses to generate a final output.\n- **Process**:\n  - Checks if there is one or more results from the teams.\n  - Collects messages and stop reasons, formats the transcripts, and prepares them for final evaluation.\n  - Constructs a prompt that instructs other AI models to evaluate the gathered inputs and provide a final answer based on the specific response format defined in the task.\n\n## Main Function Logic\n\n### main() Function\n\n- **Purpose**: Serves as the entry point for the program. Here, it orchestrates the loading of configuration files, initialization of team agents, and management of asynchronous tasks.\n- **Steps**:\n  1. **Load Configuration**: Reads model configuration from a YAML file.\n  2. **Set up Teams**: Initializes several agents for each team, including coder, executor, file surfer, and web surfer agents.\n  3. **Task Preparation**: Constructs tasks based on user-defined prompts, which could involve document analysis or coding tasks.\n  4. **Cleanup**: Removes any previous log files to reset the environment.\n  5. **Assemble Task Execution**: Uses `asyncio.create_task()` for each team to run concurrently while also collecting their results.\n\n### Completion and Finalization\n\nAfter executing the assigned tasks, the code checks for task completion and aggregates results. If the number of satisfactory results (answers) meets the threshold, it cancels any remaining tasks and collects responses for aggregation.\n\nFinally, the program prints out the final aggregated answer that summarizes the collective responses from all engaged teams.\n\n## Execution Trigger\n\nThe execution of the main function occurs under the `if __name__ == \"__main__\":` condition, defining default parameters for the number of teams and answers, setting up appropriate loggers for task tracking, and invoking the asynchronous event loop.\n\nIn summary, this code provides a structured approach to executing multi-agent collaborative tasks with clear delineations for logging, task management, and response aggregation, ultimately tailored for systems relying on automated AI-driven responses.",
    "filename": "python/packages/agbench/benchmarks/GAIA/Templates/ParallelAgents/scenario.py"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis script is a Python asynchronous application that utilizes various agents to collaboratively perform a task based on a prompt. It leverages a configuration loaded from a YAML file and executes different operations including web browsing and file handling to generate a final answer based on user input. The LLM (Large Language Model) is used to evaluate when the task is complete.\n\n## Imports and Dependencies\n\nThe script begins by importing several modules, including:\n\n- **asyncio** for asynchronous programming.\n- **os** for operating system dependent functionality.\n- **yaml** for reading YAML configuration files.\n- **warnings** to manage warning messages.\n- Various classes and functions from a package related to multiple autonomous agents, including text and code execution.\n\nThese imported modules serve functions like client creation, message handling, managing termination conditions, and defining agent roles.\n\n## Configuration Loading\n\nWithin the `main` function:\n\n- The first operation is to load a configuration file named `config.yaml`. This file contains parameters crucial for initializing different client connections to the LLMs.\n  \n```python\nwith open(\"config.yaml\", \"r\") as f:\n    config = yaml.safe_load(f)\n```\n\n- Based on the loaded configuration, instances of `ChatCompletionClient` are created for orchestrator, coder, web surfer, and file surfer. This lays the groundwork for the models that will be used later in the script.\n\n## Prompt Initialization\n\nThe script reads a prompt from a file named `prompt.txt` which is stripped of unnecessary whitespace. It also sets up a filename variable intended for file operations. If the filename is not empty, it adds context to the prompt concerning the file potentially relevant to the user's query.\n\n```python\nwith open(\"prompt.txt\", \"rt\") as fh:\n    prompt = fh.read().strip()\n```\n\n## Agent Initialization\n\nThe application sets up multiple agents, each responsible for different tasks such as coding assistance, executing code, web browsing, or file handling. This includes:\n\n1. **MagenticOneCoderAgent**: Acts as an assistant for coding-related tasks.\n2. **CodeExecutorAgent**: Executes code using a local command line interface.\n3. **FileSurfer**: Handles operations related to file management.\n4. **MultimodalWebSurfer**: Browses the web and supports downloads and visual content handling.\n\nEach agent is instantiated with appropriate model clients, including configurations for storage locations and debug information.\n\n## Task Preparation and Termination Conditions\n\nThe core task is structured by combining the user prompt with contextual information derived from the provided filename (if available). Two termination conditions are established:\n\n1. **MaxMessageTermination**: Stops execution after a maximum number of messages (20 in this case).\n2. **LLMTermination**: A custom termination condition that evaluates if the task has been completed based on LLM's response.\n\n```python\ntermination = max_messages_termination | llm_termination\n```\n\nThe termination conditions guide the agents in deciding when the collaborative task can conclude.\n\n## Team Creation and Task Execution\n\nA `SelectorGroupChat` instance is created to coordinate communication among the agents. This structure allows agents to contribute to the conversation and problem-solving process based on the defined termination conditions.\n\n```python\nteam = SelectorGroupChat([...], termination_condition=termination)\n```\n\nNext, the task is executed via a running stream, and the results are captured asynchronously. The `Console` is employed for output display during this streaming process.\n\n## Final Context and Response Formatting\n\nOnce the task is completed, the application prepares a final context for additional inference. The collected messages are processed, adding user messages based on whether the messages were text or multimodal.\n\n```python\nfinal_context: Sequence[LLMMessage] = []\n```\n\nA structured message prompts the model for a final answer while adhering to specific output formatting rules detailed in the comments.\n\n## LLMTermination Class\n\nThe script contains a definition for the `LLMTermination` class which implements the `TerminationCondition` interface. This allows for dynamic evaluation to check if the task is finished based on LLM-generated messages.\n\n### Initialization and Properties\n\nThe class constructor takes a prompt, model client, and optional termination phrase. It manages the termination state and context required for making assessment calls to the LLM.\n\n### Call Method\n\nThe `__call__` method iterates through messages, builds context, and queries the model. If a specific termination phrase is detected in the response, it modifies the termination state.\n\n### Reset Method\n\nThe `reset` method clears the termination state and context, allowing for a fresh evaluation cycle if required.\n\n## Main Execution Trigger\n\nThe script concludes with a check to execute the `main` function if the file is run as the main module. This triggers the asynchronous task flow.\n\n```python\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nIn summary, this script orchestrates multiple AI agents, leveraging their collective skills to process user queries and produce well-structured results from given prompts.",
    "filename": "python/packages/agbench/benchmarks/GAIA/Templates/SelectorGroupChat/scenario.py"
  },
  {
    "content": "# HumanEval Benchmark\n\nThis scenario implements a modified version of the [HumanEval](https://arxiv.org/abs/2107.03374) benchmark.\nCompared to the original benchmark, there are **two key differences** here:\n\n- A chat model rather than a completion model is used.\n- The agents get pass/fail feedback about their implementations, and can keep trying until they succeed or run out of tokens or turns.\n\n## Running the tasks\n\n\nNavigate to HumanEval\n\n```bash\ncd benchmarks/HumanEval\n```\n\nUpdate `config.yaml` to point to your model host, as appropriate. The default configuration points to 'gpt-4o'.\n\n\nNow initialize the tasks.\n\n```bash\npython Scripts/init_tasks.py\n```\n\nNote: This will attempt to download HumanEval\n\nThen run `Scripts/init_tasks.py` again.\n\nOnce the script completes, you should now see a folder in your current directory called `Tasks` that contains one JSONL file per template in `Templates`.\n\nNow to run a specific subset of HumanEval use:\n\n```bash\nagbench run Tasks/human_eval_AgentChat.jsonl\n```\n\nYou should see the command line print the raw logs that shows the agents in action To see a summary of the results (e.g., task completion rates), in a new terminal run the following:\n\n```bash\nagbench tabulate Results/human_eval_AgentChat\n```\n\n\n## References\n\n**Evaluating Large Language Models Trained on Code**`<br/>`\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba`<br/>`\n[https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)",
    "filename": "python/packages/agbench/benchmarks/HumanEval/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script serves as a command-line utility that utilizes a function from the `agbench.tabulate_cmd` module. It is designed to process command-line arguments and subsequently present data in a tabulated format based on those arguments.\n\n## Imports\nThe script imports three modules:\n- `os`: This module provides a way of using operating system-dependent functionality, such as reading or writing to the file system.\n- `sys`: This module allows interaction with the Python interpreter, particularly with command-line arguments and system-level attributes.\n- `default_tabulate` from `agbench.tabulate_cmd`: This specific import suggests that the script is intended to format or display data in tabular form, leveraging functionality defined within the `agbench` package.\n\n## Main Function\n### Definition\nThe `main` function is defined to accept a single parameter, `args`, which is expected to be a list of command-line arguments.\n\n### Purpose\nInside the `main` function, the `default_tabulate` function is called with `args` as its argument. The role of this call is likely to process the command-line inputs and generate output in a tabulated structure, making it easier for the user to interpret the data.\n\n## Entry Point\n### Conditional Execution\nThe script checks if it is being executed as the main module using the `if __name__ == \"__main__\" and __package__ is None:` condition. This is a common pattern in Python to allow or prevent parts of code from being run when the modules are imported. \n\n### Executing the Main Function\nIf the conditions are met, the `main` function is invoked with `sys.argv` passed as the argument. `sys.argv` is a list in Python, representing command-line arguments. By doing this, the script takes whatever arguments are provided when the script is called from the command line.\n\n## Usage Scenario\nTo use this script:\n1. It must be executed from a command line interface or terminal.\n2. The user can provide any relevant arguments following the script name. These arguments would typically dictate the behavior of the `default_tabulate` function regarding how data is processed or displayed.\n3. The output will be a formatted table based on the input arguments, although the exact formatting and data handled depend on the implementation of `default_tabulate`.\n\n## Summary\nIn conclusion, the script acts as a command-line interface that primarily serves to display data in a tabulated format, configured by user-provided arguments. It relies on the `agbench` package to accomplish this task. The script\u2019s structure ensures that the `main` function is only executed when the script is run directly, providing a clean and logical entry point for command-line interaction.",
    "filename": "python/packages/agbench/benchmarks/HumanEval/Scripts/custom_tabulate.py"
  },
  {
    "code": false,
    "content": "# Documentation for the HumanEval Dataset Download Script\n\nThis document provides a high-level overview of a Python script designed to download the HumanEval dataset from a specified URL, extract the dataset, and create JSON Lines (JSONL) format scenario files based on the tasks contained within the dataset. It also includes deprecated functionality for working with a reduced task set.\n\n## Overview\n\nThe purpose of this script is to facilitate the initialization of a testing environment leveraging the HumanEval dataset. This dataset is instrumental in evaluating the performance of automated programming systems. The script achieves this by performing a series of tasks, including downloading, extracting, and formatting data into usable scenarios for subsequent processing.\n\n## Importing Necessary Modules\n\nThe script begins by importing essential Python libraries, which include:\n\n- **`base64`**: For potential encoding/decoding functionalities (not explicitly used in the provided code).\n- **`gzip`**: To handle the gzipped dataset.\n- **`io`**: To interact with in-memory byte streams.\n- **`json`**: For JSON data serialization and deserialization.\n- **`os`**: For directory and file path manipulation.\n- **`re`**: For regular expression operations.\n- **`requests`**: To perform HTTP requests for downloading the dataset.\n\n## Constants and Directory Setup\n\n### URLs and Paths\n\nThe script defines several constants to streamline the process of accessing various directories and files:\n\n- **`URL`**: The target URL from which the HumanEval dataset is downloaded.\n- **`SCRIPT_PATH`, `SCRIPT_NAME`, `SCRIPT_DIR`**: These variables identify the paths related to the running script.\n- **`SCENARIO_DIR`, `TEMPLATES_DIR`, `TASKS_DIR`**: Define where the scenario files and templates reside or will be created.\n\n### Deprecated Reduced Set\n\nA list named `REDUCED_SET` contains a selection of task identifiers that are marked as deprecated. This indicates a shift in focus; users should now utilize the full dataset or a newly defined subset/functionality instead.\n\n## Function Definitions\n\n### `download_human_eval()`\n\nThis function is responsible for downloading the HumanEval dataset, extracting its contents, and returning a list of parsed JSON objects corresponding to each task in the dataset. \n\n- **HTTP Request**: It sends a GET request to the specified URL.\n- **Error Handling**: If the download fails, an error is raised.\n- **Data Extraction**: Using the `gzip` library, the downloaded content is processed, and each line is parsed as JSON and appended to a results list.\n- **Return Value**: It returns a list of tasks, where each task is represented as a JSON object.\n\n### `create_jsonl(name, tasks, template)`\n\nThis function is responsible for creating a JSONL file that serves a specific task scenario. It takes three parameters: `name`, `tasks`, and `template`.\n\n- **Directory Creation**: It first checks if the `TASKS_DIR` exists, creating it if necessary.\n- **File Writing**: For each task, the function constructs a dictionary to represent that task's information, including its ID and related template. This information is then written to a JSONL file, which allows for efficient processing of task information.\n\n## Main Execution Flow\n\n### `main()`\n\nThe `main` function orchestrates the primary operations of the script.\n\n1. **Data Download**: It calls `download_human_eval()` to retrieve the dataset.\n2. **Template Discovery**: It scans the `TEMPLATES_DIR`, populating a dictionary with directory names and paths corresponding to available templates for the tasks.\n3. **Scenario File Creation**: The script iterates over the discovered templates, invoking `create_jsonl()` to generate scenario files for each template.\n\nThe script concludes by checking if it is being run directly and not as part of a larger package context, in which case it executes the `main()` function.\n\n## Conclusion\n\nIn summary, this script serves as a utility for acquiring and organizing the HumanEval dataset into a suitable format for development or testing purposes. Its modular functions encapsulate distinct functionalities, ensuring clarity and maintaining focus on specific tasks such as downloading data and creating scenario files. The deprecation notice for reduced sets signals an evolution in how tasks are selected for evaluation, promoting a more comprehensive approach to leveraging the dataset. While specific features are marked as deprecated, the core functionalities remain relevant for creating and managing test scenarios for automated programming assessments.",
    "filename": "python/packages/agbench/benchmarks/HumanEval/Scripts/init_tasks.py"
  },
  {
    "code": false,
    "content": "# Documentation for `CustomCodeExecutorAgent`\n\n## Overview\n\nThe `CustomCodeExecutorAgent` class is a specialized implementation of the `CodeExecutorAgent` that is designed to handle the execution of Python and shell scripts. It enhances the functionality of the base class by extracting code blocks from markdown text and adding a testing harness for Python code. This agent is particularly useful for running and validating code snippets in a controlled environment.\n\n## Dependencies\n\nThe class relies on several imports that contribute to its functionality:\n\n- **`re`**: This module provides support for regular expressions, which can facilitate text processing.\n- **`List`, `Sequence`** from `typing`: These are type hints that help specify the expected types for function arguments and return values.\n- **`CodeBlock`, `CodeExecutor`** from `autogen_core.code_executor`: These are likely custom classes that handle the execution of code blocks.\n- **`CodeExecutorAgent`** from `autogen_agentchat.agents`: The base class from which `CustomCodeExecutorAgent` inherits, providing a framework for executing code.\n\n## Class Definition\n\n### `CustomCodeExecutorAgent`\n\nThis class extends `CodeExecutorAgent`, adding additional functionality for handling specific types of code input, particularly in the context of markdown. \n\n### Constructor: `__init__`\n\n```python\ndef __init__(\n    self,\n    name: str,\n    code_executor: CodeExecutor,\n    *,\n    description: str = \"A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).\",\n    sources: Sequence[str] | None = None,\n) -> None:\n```\n\n- **Parameters**:\n  - `name`: The name of the agent.\n  - `code_executor`: An instance of `CodeExecutor` that will execute the code blocks.\n  - `description`: A brief description of the agent's capabilities. Defaults to a provided string.\n  - `sources`: An optional sequence of strings representing sources, potentially used for documentation or reference.\n  \n- **Functionality**: \n  - Initializes the base class with the provided parameters.\n  - Reads a file named `test.txt` and stores its content into the `_test_code` attribute. This content will serve as a prefix for any Python code that the agent executes.\n\n## Method: `_extract_markdown_code_blocks`\n\n```python\ndef _extract_markdown_code_blocks(self, markdown_text: str) -> List[CodeBlock]:\n```\n\nThis method extracts code blocks from a provided markdown text, processes them, and returns a list of `CodeBlock` instances.\n\n### Functionality\n\n1. **Calling Superclass Method**: The method first calls the superclass implementation of `_extract_markdown_code_blocks` to get the initial list of code blocks from the markdown input.\n\n2. **Processing Code Blocks**: It initializes an empty list called `new_blocks` to hold the modified code blocks:\n   - It iterates over each `CodeBlock` instance retrieved.\n   - For each block, it checks if the language is Python; if so, it wraps the extracted code with a testing harness using the `_test_code` read from `test.txt`.\n   - The harness includes a `run_tests` function that calls a hypothetical `check` function, and it prints messages based on whether the tests passed or failed.\n\n3. **Returning Code Blocks**: After processing, the modified code blocks (if any changes were made) are appended to the `new_blocks` list, which is ultimately returned.\n\n### Note on Regex\n\nThe commented-out regex code within this method suggests a placeholder for additional processing, potentially aimed at handling custom tags (like `<think>`) that may appear in code blocks. However, this functionality is not currently active.\n\n## Conclusion\n\nThe `CustomCodeExecutorAgent` class builds upon the foundation laid by `CodeExecutorAgent` to create a specialized agent capable of executing Python and shell scripts, while also providing a means to validate Python code with automated tests. Its design promotes extensibility and modularity by leveraging existing components, making it a powerful tool for working with dynamic script execution in a markdown context.",
    "filename": "python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/custom_code_executor.py"
  },
  {
    "code": false,
    "content": "# Documentation for `ReasoningModelContext`\n\n## Overview\nThe `ReasoningModelContext` class is a specialized extension of the `UnboundedChatCompletionContext` class from the `autogen_core.model_context` module. This class is designed to handle chat-based interactions specifically for reasoning models. It overrides the `get_messages` method to filter out sensitive data from the message objects before they are returned, making it suitable for use cases where certain information (like the `thought` field) should not be exposed.\n\n## Class Definition\n\n### `class ReasoningModelContext(UnboundedChatCompletionContext)`\nThis class inherits from `UnboundedChatCompletionContext`, which suggests that it is part of a chat-based AI model framework. The purpose of this class is to provide a custom implementation that better suits the needs of reasoning models by modifying how messages are processed.\n\n### Purpose\n- **Filtering Messages**: The primary purpose of the `ReasoningModelContext` class is to ensure that when messages are retrieved, any extraneous or sensitive information\u2014specifically, the `thought` field of `AssistantMessage`\u2014is excluded. This can be particularly useful in situations where user privacy or data security is a concern.\n\n## Methods\n\n### `async def get_messages(self) -> List[LLMMessage]`\n- **Description**: This asynchronous method overrides the equivalent method in the parent class. It retrieves a list of message objects from the superclass and processes them to filter out certain fields.\n- **Returns**: A list of `LLMMessage` objects which do not include the `thought` attribute from any `AssistantMessage`.\n\n#### Step-by-step Breakdown:\n1. **Call Superclass Method**: The method begins by invoking `super().get_messages()` to obtain the current messages stored in the context.\n2. **Initialize Message List**: An empty list named `messages_out` is created to store the processed messages.\n3. **Message Filtering**:\n   - The method iterates over each message in the retrieved list. \n   - It checks if the message is an instance of `AssistantMessage`.\n   - For any `AssistantMessage` instances found, it sets the `thought` attribute to `None`, effectively filtering this piece of information from being returned.\n4. **Appending Messages**: Each (potentially modified) message is appended to the `messages_out` list.\n5. **Return Processed Messages**: After processing all messages, the method returns the `messages_out` list, which now contains filtered messages ready for further use.\n\n## Use Cases\n- **Privacy Protection**: This class is well-suited for applications where user interaction includes potentially sensitive information or internal states that should not be shared.\n- **Message Handling in AGI Systems**: In Artificial General Intelligence (AGI) systems, managing how messages are presented can be critical for maintaining a controlled narrative or user interaction.\n\n## Conclusion\nThe `ReasoningModelContext` serves as a tailored solution for managing chat messages in reasoning frameworks by ensuring that non-essential and sensitive data is properly filtered out. This class encapsulates this specific functionality within a clear and efficient method, making it a valuable addition to the `autogen_core` framework for developing chat-based AI systems.",
    "filename": "python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/reasoning_model_context.py"
  },
  {
    "code": false,
    "content": "# Code Overview\n\nThis Python script serves as the main entry point for a collaborative agent-based architecture that utilizes artificial intelligence to complete a Python function based on user-provided input. The script employs asynchronous programming to manage multiple interacting agents that work together to achieve the goal.\n\n---\n\n## Libraries and Dependencies\n\nThe script imports several libraries and modules:\n\n- **asyncio**: A library in Python for concurrent code execution using the async/await syntax.\n- **os**: Provides a portable way of using operating system dependent functionality.\n- **yaml**: Used for parsing YAML configuration files.\n- **autogen_ext.agents.magentic_one**: Contains the `MagenticOneCoderAgent`, an AI agent that generates code.\n- **autogen_agentchat.teams**: Contains the `RoundRobinGroupChat` class for managing interactions among agents.\n- **autogen_agentchat.ui**: Contains `Console`, which streams messages to the console.\n- **autogen_core.models**: Includes `ModelFamily` for identifying different model categories.\n- **autogen_core.model_context**: Defines contexts used for chat completion, specifically `UnboundedChatCompletionContext` and `ChatCompletionContext`.\n- **autogen_ext.code_executors.local**: Provides the `LocalCommandLineCodeExecutor` for executing code in a local environment.\n- **autogen_agentchat.conditions**: Contains `TextMentionTermination` to define when interactions should stop.\n- **custom_code_executor**: The `CustomCodeExecutorAgent` likely defines additional behavior for executing code.\n- **reasoning_model_context**: Imports `ReasoningModelContext` for managing reasoning-based interactions.\n- **autogen_core.models**: Again imports `ChatCompletionClient` for creating components based on the model configuration.\n\n---\n\n## Main Function Execution\n\n### Configuration Loading\n\nThe execution begins with the `main` asynchronous function, which starts by loading a configuration file named `config.yaml`. This file is expected to contain settings related to the model configuration.\n\n```python\nwith open(\"config.yaml\", \"r\") as f:\n    config = yaml.safe_load(f)\n```\n\n### Model Client and Context Setup\n\nNext, the script creates a `ChatCompletionClient` based on the loaded configuration. It checks the model family type, and initializes either a `ReasoningModelContext` or an `UnboundedChatCompletionContext`. This setup helps the agents understand the context within which they'll operate.\n\n```python\nmodel_client = ChatCompletionClient.load_component(config[\"model_config\"])\n\n# Model context\nif model_client.model_info[\"family\"] == ModelFamily.R1:\n    model_context = ReasoningModelContext()\nelse:\n    model_context = UnboundedChatCompletionContext()\n```\n\n### Agent Initialization\n\nTwo agents are instantiated: \n\n1. **Coder Agent**: An instance of `MagenticOneCoderAgent`, configured with the model client and its context.\n    ```python\n    coder_agent = MagenticOneCoderAgent(\n        name=\"coder\",\n        model_client=model_client,\n    )\n    coder_agent._model_context = model_context  # type: ignore\n    ```\n\n2. **Executor Agent**: An instance of `CustomCodeExecutorAgent` that takes a command line executor for running code and sources the coder agent for inputs.\n    ```python\n    executor = CustomCodeExecutorAgent(\n        name=\"executor\",\n        code_executor=LocalCommandLineCodeExecutor(),\n        sources=[\"coder\"],\n    )\n    ```\n\n### Termination Condition\n\nA termination condition is defined to stop the interaction when the word \"TERMINATE\" is mentioned in messages from the executor agent.\n\n```python\ntermination = TextMentionTermination(text=\"TERMINATE\", sources=[\"executor\"])\n```\n\n---\n\n## Team Formation and Interaction\n\n### Agent Team Setup\n\nA team is established using `RoundRobinGroupChat`, allowing the coder and executor agents to collaborate for a maximum of 12 turns while adhering to the defined termination condition.\n\n```python\nagent_team = RoundRobinGroupChat([coder_agent, executor], max_turns=12, termination_condition=termination)\n```\n\n### Prompt Preparation\n\nThe script reads an input prompt from a text file named `prompt.txt`, which expects a partially defined Python function.\n\n```python\nprompt = \"\"\nwith open(\"prompt.txt\", \"rt\") as fh:\n    prompt = fh.read()\n```\n\n### Task Definition\n\nA task is formatted that instructs the agents to complete the Python function and expects the output to be formatted as a Markdown block.\n\n```python\ntask = f\"\"\"Complete the following python function. Format your output as Markdown python code block containing the entire function definition:\n\n```python\n{prompt}\n```\n\"\"\"\n```\n\n---\n\n## Running the Team and Streaming Output\n\nFinally, the team is run with the task provided, and the output is streamed directly to the console for real-time visibility.\n\n```python\nstream = agent_team.run_stream(task=task)\nawait Console(stream)\n```\n\n### Main Function Invocation\n\nThe `main` function is called using `asyncio.run`, effectively executing the entire script asynchronously. This allows the agents to communicate and respond without blocking other operations.\n\n```python\nasyncio.run(main())\n```\n\n---\n\n## Conclusion\n\nIn summary, this script orchestrates a collaborative interaction between AI agents to complete a Python function. It leverages asynchronous programming to manage multiple operations concurrently, making it suitable for real-time applications where timely responses are crucial. The use of YAML configuration and structured task execution promotes flexibility and configurability for various coding tasks.",
    "filename": "python/packages/agbench/benchmarks/HumanEval/Templates/AgentChat/scenario.py"
  },
  {
    "content": "# Benchmarking Agents\n\nThis directory provides ability to benchmarks agents (e.g., built using Autogen) using AgBench. Use the instructions below to prepare your environment for benchmarking. Once done, proceed to relevant benchmarks directory (e.g., `benchmarks/GAIA`) for further scenario-specific instructions.\n\n## Setup on WSL\n\n1. Install Docker Desktop. After installation, restart is needed, then open Docker Desktop, in Settings, Ressources, WSL Integration, Enable integration with additional distros \u2013 Ubuntu\n2. Clone autogen and export `AUTOGEN_REPO_BASE`. This environment variable enables the Docker containers to use the correct version agents.\n    ```bash\n    git clone git@github.com:microsoft/autogen.git\n    export AUTOGEN_REPO_BASE=<path_to_autogen>\n    ```\n3. Install `agbench`. AgBench is currently a tool in the Autogen repo.\n\n    ```bash\n    cd autogen/python/packages/agbench\n    pip install -e .\n    ```",
    "filename": "python/packages/agbench/benchmarks/README.md"
  },
  {
    "code": false,
    "content": "# Documentation for Log Processing Code\n\nThis Python script serves as a log processing tool for benchmarking AI agents, specifically for two known benchmarks named \"Gaia\" and \"WebArena\". It analyzes logs generated by the agents during their tasks to assess performance and correctness. The primary output of the script is a structured dataset represented as a Pandas DataFrame that contains extensive details about the benchmarks processed.\n\n## Overview of Main Functionality\n\nThe core function of the code is `process_logs`, which takes the path to a directory containing log files and processes the logs within those directories. It checks if the directory exists, retrieves the names of all benchmark subsets, and iterates over each subset to collect relevant information. This includes expected answers, final agent responses, messages exchanged during tasks, and various performance metrics.\n\n## Key Functions and Their Roles\n\n### 1. `process_logs(logs_path, single_benchmark=False)`\n\n- **Parameters**: \n  - `logs_path`: Directory path where logs are stored.\n  - `single_benchmark`: (Optional) Boolean indicating if only a specific benchmark should be processed.\n  \n- **Returns**: A Pandas DataFrame containing processed log data.\n\n- **Functionality**:\n  - Checks if the provided path exists. If not, it raises a `FileNotFoundError`.\n  - Handles directories for a single benchmark or all benchmarks found in the logs path.\n  - Iterates over log subsets, extracting instance data and invoking other helper functions to analyze each log.\n\n### 2. `scorer(instance_dir, benchmark_name)`\n\n- **Parameters**: \n  - `instance_dir`: Directory path for a single benchmark instance.\n  - `benchmark_name`: Name of the benchmark being processed.\n  \n- **Returns**: A tuple containing correctness of the answer, expected answer, and the final answer if applicable.\n\n- **Functionality**:\n  - Reads the expected answer and the final answer from log files.\n  - Normalizes both answers and checks their correctness for benchmarks \"Gaia\" or \"WebArena\".\n  - Returns results based on the correctness or score derived from the final answer.\n\n### 3. `normalize_answer(a)`\n\n- **Parameters**: \n  - `a`: A string representing the answer to be normalized.\n  \n- **Returns**: A normalized string.\n\n- **Functionality**:\n  - Processes and formats the answer by trimming, lowering the case, and cleaning up punctuation and extra spaces.\n\n### 4. `get_message_logs(instance_dir)`\n\n- **Parameters**: \n  - `instance_dir`: Directory path for a single benchmark instance.\n  \n- **Returns**: A list of message logs if available, else `None`.\n\n- **Functionality**:\n  - Reads a log file in JSON format to extract messages and their sources and returns these in list form.\n\n### 5. `get_task_information(instance_dir, benchmark_name)`\n\n- **Parameters**: \n  - `instance_dir`: Directory path for a single benchmark instance.\n  - `benchmark_name`: Name of the benchmark being processed.\n  \n- **Returns**: The task information as a string if available, else `None`.\n\n- **Functionality**:\n  - Retrieves detailed task information from relevant files, distinguishing between \"Gaia\" and \"WebArena\".\n\n### 6. `did_agent_stall(instance_dir)`\n\n- **Parameters**: \n  - `instance_dir`: Directory path for a single benchmark instance.\n  \n- **Returns**: Boolean indicating whether the agent has stalled.\n\n- **Functionality**:\n  - Checks logs for indications of stalling behavior, particularly looking for specific phrases in log entries.\n\n### 7. `is_progress_not_being_made(instance_dir)`\n\n- **Parameters**: \n  - `instance_dir`: Directory path for a single benchmark instance.\n  \n- **Returns**: Boolean indicating if progress is not being made.\n\n- **Functionality**:\n  - Analyzes log entries to assess if there are moments when the agent is deemed to make no progress, thus providing insight into performance bottlenecks.\n\n## DataFrame Construction\n\nAt the end of `process_logs`, the results of all analyzed instances are compiled into a Pandas DataFrame. Each entry in the DataFrame corresponds to a combination of benchmark name, subset, instance, correctness, answers, messages, and various performance indicators. This structured dataset enables further analysis and visualization, allowing users to evaluate the performance of the agent across benchmarks efficiently.\n\n## Logging for Monitoring\n\nThe script employs Python's built-in logging module, which helps by providing informational logs during processing. The logging system informs users when benchmarks and instances are being processed and captures any errors or exceptions that may arise, contributing to effective troubleshooting.\n\n## Conclusion\n\nThis code serves as a robust tool for analyzing and processing logs from AI agent benchmarks. With its structured approach and helper functions, it provides essential insights into the performance and correctness of agent actions through detailed examinations of task results, making it invaluable for researchers and developers in artificial intelligence domains.",
    "filename": "python/packages/agbench/benchmarks/process_logs.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis source code is a minimal Python script intended for packaging a Python project using `setuptools`. It uses the `setup()` function provided by the `setuptools` library to facilitate the creation of a Python package.\n\n## Purpose of the Code\n\nThe primary function of this script is to call `setup()`, which is a pivotal function in `setuptools` that allows developers to specify the details of their Python package. This includes metadata and instructions on how the package should be built, installed, and distributed. The script as shown does not include any arguments within the `setup()` function, indicating that it uses the default settings.\n\n## Packaging with Setuptools\n\nPackaging in Python involves creating a distributable format for your project. When you package a Python project, it typically includes all necessary files, documentation, and dependencies that users need to install and run the code. The `setuptools` library simplifies the packaging process, and calling `setup()` is a critical step in that process.\n\n## Default Behavior\n\nSince the `setup()` function is called without any parameters or arguments, it will rely on default values. This typically means that the script will look for certain files and metadata, such as `setup.py` itself, a `__version__` attribute, and others defined in the project directory. If these are not available or defined, the packaging process may not work as intended.\n\n## Including Metadata\n\nIn a more comprehensive setup script, one would typically include metadata such as `name`, `version`, `description`, `author`, `license`, and other fields that describe the package. These details are important because they inform potential users about the package\u2019s purpose, its usage, and who maintains it. This script lacks such metadata, which may lead to limitations in functionality when distributing the package.\n\n## Adding Dependencies\n\nAnother key aspect of using `setuptools` is the ability to declare dependencies that your package requires. In larger scripts, you might see a parameter like `install_requires` in the `setup()` call, specifying other Python packages that need to be installed alongside yours for it to function correctly. This script does not contain any such specifications.\n\n## Customization Options\n\nThe `setup()` function contains numerous optional parameters to customize the behavior of the package creation process. These may include classifiers (to help users find the package), entry points (to define where the application will start), and package data (to include non-Python files). Again, because this script does not specify any arguments, it relies on the defaults. Without customization, the usability of the package may be limited.\n\n## Invoking the Script\n\nTypically, this script would be run from the command line using a command like `python setup.py install` or `python setup.py sdist`. The absence of additional arguments in the `setup()` means that the script will perform a simple installation or source distribution, which may not be sufficient if this is intended for broader distribution.\n\n## Conclusion\n\nIn summary, this minimal script serves as a starting point for packaging a Python project but lacks the necessary metadata and configurations for a fully functional package. Users looking to effectively package their Python applications would generally expand upon this template by incorporating essential details and dependencies in the arguments to the `setup()` function. To enhance this script, one would typically add parameters to `setup()` that provide crucial information about the package, improving its accessibility and utility.",
    "filename": "python/packages/agbench/setup.py"
  },
  {
    "code": false,
    "content": "# Module Documentation\n\nThis documentation provides an overview of the presented Python code, which imports a version identifier from a separate module.\n\n## Overview\n\nThe code snippet is a single line that is part of a larger module. It imports the `__version__` variable from a module named `version` located in the same package as the current module (denoted by the dot prefix). The primary purpose of this line is to make the version information of the package accessible within the current module.\n\n## Import Statement\n\nThe line of code can be broken down as follows:\n\n```python\nfrom .version import __version__\n```\n\n### Relative Import\n\n- The dot (`.`) in the import statement suggests that it is a **relative import**. This means that the `version` module is expected to be in the same directory as the current module. Relative imports are useful in package structures to refer to modules without specifying the full package path.\n\n### Importing the Version Variable\n\n- `__version__`: This variable typically holds a string representing the version of the package or module. It is a common convention in Python libraries to define this variable to provide users with information about the current version of the library they are using.\n\n## Purpose of Versioning\n\nVersioning in software development serves several critical purposes:\n\n- **Dependency Management:** By adhering to versioning, developers can manage dependencies effectively. Other libraries or software that depend on this module can specify their requirements based on the version.\n\n- **Backward Compatibility:** Version numbers can signify updates, including backward-compatible changes or breaking changes. Semantic versioning (e.g., MAJOR.MINOR.PATCH) is often used to communicate these changes clearly.\n\n- **Documentation:** Including the `__version__` variable in a module allows for easier documentation and user reference. Developers or users can quickly check the version they are working with for compatibility.\n\n## Use Cases\n\nWhile the provided code snippet does not execute any operations, it is generally used in contexts where:\n\n- A developer needs to check or log the version of the module they are importing.\n- Libraries require a consistent way to retrieve version information, which can be especially helpful in APIs and command-line tools that display version information.\n\n## Conclusion\n\nIn summary, this code snippet serves as a straightforward means to access the version of a package defined in another module within the same package. It exemplifies good practices in software development by promoting version visibility and management. Further functionality and context would typically be provided by the code surrounding this import statement, where the `__version__` could be utilized in various functions or classes designed for the module's operations.",
    "filename": "python/packages/agbench/src/agbench/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis documentation provides a high-level overview and explanation of a simple Python script that imports and executes a function from a module named `cli`. \n\n## Overview\n\nThe script acts as an entry point for a command-line interface (CLI) application. It checks whether the script is executed as the main program and, if true, calls the `main` function from the `cli` module.\n\n## Module Import\n\n### `from .cli import main`\n\nThe line imports the `main` function from a module named `cli`. The use of the dot (`.`) at the beginning of the import statement indicates that `cli` is in the same package or directory as this script. The `main` function presumably contains the core logic for the command-line interface.\n\n- **Purpose of the import**: This line facilitates access to the `main` function defined in another module, allowing this script to serve as a launcher or facilitator for executing that function.\n\n## Main Guard Section\n\n### `if __name__ == \"__main__\":`\n\nThis conditional checks if the script is being run directly (as opposed to being imported as a module in another script). If the script is executed directly, the body of the conditional will execute.\n\n- **Purpose**: This guard ensures that the code within it runs only when the script is the entry point of the execution. This is a common pattern in Python for script usability and modularity.\n\n### `main()`\n\nWhen the script runs directly, it will call the `main` function imported from the `cli` module. \n\n- **Purpose of the `main` function**: Although the functionality of `main` isn't defined within this script, it's implied that this function is responsible for the primary interactions of the CLI application, likely handling user inputs, executing commands, and managing application logic.\n\n## Execution Flow\n\n1. **Script Initialization**: When the script is executed, Python initializes the script.\n  \n2. **Importing Required Functions**: The `main` function from the `cli` module is imported, making it available for use later in the script.\n\n3. **Checking the Execution Context**: The script checks if it is being run as a standalone program with `if __name__ == \"__main__\"`.\n\n4. **Calling the Main Function**: If the script passes the execution context check, it will call the `main` function to start the CLI application.\n\n## Final Thoughts\n\nThis script's simplicity is indicative of its role as a launcher for a CLI application. It delegates the core functionality to the `main` function found in the `cli` module, maintaining a clean separation of concerns. By following this pattern, the script remains modular and can easily be imported into other Python scripts, if needed, without executing the `main` function unintentionally. \n\nIn modular applications, such patterns increase code reusability and maintainability, making it easier for developers to build and manage larger systems.",
    "filename": "python/packages/agbench/src/agbench/__main__.py"
  },
  {
    "code": false,
    "content": "# AutoGenBench Command-Line Interface Documentation\n\nThis documentation provides a high-level overview of the `AutoGenBench` command-line tool, detailing its functionalities, commands, and how to use it effectively.\n\n## Overview\n\nThe `AutoGenBench` tool is designed for the execution and management of benchmark scenarios. The primary interface is a command-line interface (CLI) that allows users to perform various operations related to benchmarks. The script functions by interpreting command-line arguments, routing them to the corresponding functionality, and providing help or usage instructions if necessary.\n\n## Command Structure\n\nThe CLI recognizes several commands that can be executed by the user. Each command has an associated description and function that dictates its behavior:\n\n- **Commands**:\n  - `run`: Executes a given benchmark configuration.\n  - `tabulate`: Formats and displays the results of a previously executed benchmark run.\n  - `lint`: Checks the benchmark configuration for any errors or issues.\n  - `remove_missing`: Deletes directories that do not contain any results.\n  - `--version`: Outputs the version of `AutoGenBench`.\n  - `--help`: Displays help information regarding available commands.\n\nThe commands are encapsulated within a list of `CommandSpec` dictionaries, each containing a command name, a description, and a callable associated with the command (if applicable).\n\n## Main Function\n\nThe script's execution starts with the `main` function, which serves as the entry point. The function accepts an optional list of arguments, which defaults to the command-line arguments received when the script is invoked. \n\n### Argument Processing\n\n1. If no arguments are provided, it takes the entire command-line input.\n2. It sets the `invocation_cmd` variable to identify the tool name, `autogenbench`.\n3. The version string is constructed using the `__version__` variable imported from the `version` module.\n\n## Command Help and Usage\n\nThe script generates usage help text that clearly states how to use the command-line tool:\n\n- **Usage Template**:\n  ```\n  usage: autogenbench COMMAND ARGS\n  Where, COMMAND is one of: ...\n  ```\n\nThe commands and their descriptions are formatted for better readability, and any command can be paired with `--help` to access command-specific assistance.\n\n## Command Invocation Logic\n\nThe script handles command invocations as follows:\n\n1. It checks if the user has supplied enough arguments.\n2. It iterates through the defined commands and compares the command provided by the user (in `args[1]`) with the available commands.\n3. If a matching command is found:\n   - If it is the `--help` command, the help text is printed.\n   - For other commands, the associated function is called, passing along any additional arguments (i.e., `args[2:]`).\n\n## Error Handling\n\nIf a user supplies fewer than two arguments or specifies an invalid command:\n\n- The script will provide a usage message indicating the correct format and available commands.\n- If an unrecognized command is entered, an error message is displayed informing the user of the invalid command and provides a list of valid options.\n\n## Exiting the Script\n\nThe script uses `sys.exit()` to terminate the execution upon successful completion of a command, or in case of an error (returning appropriate exit codes). This ensures that the command-line environment is properly notified of the script's execution status, allowing for error handling in scripts that might invoke this tool programmatically.\n\n## Conclusion\n\nThe `AutoGenBench` CLI provides a straightforward way for users to run and manage benchmark configurations with various utility commands. The careful design of command handling, help messages, and error reporting contributes to a user-friendly experience, allowing effective interaction with benchmark scenarios. The structured command specification allows for expansion or modification of functionalities as needed in future iterations of the tool.",
    "filename": "python/packages/agbench/src/agbench/cli.py"
  },
  {
    "code": false,
    "content": "# Documentation for `__init__.py`\n\nThis `__init__.py` file is part of a Python package and serves as an entry point for the module. Its primary purpose is to facilitate the importation of specific classes and functions, making them accessible when the package is imported.\n\n## Purpose\n\nThe `__init__.py` file is executed when its containing package is imported, and it defines the public interface of the package. By specifying what is available for import with the `__all__` variable, it ensures that certain classes are explicitly included in the package's namespace.\n\n## Imports\n\nThe file imports the following items from the `_base` module:\n\n- **BaseQualitativeCoder**: A class that likely serves as a foundational or base class for qualitative coding functionality.\n- **Code**: This class might represent a coding mechanism used for qualitative analysis.\n- **CodedDocument**: This class probably encapsulates documents that have been coded, allowing for interaction with the coded data.\n- **Document**: This class likely represents a document within the system, possibly for qualitative research purposes.\n\n## Public API\n\nThe `__all__` variable indicates the public API of the package. This means that when someone uses a wildcard import (e.g., `from package import *`), only the names listed in `__all__` will be imported. This is useful for controlling the exposure of certain classes or functions and helps prevent namespace pollution.\n\n### Components in `__all__`:\n\n1. **Code**: It is expected to provide functionalities related to defining or managing codes used in qualitative data analysis.\n   \n2. **Document**: This class will likely have methods and attributes pertinent to handling textual or structured documents in the analysis process.\n\n3. **CodedDocument**: It serves as a wrapper around documents that have been processed with coding, permitting users to work with coded texts efficiently.\n\n4. **BaseQualitativeCoder**: This class is essential for defining a standard interface or behavior for various qualitative coding strategies.\n\n## Implications for Users\n\nBy including this `__init__.py` file in a package, users are provided with a clean and manageable way to utilize the key components of the package with minimal effort. They can directly access the classes without having to delve into the internal structure of the `_base` module.\n\n## Conclusion\n\nThis `__init__.py` file plays a critical role in streamlining the usability of the defined classes for qualitative analysis. By managing imports efficiently and controlling the public API, it provides clarity and ease of use for anyone wanting to leverage the functionality of this package in their qualitative research tasks.",
    "filename": "python/packages/agbench/src/agbench/linter/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Source Code\n\nThis documentation outlines the purpose and functionalities of the provided source code, which centers around modeling documents and associated coding examples using Pydantic data models.\n\n## Overview\n\nThe code utilizes Python's Pydantic library to define data models for representing documents and coding errors, encapsulated within three primary classes: `Document`, `CodeExample`, and `Code`. These models facilitate structured data handling and validation. Additionally, the code allows for the creation of a composite model, `CodedDocument`, which aggregates a `Document` and a set of associated `Code` instances.\n\n## Document Class\n\n### Purpose\n\nThe `Document` class is designed to represent textual documents. Each instance of this class includes a text content field and an optional name field.\n\n### Fields\n\n- **text**: Required field that contains the main content of the document.\n- **name**: Optional field for naming the document.\n\n### Methods\n\n- **`__hash__`**: This method overrides the default hashing mechanism to compute a hash value based on the document's text. It uses the MD5 hashing algorithm, encoding the text into UTF-8 for consistent results.\n\n## CodeExample Class\n\n### Purpose\n\nThe `CodeExample` class models a coding error linked to specific lines within a codebase. It provides a structured way to capture the reasons and contents of code examples.\n\n### Fields\n\n- **reason**: A mandatory human-readable explanation associating the example with a coding error.\n- **line_content**: Contains the precise content of the line where a coding issue is identified.\n- **line**: Indicates the starting line number of the error.\n- **line_end**: Specifies the ending line number of the identified error.\n\n## Code Class\n\n### Purpose\n\nThe `Code` class acts as a container for defining a coding error, including its associated examples, severity, and unique identification.\n\n### Fields\n\n- **name**: Required normalized unique name for the code, which should be lowercase and hyphen-separated.\n- **definition**: Provides a textual definition of the coding error.\n- **examples**: A list of `CodeExample` instances that cannot be empty, providing context for the code.\n- **severity**: An integer representing the severity level of the coding error, constrained to values 0, 1, or 2.\n- **id**: An optional identifier auto-computed based on the name and definition.\n- **merged_from**: An optional list of identifiers representing previously merged codes.\n\n### Methods\n\n- **`__init__`**: The constructor initializes the code attributes, applying normalization to the name and generating a unique ID if not provided. It also sets the `merged_from` attribute to `None`.\n- **`__hash__`**: Computes a unique hash based on the `id`, raising an error if the `id` has not been set.\n- **add_merged_from**: This method allows adding an identifier to the `merged_from` list if it is not already present, facilitating tracking of merged codes.\n\n## CodedDocument Class\n\n### Purpose\n\nThe `CodedDocument` class serves as a composite model that encapsulates a `Document` and its associated coding examples through `Code` instances.\n\n### Fields\n\n- **doc**: An instance of the `Document` class representing the document's content.\n- **codes**: A set of `Code` instances that provide additional coding error details related to the document.\n\n### Methods\n\n- **from_json**: A class method that constructs a `CodedDocument` instance from a JSON string. It parses the JSON, creating the `Document` and sets of `Code` instances for inclusion in the `CodedDocument`.\n\n## BaseQualitativeCoder Protocol\n\n### Purpose\n\nThe `BaseQualitativeCoder` is defined as a protocol to establish a standard interface for coding documents.\n\n### Method\n\n- **code_document**: This method is expected to take a `Document` and an optional set of `Code` instances, returning a `CodedDocument` if successful. It serves as a blueprint for any class implementing this protocol, ensuring consistent functionality across qualitative coders.\n\n## Summary\n\nIn essence, this code base lays the groundwork for a structured system that captures documents and their associated errors through well-defined models using Pydantic. The use of hashing for document and code identification ensures efficient data handling while validating data integrity. The inclusion of a protocol allows for extensibility in coding methodologies, making the system adaptable for future enhancements.",
    "filename": "python/packages/agbench/src/agbench/linter/_base.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\nThis document provides a high-level overview of a Python script designed for analyzing log files. The script interacts with an external OpenAI API to process log data, identify errors or inefficiencies, and produce a summary or structured output detailing its findings. Below is a concise breakdown of its components and their functionalities.\n\n## Imports and Dependencies\n\nThe script begins by importing several essential libraries and modules:\n\n- **argparse**: For command-line argument parsing.\n- **os**: For operating system functionalities like file handling.\n- **typing**: For type hinting, specifically `List`, `Optional`, and `Sequence`.\n\nAdditionally, it imports classes and functions from local modules:\n- **OpenAI**: A client for interfacing with OpenAI's APIs.\n- **CodedDocument** and **Document**: Data structures presumably defined in the `_base` module.\n- **OAIQualitativeCoder**: A class responsible for coding log documents, imported from the `coders.oai_coder` module.\n\n## Function: `prepend_line_numbers`\n\n```python\ndef prepend_line_numbers(lines: List[str]) -> List[str]:\n```\nThis function takes a list of strings (lines) and returns a new list where each line is prefixed with its line number, right-justified to align properly.\n\n### Parameters:\n- `lines`: A list of strings representing lines of the log file.\n\n### Returns:\n- A list of strings with each line prefixed by its corresponding line number.\n\n## Function: `load_log_file`\n\n```python\ndef load_log_file(path: str, prepend_numbers: bool = False) -> Document:\n```\nThis function loads a log file from the provided path and creates a `Document` object.\n\n### Parameters:\n- `path`: A string representing the file path.\n- `prepend_numbers`: A boolean indicating whether to prepend line numbers.\n\n### Returns:\n- A `Document` object containing the log text and its absolute path.\n\n### Logic:\n- Reads the log file content, prepends line numbers if specified, and constructs a Document object.\n\n## Function: `code_log`\n\n```python\ndef code_log(path: str) -> Optional[CodedDocument]:\n```\nThe `code_log` function utilizes the `OAIQualitativeCoder` to analyze the log file located at the specified path.\n\n### Parameters:\n- `path`: A string representing the file path.\n\n### Returns:\n- An optional `CodedDocument` object containing coded results, or raises a `FileNotFoundError` if the file does not exist.\n\n### Logic:\n- Checks if the input path is a file, loads the corresponding log file, and applies the coder to the document.\n\n## Function: `print_coded_results`\n\n```python\ndef print_coded_results(input_path: str, coded_doc: CodedDocument) -> None:\n```\nThis function formats and prints the results of the coding process to the console, highlighting any errors found in the log.\n\n### Parameters:\n- `input_path`: A string representing the path to the log file.\n- `coded_doc`: A `CodedDocument` object containing results of the assessment.\n\n### Logic:\n- Counts the number of errors and categorizes them by severity, using ANSI colors for visual distinction.\n- Prints the severity level, name, and definition of each error, along with examples of each error from the log file.\n\n## Function: `get_log_summary`\n\n```python\ndef get_log_summary(input_path: str) -> str:\n```\nThis function generates a one-sentence summary of the log file using the OpenAI API.\n\n### Parameters:\n- `input_path`: A string representing the path to the log file.\n\n### Returns:\n- A summary string generated by OpenAI based on the log contents.\n\n### Logic:\n- Loads the log file without line numbers and submits its text to the OpenAI client for summarization.\n\n## Function: `code_command`\n\n```python\ndef code_command(input_path: str) -> None:\n```\nThis function encapsulates the process of coding a log file, calling various helper functions and managing their output.\n\n### Parameters:\n- `input_path`: A string representing the path to the log file.\n\n### Logic:\n- Validates the existence of the file, prints a summary, codes the log, and prints the results. If any step fails, it raises appropriate errors.\n\n## Function: `lint_cli`\n\n```python\ndef lint_cli(args: Sequence[str]) -> None:\n```\nThis function serves as the entry point for command-line interaction, parsing arguments related to log file analysis.\n\n### Parameters:\n- `args`: A sequence of string arguments from the command line.\n\n### Logic:\n- Sets up argument parsing with `argparse`, collects the log file path, and then calls `code_command` with the parsed log file path.\n\n## Summary\n\nThe Python script is an effective tool for analyzing log files. By integrating functionality for reading, coding, and summarizing log entries, it helps users identify errors efficiently. Its design allows users to interact with the script through a command-line interface, making it accessible for various use cases in log analysis.",
    "filename": "python/packages/agbench/src/agbench/linter/cli.py"
  },
  {
    "code": false,
    "content": "It seems there was no code provided for analysis. Please share the source code you would like me to review, and I'll be glad to help you with a high-level description and documentation.",
    "filename": "python/packages/agbench/src/agbench/linter/coders/__init__.py"
  },
  {
    "code": false,
    "content": "# OAIQualitativeCoder Documentation\n\nThis documentation provides a high-level overview of the `OAIQualitativeCoder` class and its associated methods. The primary purpose of this code is to facilitate qualitative coding of documents through interaction with an OpenAI model, effectively identifying and categorizing errors found in logs.\n\n## Overview\n\nThe `OAIQualitativeCoder` class is designed to process documents by generating a set of error codes based on qualitative analysis. By leveraging the capabilities of an OpenAI model, the coder can recognize specific issues in text logs and provide structured and meaningful codes for those issues. This is particularly useful in maintaining clarity and consistency when reviewing logs containing complex information.\n\n### Dependencies\n\nThe implementation imports several libraries and modules:\n- `os`: For handling directory paths and file operations.\n- `re`: For regular expressions, specifically used to sanitize the text by removing control characters.\n- `List`, `Optional`, and `Set` from `typing`: Used for type hinting.\n- `OpenAI`: A client library to interact with OpenAI services.\n- `BaseModel` from `pydantic`: For data validation and settings management.\n- Custom imports from a base module (`.._base`) which includes essential classes like `BaseQualitativeCoder`, `Code`, `CodedDocument`, and `Document`.\n\n## CodeList Class\n\n### Purpose\n\nThe `CodeList` class extends `BaseModel` from Pydantic to handle a list of qualitative codes (`code_list`). It acts as a data structure to manage and validate a collection of coding terms that the OpenAI model generates during analysis.\n\n### Attributes\n- `code_list`: A list of `Code` instances, which contains the generated error codes and their descriptions.\n\n## Utility Function: remove_control_characters\n\n### Purpose\n\nThe `remove_control_characters` function is a utility method designed to clean the input text by removing non-printable control characters that might interfere with processing.\n\n### Implementation\n\n- Accepts a single string `text` as input.\n- Uses regular expressions to identify and replace control characters with an empty string, resulting in sanitized text.\n\n## OAIQualitativeCoder Class\n\n### Initialization\n\nThe constructor (`__init__`) sets up the `OAIQualitativeCoder` instance:\n- `cache_dir`: Path to the directory used for caching results. Default is `.cache`.\n- `model`: Specifies the OpenAI model to use, defaulting to `\"gpt-4o\"`.\n- `cache_enabled`: A boolean flag to determine whether caching functionality should be activated.\n\n## Method: code_document\n\n### Purpose\n\nThe `code_document` method is the core functionality of the `OAIQualitativeCoder` class. It handles the process of coding a document either by generating new codes from scratch or by modifying existing codes based on a supplied set.\n\n### Parameters\n- `doc`: An instance of `Document` to be analyzed and coded.\n- `code_set`: An optional set of `Code` instances that may contain pre-defined codes for the analysis.\n\n### Steps\n\n1. **Hashing and Caching**:\n   - A hash of the document is computed to uniquely identify it.\n   - If caching is enabled, the method checks for an existing cache file to reuse previous results.\n\n2. **Cleaning the Document**:\n   - The text of the document is sanitized using the `remove_control_characters` function to ensure that incompatible characters do not disrupt processing.\n\n3. **Coding Process**:\n   - If no `code_set` is provided, a new set of error codes is generated by prompting the OpenAI model with specific instructions regarding clarity and naming conventions.\n   - If a `code_set` is provided, the existing codes are modified based on the provided document's content. This includes adapting the examples associated with each error code to better fit the context of the document.\n\n4. **Handling the Response**:\n   - The results from the OpenAI model are parsed, and error codes are extracted and validated.\n   - If coding is successful, a `CodedDocument` instance is created which includes the original document and its associated codes.\n\n5. **Caching the Result**:\n   - If caching is enabled and coding was successful, the results are stored in the designated cache file.\n\n6. **Error Handling**:\n   - Clear error messages are raised if the coding process fails at any stage, ensuring that problems are promptly identified for debugging.\n\n### Returns\n\nReturns an instance of `CodedDocument` containing the original document and its corresponding codes. \n\n## Conclusion\n\nThe `OAIQualitativeCoder` class integrates qualitative analysis with advanced machine learning capabilities to provide structured coding for documents. By allowing both new code generation and existing code appropriation, it facilitates a flexible yet rigorous approach to managing qualitative data. This implementation is particularly useful for researchers and analysts who require precise categorization and understanding of error logs.",
    "filename": "python/packages/agbench/src/agbench/linter/coders/oai_coder.py"
  },
  {
    "code": false,
    "content": "# Module Loader Documentation\n\nThis document provides a high-level overview and detailed description of the functionality implemented in the provided module-loading script.\n\n## Overview\n\nThe script defines a function named `load_module` that facilitates the dynamic loading of Python modules from a specified file path. This is achieved using the `importlib` library, which provides an interface for importing modules in a programmatic manner. The function extracts the module's name from the file path, creates a module specification, and executes it to make the module available during runtime.\n\n## Function: `load_module`\n\n### Purpose\n\nThe `load_module` function is designed to load a Python module from a given file path dynamically. It takes care of creating a module specification, assigning the module to the system modules, and executing the module so all its contents can be utilized.\n\n### Parameters\n\n- `module_path` (str): A string representing the file system path to the Python file (e.g., `.py` file) that is to be loaded as a module.\n\n### Steps and Functionality\n\n1. **Extracting the Module Name**:\n    - The function uses `os.path.basename` to obtain the base name of the module file (i.e., discarding the directory path) and subsequently replaces the `.py` extension with an empty string to form the module name.\n\n    ```python\n    module_name = os.path.basename(module_path).replace(\".py\", \"\")\n    ```\n\n2. **Creating a Module Specification**:\n    - It utilizes `importlib.util.spec_from_file_location` to create a module specification based on the derived module name and the provided file path. This specification acts as a blueprint for the module's loading process.\n\n    ```python\n    spec = importlib.util.spec_from_file_location(module_name, module_path)\n    ```\n\n3. **Validation of Module Specification**:\n    - The function checks if the `spec` is `None`. If it is, it raises a `ValueError` to indicate that it could not load the module from the provided path.\n\n    ```python\n    if spec is None:\n        raise ValueError(f\"Could not load module from path: {module_path}\")\n    ```\n\n4. **Creating the Module Object**:\n    - Following the successful creation of the specification, `importlib.util.module_from_spec` is called to create a new module object based on the specification.\n\n    ```python\n    module = importlib.util.module_from_spec(spec)\n    ```\n\n5. **Registering the Module**:\n    - The newly created module is registered in `sys.modules` using the derived module name. This ensures that the module can be accessed globally from within the program after its loading.\n\n    ```python\n    sys.modules[module_name] = module\n    ```\n\n6. **Executing the Module**:\n    - The function then asserts that the module loader is not `None` and calls `spec.loader.exec_module(module)` to execute the module code. This step executes any top-level code in the loaded module and initializes its namespace.\n\n    ```python\n    assert spec.loader is not None\n    spec.loader.exec_module(module)\n    ```\n\n7. **Returning the Loaded Module**:\n    - Upon successful execution, the function returns the imported module, allowing the caller to use its attributes, functions, and classes.\n\n    ```python\n    return module\n    ```\n\n## Conclusion\n\nThe `load_module` function provides an effective way to dynamically load Python modules from specified file paths. This functionality is particularly useful in scenarios such as plugin systems, dynamic imports based on user input, or when loading modules for testing purposes. By utilizing `importlib`, the function ensures compatibility with Python\u2019s import system, providing a robust and versatile method for working with Python modules.",
    "filename": "python/packages/agbench/src/agbench/load_module.py"
  },
  {
    "code": false,
    "content": "# Documentation: Remove Missing Results Script\n\nThis script is designed to manage and clean up directories containing run logs for tasks, specifically focusing on removing folders that lack expected results. It checks the contents of log files to determine if a task has completed successfully and provides options to delete folders based on the results of these checks.\n\n## Overview\n\nThe script utilizes several Python modules, including `argparse`, `os`, `shutil`, and `sys`, to facilitate file handling, command-line argument parsing, and logging behaviors. The core functionalities include:\n1. Checking for completion based on console log contents.\n2. Deleting directories with missing results after user confirmation or unconditionally.\n3. Handling command-line inputs to guide the user through the cleanup process.\n\n## Functions\n\n### 1. `default_scorer(instance_dir: str) -> bool`\n\nThe `default_scorer` function assesses the state of a specific instance directory by examining its `console_log.txt` file. It will return `True` if the log file indicates successful completion of a task without errors, determined by the following checks:\n- Presence of specific keywords indicating task completion: `\"FINAL ANSWER:\"`, `\"SCENARIO.PY COMPLETE !#!#\"`, and `\"RUN.SH COMPLETE !#!#\"`.\n- Absence of any error codes in the last ten lines of the log content.\n\nIf the log file does not exist or the checks fail, it will print the log contents to the console for assessment and return `False`.\n\n### 2. `delete_folders_with_missing_results(runlogs_path: str, noconfirm: bool = False) -> None`\n\nThis function iterates through subdirectories under the specified `runlogs_path`. It performs the following operations:\n- For each subdirectory, it performs checks using the `default_scorer` function to assess if the instance directories contain missing results.\n- If an instance is not successful (missing results), it will either prompt the user for confirmation before deletion or delete the folder outright if `noconfirm` is set to `True`.\n- The function counts and reports the total number of deleted folders.\n\n### 3. `remove_missing_cli(args: Sequence[str]) -> None`\n\nThis function is designed to handle command-line inputs and orchestrate the main execution of the script. Its tasks include:\n- Parsing command-line arguments to specify the log directory and the `noconfirm` flag.\n- Validating that the provided path is valid.\n- Asking the user if they have modified the `default_scorer` function before commencing the deletion process.\n- Calling `delete_folders_with_missing_results` with the appropriate parameters based on parsed arguments.\n\n## Main Execution Flow\n\nThe script begins execution at the main block, which performs the following steps:\n1. Validates input arguments to ensure at least one positional argument (the `runlogs_path`) is provided.\n2. Checks whether the `runlogs_path` is a valid directory.\n3. Prompts the user to confirm that the `default_scorer` function is appropriately modified.\n4. Invokes the `delete_folders_with_missing_results` function to begin the cleanup process.\n\n## User Interaction\n\nThe script allows for user interaction mainly through command-line prompts:\n- If `noconfirm` is not set, the user is asked to confirm whether they have modified the scoring function. This is crucial as the scoring logic determines whether directories are marked for deletion.\n- During the deletion process, if a directory is identified as having missing results, a confirmation prompt is displayed before any deletion occurs.\n\n## Error Handling\n\nThroughout the script, there are checks to ensure:\n- Valid paths are provided to prevent runtime errors.\n- Proper feedback is given to users when files or directories do not meet expectations, ensuring they are well informed about the script's operations.\n\n## Summary\n\nThis Python script serves as a utility for managing and cleaning directories of task results by effectively checking for missing data in log files and providing the necessary means to delete incomplete instances, adapting to user preferences on confirmation handling.",
    "filename": "python/packages/agbench/src/agbench/remove_missing_cmd.py"
  },
  {
    "code": false,
    "content": "# AGBench Scenario Runner Documentation\n\n## Overview\n\nThis script serves as a command-line interface (CLI) tool for running scenarios defined in JSONL format using the AGBench system. It provides functionality for executing these scenarios in different contexts, including natively on a local machine or within Docker containers. This tool is designed for researchers and developers working on AI-driven applications, allowing them to run experiments under controlled environments while recording results systematically.\n\n## Key Components\n\n### Imports and Dependencies\n\nThe script imports several modules to provide functionality for various tasks:\n\n- Standard libraries: `argparse`, `errno`, `json`, `logging`, `os`, `pathlib`, `random`, `re`, `shutil`, `stat`, `subprocess`, `sys`, `time`, and `traceback`.\n- Multithreading: `multiprocessing.Pool` allows parallel execution of scenarios.\n- Docker interface: The `docker` library is used for interaction with Docker containers.\n- Azure SDK: To authenticate Azure services, it imports specific classes and functions for token management.\n\n### Constants and Variables\n\nThis section defines important constants that dictate the behavior of the script:\n- `SCRIPT_PATH`, `SCRIPT_NAME`, `SCRIPT_DIR`: Variables to determine the script's location.\n- `TASK_TIMEOUT`: The maximum execution time for tasks (set to 120 minutes).\n- Default paths and filenames, such as template paths and environment files.\n- Random number generator for sampling tasks.\n\n### Class Definitions\n\n#### ScenarioInstance\n\nA `TypedDict` representing the structure of a scenario instance defined in JSON format. \n\n```python\nclass ScenarioInstance(TypedDict):\n    id: str\n    template: Union[str, List[Union[str, List[str]]]]\n    substitutions: Dict[str, Dict[str, str]]\n    values: Dict[str, Dict[str, str]]\n```\n\n## Main Functions\n\n### run_scenarios\n\n```python\ndef run_scenarios(\n    scenario: str, \n    n_repeats: int, \n    is_native: bool,\n    config_file: Union[None, str],\n    token_provider: Optional[Callable[[], str]],\n    docker_image: Optional[str] = None,\n    results_dir: str = \"Results\",\n    subsample: Union[None, int, float] = None,\n    env_file: Union[None, str] = None,\n) -> None\n```\n\nThis function executes a set of scenarios from specified files or folders. Key features include:\n\n- Reading scenario definitions from JSONL files, potentially applying subsampling to reduce the number of runs.\n- Creating structured directories to save results, ensuring data is organized by scenario and instance.\n- Utilizing environment variables for configuration and authentication (e.g., Azure).\n\n### expand_scenario\n\n```python\ndef expand_scenario(\n    scenario_dir: str, \n    scenario: ScenarioInstance, \n    output_dir: str, \n    config_file: Union[str, None]\n) -> None\n```\n\nThis function handles the preparation of scenario instances by copying necessary files and directories. It populates template files with specific substitutions before executing the scenario. This includes initialization of configuration files if present.\n\n### get_scenario_env\n\n```python\ndef get_scenario_env(token_provider: Optional[Callable[[], str]] = None, env_file: str | None = None) -> Dict[str, str]\n```\n\nThis function retrieves a dictionary of environment variables necessary for running the scenario. It supports reading from an optional external environment file (YAML or JSON) and includes Azure token generation if required.\n\n### substitute_env_variables\n\n```python\ndef substitute_env_variables(json_data: Any) -> None\n```\n\nThis utility function recursively replaces placeholders in environment variables (in the form of `${VARIABLE_NAME}`) within provided JSON data using the current environment values.\n\n## Scenario Execution\n\n### run_scenario_natively\n\n```python\ndef run_scenario_natively(work_dir: str, env: Dict[str, str], timeout: int = TASK_TIMEOUT) -> None\n```\n\nExecutes a scenario in the local Python environment, creating a virtual environment specifically for that execution. It generates a script (`run.sh`) that sets up the environment and runs the defined scenario.\n\n### run_scenario_in_docker\n\n```python\ndef run_scenario_in_docker(\n    work_dir: str, \n    env: Dict[str, str], \n    timeout: int = TASK_TIMEOUT, \n    docker_image: Optional[str] = None\n) -> None\n```\n\nThis function manages the Docker execution of scenarios. It prepares a Docker container with a specified or default image, runs the scenario, collects logs, and handles graceful termination if necessary. It also includes facilities for mounting relevant volumes to ensure data continuity between the host and container.\n\n## Utility Functions\n\n### build_default_docker_image\n\n```python\ndef build_default_docker_image(docker_client: docker.DockerClient, image_tag: str) -> None\n```\n\nThis function builds a Docker image from a specified Dockerfile if the image is not found in the local repository. \n\n### find_autogen_repo\n\n```python\ndef find_autogen_repo(path: str) -> Optional[str]\n```\n\nUtility function to locate the root directory of the AutoGen repository by checking directory structure for specific files.\n\n### split_jsonl\n\n```python\ndef split_jsonl(file_path: str, num_parts: int) -> List[List[Dict[str, Any]]]\n```\n\nSplits a JSONL file into smaller parts for parallel processing, shuffling the data for better distribution among workers.\n\n### mkdir_p\n\n```python\ndef mkdir_p(path: str) -> None\n```\n\nCreates a directory and handles potential race conditions gracefully, ensuring that existing directories do not cause failures.\n\n## CLI Execution\n\n### run_cli\n\n```python\ndef run_cli(args: Sequence[str]) -> None\n```\n\nThe main entry-point function for executing the CLI tool. It parses arguments provided via the command line, including options for the scenario file, repetition count, subsampling, parallel processing, and environment configurations.\n\nThis function also validates input, checks file availability, and triggers appropriate execution pathways based on user choices.\n\n---\n\nThis documentation serves as an overview of the AGBench script, detailing its purpose, structure, and key functionalities to help users understand and utilize its features effectively.",
    "filename": "python/packages/agbench/src/agbench/run_cmd.py"
  },
  {
    "code": false,
    "content": "# Overview of the Script\n\nThis script is designed to facilitate the tabulation of results from a previous run of tasks scattered across multiple directories. It allows users to assess the success and timing of these runs through command line arguments. The script employs a combination of file operations, regular expressions, and data structures to collect and present this information in a human-readable format. It optionally supports outputting results in CSV or Excel formats using the popular `pandas` and `tabulate` libraries.\n\n## Imports and Constants\n\n### Imports\n\nThe script begins by importing necessary modules:\n\n- **argparse:** To handle command-line arguments.\n- **os, re, sys:** For file and directory management, regular expressions, and system-level interactions.\n- **pandas:** For creating and manipulating DataFrames to structure the results.\n- **tabulate:** For formatting DataFrames into appealing tables.\n\n### Constants\n\nSeveral string constants and configurations are defined at the start of the script:\n\n- **SCRIPT_PATH, SCRIPT_NAME, SCRIPT_DIR:** Variables to help understand the current script's location.\n- **TABULATE_FILE:** The expected name of a custom tabulation module file.\n- **SUCCESS_STRINGS and COMPLETED_STRINGS:** Lists of strings to identify successful and complete runs in log files.\n- **EXCLUDE_DIR_NAMES:** Directories that should be ignored during the execution.\n- **TIMER_REGEX:** A regex pattern to capture runtime duration from log files.\n\n## Functions Overview\n\n### `find_tabulate_module`\n\nThe `find_tabulate_module` function searches for the specified `TABULATE_FILE` in a given directory and its parent directories, stopping when it reaches the root or a specified stopping directory. If found, it returns the path to that file; otherwise, it returns `None`.\n\n#### Key Features:\n\n- Handles both directory checks and extraction of absolute paths.\n- Supports searching within nested directories (`Scripts` and `scripts`).\n- Stops searching either upon reaching a pre-defined stop directory or the system root.\n\n### `default_scorer`\n\nThe `default_scorer` function checks for a logfile and determines if a run was successful or completed without success based on predefined success and completion strings. It returns:\n\n- `True` if successful.\n- `False` if completed without success.\n- `None` if the run has not completed.\n\n#### Key Features:\n\n- Reads a specific 'console_log.txt' to gather results.\n- Uses string matching to determine the final state of the run.\n\n### `default_timer`\n\nThe `default_timer` function extracts the runtime of the operation from the specified log file using a regex pattern. It returns the runtime as a float if available, or `None` otherwise.\n\n#### Key Features:\n\n- Similar mechanism as `default_scorer` for locating and reading 'console_log.txt'.\n- Employs regex for flexible extraction of runtime data.\n\n## Main Functionality: `default_tabulate`\n\nThe `default_tabulate` function orchestrates the results tabulation process. It performs the following tasks:\n\n- Defines a command-line interface for specifying options like the run log path, and output formats (CSV or Excel).\n- Collects results from subdirectories representing different tasks and their instances.\n- Uses the scorer and timer functions to populate a structured list of task results.\n- Generates a Pandas DataFrame from the collected results and prints it in either tabulated or CSV format.\n\n### Detailed Workflow:\n\n1. **Argument Parsing:** Parses command-line arguments using `argparse`.\n2. **Directory Traversal:** Iterates through run log directories, collecting results for each subdirectory labeled as a task.\n3. **DataFrame Creation:** Produces a Pandas DataFrame from the results, ensuring consistency by padding missing trials with `None`.\n4. **Output Handling:** Outputs results in the specified format (CSV or tabulated) and computes summary statistics (success rates, timings).\n\n## Summary Statistics\n\nThe script calculates and prints aggregate statistics, including:\n\n- **Success Counts:** The total number of successes, failures, and missing trials for each instance.\n- **Average Success Rates:** Derived from the success counts amidst all attempts.\n- **Average Timings:** Both total and average time taken for each instance.\n\nThis information is presented in well-formatted tables using the `tabulate` library, ensuring clarity in results presentation.\n\n## CLI Entry Point: `tabulate_cli`\n\nThe `tabulate_cli` function serves as the main entry point for the command-line interface. It manages argument parsing and loads either a custom tabulation module (if found) or defaults to the `default_tabulate` implementation. \n\n### Role and Features:\n\n1. **Argument Examination:** It examines the command line arguments to ascertain the correct execution context and format.\n2. **Module Loading:** It attempts to load a custom user-defined module dynamically if specified, providing flexibility in how tabulation can be customized.\n3. **Fallback Mechanism:** Default behavior is provided through `default_tabulate` if no custom module is found, ensuring the script is robust against variations in user requirements.\n\n## Conclusion\n\nThis script is a well-structured tool for users needing to analyze and summarize the results from a series of computational tasks. With flexible output options and a straightforward command-line interface, it serves both developers and researchers who need to interpret and present performance metrics from automated runs. The organization of the script, alongside its internal functions, reflects good design principles that factor in usability and maintainability.",
    "filename": "python/packages/agbench/src/agbench/tabulate_cmd.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Version Information\n\nThe line of code defines a special variable `__version__` which holds a string indicating the version of the software. In this case, the version is `\"0.0.1a1\"`. \n\n### Purpose of the Version Variable\n- **Versioning Standards**: The format `\"0.0.1a1\"` follows semantic versioning conventions, which can be broken down as follows:\n  - **Major Version (0)**: Indicates a potentially unstable version with major changes. Often used for pre-release versions.\n  - **Minor Version (0)**: Zero indicates that no minor features have been added, still early in development.\n  - **Patch Version (1)**: Indicates initial fixes or smaller updates that may patch issues in the pre-release.\n  - **Pre-release Label (a1)**: The `a` stands for alpha, marking this as an early testing version, and `1` means it is the first alpha release.\n\n## Importance in Development \nThe `__version__` variable serves several important functions in software development:\n\n- **Identifying Software State**: It informs users and developers about the state and stability of the software. \n- **Dependency Management**: Tools that manage packages (like `pip` in Python) can use this versioning to determine whether updates or specific versions of dependencies are required.\n- **Documentation and Communication**: It allows developers to convey important changes or features associated with specific versions, aiding users in understanding updates.\n\n## Usage Context\nIn practical scenarios, `__version__` is typically found in:\n\n- **Modules or Packages**: It can be included in Python modules or packages to allow for easy identification of their version.\n- **Documentation and Changelogs**: This variable often appears alongside documentation to clarify the specific version being referred to.\n- **Version Checks**: Conditional statements in code may use this variable to enable or disable features based on the version installed.\n\n## Summary\nWhile the provided code snippet is simply defining a version number, it encapsulates a critical aspect of software management that helps in version control, dependency handling, and communication about software states between developers and users. The designation of this version as a pre-release implies that it is still undergoing testing and should be used cautiously by those who choose to adopt it early in its lifecycle.",
    "filename": "python/packages/agbench/src/agbench/version.py"
  },
  {
    "content": "# AutoGen AgentChat\n\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html)\n\nAgentChat is a high-level API for building multi-agent applications.\nIt is built on top of the [`autogen-core`](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html) package.\nFor beginner users, AgentChat is the recommended starting point.\nFor advanced users, [`autogen-core`](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)'s event-driven\nprogramming model provides more flexibility and control over the underlying components.\n\nAgentChat provides intuitive defaults, such as **Agents** with preset\nbehaviors and **Teams** with predefined [multi-agent design patterns](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/intro.html).",
    "filename": "python/packages/autogen-agentchat/README.md"
  },
  {
    "code": false,
    "content": "# Module Overview: `autogen_agentchat`\n\nThis module serves as the main entry point for the `autogen_agentchat` package. It is primarily responsible for setting up loggers and retrieving the version of the package. Below is a detailed breakdown of its components.\n\n## Logger Definitions\n\n### TRACE_LOGGER_NAME\n\n```python\nTRACE_LOGGER_NAME = \"autogen_agentchat\"\n```\n\n- **Purpose**: This constant defines the logger name used for trace logs within the package.\n- **Role**: The trace logger is typically employed to capture detailed logs that can be useful for debugging and tracing the flow of execution within the application. By using a specific logger name, it allows for better organization and filtering of log messages related to the package's internal operations.\n\n### EVENT_LOGGER_NAME\n\n```python\nEVENT_LOGGER_NAME = \"autogen_agentchat.events\"\n```\n\n- **Purpose**: This constant defines the logger name intended for event logs.\n- **Role**: The event logger focuses on recording significant events that occur within the application, such as user actions or system milestones. This separation from trace logs allows developers to distinguish between detailed debugging information and high-level events that indicate the overall status or behavior of the application.\n\n## Version Retrieval\n\n```python\n__version__ = importlib.metadata.version(\"autogen_agentchat\")\n```\n\n- **Purpose**: The module retrieves and stores the current version of the `autogen_agentchat` package.\n- **Role**: By using the `importlib.metadata` module, it ensures that the package version is automatically kept in sync with the installed version. This is particularly useful for applications that may need to check compatibility, log versions, or inform users about the current version in use.\n\n## Summary\n\nIn summary, this module is foundational for the `autogen_agentchat` package, as it establishes important logging mechanisms and provides the package version information. The defined loggers help facilitate effective monitoring and troubleshooting of the application, ensuring both developers and users can maintain a clear understanding of the application's operational state. This groundwork paves the way for further implementations that will leverage these logging capabilities and version information.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an initializer for various predefined agents that are part of the `AgentChat` package. Its purpose is to streamline access to different agent classes, providing a cohesive interface for users or developers who want to utilize these agents in their applications. The module primarily focuses on importing and cataloging different agent types from various submodules.\n\n## Imports\n\nThe module imports several classes from different submodules, each representing a specific type of agent. The imported components include:\n\n- **`AssistantAgent`**: A specialized agent that likely assists users in task completion or information retrieval.\n- **`BaseChatAgent`**: The foundational class for all agents within `AgentChat`, providing shared functionalities and structures for other agents.\n- **`CodeExecutorAgent`**: This agent is capable of executing code and possibly handling requests and responses associated with code execution through types such as `ApprovalRequest` and `ApprovalResponse`.\n- **`MessageFilterAgent`**: An agent focused on filtering messages, possibly determining which messages to allow through based on certain criteria. It utilizes configs such as `MessageFilterConfig` and supports specific filtering methods through `PerSourceFilter`.\n- **`SocietyOfMindAgent`**: This agent may represent a complex architecture mimicking human-like cognitive processes, integrating various functions or processes to simulate thought patterns.\n- **`UserProxyAgent`**: An agent that acts as a proxy for user interactions, potentially managing user communications or decisions on behalf of the user.\n\n## Agent Overview\n\n### BaseChatAgent\n\nThe `BaseChatAgent` is the core class on which all other agents are built. It likely provides essential methods and properties that facilitate interaction between agents and users, establishing a common ground for communication and action.\n\n### AssistantAgent\n\n`AssistantAgent` derives from `BaseChatAgent` and serves a dedicated role of assisting users. This agent presumably incorporates functionalities designed to help users with inquiries, automate tasks, or provide recommendations based on user input.\n\n### CodeExecutorAgent\n\nThe `CodeExecutorAgent` is designed for executing code snippets or programs. With the associated `ApprovalRequest` and `ApprovalResponse` types, it may implement mechanisms for validating code execution requests and returning results, thus ensuring secure and efficient code handling.\n\n### MessageFilterAgent\n\nThe `MessageFilterAgent` serves the role of managing the flow of messages in the system. Using `MessageFilterConfig` for configuration and `PerSourceFilter` for filter criteria, this agent ensures that only relevant or permitted messages reach their intended targets, adding a layer of control and safety to communications.\n\n### SocietyOfMindAgent\n\nThe `SocietyOfMindAgent` represents a more intricate agent implementation that likely mimics the multi-faceted nature of human thought. By integrating various functionalities, it may facilitate a more nuanced communication approach, enabling complex interactions and decision-making processes.\n\n### UserProxyAgent\n\nThe `UserProxyAgent` acts on behalf of users in the agent framework. By managing user interactions, this agent helps streamline communications, enabling a layer of abstraction that simplifies user requests and enhances task automation.\n\n## Exported Members\n\nThe list categorized under `__all__` specifies which agents and components are publicly accessible when the module is imported. This encapsulation helps maintain a clean API, ensuring that only intended classes and types are exposed to users. It includes:\n\n- Essential agent classes like `BaseChatAgent`, `AssistantAgent`, `CodeExecutorAgent`, etc.\n- Important types such as `ApprovalRequest`, `ApprovalResponse`, and configurations related to message filtering.\n\n## Conclusion\n\nIn summary, this module is a foundational component of the `AgentChat` package, providing an organized way to initialize and access various specialized agents. Each imported agent class serves a distinct purpose, contributing to the overall functionality of the package. This design not only promotes modularity but also enhances the ability to create sophisticated, interactive applications centered around chat and automation.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/__init__.py"
  },
  {
    "code": false,
    "content": "# Assistant Agent Documentation\n\n## Overview\n\nThe `AssistantAgent` class is part of a larger framework designed to create interactive AI assistants that can process user inputs, perform computations, and utilize external tools and memory. The agent is structured to provide responses in both synchronous and asynchronous modes, with various configurations available to optimize performance based on the specific requirements of the application.\n\n### Imports and Dependencies\n\nThis module imports necessary libraries and modules, including:\n- `asyncio` for asynchronous programming.\n- `json` for handling JSON data.\n- Various components from the `autogen_core` library, which include models for messages, tools, and memory management.\n- `pydantic` for data validation and settings management using Python type annotations.\n\n### Agent Configuration\n\nThe `AssistantAgentConfig` class serves as a configuration data model for the assistant agent, specifying various parameters such as:\n- `name`: The name of the agent.\n- `model_client`: The model client responsible for generating responses.\n- `tools`: Optional tools that the agent can use.\n- `memory`: Optional components for memory, allowing the agent to recall prior interactions.\n- `max_tool_iterations`: Controls how many times the agent can call tools during a session.\n- `system_message`: A message that sets the context for the agent's behavior.\n\n## AssistantAgent Class\n\nThe `AssistantAgent` class extends the `BaseChatAgent` and implements the `Component` interface. It allows for interaction with users via messages, the execution of tool calls, and management of context and state.\n\n### Initialization\n\nThe `__init__` method handles the configuration and state initialization. Key features include:\n- Registering tools and workbenches, ensuring names are unique to prevent conflicts.\n- Loading memory components and configuring handoffs.\n- Setting a model context for managing the conversation history.\n\n### Message Handling\n\n#### `on_messages` Method\n\nThe `on_messages` method processes incoming messages and generates a response. It internally calls `on_messages_stream` to handle the responses asynchronously and yields the final response when ready.\n\n```python\nasync def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n    # Processes messages and generates a response.\n```\n\n#### `on_messages_stream` Method\n\nThis method allows for streaming responses. It handles the processing of incoming messages and produces outputs as events or messages, yielding each chunk of information as it is generated.\n\n```python\nasync def on_messages_stream(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[Union[BaseAgentEvent, BaseChatMessage, Response], None]:\n    # Processes messages and streams the response.\n```\n\n### Tool and Handoff Management\n\nThe agent is capable of calling predefined tools or running handoff operations when necessary. Tool call behavior varies depending on the configuration of `reflect_on_tool_use`, affecting how responses are generated when tools are utilized.\n\n#### Tool Call Execution\n\nTool calls are handled through private methods like `_execute_tool_call`, which processes the requests and returns the results to the caller. Any associated events are yielded during tool execution to ensure that users receive real-time feedback.\n\n### Streaming Responses\n\nThe agent supports both synchronous and asynchronous response modes. By enabling `model_client_stream`, the agent can stream model responses in smaller chunks, allowing for live updates. The response format can be customized to return either structured JSON data or plain text.\n\n## Context and Memory Management\n\n### Model Context\n\nThe `model_context` property manages the conversation state, allowing the agent to maintain a coherent memory of the interaction history. Options for context size can be limited to conserve resources.\n\n### Memory Integration\n\nThe agent can utilize memory components to recall past interactions, enriching the conversational context. `Memory` works with memory queries to fetch information that can be included in the conversation flow.\n\n```python\nasync def _update_model_context_with_memory(memory: Optional[Sequence[Memory]], model_context: ChatCompletionContext, agent_name: str) -> List[MemoryQueryEvent]:\n    # Updates the model context with memory content.\n```\n\n## Summary and Reflection\n\nAfter executing a task or tool calls, the agent can reflect on the outputs and summarize the results. The final output can either represent a comprehensive account of the tool calls or be a straightforward response generated from the agent's logic.\n\n### Example Configurations\n\nThe class documentation includes several examples demonstrating various configurations, such as:\n1. **Basic agent setup**: Instantiating an agent to perform simple tasks.\n2. **Streaming token responses**: Generating responses in a fluid, real-time format.\n3. **Tools integration**: Enabling the agent to use defined functions for enhanced interactions.\n\nThis framework effectively allows developers to construct versatile AI assistants capable of sophisticated interactions through structured configurations and smooth integration with external tools and services.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_assistant_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for BaseChatAgent Class\n\n## Overview\n\nThe `BaseChatAgent` class serves as an abstract base implementation for chat agents within a larger framework. It extends functionalities from the `ChatAgent`, `ABC`, and `ComponentBase` classes while incorporating asynchronous operations. The main goal of the `BaseChatAgent` class is to provide a structure for agents that can handle and respond to chat messages, maintain state across interactions, and allow for customization by subclassing.\n\n## Class Definition\n\n### Class Hierarchy\n\n- **ChatAgent**: Parent class that provides basic chat interaction functionalities.\n- **ABC**: Abstract base class that enforces implementation of specified abstract methods.\n- **ComponentBase**: A generic component base class which can be leveraged to define additional functionalities common to different components.\n  \n### Purpose\n\nTo create a new chat agent, developers are encouraged to subclass `BaseChatAgent` and implement several methods:\n\n- `on_messages`: to handle incoming messages.\n- `on_reset`: to reset the agent to its initial state.\n- `produced_message_types`: to specify the types of messages the agent can produce.\n\nIf streaming capabilities are required, subclasses should also implement the `on_messages_stream` method.\n\n## Initialization\n\n### Constructor\n\nThe `__init__` method initializes the agent with specific parameters:\n\n- **name**: The unique identifier for the agent.\n- **description**: A brief description outlining the capabilities of the agent.\n\nIt wraps the initialization process in a context manager to trace the creation of the agent. If the name provided is not a valid Python identifier, it raises a `ValueError`.\n\n### Properties\n\n- **name**: Returns the agent's unique name.\n- **description**: Returns the agent's description, detailing its capabilities.\n\n## Abstract Methods\n\n### `produced_message_types`\n\nThis property must be overridden in subclasses to define what types of messages the agent can generate. All types must derive from `BaseChatMessage`.\n\n### `on_messages`\n\nThis abstract method processes incoming messages and returns a response. It requires that new messages (not the entire conversation history) be passed on each call, and the agent maintains internal state across calls.\n\n### `on_reset`\n\nThis abstract method needs implementation in subclasses, providing functionality to reset the agent to its initialization state.\n\n## Message Handling\n\n### `on_messages_stream`\n\nThis asynchronous method handles incoming messages in a streaming fashion, ultimately yielding messages until the final response is produced. The method simply invokes `on_messages` and yields any messages found in the response alongside the response itself.\n\n### `run`\n\nThis method orchestrates the running of the agent based on a provided task. It accepts strings, single `BaseChatMessage` objects, or sequences of messages, constructing the appropriate input messages for processing. The method also captures and returns task results as a `TaskResult`, which includes all messages exchanged during the process.\n\n### `run_stream`\n\nSimilar to `run`, this method allows running the agent with the addition of yielding messages during processing. It supports streaming responses to consumers, with the final task result emitted at the end of the asynchronous stream.\n\n## State Management\n\n### Methods for State Preservation\n\n- **save_state**: Exports the agent's current state, with a default implementation that provides a stateless representation.\n- **load_state**: Accepts a state mapping and restores the agent\u2019s state, validating against a predefined schema.\n\n### Resource Management\n\nThe `close` method is included to release any resources used by the agent when it is no longer needed. By default, it is a no-operation method that subclasses can override to provide custom cleanup behavior.\n\n## Control Flow\n\n### Contextual Operations\n\n- **on_pause**: This asynchronous method can be overridden to implement custom functionality for when the agent is paused during message handling.\n- **on_resume**: Similar to `on_pause`, it provides hooks for resuming the agent's operation after being paused.\n\n## Conclusion\n\nThe `BaseChatAgent` class establishes a robust framework for developing stateful chat agents that can process and respond to messages asynchronously. By adhering to the outlined architecture and rules, developers can create customized agents that fit a wide range of applications in chat-driven environments.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_base_chat_agent.py"
  },
  {
    "code": false,
    "content": "# Code Executor Agent Documentation\n\nThis document provides an overview of the `CodeExecutorAgent`, a special type of bot agent designed to generate and execute code snippets based on user instructions. This agent integrates with a code execution environment, enabling it to run code while providing feedback and reflection on outcomes.\n\n## Overview\n\nThe `CodeExecutorAgent` is designed to create a dynamic interaction where users can request code executions directly or indirectly through other agents. It can generate code snippets using an LLM (Language Model) and, if connected to a `model_client`, it can also reflect on the results produced by the code.\n\nThis agent is customizable and features various configurations for code execution, approval processes, and handling results. The default behavior is to run code blocks encapsulated in markdown (triple backticks), permitting executions in specified programming languages.\n\n---\n\n## Key Classes and Functions\n\n### CodeExecutorAgentConfig\n\nThis class contains configuration options for the `CodeExecutorAgent`. Key parameters include:\n\n- **name**: Identifies the agent.\n- **code_executor**: Component model for running code.\n- **model_client**: Optional client for generating code.\n- **system_message**: Initial message that provides context for the model.\n- **supported_languages**: Lists the languages supported by the agent.\n\n```python\nclass CodeExecutorAgentConfig(BaseModel):\n    name: str\n    code_executor: ComponentModel\n    model_client: ComponentModel | None = None\n    description: str | None = None\n    sources: List[str] | None = None\n    system_message: str | None = None\n    model_client_stream: bool = False\n    supported_languages: List[str] | None = None\n```\n\n### Approval Models\n\nThe agent supports both synchronous and asynchronous approval functions through the following classes:\n\n- **ApprovalRequest**: Contains the code to execute and context information.\n- **ApprovalResponse**: Indicates whether the execution is approved.\n\n```python\nclass ApprovalRequest(BaseModel):\n    code: str\n    context: List[LLMMessage]\n\nclass ApprovalResponse(BaseModel):\n    approved: bool\n    reason: str\n```\n\n### CodeExecutorAgent\n\nThe `CodeExecutorAgent` class inherits from `BaseChatAgent`. It is responsible for the primary logic of executing code, managing approvals, and handling user interactions.\n\nKey methods in `CodeExecutorAgent` include:\n\n- **on_messages**: Processes incoming messages and returns responses.\n- **execute_code_block**: Executes the provided code blocks, including approval checks.\n- **extract_code_blocks_from_messages**: Extracts code blocks formatted in markdown from user messages.\n\n```python\nclass CodeExecutorAgent(BaseChatAgent, Component[CodeExecutorAgentConfig]):\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Logic to process messages\n        pass\n    \n    async def execute_code_block(self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken) -> CodeResult:\n        # Logic to execute provided code blocks after approval\n        pass\n```\n\n## Execution Workflow\n\nThe workflow for using the `CodeExecutorAgent` can be described step-by-step:\n\n1. **Receive Messages**: The agent listens for incoming messages, particularly those that may contain code blocks formatted in markdown.\n   \n2. **Extract Code Blocks**: Upon receiving a message, it extracts any valid code blocks using the markdown format.\n\n3. **Approval Process**: Before executing the code, the agent checks with an approval function (if configured). This function evaluates whether the code is safe to execute.\n\n4. **Execute Code**: If the code execution is approved, the agent runs the code in an appropriate environment (recommended to use Docker for isolation).\n\n5. **Handle Execution Results**: After execution, the agent captures the results, including any outputs or errors encountered during execution.\n\n6. **Reflect and Respond**: The agent reflects on the execution results and embeds this reflection in the output response, providing a final message to the user.\n\n---\n\n## Event Logging\n\nThe agent utilizes Python's logging framework. `EventLogger` captures significant events throughout the execution process, ensuring that all interactions and key decisions are logged for debugging and auditing purposes.\n\n## Configuration Considerations\n\n- **Safety and Security**: The approval function is crucial for ensuring that potentially harmful operations are not executed without consent. It is highly recommended to always have a robust approval mechanism in place.\n  \n- **Language Support**: The agent supports specific programming languages configured during initialization. Users must ensure that code snippets conform to these language restrictions.\n\n- **Docker Requirement**: For executing code in an isolated and secured environment, it is recommended to use a Docker container. Users must have Docker installed and operational.\n\n---\n\n## Examples of Use\n\n1. **Basic Execution with Approval**: \n    - Users submit code and receive a prompt for approval prior to actual execution.\n  \n2. **Group Chats with Multiple Agents**: \n    - The agent can operate within a team setting, processing commands and executing code generated by other agents in chat discussions.\n\n3. **Dynamic Code Generation**:\n    - Combined with a language model, the agent can autonomously generate code based on user queries, execute it, and provide contextual responses.\n\nThe `CodeExecutorAgent` provides flexible and powerful capabilities for executing code within a collaborative environment, integrating dynamically with various configurations according to project requirements.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_code_executor_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Message Filter Agent\n\n## Overview\n\nThe `MessageFilterAgent` is a component of a chatbot framework designed to filter incoming messages before they are processed by an internal agent. It is particularly useful in multi-agent workflows where context specificity is essential. By configuring filters, the agent can selectively decide which messages to pass on to the wrapped agent based on their source and position.\n\n## Message Filter Configuration\n\n### Classes and Data Structures\n\n#### `PerSourceFilter`\n\nThis class holds the configuration for filtering messages from specified sources. It has the following attributes:\n\n- `source`: A string identifying the message source (e.g., \"user\" or another agent).\n- `position`: An optional parameter (either \"first\" or \"last\") that specifies the position of the messages to be filtered.\n- `count`: An optional integer indicating the number of messages to keep from the specified position.\n\n#### `MessageFilterConfig`\n\nThis class encapsulates a list of `PerSourceFilter` instances. It serves as the main configuration for filtering messages from various sources. The `per_source` attribute consists of a list of `PerSourceFilter` objects.\n\n### `MessageFilterAgentConfig`\n\nThis class models the configuration for the `MessageFilterAgent`. It includes:\n\n- `name`: A string identifier for the agent.\n- `wrapped_agent`: An instance of `ComponentModel` representing the internal agent that will process the filtered messages.\n- `filter`: An instance of `MessageFilterConfig` that details how messages should be filtered.\n\n## Message Filter Agent Implementation\n\n### Class Definition: `MessageFilterAgent`\n\nThe `MessageFilterAgent` inherits from both `BaseChatAgent` and `Component`. Its primary role is to act as a wrapper that implements the message filtering behavior. \n\n#### Initialization\n\nThe constructor of `MessageFilterAgent` accepts three parameters:\n\n- `name`: The name of the agent.\n- `wrapped_agent`: An instance of a chat agent that will process filtered messages.\n- `filter`: An instance of `MessageFilterConfig` which defines the filtering rules.\n\nDuring initialization, it also sets a descriptive message for the agent.\n\n### Filtering Logic\n\n#### Method: `_apply_filter`\n\nThis method applies the specified filters to the incoming messages. It iterates over each `PerSourceFilter` in the configuration and extracts messages based on the defined criteria, whether to take the \"first\" or \"last\" messages for each source specified. The filtered results are collected and returned.\n\n### Handling Incoming Messages\n\n#### Method: `on_messages`\n\nAn asynchronous method, `on_messages`, handles incoming messages by first applying the filter through `_apply_filter`. It then passes the filtered messages to the `wrapped_agent` for additional processing and returns the response.\n\n#### Method: `on_messages_stream`\n\nThis method handles message streams asynchronously. Similar to `on_messages`, it first filters the messages and then yields responses from the `wrapped_agent`, allowing for real-time data handling.\n\n#### Method: `on_reset`\n\nThis method is responsible for resetting the internal state of the `wrapped_agent`, if necessary. It accepts a cancellation token to manage task cancellation.\n\n## Configuration Methods\n\n### `_to_config`\n\nThis method converts the current state of the `MessageFilterAgent` back into its configuration schema. It utilizes the `MessageFilterAgentConfig` class to return a structured representation of the agent's state.\n\n### `_from_config`\n\nThis class method constructs a `MessageFilterAgent` instance from a given configuration. It extracts the wrapped agent and filter configurations and creates a new instance.\n\n## Usage Examples\n\n### Basic Example\n\nAn example of creating a `MessageFilterAgent` might look like the following:\n\n```python\nagent_a = MessageFilterAgent(\n    name=\"A\",\n    wrapped_agent=some_other_agent,\n    filter=MessageFilterConfig(\n        per_source=[\n            PerSourceFilter(source=\"user\", position=\"first\", count=1),\n            PerSourceFilter(source=\"B\", position=\"last\", count=2),\n        ]\n    ),\n)\n```\n\nIn this configuration, agent A will only see the first message from the user and the last two messages from agent B.\n\n### Multi-Agent Interaction\n\nThe `MessageFilterAgent` can be used in complex interactions such as a looping multi-agent graph, where each agent is wrapped to ensure they only process relevant messages for their functionality. This leads to efficient decision-making without being cluttered by irrelevant message history. \n\nIn summary, the `MessageFilterAgent` provides flexible and powerful message filtering capabilities in a chatbot framework, enhancing the agent's ability to handle intricate workflows efficiently.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_message_filter_agent.py"
  },
  {
    "code": false,
    "content": "# SocietyOfMindAgent Documentation \n\n## Overview\n\nThe `SocietyOfMindAgent` is a sophisticated agent designed to harness the power of multiple inner agents, or a \"team,\" to collaboratively generate responses to user queries. This class is an extension of the `BaseChatAgent` and is composed with a direct integration of components that facilitate message handling and context management. The fundamental design framework employs a multi-agent approach to process and respond to messages using natural language models effectively.\n\n## Configuration\n\n### SocietyOfMindAgentConfig\n\nBefore creating an instance of `SocietyOfMindAgent`, a configuration must be defined through `SocietyOfMindAgentConfig`. This configuration includes essential parameters that determine the behavior and features of the agent:\n\n- **name**: The unique identifier for the agent.\n- **team**: A model defining the group of inner agents that will work together.\n- **model_client**: The ChatCompletion model client used to formulate responses.\n- **description**: Optional descriptive text about the agent's role.\n- **instruction**: Optional system-level instruction guiding the agent in response generation.\n- **response_prompt**: An optional specific prompt to generate the final output.\n- **model_context**: An optional context model for managing conversations with the underlying LLM (Language Model).\n\n## Core Class: SocietyOfMindAgent\n\nThe `SocietyOfMindAgent` class inherits from `BaseChatAgent` and implements methods for processing incoming messages and generating responses, utilizing its configured team of agents. The key features and properties of this class are:\n\n### Constructor\n\nThe constructor initializes the agent with its name, team, model client, and various optional parameters like description, instruction, response prompt, and model context:\n\n```python\ndef __init__(self, name: str, team: Team, model_client: ChatCompletionClient, ...):\n```\n\n### Default Settings\n\nThe class contains several constants that define default values for the instruction, response prompt, and description. These are applied when specific parameters are not provided during instantiation.\n\n### Message Handling\n\nThe primary mechanism for processing messages is through two asynchronous methods:\n- **on_messages**: This method consolidates and waits for responses from the inner team of agents.\n  \n```python\nasync def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n```\n\n- **on_messages_stream**: This method processes incoming messages in a streaming format and coordinates the interaction between the model client and the inner agents.\n\n```python\nasync def on_messages_stream(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n```\n\nIn these methods, the inner agents are called upon to generate their own messages based on the initial input, which are then compiled into a model-appropriate format for final output generation.\n\n## Team Collaboration\n\nThe agent utilizes a team-based approach wherein multiple sub-agents collaborate to produce a unified output. The entire team can run and yield results in a streaming manner through the `run_stream` method of the defined team:\n\n```python\nasync for inner_msg in self._team.run_stream(task=task, cancellation_token=cancellation_token, output_task_messages=False):\n```\n\nAs per the design, inner messages from this collaboration are processed and filtered before being wrapped into a final response that the agent sends back.\n\n## Context Management\n\nThe context within which messages are processed can be tailored using the `model_context`. This is particularly useful for limiting the amount of conversational history sent to the model, thus complying with any token limitations imposed by the underlying language model. The management functionalities include:\n\n- Adding messages to the context.\n- Clearing the context during resets.\n- Retrieving past messages for continuity.\n\n## State Management\n\nThe `SocietyOfMindAgent` can save and load its internal state, allowing for persistence across executions. This is crucial for scenarios where an agent may need to continue where it left off after being interrupted. \n\n```python\nasync def save_state(self) -> Mapping[str, Any]:\nasync def load_state(self, state: Mapping[str, Any]) -> None:\n```\n\nThese methods ensure that the configuration of the agent's inner team can be stored and restored, preserving ongoing conversations and configurations.\n\n## Example Use Case\n\nA typical implementation of the `SocietyOfMindAgent` might involve a scenario where multiple specialized agents work together on a creative writing task. The example code provided in the class docstring illustrates how to create a framework where:\n\n1. Two assistant agents collaborate in generating narratives.\n2. An additional language translation agent is involved ultimately in refining the output.\n   \nThis showcases the flexibility and modularity of the agent system in practical applications.\n\n## Conclusion\n\nThe `SocietyOfMindAgent` presents a powerful framework for leveraging a multi-agent system in collaborative response generation. With configurable parameters, context management, and stateful capabilities, it is well-suited for complex conversational AI applications that require layered processing and intelligent output production.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_society_of_mind_agent.py"
  },
  {
    "code": false,
    "content": "# UserProxyAgent Module Documentation\n\n## Overview\n\nThe `UserProxyAgent` module is designed to facilitate interaction in chat systems by representing a human user. It leverages an input function to solicit user responses, effectively acting as a bridge in automated messaging scenarios where human input is required. The class is part of a larger framework for building chat agents and integrates seamlessly with cancellation tokens to manage user input timeouts and exceptions.\n\n## Key Components\n\n### Imports\n\nThe code imports several modules:\n- `asyncio`: For asynchronous programming.\n- `uuid`: For generating unique identifiers.\n- Various types and utilities from `contextlib`, `contextvars`, and `inspect` to manage concurrency.\n- Types from `pydantic` and `typing` to ensure proper data modeling and type checking.\n- Custom components like `CancellationToken`, `Component`, and messaging models from `autogen_core`.\n\n### Input Functions\n\nThe code defines an input method that can either be synchronous or asynchronous, encapsulated in:\n```python\nInputFuncType = Union[SyncInputFunc, AsyncInputFunc]\n```\nThis flexibility allows the `UserProxyAgent` to operate under various contexts where user interaction may be required.\n\n### Cancellable Input Function\n\nAn essential feature of the component is the `cancellable_input` function. This asynchronous function:\n- Takes a prompt string and an optional `CancellationToken`.\n- Utilizes `asyncio.to_thread` to run the synchronous `input` function in a separate thread, allowing for non-blocking behavior.\n- Links the task to the cancellation token to provide functionality for cancelling the user input if it takes too long.\n\n## UserProxyAgent Class\n\n### Class Declaration\n\nThe main class, `UserProxyAgent`, extends `BaseChatAgent` and the generic `Component`. It encapsulates the attributes and methods necessary for facilitating user interaction within a chat framework.\n\n### Configuration Model\n\nThe class employs a Pydantic model, `UserProxyAgentConfig`, to define and enforce configuration parameters needed for the agent's instantiation. These include:\n- `name`: The agent's identifier.\n- `description`: An optional descriptor.\n- `input_func`: Specifically designed for input handling, which can represent various input methods.\n\n### Initialization\n\nDuring initialization (`__init__`), a check is made to determine if the provided `input_func` is asynchronous. This helps dictate how user input will be processed later.\n\n## Context Management\n\n### InputRequestContext\n\nThe class defines an inner static context management class, `InputRequestContext`, responsible for handling the context of user input requests. It utilizes `ContextVar` to manage contextual information across asynchronous calls effectively.\n\n### Context Population\n\nThe context manager provided in `InputRequestContext` allows the agent to track input requests by setting a unique request ID when soliciting user input. This context can be accessed within the `UserProxyAgent` methods, ensuring stateful behavior during interactions.\n\n## Handling Messages\n\n### on_messages Method\n\nThe `on_messages` method processes incoming messages and requests user input. It utilizes asynchronous streaming to allow for potential delays in user responses. It checks for a valid response and handles errors gracefully.\n\n### on_messages_stream Method\n\nThe `on_messages_stream` method is crucial in managing the input gathering process. It:\n- Generates an event to indicate user input is requested.\n- Yields user input through a prompt, accommodating both handoff messages and standard text messages, depending on the context.\n- Handles exceptions that arise during input retrieval and manages cancellation scenarios.\n\n## Handoff Handling\n\nThe `_get_latest_handoff` method scans messages for any `HandoffMessage`, which indicates a transition of conversation control. This functionality is pivotal for interactions requiring more than just a straightforward user response.\n\n## Cleanup and Configuration\n\n### on_reset Method\n\nThe `on_reset` method provides a mechanism to reset the agent's state, ensuring that previous interactions do not persist unintentionally across new sessions.\n\n### Configuration Methods\n\nTwo methods handle configuration serialization and deserialization, `_to_config` and `_from_config`, although the current implementation notes that the serialization of the input function remains to be addressed.\n\n## Example Usages\n\nThe documentation includes example usages demonstrating simple interactions with the `UserProxyAgent`. These examples show how to initialize the agent, handle user queries, and integrate cancellation mechanisms, providing clear insights into practical applications of the module within asynchronous chat systems.\n\n## Conclusion\n\nThe `UserProxyAgent` class is a robust component designed to handle user interactions in chat applications while effectively managing asynchronous tasks and cancellations. Its structure and methods offer developers the necessary tools to implement human-in-the-loop mechanisms within automated dialogue systems, making it a valuable asset in interactive AI applications.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/agents/_user_proxy_agent.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface for various components related to a chat agent system. It imports and exposes several classes that are essential for executing tasks, managing teams, and handling termination conditions in a chat-based environment.\n\n## Imported Classes\n\nThe module imports several classes from sibling modules, which are likely defined in other Python files within the same package. Below is a description of each of the imported classes:\n\n### ChatAgent\n\nThe `ChatAgent` class likely represents an intelligent agent designed to interact with users via chat. This may include processing user input, generating responses, and possibly maintaining a conversation state. It serves as the main interface for user interactions.\n\n### Response\n\nThe `Response` class presumably encapsulates the responses generated by the `ChatAgent`. This class might handle various aspects of a response, such as formatting, delivering, or managing different response types (text, images, etc.).\n\n### Handoff\n\nThe `Handoff` class is likely responsible for managing the transition of tasks or interactions from one agent to another. This can be crucial in scenarios where a query or task needs to be escalated to a different service or agent for resolution.\n\n### TaskResult\n\nThe `TaskResult` class probably represents the outcomes or results of tasks that the chat agent undertakes. It may store information about completion status, data produced, and any errors encountered during task execution.\n\n### TaskRunner\n\nThe `TaskRunner` class is likely responsible for executing and managing tasks assigned to the chat agent. It may handle the scheduling, execution, and reporting of tasks, ensuring that they run to completion or in accordance with defined conditions.\n\n### Team\n\nThe `Team` class may represent a group of chat agents or other related entities that work together to achieve common objectives. This is particularly useful for coordinating efforts in chat systems where multiple agents collaborate.\n\n### TerminationCondition\n\nThe `TerminationCondition` class likely encapsulates the conditions under which a chat session or task should be terminated. This can include timeout conditions, user requests, or other criteria that signify the end of an interaction.\n\n### AndTerminationCondition and OrTerminationCondition\n\nThese two classes are specialized types of `TerminationCondition`. They may define combinations of conditions using logical AND and OR operations, respectively, allowing for more flexible termination criteria based on multiple conditions.\n\n### TerminatedException\n\nThe `TerminatedException` class is probably used to signal that a task or chat session has been terminated prematurely or under specific conditions. This exception could be raised by various components to manage termination scenarios effectively.\n\n## Exposed API\n\nThe module exposes a set of classes through its `__all__` variable, which is a convention to indicate the public API of a module. This allows users of the module to easily understand which components are intended for external use.\n\nThe classes included in the `__all__` list are:\n\n- `ChatAgent`\n- `Response`\n- `Team`\n- `TerminatedException`\n- `TerminationCondition`\n- `AndTerminationCondition`\n- `OrTerminationCondition`\n- `TaskResult`\n- `TaskRunner`\n- `Handoff`\n\nThese components work together to provide a comprehensive framework for building, managing, and terminating chat-based interactions.\n\n## Conclusion\n\nIn summary, this module serves as a foundational component for a chat agent system, providing essential classes for interaction, task management, team coordination, and termination criteria. By organizing these elements in a cohesive structure, the module facilitates the development of sophisticated chat-based applications, enhancing user engagement and operational efficiency.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for ChatAgent Module\n\n## Overview\n\nThis module provides the definition of a `ChatAgent` interface that outlines the structure and expected behaviors of a chat agent within a conversational AI system. The `ChatAgent` class is an abstract base class (ABC), meaning it is intended to be subclassed to implement concrete chat agents. The design allows for flexibility and defines a contract for agent behavior, including message handling and state management.\n\n## Class: Response\n\n```python\n@dataclass(kw_only=True)\nclass Response:\n    ...\n```\n\n### Description\nThe `Response` dataclass serves as a structured format for responses generated by chat agents. It encapsulates the chat messages and any inner messages produced during the interactions.\n\n### Attributes\n- **chat_message**: This attribute contains a serialized chat message of type `BaseChatMessage`. It represents the main response output from the agent.\n- **inner_messages**: An optional sequence that can hold any inner messages produced during the interaction, which may be of type `BaseAgentEvent` or `BaseChatMessage`.\n\n## Class: ChatAgent\n\n```python\nclass ChatAgent(ABC, TaskRunner, ComponentBase[BaseModel]):\n    ...\n```\n\n### Description\n`ChatAgent` acts as a protocol for chat agents in the system. Being an abstract class, it defines multiple abstract methods and properties that any derived class must implement, ensuring that every chat agent adheres to a standardized interface.\n\n### Inheritance\n- **TaskRunner**: Suggests that the agents can execute tasks of some kind, perhaps involving asynchronous operations.\n- **ComponentBase**: Implies that the agent is part of a larger system with a structured component design.\n\n### Properties\n\n- **name**: An abstract property that should return the agent's unique name for identification within a team.\n- **description**: An abstract property describing the agent's capabilities to help users decide which agent to utilize.\n- **produced_message_types**: Specifies the types of messages the agent produces, enforcing the types to be subclasses of `BaseChatMessage`.\n\n### Abstract Methods\n\n1. **on_messages**\n   - **Signature**: `async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response`\n   - **Purpose**: Designed to process incoming messages and return an appropriate `Response`. This method is essential for handling interactions.\n\n2. **on_messages_stream**\n   - **Signature**: `async def on_messages_stream(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]`\n   - **Purpose**: Similar to `on_messages`, but this method is meant for streaming responses, returning a generator that yields inner messages and concludes with the final response.\n\n3. **on_reset**\n   - **Signature**: `async def on_reset(self, cancellation_token: CancellationToken) -> None`\n   - **Purpose**: Resets the agent to its default state, crucial for scenarios where state management is necessary.\n\n4. **on_pause**\n   - **Signature**: `async def on_pause(self, cancellation_token: CancellationToken) -> None`\n   - **Purpose**: Invoked when the agent needs to be paused, allowing the agent to handle ongoing processes gracefully.\n\n5. **on_resume**\n   - **Signature**: `async def on_resume(self, cancellation_token: CancellationToken) -> None`\n   - **Purpose**: Calls the agent to resume its operations, potentially following a pause.\n\n6. **save_state**\n   - **Signature**: `async def save_state(self) -> Mapping[str, Any]`\n   - **Purpose**: Saves necessary state information for the agent, making it possible to restore it later.\n\n7. **load_state**\n   - **Signature**: `async def load_state(self, state: Mapping[str, Any]) -> None`\n   - **Purpose**: Allows loading the previously saved state to restore the agent.\n\n8. **close**\n   - **Signature**: `async def close(self) -> None`\n   - **Purpose**: Responsible for releasing any resources the agent may be holding, essential for resource management.\n\n## Summary\n\nThe `ChatAgent` module is integral to building flexible and robust conversational agents within AI systems. By implementing the defined interface and structure, developers can create specific agents tailored to various tasks while ensuring they comply with a standard methodology for message processing, state management, and resource handling. The design emphasizes extensibility, enabling future agents to be incorporated into the system with minimal effort.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/_chat_agent.py"
  },
  {
    "code": false,
    "content": "# Handoff Configuration in Python\n\nThis document describes a Python module that defines a `Handoff` class intended for managing handoff configurations between agents in an automated system. The class utilizes Pydantic for data validation and type specification, making it suitable for use in scenarios where structured data is essential.\n\n## Overview\n\nThe primary purpose of the `Handoff` class is to facilitate handing off responsibilities or tasks from one agent to another. It allows defining specific target agents, descriptions, names, and messages that accompany the handoff process. An essential feature of this class is its ability to automatically generate default values for some properties if they are not provided during initialization.\n\n### Dependencies\n\nThe module imports several key libraries and modules:\n- `logging`: To log events and issues that arise during the execution of the code.\n- `typing`: To specify the expected types for function parameters and return values.\n- `BaseTool` and `FunctionTool`: Tools from the `autogen_core` library that likely facilitate the execution of functions in an automated way.\n- `BaseModel` and `Field`: From Pydantic, they are used for creating models with validation.\n- `model_validator`: A decorator used for validating model fields.\n\n## Class Definition\n\n### Handoff Class\n\nThe `Handoff` class inherits from `BaseModel`, providing a structured way to define configurations for a handoff. This class includes the following attributes:\n\n- `target` (str): The name of the target agent for the handoff. This is a required field.\n- `description` (str): Text that describes the handoff's conditions and the capabilities of the target agent. It defaults to a generated string if not provided.\n- `name` (str): A human-readable name for the handoff. If omitted, it will be auto-generated based on the target agent's name.\n- `message` (str): A message intended for the target agent detailing the handoff. Similar to other attributes, if not specified, it will have a default generated value.\n\n### Data Validation\n\nThe class includes a validator method, `set_defaults`, which is called before the model's initialization:\n- It checks if the `description`, `name`, or `message` attributes are empty. If they are, default values are assigned based on the value of the `target`.\n- The `name` field is validated to ensure it is a string and a valid Python identifier. If not, it raises a `ValueError`.\n\n## Properties and Methods\n\n### handoff_tool Property\n\nThe class features a property method called `handoff_tool`:\n- This property generates a `FunctionTool` based on the current instance of `Handoff`.\n- The `_handoff_tool` function returns the `message` attribute when invoked, serving as the core functionality of the `handoff_tool`.\n- The `FunctionTool` is instantiated with parameters like the name and description, making it ready for execution in the context of the handoff.\n\n### Purpose of the Handoff Tool\n\nThe resulting `handoff_tool` acts as a mechanism for carrying out the handoff to the specified target agent. When executed, it provides the message defined in the model, effectively communicating the necessary information to the target agent.\n\n## Logging Mechanism\n\nThe module includes a logging setup through the `logging` library. An `event_logger` is created using a logger name defined as `EVENT_LOGGER_NAME`. Logging can play a crucial role in tracking the handoff actions and any errors that may occur during the process, aiding in debugging and monitoring.\n\n## Conclusion\n\nThe `Handoff` class serves as a robust framework for automating agent task handoffs, ensuring that necessary information is efficiently communicated to target agents. With built-in validation and default value generation, it reduces the potential for errors during the handoff process. The integration of logging enhances the traceability and reliability of handoffs in automated systems.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/_handoff.py"
  },
  {
    "code": false,
    "content": "# Task Management System Documentation\n\nThis document provides an overview of the task management system implemented in the given code. The system is designed for managing tasks through a standardized running and streaming framework, which is particularly suited for asynchronous environments.\n\n## Overview\n\nThe code defines a task runner interface using Python's `Protocol`, aimed at handling various types of tasks. It also includes a data model, `TaskResult`, for encapsulating the outcome of executed tasks. The framework is flexible enough to manage both the execution and streaming of tasks while handling cancellations gracefully.\n\n## Core Components\n\n### TaskResult Class\n\nThe `TaskResult` class is a Pydantic model that encapsulates the result of a completed task. \n\n```python\nclass TaskResult(BaseModel):\n    \"\"\"Result of running a task.\"\"\"\n    \n    messages: Sequence[SerializeAsAny[BaseAgentEvent | BaseChatMessage]]\n    \"\"\"Messages produced by the task.\"\"\"\n    \n    stop_reason: str | None = None\n    \"\"\"The reason the task stopped.\"\"\"\n```\n\n- **Attributes:**\n  - `messages`: A sequence that holds messages generated during the task's execution. It can contain either agent events or chat messages.\n  - `stop_reason`: An optional string that indicates why the task was halted, providing context for termination.\n\n### TaskRunner Protocol\n\nThe `TaskRunner` is defined as a `Protocol`, which means it outlines a blueprint for any class intending to implement this interface. It includes two main methods for running and streaming tasks.\n\n```python\nclass TaskRunner(Protocol):\n    \"\"\"A task runner.\"\"\"\n    \n    async def run(...)\n    def run_stream(...)\n```\n\n#### run Method\n\nThe `run` method executes a task and returns a `TaskResult`. \n\n- **Parameters:**\n  - `task`: Can be a string, a single chat message, or a sequence of messages. This defines what the task will be.\n  - `cancellation_token`: An optional token to cancel the task immediately, aiding in resource management.\n  - `output_task_messages`: A boolean indicating whether task messages should be included in the result. Defaults to `True`.\n\n- **Behavior:**\n  - This method is stateful, meaning subsequent calls will continue from where the last execution left off if no new task is specified.\n\n#### run_stream Method\n\nThe `run_stream` method provides an asynchronous generator that yields messages during task execution and concludes with a `TaskResult`.\n\n- **Parameters:** The same as the `run` method.\n\n- **Behavior:**\n  - It allows the user to receive intermediate messages in real-time, which can be beneficial in interactive applications. Like `run`, it continues from the last state if a new task isn\u2019t provided.\n\n## Asynchronous Handling\n\nBoth methods in the `TaskRunner` protocol leverage Python's asynchronous programming capabilities, making them well-suited for I/O-bound operations like network calls or waiting for user input. This is evident in the `run` method being defined as `async`, allowing it to be awaited in an asynchronous context.\n\n## Reusability and Extensibility\n\nThe use of a `Protocol` for the `TaskRunner` enables other classes or modules to implement this interface, fostering a plugin-like architecture. This allows for easy extension and customization, making it possible to introduce new task types without modifying existing code.\n\n## Conclusion\n\nThe task management system provided in the code sample is a robust framework for task execution and messaging in a seamless asynchronous paradigm. By defining clear interfaces and structured data models, the system is designed to be both extensible and maintainable, catering to various applications that require task handling and output messaging.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/_task.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code defines an abstract base class `Team` that extends multiple components from different modules. It acts as a blueprint for specific implementations of teams that have certain functionalities, which need to be fully defined in derived classes. The use of abstractions and the Pydantic library suggests that this code is likely part of a larger asynchronous system that deals with components needing state management and task coordination.\n\n## Dependencies\n\n1. **abc and abstractmethod:** These modules are used to define abstract base classes and abstract methods. This enforces that subclasses of `Team` must implement specified methods and properties.\n2. **typing:** The `Any` and `Mapping` types are used for type hints which enhance code clarity and type safety.\n3. **autogen_core:** This module contains `ComponentBase`, which `Team` extends, indicating that `Team` is a type of component in the autogen framework.\n4. **pydantic:** Likely used for data validation and settings management. `BaseModel` serves as a base class for creating data models, promoting structured data handling.\n5. **.task:** This likely contains `TaskRunner`, which provides underlying functionality for running tasks in the context of the team.\n\n## The Team Class\n\n### Introduction\n\nThe `Team` class is an abstract representation of a team within the context of a larger application. As it inherits both `TaskRunner` and `ComponentBase`, it combines functionalities that allow it to perform tasks while maintaining a structured component structure. \n\n### Properties\n\n1. **name (abstract):** \n   - This property must return a string that represents the unique identifier of the team. The name helps in distinguishing this team from other teams within the system.\n\n2. **description (abstract):** \n   - This property returns a string that provides context about the team\u2019s role or purpose. It aids parent orchestrators in understanding what the team does.\n\n### Methods\n\n1. **reset (abstract):** \n   - This is an asynchronous method responsible for resetting the team and all its participants to their initial states. It ensures that after any operations, the team can start afresh.\n\n2. **pause (abstract):** \n   - This asynchronous method is designed to pause the team and all its participants. This functionality is particularly useful to temporarily halt operations without losing the current state.\n\n3. **resume (abstract):** \n   - After calling the `pause` method, this method can be invoked to continue the operations of the team and its participants. It represents a means of controlling the process flow.\n\n4. **save_state (abstract):** \n   - This asynchronous method should be implemented to capture the current state of the team and return it in a mapping format. Saving state is crucial for scenarios where the system needs to persist data.\n\n5. **load_state (abstract):** \n   - This method allows for re-establishing the team\u2019s state using a previously saved state mapping. It is essential for resuming from a given point after a pause or reboot.\n\n## Usage Context\n\nThe `Team` class serves as a vital component in applications where teams or participants need to work in a coordinated and stateful manner. Given its design, it would be suitable for scenarios like game development, collaborative tasks, or even complex workflow management systems where multiple entities interact dynamically.\n\n### Conformity to Interface\n\nAs an abstract class, `Team` does not provide concrete implementations but requires derived classes to adhere to its structure. This encourages consistency across different teams, as each must provide specific implementations for managing their states and task execution methods outlined in the abstract methods.\n\n## Conclusion\n\nThe `Team` class encapsulates essential functionalities for team management within an asynchronous context, focusing on state control and operational flow. Its design promotes modular development, making it easier to create various implementations tailored to specific needs while adhering to a shared interface. This abstraction can significantly enhance the maintainability and expandability of the system in which it is used.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/_team.py"
  },
  {
    "code": false,
    "content": "# Termination Conditions in Agent Conversations\n\nThis document describes the implementation of termination conditions used in agent conversations. These conditions determine when a conversation should end based on specific criteria defined within the model classes. The implementation allows for complex combinations of these conditions to facilitate various use cases.\n\n## Overview of Classes\n\nThe key components in this implementation are:\n\n- **TerminationCondition (abstract class)**: Defines the interface and common behavior for all termination conditions.\n- **AndTerminationCondition (class)**: Represents a composite termination condition where all specified conditions must be satisfied for the conversation to end.\n- **OrTerminationCondition (class)**: Represents a composite termination condition where satisfying any one of the specified conditions triggers the conversation\u2019s termination.\n\n### TerminationCondition Class\n\nThe `TerminationCondition` class is an abstract base class that sets a standard for defining termination conditions in conversations. Its primary responsibilities include:\n\n- **Properties and Methods**:\n    - **`terminated`**: A property that checks if the termination condition has been met.\n    - **`__call__()`**: An asynchronous method that evaluates a sequence of messages to determine if the termination condition is satisfied. It raises a `TerminatedException` if already reached.\n    - **`reset()`**: An asynchronous method that resets the termination condition, preparing it for reuse.\n\n- **Combining Conditions**:\n    - Implements logical operators (`AND` and `OR`) to combine multiple termination conditions, allowing for greater flexibility in defining when conversations should terminate.\n\n### AndTerminationCondition Class\n\nThe `AndTerminationCondition` class inherits from `TerminationCondition` and is designed to aggregate multiple conditions that must all return true for the conversation to terminate. \n\n- **Key Features**:\n    - **Initialization**: Takes multiple `TerminationCondition` instances and stores them internally.\n    - **`terminated` Property**: Checks if every included condition has been satisfied.\n    - **`__call__()` Method**: Evaluates each condition in parallel (using `asyncio.gather`). If any condition indicates that the conversation should continue, it returns `None`. If all are satisfied, it returns a `StopMessage` summarizing the termination reason.\n    - **`reset()` Method**: Resets all included conditions and clears stored stop messages.\n\n### OrTerminationCondition Class\n\nSimilar to `AndTerminationCondition`, the `OrTerminationCondition` class also aggregates multiple conditions but operates under a different logic: meeting any single condition is sufficient to trigger termination.\n\n- **Functionalities**:\n    - **Initialization**: Accepts multiple `TerminationCondition` instances to monitor.\n    - **`terminated` Property**: Checks if any of the included conditions have been satisfied.\n    - **`__call__()` Method**: Collects results from all included conditions. If at least one indicates a need for termination, it returns a `StopMessage`. Otherwise, it returns `None`.\n    - **`reset()` Method**: Resets each of the contained conditions for future evaluations.\n\n## Composite Configuration Classes\n\nThe document includes configuration classes, `AndTerminationConditionConfig` and `OrTerminationConditionConfig`, to facilitate the creation and management of these termination condition types.\n\n### AndTerminationConditionConfig\n\nThis class is a Pydantic model designed to validate and structure the configuration for `AndTerminationCondition`. It contains a list of conditions that are evaluated together.\n\n### OrTerminationConditionConfig\n\nSimilar to `AndTerminationConditionConfig`, this Pydantic model structures the configuration for `OrTerminationCondition`. The key differentiation is that it allows any included condition to be satisfied for termination.\n\n## Exception Handling\n\nAn important aspect of the `TerminationCondition` class is the definition of `TerminatedException`. This exception is raised when an attempt to evaluate a termination condition that has already been satisfied is made. This protects the integrity and statefulness of termination handling.\n\n## Usage Example\n\nTo demonstrate how termination conditions can be employed in a practical context, consider a scenario where a conversation should terminate after a specific number of message exchanges, or if a particular phrase is mentioned. The example provided in the documentation shows how to create an `AndTerminationCondition` or `OrTerminationCondition`, evaluate them, and reset their state.\n\n## Conclusion\n\nThe implementation of termination conditions provides significant flexibility and control over how agent conversations are managed, making it easier to define when conversations should end based on multiple criteria. The design allows for intuitive combinations of conditions, ensuring agents can respond appropriately based on user interactions while adhering to business rules or requirements. \n\nThis framework is essential for maintaining robust, stateful, and user-centered interactive agents.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/base/_termination.py"
  },
  {
    "code": false,
    "content": "# Module Overview: Termination Conditions for Multi-Agent Teams\n\nThis module is designed to supply various termination conditions that help govern the interactions and decision-making processes of multi-agent systems. These conditions provide the necessary controls to determine when a set of activities or communications between agents should cease, either based on specific triggers or predefined criteria.\n\n## Imports Overview\n\nThe module imports several classes from the `_terminations` submodule. Each of these classes represents a distinct type of termination condition that can be applied within the context of managing a multi-agent team. The termination conditions reflect different operational scenarios, allowing for flexibility in handling various team dynamics and communication strategies.\n\n```python\nfrom ._terminations import (\n    ExternalTermination,\n    FunctionalTermination,\n    FunctionCallTermination,\n    HandoffTermination,\n    MaxMessageTermination,\n    SourceMatchTermination,\n    StopMessageTermination,\n    TextMentionTermination,\n    TextMessageTermination,\n    TimeoutTermination,\n    TokenUsageTermination,\n)\n```\n\n### Termination Classes Explained\n\n1. **ExternalTermination**: This class likely defines a termination condition based on external inputs or signals, allowing the system to respond to events outside its normal processing scope.\n\n2. **FunctionalTermination**: This termination may operate based on specific functional criteria or outcomes, signaling when a function or task has sufficiently completed its objective.\n\n3. **FunctionCallTermination**: This could be related to conditions where a function call triggers a conclusion of the current operation, either after a certain result is achieved or a specified number of function calls are made.\n\n4. **HandoffTermination**: This termination class may be designed to manage scenarios where a task or communication should be passed from one agent to another, indicating the end of the current agent's involvement.\n\n5. **MaxMessageTermination**: It likely enforces a cap on the number of messages exchanged during an interaction, terminating the communication once the limit is reached.\n\n6. **SourceMatchTermination**: This condition could be concerned with ensuring that messages or interactions from matching sources lead to a termination of actions, perhaps to prevent redundant communications.\n\n7. **StopMessageTermination**: This class might define a specific message that, when received, signals agents to stop their activities.\n\n8. **TextMentionTermination**: This could involve identifying when certain text mentions trigger a termination, useful in contexts like natural language processing and chatbot interactions.\n\n9. **TextMessageTermination**: Similar to the previous class, but may focus on general text messages as sources for termination signals.\n\n10. **TimeoutTermination**: This class likely controls termination based on time, ensuring operations cease after a designated time period.\n\n11. **TokenUsageTermination**: This might relate to the consumption of resources (tokens), terminating processes once a specified usage threshold is reached.\n\n## Exported Elements\n\nThe module specifies which classes are intended for public use through the `__all__` directive. This limits the elements accessible when the module is imported, enhancing encapsulation while providing a clear public API for users. \n\n```python\n__all__ = [\n    \"MaxMessageTermination\",\n    \"TextMentionTermination\",\n    \"StopMessageTermination\",\n    \"TokenUsageTermination\",\n    \"HandoffTermination\",\n    \"TimeoutTermination\",\n    \"ExternalTermination\",\n    \"SourceMatchTermination\",\n    \"TextMessageTermination\",\n    \"FunctionCallTermination\",\n    \"FunctionalTermination\",\n]\n```\n\n### Purpose of Exported Classes\n\nThe inclusion of termination classes in the `__all__` list indicates their relevance for users looking to control agent behavior. Each class provides different functionality and options that practitioners can leverage when developing multi-agent systems, supporting various needs like resource management, interaction control, and contextual responsiveness.\n\nThis module serves a crucial role in the development of flexible multi-agent systems, helping define clear boundaries for communication and operational continuity based on diverse conditions.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/conditions/__init__.py"
  },
  {
    "code": false,
    "content": "# Termination Conditions Module Documentation\n\nThis module defines various termination conditions for a conversation system, utilizing the asynchronous capabilities of Python through the `asyncio` library. Each termination condition checks for specific criteria where the ongoing conversation should be halted. This document offers a high-level overview of the classes contained within the module.\n\n## Overview of Classes\n\nThe module contains a series of classes, each implementing specific termination logic. The base class for all termination conditions is `TerminationCondition`. This serves as a foundation upon which various conditions can be built:\n\n- **StopMessageTermination**: Halts conversation when a `StopMessage` is received.\n- **MaxMessageTermination**: Terminates the conversation after a specified number of messages.\n- **TextMentionTermination**: Ends the conversation if specific text is mentioned.\n- **FunctionalTermination**: Uses a user-defined function to decide whether to terminate.\n- **TokenUsageTermination**: Monitors token usage and stops if limits are reached.\n- **HandoffTermination**: Ends the conversation upon receiving a specific `HandoffMessage`.\n- **TimeoutTermination**: Terminates after a pre-defined duration.\n- **ExternalTermination**: Allows external control for termination.\n- **SourceMatchTermination**: Ends the conversation when a specific source responds.\n- **TextMessageTermination**: Halts if a `TextMessage` from a specified source is received.\n- **FunctionCallTermination**: Closes the conversation when a specific function execution is detected.\n\nEach class is designed to maintain its state and can reset to an initial condition as needed.\n\n## StopMessageTermination\n\n```python\nclass StopMessageTermination(TerminationCondition, Component[StopMessageTerminationConfig]):\n    ...\n```\n\nThe `StopMessageTermination` class listens for a `StopMessage` in the conversation. If such a message is found, it triggers the termination. The internal state `_terminated` keeps track of whether the condition has already been met. The class provides:\n\n- **Constructor**: Initializes `_terminated` to `False`.\n- **terminate property**: Returns the state of termination.\n- **__call__ method**: Checks messages for `StopMessage` and sets the terminated state if found.\n- **reset method**: Resets the termination state.\n\n## MaxMessageTermination\n\n```python\nclass MaxMessageTermination(TerminationCondition, Component[MaxMessageTerminationConfig]):\n    ...\n```\n\nThis class terminates the conversation after a specified maximum number of messages have been exchanged. Here are its components:\n\n- **Constructor**: Takes maximum messages and a flag for including agent events.\n- **terminate property**: Evaluates if the message count has reached the maximum.\n- **__call__ method**: Counts incoming messages based on configuration and checks against limits.\n- **reset method**: Resets the message count.\n\n## TextMentionTermination\n\n```python\nclass TextMentionTermination(TerminationCondition, Component[TextMentionTerminationConfig]):\n    ...\n```\n\nThe `TextMentionTermination` class monitors for specific text mentions in the messages. It offers:\n\n- **Constructor**: Initializes termination conditions based on specified text and sources.\n- **terminate property**: Indicates if the condition has been met.\n- **__call__ method**: Scans messages for the specified text from defined sources.\n- **reset method**: Resets the termination state.\n\n## FunctionalTermination\n\n```python\nclass FunctionalTermination(TerminationCondition):\n    ...\n```\n\n`FunctionalTermination` uses a functional expression to determine whether to terminate. Key features include:\n\n- **Constructor**: Accepts a callable for custom termination logic.\n- **terminate property**: Checks the state of termination.\n- **__call__ method**: Executes the function to evaluate the termination condition.\n- **reset method**: Resets the termination flag.\n\n## TokenUsageTermination\n\n```python\nclass TokenUsageTermination(TerminationCondition, Component[TokenUsageTerminationConfig]):\n    ...\n```\n\nThis class ensures that a conversation does not exceed specific token usage limits. It includes attributes and methods to:\n\n- **Constructor**: Sets limits for total, prompt, and completion tokens.\n- **terminate property**: Checks whether the usage has exceeded specified limits.\n- **__call__ method**: Accumulates token usage from messages to evaluate termination.\n- **reset method**: Resets all token counts.\n\n## HandoffTermination\n\n```python\nclass HandoffTermination(TerminationCondition, Component[HandoffTerminationConfig]):\n    ...\n```\n\n`HandoffTermination` checks for a specific `HandoffMessage`. Features include:\n\n- **Constructor**: Takes a targeted source for the handoff.\n- **terminate property**: Indicates if termination has occurred.\n- **__call__ method**: Inspects messages for handoff and triggers termination if found.\n- **reset method**: Resets termination state.\n\n## TimeoutTermination\n\n```python\nclass TimeoutTermination(TerminationCondition, Component[TimeoutTerminationConfig]):\n    ...\n```\n\nThis class implements a timeout mechanism to terminate the conversation. Its structure includes:\n\n- **Constructor**: Initializes timeout durations.\n- **terminate property**: Indicates if the timeout condition has been reached.\n- **__call__ method**: Checks elapsed time against the set duration.\n- **reset method**: Resets the timer state.\n\n## ExternalTermination\n\n```python\nclass ExternalTermination(TerminationCondition, Component[ExternalTerminationConfig]):\n    ...\n```\n\nDesigned for externally controlled termination, `ExternalTermination` allows users to control when to stop the conversation. It provides:\n\n- **Constructor**: Initializes state variables.\n- **terminate property**: Checks if the termination has been set.\n- **set method**: Changes state to terminated externally.\n- **__call__ method**: Checks if termination has been requested.\n- **reset method**: Resets internal flags.\n\n## SourceMatchTermination\n\n```python\nclass SourceMatchTermination(TerminationCondition, Component[SourceMatchTerminationConfig]):\n    ...\n```\n\nThis class listens for responses from specific sources and terminates the conversation based on those responses. Its components include:\n\n- **Constructor**: Accepts a list of sources.\n- **terminate property**: Returns whether the condition is satisfied.\n- **__call__ method**: Inspects messages for matching sources.\n- **reset method**: Resets the terminated state.\n\n## TextMessageTermination\n\n```python\nclass TextMessageTermination(TerminationCondition, Component[TextMessageTerminationConfig]):\n    ...\n```\n\n`TextMessageTermination` looks for `TextMessage` instances. Its functionalities include:\n\n- **Constructor**: Initializes the source property.\n- **terminate property**: Reflects termination state.\n- **__call__ method**: Checks if a `TextMessage` is received from the specified source.\n- **reset method**: Resets the termination state.\n\n## FunctionCallTermination\n\n```python\nclass FunctionCallTermination(TerminationCondition, Component[FunctionCallTerminationConfig]):\n    ...\n```\n\nThis class terminates conversations based on specific function calls. Key features:\n\n- **Constructor**: Accepts the name of the function to check for.\n- **terminate property**: Provides the terminated state.\n- **__call__ method**: Scans messages for executed functions that match the specified name.\n- **reset method**: Resets the termination flag.\n\n## Conclusion\n\nThis module provides a robust framework for determining when to terminate a conversation based on various signals, with specific classes catering to different termination conditions. The design encourages extensibility, allowing for the implementation of additional termination conditions as necessary. Each class maintains its own state and offers means to reset and configure settings, facilitating dynamic conversations within automated systems.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/conditions/_terminations.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: Agent Communication Messages\n\nThis module defines a comprehensive framework for various message types used in agent-to-agent communication. Each message is represented by a specialized class that inherits from either `BaseChatMessage` or `BaseAgentEvent`, encapsulating specific fields and behaviors relevant to the type of message being sent.\n\n## Core Classes and Their Functions\n\n### BaseMessage\n\n`BaseMessage` is an abstract base class for all message types within the agent chat system. It provides essential methods for serialization and deserialization while ensuring that any inheriting class implements a method to convert messages to a textual representation.\n\n- **Methods:**\n  - `to_text()`: An abstract method that needs to be defined in subclasses for converting message content into a string format.\n  - `dump()`: Offers a default mechanism for converting the message into a JSON-serializable dictionary.\n  - `load()`: Creates an instance of the message from a dictionary, utilizing Pydantic's validation features.\n\n**Important Note:** When creating a new message type, subclasses should inherit from `BaseChatMessage` or `BaseAgentEvent` to distinguish their purposes.\n\n### BaseChatMessage\n\n`BaseChatMessage` extends `BaseMessage` and is aimed at messages exchanged in chat-based interactions between agents.\n\n- **Attributes:**\n  - `id`: A unique identifier for each message, generated using UUID.\n  - `source`: Represents the agent that sends the message.\n  - `models_usage`: Keeps track of the model usage associated with sending the message.\n  - `metadata`: A dictionary storing additional details relevant to the message.\n  - `created_at`: Timestamp indicating when the message was created.\n\n- **Methods:**\n  - `to_model_text()`: Converts the content into a format suitable for model consumption.\n  - `to_model_message()`: Converts the message into a `UserMessage` format.\n\n### BaseTextChatMessage\n\nBuilding upon `BaseChatMessage`, `BaseTextChatMessage` is a base class specifically for text-only messages. It provides concrete implementations for the transformation of message content into various formats.\n\n- **Attributes:**\n  - `content`: Holds the string content of the message.\n\n- **Methods:**\n  - `to_text()`, `to_model_text()`, `to_model_message()`: These methods return the content in string form, as model text, or as a `UserMessage`, respectively.\n\n### BaseAgentEvent\n\nThis abstract class represents events triggered by agent actions or states, not intended for direct agent-to-agent communication. \n\n- **Attributes:**\n  - Similar to `BaseChatMessage` (e.g., `id`, `source`, `metadata`, `created_at`).\n\n- **Methods:** \n  - Defines an abstract `to_text()` method for custom rendering of event content.\n\n## Specialized Message Types\n\n### StructuredMessage\n\n`StructuredMessage` allows for more complex message formats by utilizing generics with Pydantic models, enabling structured content to be packaged neatly.\n\n- **Attributes:**\n  - `content`: Defined as a subclass of `BaseModel`.\n  - `format_string`: An optional string that can format the content for improved readability.\n\n- **Methods:**\n  - Overrides `to_text()`, `to_model_text()`, and `to_model_message()` to leverage the format string.\n\n### Concrete Message Implementations\n\nSeveral concrete message classes extend `BaseTextChatMessage` and `BaseChatMessage` for specific functionalities:\n\n- **TextMessage**: Represents a simple text message.\n- **MultiModalMessage**: Designed for messages containing multiple content types (e.g., text and images).\n- **StopMessage**: A message that requests termination of a conversation.\n- **HandoffMessage**: Signals the transition of a conversation to another agent.\n- **ToolCallSummaryMessage**: Summarizes results from tool calls made during an interaction.\n\n### Agent Event Implementations\n\n- **ToolCallRequestEvent**: An event indicating a request to use tools.\n- **CodeGenerationEvent**: Represents a signaling event for code generation.\n- **CodeExecutionEvent**: Signals the results of code execution.\n- **UserInputRequestedEvent**: Indicates that user input is needed.\n- **MemoryQueryEvent**: Signals results from querying the agent's memory.\n\n## Message Factory\n\nThe `MessageFactory` class offers a mechanism for creating message instances from JSON-serializable data. It maintains a registry of available message types, facilitating easy creation based on message type identifiers.\n\n- **Methods:**\n  - `is_registered()`: Checks if a message type is already in the registry.\n  - `register()`: Allows the addition of new message types.\n  - `create()`: Instantiates a message from given JSON data, ensuring type validity.\n\n## Summary\n\nThis module encapsulates frameworks for creating and managing a broad spectrum of message types for agent-to-agent communication, emphasizing flexibility, structure, and type safety through a combination of abstract and concrete message classes. The usage of Pydantic models enhances validation and serialization, ensuring robustness and reliability in message handling within an agent chat framework.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/messages.py"
  },
  {
    "code": false,
    "content": "# State Management Module Documentation\n\n## Overview\nThis module provides state management for agents, teams, and termination conditions in a multi-agent system. The functionality is organized around several state classes, each serving a distinct role in maintaining and managing various aspects of the agent's lifecycle and interactions within their environment.\n\n## Imports\nThe module begins by importing several state classes from the `_states` submodule. The specific classes imported are:\n- `AssistantAgentState`\n- `BaseGroupChatManagerState`\n- `BaseState`\n- `ChatAgentContainerState`\n- `MagenticOneOrchestratorState`\n- `RoundRobinManagerState`\n- `SelectorManagerState`\n- `SocietyOfMindAgentState`\n- `SwarmManagerState`\n- `TeamState`\n\nThese classes encapsulate the various states that different components and types of agents can possess, enabling structured and organized state management.\n\n## State Classes Overview\nEach imported state class is crucial for handling the internal workings of agents and their interactions within teams or groups.\n\n### `BaseState`\nThis is likely a foundational class that defines basic properties and methods common to all states. It potentially provides the necessary infrastructure for managing state-related information in derived classes.\n\n### `AssistantAgentState`\nThis class probably represents the state of an assistant agent, encapsulating data and functionalities specific to its role. It may handle the agent's current tasks, performance, and any other pertinent information connected to its objectives.\n\n### `BaseGroupChatManagerState`\nThis class likely deals with managing the state of group chat interactions among agents. It might include features for moderating conversations, tracking messages, or managing participant states during discussions.\n\n### `ChatAgentContainerState`\nThis class probably acts as a container for other chat agents, managing their states collectively. It is likely responsible for handling state transitions and interactions between multiple agents engaged in conversations.\n\n### `MagenticOneOrchestratorState`\nThis state class might be dedicated to the orchestrator of the system, which oversees and coordinates interactions and tasks among agents. It likely maintains the overall system's operational state and ensures harmony in agent interactions.\n\n### `RoundRobinManagerState`\nThis class suggests a management approach where agents are organized in a round-robin fashion. It likely maintains the current state of each agent and manages the order of operations, ensuring each agent gets a turn to interact or perform tasks.\n\n### `SelectorManagerState`\nThis state class likely implements a selection mechanism to manage and choose among various agents based on certain criteria or conditions. It might allow dynamic decision-making regarding which agent should perform specific tasks at any given time.\n\n### `SocietyOfMindAgentState`\nThis class probably represents a complex agent structure based on the \"Society of Mind\" concept, where multiple sub-agents or modules work together. It likely includes mechanisms to manage inter-agent communication and collaborative problem-solving.\n\n### `SwarmManagerState`\nThis class likely represents a state management system for swarm-based agents. It could facilitate coordination and management of a group of agents working together to achieve complex goals, echoing principles found in nature's swarm behaviors.\n\n### `TeamState`\nThis state class is likely designed to handle the overall state of teams composed of agents. It might encompass team dynamics, performance tracking, and coordination strategies among team members to achieve shared objectives.\n\n## Exported Classes\nThe module uses the `__all__` variable to define which classes should be publicly accessible when the module is imported. This facilitates easier management of the public API while keeping the internal classes or functions hidden from users who do not need to interact with them directly. The specific classes included in `__all__` are as follows:\n- `BaseState`\n- `AssistantAgentState`\n- `BaseGroupChatManagerState`\n- `ChatAgentContainerState`\n- `RoundRobinManagerState`\n- `SelectorManagerState`\n- `SwarmManagerState`\n- `MagenticOneOrchestratorState`\n- `TeamState`\n- `SocietyOfMindAgentState`\n\n## Conclusion\nThis state management module is foundational in structuring and managing multiple agents and their interactions within a system. By utilizing a suite of dedicated classes, it ensures that the state information is well-organized, allowing for efficient operation, coordination, and collaboration among diverse agent types in complex environments.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/state/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation of State Management Classes\n\nThis document provides an overview of a set of classes designed to manage various states in a chat and agent-based system. The classes leverage Pydantic, a data validation and settings management library, to enforce type checking and ensure data integrity.\n\n## Overview of State Classes\n\nThe classes defined in this code serve as structured models that represent the state of different components within a chat system. By inheriting from a base class (`BaseState`), they standardize both the attributes and methods that can be used across various states within the application. This design promotes a clear hierarchy and reduces redundancy.\n\n### BaseState Class\n\n```python\nclass BaseState(BaseModel):\n    \"\"\"Base class for all saveable state\"\"\"\n\n    type: str = Field(default=\"BaseState\")\n    version: str = Field(default=\"1.0.0\")\n```\n\nThe `BaseState` class is the foundational model for all state classes in this system. It holds two key attributes:\n- **type:** A string that defaults to \"BaseState,\" indicating the kind of state the instance represents.\n- **version:** A string that denotes the version of the state model, defaulting to \"1.0.0.\"\n\nThis class does not implement any specific logic but serves as a common structure for all derived classes.\n\n### AssistantAgentState Class\n\n```python\nclass AssistantAgentState(BaseState):\n    \"\"\"State for an assistant agent.\"\"\"\n\n    llm_context: Mapping[str, Any] = Field(default_factory=lambda: dict([(\"messages\", [])]))\n    type: str = Field(default=\"AssistantAgentState\")\n```\n\nThe `AssistantAgentState` class extends `BaseState` and captures the state specific to an assistant agent. Its attributes include:\n- **llm_context:** A dictionary designed to hold contextual information specific to the assistant agent. It is initialized with a list of messages to manage conversation context.\n- **type:** Overrides the type to specify that this instance represents \"AssistantAgentState.\"\n\n### TeamState Class\n\n```python\nclass TeamState(BaseState):\n    \"\"\"State for a team of agents.\"\"\"\n\n    agent_states: Mapping[str, Any] = Field(default_factory=dict)\n    type: str = Field(default=\"TeamState\")\n```\n\nThe `TeamState` class is tailored for the state management of teams consisting of multiple agents. Its attributes are as follows:\n- **agent_states:** A dictionary initialized as an empty mapping that will hold the states of individual agents within the team.\n- **type:** Indicates that this instance is of type \"TeamState.\"\n\n### Group Chat Manager State Classes\n\nMultiple classes based on `BaseGroupChatManagerState` manage the state of group chat management systems, ensuring that each type of manager can track the necessary attributes and behaviors. This class has several important attributes:\n\n```python\nclass BaseGroupChatManagerState(BaseState):\n    \"\"\"Base state for all group chat managers.\"\"\"\n\n    message_thread: List[Mapping[str, Any]] = Field(default_factory=list)\n    current_turn: int = Field(default=0)\n    type: str = Field(default=\"BaseGroupChatManagerState\")\n```\n\n- **message_thread:** A list that maintains a record of messages exchanged in the chat.\n- **current_turn:** An integer representing the current turn of the conversation.\n- **type:** Indicates this instance's role as a group chat manager.\n\n#### RoundRobinManagerState Class\n\n```python\nclass RoundRobinManagerState(BaseGroupChatManagerState):\n    \"\"\"State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.\"\"\"\n\n    next_speaker_index: int = Field(default=0)\n    type: str = Field(default=\"RoundRobinManagerState\")\n```\n\nThis class manages the state for a round-robin style chat where speakers take turns in a predefined order. It includes:\n- **next_speaker_index:** An index tracking whose turn it is to speak.\n\n#### SelectorManagerState Class\n\n```python\nclass SelectorManagerState(BaseGroupChatManagerState):\n    \"\"\"State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.\"\"\"\n\n    previous_speaker: Optional[str] = Field(default=None)\n    type: str = Field(default=\"SelectorManagerState\")\n```\n\nThis state class is used when a single speaker is selected from a pool of agents, keeping track of:\n- **previous_speaker:** The last speaker in the chat, allowing the system to manage turn-taking more effectively.\n\n#### SwarmManagerState Class\n\n```python\nclass SwarmManagerState(BaseGroupChatManagerState):\n    \"\"\"State for :class:`~autogen_agentchat.teams.Swarm` manager.\"\"\"\n\n    current_speaker: str = Field(default=\"\")\n    type: str = Field(default=\"SwarmManagerState\")\n```\n\nIn this class, the state of a \"swarm\" chat system is managed. It adds:\n- **current_speaker:** A string that indicates who is currently speaking.\n\n### MagenticOneOrchestratorState Class\n\n```python\nclass MagenticOneOrchestratorState(BaseGroupChatManagerState):\n    \"\"\"State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.\"\"\"\n\n    task: str = Field(default=\"\")\n    facts: str = Field(default=\"\")\n    plan: str = Field(default=\"\")\n    n_rounds: int = Field(default=0)\n    n_stalls: int = Field(default=0)\n    type: str = Field(default=\"MagenticOneOrchestratorState\")\n```\n\nThis class encapsulates the state for a specific orchestrator managing a collaborative effort called \"MagneticOne.\" It includes:\n- **task:** The current task being addressed by the group.\n- **facts, plan:** Strings for additional information to inform the group\u2019s discussion.\n- **n_rounds, n_stalls:** Integers that track the number of discussion rounds and stalls (i.e., delays) that have occurred.\n\n### SocietyOfMindAgentState Class\n\n```python\nclass SocietyOfMindAgentState(BaseState):\n    \"\"\"State for a Society of Mind agent.\"\"\"\n\n    inner_team_state: Mapping[str, Any] = Field(default_factory=dict)\n    type: str = Field(default=\"SocietyOfMindAgentState\")\n```\n\nFinally, the `SocietyOfMindAgentState` class addresses the state of agents that embody a \"Society of Mind\" concept. It includes:\n- **inner_team_state:** A mapping to hold states of the internal team, allowing this agent to manage multiple subordinate states effectively.\n- **type:** Identifies this as a \"SocietyOfMindAgentState.\"\n\n## Conclusion\n\nIn summary, the code consists of a hierarchy of state classes that facilitate the management of various agents and group chat dynamics in an artificial intelligence environment. Each class is specifically tailored to different functionalities and contexts, promoting flexibility and scalability within chat-based interactions. By employing Pydantic's data validation capabilities, the design ensures that the data remains consistent and adheres to predefined formats.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/state/_states.py"
  },
  {
    "code": false,
    "content": "# Multi-Agent Teams Module Documentation\n\n## Overview\n\nThis module is designed to facilitate the implementation of various pre-defined multi-agent teams, utilizing object-oriented programming principles. The primary foundation for the teams is the `BaseGroupChat` class, from which specific team classes inherit. These team classes are tailored to execute different strategies for agent coordination and communication in a group setting, which is critical in multi-agent systems.\n\n## Imports and Dependencies\n\nThe module imports several essential components necessary for functionality:\n\n- **BaseGroupChat**: Serves as the superclass for various group chat implementations, likely containing core functionalities needed for any group chat feature.\n- **DiGraph, DiGraphBuilder, DiGraphEdge, DiGraphNode, GraphFlow**: These classes are imported from another module (`_graph`) and are crucial for managing the relationships and interactions between agents in a multi-agent system using directed graphs.\n\n## Classes\n\n### BaseGroupChat\n\n`BaseGroupChat` is the central class that provides foundational methods and properties for all other group chat implementations. The classes that inherit from it must implement specific behaviors and functionalities pertinent to the agents\u2019 communication and interaction strategies.\n\n### RoundRobinGroupChat\n\n`RoundRobinGroupChat` is a specialized class that likely implements a round-robin scheduling algorithm for agent interactions. This approach ensures that every agent has an opportunity to communicate or perform actions sequentially, which can help in scenarios where balanced participation is critical.\n\n### SelectorGroupChat\n\n`SelectorGroupChat` is another derived class that probably employs a selection mechanism to determine which agents participate in the group chat or take actions at any given time. This could be based on specific criteria or conditions, ensuring that the most relevant agents are prioritized for communication or action.\n\n### Swarm\n\nThe `Swarm` class takes inspiration from swarm intelligence principles, facilitating interactions that mimic natural swarming behavior among agents. This class is useful for applications that require coordinated group actions, similar to those seen in nature, such as flocks of birds or schools of fish.\n\n### MagenticOneGroupChat\n\n`MagenticOneGroupChat` offers a unique implementation, possibly focusing on a specific type of group interaction or communication pattern distinct from the other classes. The naming suggests it might employ a form of attraction or magnetism in agent behavior.\n\n## Graph Representation Classes\n\n### DiGraph and Related Classes\n\nThe module imports several classes related to directed graphs, indicating that relationships and communication pathways between agents are structured as a graph. The importance of these classes includes:\n\n- **DiGraph**: Represents a directed graph structure that can be used to model complex interactions among agents.\n- **DiGraphBuilder**: Likely aids in constructing instances of directed graphs, allowing dynamic adjustments to agent relationships based on various conditions.\n- **DiGraphNode**: Represents nodes within the directed graph, likely corresponding to individual agents or communication points.\n- **DiGraphEdge**: Represents connections between nodes, signifying the communication pathways or relationships.\n- **GraphFlow**: May refer to the flow of information or interactions through the graph, facilitating analysis and management of agent communications.\n\n## Module Namespace\n\nThe `__all__` list at the end of the module is a way to define the public interface for this module. It explicitly specifies which classes and components are accessible when the module is imported elsewhere, enhancing encapsulation and usability.\n\nClasses included in `__all__`:\n\n- Base classes and implementations (e.g., `BaseGroupChat`, `RoundRobinGroupChat`, `SelectorGroupChat`, `Swarm`, `MagenticOneGroupChat`)\n- Graph representation classes (e.g., `DiGraphBuilder`, `DiGraph`, `DiGraphNode`, `DiGraphEdge`, `GraphFlow`)\n\nThis structured approach ensures that users of this module can easily access the necessary functionalities without wading through unnecessary internal components. \n\n## Conclusion\n\nThis module encapsulates a multi-agent team system supported by structured group interaction classes and directed graph constructs. By leveraging these foundational classes, developers can implement a variety of agent communication dynamics tailored to the needs of their specific applications. The hierarchical organization of classes improves maintainability and facilitates the creation of complex multi-agent behaviors in diverse environments.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/__init__.py"
  },
  {
    "code": false,
    "content": "It seems there was no code provided to analyze. Please share the source code you would like me to describe, and I'll be happy to help!",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/__init__.py"
  },
  {
    "code": false,
    "content": "# BaseGroupChat Documentation\n\n## Overview\n\nThe `BaseGroupChat` class serves as an abstract base for creating team-based chat operations. It implements functionality to manage a group chat where multiple agents can participate and share messages seamlessly. Utilizing asynchronous programming, this class simplifies the complex interactions between different agent instances in a chat environment.\n\n### Key Features\n\n- **Agent Interaction**: Facilitates context sharing among agents through broadcasting messages.\n- **Dynamic Configuration**: Supports unique participant configurations, including different types of agents.\n- **Lifecycle Management**: Provides methods to manage the state of the group chat, including starting, stopping, pausing, and resetting the team.\n- **Message Handling**: Utilizes a message factory for registering and managing various types of chat messages.\n\n## Class Definition\n\n```python\nclass BaseGroupChat(Team, ABC, ComponentBase[BaseModel]):\n```\n\n### Inheritance\n\n`BaseGroupChat` inherits from:\n- `Team`: Likely provides a framework for managing multiple participating entities.\n- `ABC`: Enables the definition of abstract base classes in Python.\n- `ComponentBase[BaseModel]`: Serves as a base class for all components, potentially enforcing structure.\n\n## Initialization\n\n### Constructor\n\n```python\ndef __init__(self, name: str, ...):\n```\n\nThe constructor initializes the group chat instance with parameters such as:\n\n- `name`: Name of the group chat.\n- `description`: A brief overview of the chat's purpose.\n- `participants`: List of agents or team instances participating in the chat.\n- `group_chat_manager_name`: Specifies the name for the associated chat manager.\n- `group_chat_manager_class`: Class type for the chat manager.\n- `termination_condition`: Defines when the chat should terminate.\n- `max_turns`: Limits on the number of exchanges in the chat.\n- `runtime`: An optional runtime environment.\n- `custom_message_types`: Allows for registration of additional message types.\n- `emit_team_events`: A flag to control event emission from the team.\n\n### Participant Setup\n\nParticipants must be unique, and there must be at least one. This is enforced in the constructor, where checks are performed, raising errors if any rules are breached.\n\n## Messaging System\n\n### Message Factory\n\nThe class utilizes a `MessageFactory`, registering both default and custom message types. When agents are added as participants, their production types are registered with the factory to ensure they can communicate effectively.\n\n### Topics\n\nThe class defines various topic types used for broadcasting messages among participants:\n- **Group Topic Type**: Facilitates communication for all participants.\n- **Manager Topic Type**: Pertains to direct communication with the chat manager.\n- **Participant Topic Types**: Unique for each participant for direct communication.\n\n## Lifecycle Management\n\n### Initialization Method\n\n```python\nasync def _init(self, runtime: AgentRuntime) -> None\n```\n\nThis private method prepares the chat environment. It registers all participants and the group chat manager with the specified runtime. Subscriptions are established to allow the appropriate message flow.\n\n### Running the Chat\n\n```python\nasync def run(self, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None)\n```\n\nThis method starts the group chat operation, processing a defined task and returning a `TaskResult`. It employs a streaming approach to handle responsiveness, wherein messages and results can be yield-managed.\n\n### Streaming Execution\n\nThe `run_stream` method provides a real-time output of messages as the chat proceeds. It allows for fine-tuned control of task execution, cancellation, and message streaming. Users can directly observe chat interactions event by event.\n\n## State Management\n\n### Saving and Loading State\n\n```python\nasync def save_state(self) -> Mapping[str, Any]\nasync def load_state(self, state: Mapping[str, Any]) -> None\n```\n\nState management is accomplished through these methods, allowing the chat's state to be saved and restored appropriately. The class emphasizes that these actions should not be performed while the chat is active to maintain integrity.\n\n## Control Mechanisms\n\n### Pausing and Resuming\n\n```python\nasync def pause(self) -> None\nasync def resume(self) -> None\n```\n\nThese methods provide the ability to pause and resume agent interactions, requiring agents to implement corresponding methods to manage their respective states during pauses seamlessly.\n\n## Conclusion\n\nThe `BaseGroupChat` class is a comprehensive framework for implementing group chat functionality in an agent-based environment. By providing features such as dynamic message registration, lifecycle control, and state management, it offers a robust foundation for developing chat systems among agents.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat.py"
  },
  {
    "code": false,
    "content": "# Group Chat Manager Documentation\n\n## Overview\n\nThe `BaseGroupChatManager` class serves as a foundational component for managing group chat interactions among multiple participants. It is built to coordinate messages, track the state of the chat, handle speaker selections, and enforce termination conditions based on specific rules. The class leverages asynchronous programming through `asyncio`, allowing it to manage multiple chat events concurrently.\n\n## Class Definition\n\n### BaseGroupChatManager\n\nThe `BaseGroupChatManager` class inherits from `SequentialRoutedAgent` and is abstract, containing essential functionality for managing a group chat. As an abstract base class, it defines a structure for concrete subclasses to implement specific behaviors (e.g., speaker selection and state validation).\n\n### Constructor: `__init__`\n\nThe constructor accepts various parameters crucial for initializing the group chat:\n\n- **name**: The name of the group chat manager.\n- **group_topic_type**: The topic type for the group chat.\n- **output_topic_type**: The topic type for output messages.\n- **participant_topic_types**: A list of topic types corresponding to each participant.\n- **participant_names**: Names of the participants.\n- **participant_descriptions**: Descriptions for the participants.\n- **output_message_queue**: A queue for output messages.\n- **termination_condition**: An optional termination condition.\n- **max_turns**: An optional limit on the number of turns in the chat.\n- **message_factory**: A factory for creating message instances.\n- **emit_team_events**: A flag to indicate if team events should be emitted.\n\n### Pre-Validation Rules\n\nThe constructor includes several validation checks to ensure the integrity of the configuration:\n\n1. Maximum turns must be greater than zero if specified.\n2. The number of participant topic types must match the number of participant descriptions.\n3. Participant topic types must be unique.\n4. The group topic type must not be part of the participant topic types.\n\n## Methods\n\n### `handle_start`\n\nThe `handle_start` method responds to the `GroupChatStart` event by:\n\n1. Checking if the chat has already terminated based on the termination condition.\n2. Validating the group state with the provided messages.\n3. Publishing messages to the output topic and forwarding them to the group chat participants.\n4. Appending messages to the message thread.\n5. Selecting active speakers to initiate or continue the conversation.\n\n### `handle_agent_response`\n\nThe `handle_agent_response` method processes responses from participants. It:\n\n1. Handles both `GroupChatAgentResponse` and `GroupChatTeamResponse`.\n2. Updates the message thread with messages received.\n3. Checks if the conversation should terminate based on the termination condition.\n4. Transitions control to the next speakers if the conversation continues.\n\n### `_transition_to_next_speakers`\n\nThis private method determines the next speakers based on current messages. It:\n\n1. Selects speakers asynchronously.\n2. Validates selected speaker names against the participant list.\n3. Sends requests for the selected speakers to publish messages.\n\n### `_apply_termination_condition`\n\nThis method evaluates the termination condition and manages the termination process. It checks if:\n\n1. The termination condition triggers termination based on the delta of new messages.\n2. The maximum number of turns has been exceeded.\n3. It increments the turn count if specified.\n\n### `_log_speaker_selection`\n\nThe `_log_speaker_selection` method logs which speakers have been chosen for the next turn to both the output message queue and the group chat.\n\n### `_signal_termination` and `_signal_termination_with_error`\n\nThese two methods are responsible for signaling the end of the group chat in standard cases or when encountering an error, respectively. They publish termination messages and queue them accordingly.\n\n### Event Handlers\n\nThe following event handler methods manage specific types of messages:\n\n- `handle_group_chat_message`: Appends received chat messages to the output message queue.\n- `handle_group_chat_error`: Logs errors and signals termination.\n- `handle_reset`: Calls the reset function to clear the state.\n\n### Abstract Methods\n\nThe class defines three abstract methods that must be implemented by any subclass:\n\n- `validate_group_state`: Validates the current state of the chat based on the incoming messages.\n- `select_speaker`: Chooses the next speakers from the participants.\n- `reset`: Resets the group chat manager to its initial state.\n\n### `on_unhandled_message`\n\nFinally, the method `on_unhandled_message` raises an error if a message type is unhandled, ensuring robustness in message processing.\n\n## Conclusion\n\nThe `BaseGroupChatManager` class provides a comprehensive structure for managing group chat interactions in a controlled and sequential manner. By including mechanisms for validation, state management, and termination conditions, it sets the groundwork for subclasses to build upon and expand the group chat capabilities according to specific requirements.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat_manager.py"
  },
  {
    "code": false,
    "content": "# ChatAgentContainer Documentation\n\n## Overview\n\nThe `ChatAgentContainer` class is a specialized agent designed to manage communication within a group chat setting. It extends from `SequentialRoutedAgent` and is responsible for delegating message handling to a specified chat agent or team. The container orchestrates events related to message reception, processing, and responding while maintaining an internal state for buffered messages. \n\n## Class Definition\n\n### Constructor\n\n```python\ndef __init__(\n        self, parent_topic_type: str, output_topic_type: str, agent: ChatAgent | Team, message_factory: MessageFactory\n    ) -> None:\n```\n\n- **Arguments**:\n  - `parent_topic_type`: Specifies the type of the parent topic orchestrator.\n  - `output_topic_type`: Defines the topic type for the output messages.\n  - `agent`: Either a `ChatAgent` or a `Team` instance, which will handle message processing.\n  - `message_factory`: An instance of `MessageFactory` used to create messages from JSON data.\n\nThis constructor initializes the agent's properties, including buffering capabilities and the event types it will handle sequentially.\n\n## Event Handlers\n\nThe class contains several asynchronous event handlers that respond to specific types of messages:\n\n### Handle Start Message\n\n```python\n@event\nasync def handle_start(self, message: GroupChatStart, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: To handle incoming group chat start messages and buffer the contained messages for processing.\n- **Implementation**: It checks if messages are provided in the incoming `GroupChatStart` and appends them to an internal buffer.\n\n### Handle Agent Response\n\n```python\n@event\nasync def handle_agent_response(self, message: GroupChatAgentResponse, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: To manage responses from a delegated agent and add any chat messages received to the buffer.\n- **Implementation**: Extracts the chat message from the `GroupChatAgentResponse` and adds it to the internal buffer.\n\n### Handle Team Response\n\n```python\n@event\nasync def handle_team_response(self, message: GroupChatTeamResponse, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: To manage responses from a team agent and buffer the resultant messages.\n- **Implementation**: Iterates through the resulting messages and buffers each valid chat message.\n\n### Reset Handling\n\n```python\n@rpc\nasync def handle_reset(self, message: GroupChatReset, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: To reset the internal message buffer and reset the agent's state if it's a team.\n- **Implementation**: Clears the message buffer and invokes the `reset` method on the agent or team.\n\n### Request Handling\n\n```python\n@event\nasync def handle_request(self, message: GroupChatRequestPublish, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: To publish buffered messages to the agent or team and publish their response.\n- **Implementation**: Depending on if the agent is a team or individual, it processes the messages differently, handling errors and ensuring responses are published back to the group chat.\n\n## Internal Methods\n\n### Buffering Messages\n\n```python\ndef _buffer_message(self, message: BaseChatMessage) -> None:\n```\n\n- **Purpose**: To add a message to the internal buffer if it is a registered message type.\n- **Error Handling**: Raises a `ValueError` if the message type is unregistered.\n\n### Logging Messages\n\n```python\nasync def _log_message(self, message: BaseAgentEvent | BaseChatMessage) -> None:\n```\n\n- **Purpose**: To log messages by publishing them to the output topic.\n- **Implementation**: Checks message registration before logging and publishes the message wrapped in a `GroupChatMessage`.\n\n## State Management\n\n### Saving State\n\n```python\nasync def save_state(self) -> Mapping[str, Any]:\n```\n\n- **Purpose**: To save the current state of the agent container, including buffered messages and the agent's state.\n- **Implementation**: Utilizes the `ChatAgentContainerState` to structure and return the relevant state information.\n\n### Loading State\n\n```python\nasync def load_state(self, state: Mapping[str, Any]) -> None:\n```\n\n- **Purpose**: To restore the agent container's state from a given mapping.\n- **Implementation**: Validates and reconstructs the message buffer and loads the state for the contained agent.\n\n## Miscellaneous Handlers\n\n### Handle Pause and Resume\n\n```python\n@rpc\nasync def handle_pause(self, message: GroupChatPause, ctx: MessageContext) -> None:\n```\n\n- Handles the event to pause operations of the agent or team.\n\n```python\n@rpc\nasync def handle_resume(self, message: GroupChatResume, ctx: MessageContext) -> None:\n```\n\n- Resumes the paused operations of the agent or team.\n\n### Unhandled Messages\n\n```python\nasync def on_unhandled_message(self, message: Any, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: Exceptionally handles unrecognized messages by raising a value error indicating the message type encountered.\n\nThe `ChatAgentContainer` serves as a facilitative wrapper for managing complex interactions in a group chat environment, effectively dealing with messages, responses, and state transitions while ensuring robust error handling and synchronization.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_chat_agent_container.py"
  },
  {
    "code": false,
    "content": "# Documentation for Group Chat Module\n\nThis module contains several classes that serve to facilitate the management and communication within a group chat system. The classes make use of the Pydantic library to enforce data validation and serialization, ensuring that the data structures used throughout the system are consistent and valid.\n\n## Exception Handling\n\n### SerializableException Class\n\n```python\nclass SerializableException(BaseModel):\n    ...\n```\n\nThe `SerializableException` class is designed to encapsulate details about an exception that occurs within the group chat system. It includes the following attributes:\n\n- `error_type`: The type of error that occurred (as a string).\n- `error_message`: A descriptive message regarding the error.\n- `traceback`: An optional string representation of the traceback, which provides context for where the error occurred in the code. \n\n#### Key Method\n\n- `from_exception(cls, exc: Exception) -> \"SerializableException\"`: This class method takes a standard Python `Exception` object and converts it into a `SerializableException`. It captures the exception's type, message, and traceback.\n  \n- `__str__(self)`: This method overrides the default string representation to include the error type, message, and traceback if available, providing a clearer understanding of the error.\n\n## Group Chat Operations\n\n### GroupChatStart Class\n\n```python\nclass GroupChatStart(BaseModel):\n    ...\n```\n\nThe `GroupChatStart` class represents a request to initiate a new group chat session. It includes:\n\n- `messages`: An optional list of messages that can be included to start the chat. This can provide context or initial discussion points.\n- `output_task_messages`: A boolean specifying whether task messages should be included in the output. Default value is set to `True` for backward compatibility.\n\n### GroupChatAgentResponse Class\n\n```python\nclass GroupChatAgentResponse(BaseModel):\n    ...\n```\n\nThis class represents a response generated by an agent during a group chat. Key attributes include:\n\n- `response`: The actual response object from an agent, serialized using `SerializeAsAny`.\n- `name`: The name of the agent that produced the response.\n\n### GroupChatTeamResponse Class\n\n```python\nclass GroupChatTeamResponse(BaseModel):\n    ...\n```\n\nSimilar to the `GroupChatAgentResponse`, but for responses from a team, this class includes:\n\n- `result`: The outcome from a team's action within the chat, also serialized.\n- `name`: The name of the team that generated the response.\n\n## Publishing and Message Handling\n\n### GroupChatRequestPublish Class\n\n```python\nclass GroupChatRequestPublish(BaseModel):\n    ...\n```\n\nWhile not fully defined here, this class is intended to represent requests for publishing messages within the group chat context.\n\n### GroupChatMessage Class\n\n```python\nclass GroupChatMessage(BaseModel):\n    ...\n```\n\nThis class is used to represent a message sent in the group chat. It includes:\n\n- `message`: The content of the message, which can be an agent event or a chat message, serialized as necessary.\n\n## Group Chat Control Operations\n\n### GroupChatTermination Class\n\n```python\nclass GroupChatTermination(BaseModel):\n    ...\n```\n\nThe `GroupChatTermination` class signals that a group chat session has ended. It holds:\n\n- `message`: A `StopMessage` that indicates the reason for termination.\n- `error`: An optional `SerializableException` that may detail an error that occurred during the session.\n\n### GroupChatReset Class\n\n```python\nclass GroupChatReset(BaseModel):\n    ...\n```\n\nThis class signifies a request to reset the state of agents in a group chat, although specific implementation details are not defined here.\n\n### GroupChatPause Class\n\n```python\nclass GroupChatPause(BaseModel):\n    ...\n```\n\nThe `GroupChatPause` class requests the pausing of ongoing activities in the group chat.\n\n### GroupChatResume Class\n\n```python\nclass GroupChatResume(BaseModel):\n    ...\n```\n\nComplementing the pause functionality, this class serves to request the resumption of activities in the group chat.\n\n## Error Handling in Group Chats\n\n### GroupChatError Class\n\n```python\nclass GroupChatError(BaseModel):\n    ...\n```\n\nThis class encapsulates information regarding errors that occur within the group chat. It contains:\n\n- `error`: An instance of `SerializableException`, providing the details of the error.\n\n## Summary\n\nThis module effectively lays the groundwork for managing group chats with a variety of operations, including initiating chats, sending messages, and managing chat lifecycle events such as pausing, resuming, or terminating the chat. It also incorporates robust error handling through the use of serialization for tracking exceptions, ensuring that issues can be reported and managed efficiently. The use of Pydantic models enhances data validation and serialization, making the interactions within the system structured and reliable.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_events.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis module serves as an interface to a set of graph-related classes and functionalities. It imports several classes from other modules within the same package, facilitating the creation, management, and manipulation of directed graphs (DiGraphs) and their components. By listing specific classes in the `__all__` variable, it defines a public API for this module, indicating what should be accessible when this module is imported.\n\n## Imports\n\nThe following classes are imported from their respective modules:\n\n1. **DiGraph**: Represents a directed graph structure that can hold nodes connected by directed edges.\n2. **DiGraphEdge**: Represents the edges that connect nodes in a directed graph, likely carrying weights or other properties.\n3. **DiGraphNode**: Represents the individual nodes within a directed graph.\n4. **GraphFlow**: Likely encapsulates the concept of a flow through a graph, possibly tracking how data or processes traverse the structure.\n5. **GraphFlowManager**: Manages the execution of flows within a graph, handling the orchestration and flow of data or tasks.\n6. **DiGraphBuilder**: A utility class designed for constructing directed graphs, potentially offering methods to easily add nodes and edges.\n\n## Public API\n\nThe `__all__` list explicitly declares which classes are part of the public interface for this module. The inclusion of these classes suggests that they are the primary components intended for use by other modules or users of this API:\n\n- **GraphFlow**: Central to the flow of data or tasks in the directed graph.\n- **DiGraph**: The main structure used to represent the graph.\n- **GraphFlowManager**: Orchestrator for managing flows within the graph.\n- **DiGraphNode**: Represents the fundamental building blocks (nodes) of the graph.\n- **DiGraphEdge**: Connects nodes, forming relationships within the graph.\n- **DiGraphBuilder**: Assists in creating the graph structure.\n\n## Purpose\n\nThe purpose of this module is to provide an organized and encapsulated way to work with directed graphs and their respective components. It abstracts the complexity involved in graph management and flow execution, making it more accessible for developers to utilize within their applications. By providing a coherent set of classes, it facilitates graph manipulation, analysis, and flow management.\n\n## Dependencies\n\nThis module depends on internal classes defined in `_digraph_group_chat` and `_graph_builder`. These modules are presumably part of the same package, which suggests a cohesive design centered around graph theory and flow management.\n\n## Use Cases\n\n1. **Graph Representation**: Utilize `DiGraph` to create a representation for complex relationships in data.\n2. **Building Graphs**: Use `DiGraphBuilder` to easily construct graphs by adding nodes and edges programmatically.\n3. **Flow Management**: Implement data or process flows through a directed graph using `GraphFlow` and `GraphFlowManager` to manage and track traversal or execution.\n  \n## Assumptions\n\nThe effectiveness of this module is predicated on the assumption that the underlying implementations of the imported classes (`DiGraph`, `DiGraphEdge`, etc.) are robust and cater to the needs of directed graph operations. Additionally, appropriate error handling and validation mechanisms should be implemented within these classes to ensure they can gracefully manage invalid operations or states.\n\n## Future Enhancements\n\nPotential enhancements for this module may include:\n\n- Additional functionalities for graph analysis (e.g., finding shortest paths, cycles).\n- Visualizational tools for representing graphs and their flows.\n- Extended support for weighted edges and nodes to accommodate diverse applications.\n  \n## Conclusion\n\nThis module provides foundational components for working with directed graphs and executing flows within those graphs. Its structured approach encourages ease of use and scalability, making it a valuable asset in graph-related programming tasks or applications. The clearly defined public API allows for straightforward integration and usage in broader software projects.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_graph/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview of the Directed Graph Execution Framework\n\nThis code is part of a framework designed for managing a directed graph execution model for AI agents, particularly within a group chat setting. It utilizes directed graphs (DiGraphs) to control the flow of messages and execution order among multiple agents. \n\n## Key Classes and Their Roles\n\n### 1. **DiGraphEdge**\nThe `DiGraphEdge` class defines a directed edge between nodes in a directed graph. It includes:\n\n- **Target Node**: Name of the node this edge points to.\n- **Condition**: Specifies when this edge can be traversed. It can be a string, callable, or None.\n- **Activation Group**: Allows for grouping edges that affect the same target for execution control.\n- **Activation Condition**: Determines if all or any of the edges in an activation group need to be satisfied for the target to execute.\n\n### 2. **DiGraphNode**\n`DiGraphNode` represents a node in the directed graph. It holds:\n\n- **Name**: Unique identifier for the agent/node.\n- **Edges**: List of edges leading to other nodes.\n- **Activation Type**: Defines whether execution requires all parent nodes or any of them to be completed.\n\n### 3. **DiGraph**\nThe `DiGraph` class encapsulates the structure of the directed graph. Its roles include:\n\n- **Node Storage**: Maintains a mapping of names to `DiGraphNode` instances.\n- **Cycle Detection**: Provides methods to check for cycles and validate edge conditions.\n- **Graph Validation**: Ensures the integrity of the graph structure during initialization.\n\n### 4. **GraphFlowManagerState**\nThis class tracks the active execution state of agents in the directed graph. It supplements the state management for nodes currently in execution.\n\n### 5. **GraphFlowManager**\n`GraphFlowManager` is responsible for executing the logic defined within the directed graph. It handles:\n\n- **Initialization**: Validates the graph and prepares execution logistics.\n- **Message Propagation**: Updates child nodes based on parent messages.\n- **Execution State Management**: Manages readiness, triggered groups, and activation conditions throughout the graph as messages flow.\n\n### 6. **GraphFlowConfig**\nThis class serves as a declarative configuration model for `GraphFlow`, allowing for easy setup of different graph executions. It organizes participant data, termination conditions, and graph structure.\n\n### 7. **GraphFlow**\n`GraphFlow` is the high-level interface that integrates all aspects of the graph execution framework. It facilitates:\n\n- **Participant Management**: Handles agent definitions and ensures they meet required types.\n- **Execution Control**: Manages the overall execution flow according to directed graph definitions.\n\n## Functionality Overview\n\n### Graph Structure and Initialization\nThe directed graph structure consists of nodes and edges that dictate the execution flow of agents. Each `DiGraph` is comprised of multiple `DiGraphNode` objects, which can have conditional edges (defined by `DiGraphEdge`) based on the state of previous agents. This allows for complex workflows, including:\n\n- **Sequential Execution**: Where agents pass messages linearly.\n- **Parallel Execution**: Agents can process messages simultaneously based on conditions.\n- **Conditional Branching**: Subsequent agent execution can depend on content conditions.\n- **Loops**: Agents can re-invoke previous nodes under specific conditions.\n\n### Message Handling and Event Propagation\nThe `GraphFlowManager` orchestrates message events by listening for messages from agents and propagating state updates to child nodes. It uses the conditions defined in `DiGraphEdge` to determine which messages should move forward in the graph's execution path.\n\n### Validation and State Management\nBoth `DiGraph` and `GraphFlowManager` include robust methods to validate the graph's structure and the active execution state. The validation checks for cycles, ensures correct conditions are set, and confirms that necessary start and leaf nodes exist. \n\n### Configuration and Component Management\nThe `GraphFlowConfig` class allows configurations to be saved and loaded, aiding in scenarios where the state of the whole execution needs to be maintained or restored. The `GraphFlow` class facilitates creating teams of agents based on this configuration and sets the platform for executing complex conversations dynamically.\n\n## Example Group Chat Scenarios\nThe framework supports a range of usages that exemplify its capabilities:\n\n1. **Sequential Flow**: Agents execute in a defined order (e.g., A \u2192 B \u2192 C).\n2. **Parallel Fan-out**: A single agent can branch its messages to multiple recipients (e.g., A \u2192 B and A \u2192 C).\n3. **Conditional Branching**: Agent execution can diverge based on the content of messages (e.g., A directs to B if 'yes' or to C otherwise).\n4. **Loops with Exit Condition**: Agents can cycle back to prior nodes based on certain conditions (e.g., feedback loops).\n\n### Conclusion\nThis code forms a robust framework that enables complex interactions between conversational agents through a directed graph model. The flexible design allows developers to create intricate workflow patterns tailored to specific application needs within a chat environment, fostering rich interactions and conditions for AI agents in collaborative tasks.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_graph/_digraph_group_chat.py"
  },
  {
    "code": false,
    "content": "# DiGraphBuilder Documentation\n\n## Overview\n\nThe `DiGraphBuilder` class is designed for the creation of execution graphs that facilitate interactions among multiple agent instances, specifically for use in a `GraphFlow` context. It allows for sophisticated control flows that can encompass sequential execution, branching conditions, and loops, ensuring each agent in the graph can interact according to defined paths. Each node in the graph corresponds to an agent, and the edges between these nodes depict possible execution routes that depend on specific conditions.\n\nThe builders and methods offered by the `DiGraphBuilder` enable users to seamlessly construct and validate directed graphs that dictate how agents communicate based on message content. However, users should note that the API is experimental and may undergo changes in future versions.\n\n## Class Initialization\n\n### `__init__`\n\n```python\ndef __init__(self) -> None:\n```\nThe constructor initializes the `DiGraphBuilder` instance. It sets up two dictionaries: one to hold nodes (agents) and another to register the actual agent instances. Additionally, it initializes a variable to keep track of the default starting node of the graph.\n\n## Node Management\n\n### `add_node`\n\n```python\ndef add_node(self, agent: ChatAgent, activation: Literal[\"all\", \"any\"] = \"all\") -> \"DiGraphBuilder\":\n```\nThis method registers a new node in the execution graph by adding an agent. Each node can specify an activation condition, either \"all\" or \"any\", dictating when the node can be activated. It constructs a `DiGraphNode` if the agent has not been previously added, and also keeps a map of agent names to their respective `ChatAgent` instances for easy access.\n\n### `get_participants`\n\n```python\ndef get_participants(self) -> list[ChatAgent]:\n```\nThis method retrieves a list of all agents added to the builder, preserving the order in which they were inserted. This is useful for acquiring all the agents involved in the constructed graph for potential use or display.\n\n## Edge Management\n\n### `add_edge`\n\n```python\ndef add_edge(self, source: Union[str, ChatAgent], target: Union[str, ChatAgent], condition: Optional[Union[str, Callable[[BaseChatMessage], bool]]] = None, activation_group: Optional[str] = None, activation_condition: Optional[Literal[\"all\", \"any\"]] = None) -> \"DiGraphBuilder\":\n```\nThe `add_edge` method creates a directed edge between two nodes, defining how messages propagate from one agent to another. The source and target can be identified either by their names or by the agent objects. If specified, the `condition` parameter determines when the edge should be activated, either as a substring or via a callable function that evaluates the message. This method raises exceptions if the source or target nodes have not been previously defined, thus ensuring graph integrity.\n\n## Conditional Edge Handling\n\n### `add_conditional_edges`\n\n```python\ndef add_conditional_edges(self, source: Union[str, ChatAgent], condition_to_target: Dict[str, Union[str, ChatAgent]]) -> \"DiGraphBuilder\":\n```\nThis method allows adding multiple edges from a single source node based on keyword checks. The provided `condition_to_target` mapping indicates which keywords to look for and which nodes to target when a keyword is detected in a message. It's worth mentioning that the interface for this method may change, so users are encouraged to utilize `add_edge` for custom conditions.\n\n## Entry Point Configuration\n\n### `set_entry_point`\n\n```python\ndef set_entry_point(self, name: Union[str, ChatAgent]) -> \"DiGraphBuilder\":\n```\nThe `set_entry_point` method designates a specific node as the starting point for the graph. It ensures that the specified node has been registered prior to being set as the entry point, providing control over the flow of execution within the graph.\n\n## Graph Validation and Construction\n\n### `build`\n\n```python\ndef build(self) -> DiGraph:\n```\nThe `build` method finalizes the creation of the directed graph, validating all nodes and edges before returning an instance of `DiGraph`. This method ensures that the constructed graph adheres to required criteria, enabling reliable execution of the defined flows.\n\n## Summary of Functionality\n\nThe `DiGraphBuilder` serves as a comprehensive tool for constructing and managing the interactions among agents in a controlled flow environment. It leverages a combination of node and edge management methods to facilitate complex execution paths including sequential, parallel, conditional and looping structures. While usage of the builder can efficiently dictate how agents engage based on message content, users should approach its implementation cautiously due to the potential for future API changes. Overall, the builder enhances flexibility and empowers developers to create intricate interaction models suitable for various applications involving agent-based architectures.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_graph/_graph_builder.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: MagenticOneGroupChat\n\n## Overview\n\nThis module is a part of a larger package and serves as an interface to the `MagenticOneGroupChat` class defined in the `_magentic_one_group_chat` module. The primary goal of this module is to facilitate controlled access to the functionalities associated with group chat management through a well-defined interface.\n\n## Imports\n\n```python\nfrom ._magentic_one_group_chat import MagenticOneGroupChat\n```\n\nThis line imports the `MagenticOneGroupChat` class from the `_magentic_one_group_chat` file located in the same package. This suggests that `_magentic_one_group_chat` is likely a private module (as indicated by the underscore) and contains the core functionalities related to the `MagenticOneGroupChat`.\n\n## Exposed Namespace\n\n```python\n__all__ = [\n    \"MagenticOneGroupChat\",\n]\n```\n\nThe `__all__` variable is defined to contain a list of public objects that should be exposed when the module is imported. By including `MagenticOneGroupChat` in this list, it indicates that this is the primary class that users of the module should work with. This also restricts access to other parts of the module, which may be used internally but are not intended for public use.\n\n## Class: MagenticOneGroupChat\n\n### Purpose\n\nThe `MagenticOneGroupChat` class serves as the primary interface for managing group chats within the application. While the specifics of the class are not defined in this module, one can infer that it likely includes methods for creating, managing, and interacting with group chat features such as messaging, user management, and chat history.\n\n### Role\n\nThe class acts as a higher-level abstraction for users who want to create and manipulate group chats, providing necessary functionalities without exposing raw implementation details or complexities found in the underlying module (`_magentic_one_group_chat`). By placing the class in the `__all__` list, the authors suggest that this is the key component users should focus on when utilizing this module.\n\n## Usage Context\n\nThis module setup suggests a design approach that emphasizes encapsulation and modular programming. By structuring the code in this way, it allows for easier maintenance and clearer organization of functionalities. Users of the module will interact primarily with `MagenticOneGroupChat`, making it simpler to understand the interface without needing to delve into the complexities of its implementation.\n\n## Conclusion\n\nThe current module serves as a straightforward fa\u00e7ade for the `MagenticOneGroupChat` class, promoting a clean and accessible interface for managing group chat features within a larger application. Further exploration of the `_magentic_one_group_chat` module will provide deeper insights into the capabilities and functionalities supported by the `MagenticOneGroupChat` class.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_magentic_one/__init__.py"
  },
  {
    "code": false,
    "content": "# MagenticOneGroupChat Documentation\n\n## Overview\n\nThe `MagenticOneGroupChat` class is a component designed to facilitate group chats among multiple agents using the Magentic-One architecture. This architecture is tailored for efficiently managing conversations and interactions among various agents to complete complex tasks. The `MagenticOneGroupChat` serves as a specialized implementation of the group chat functionality available in the `autogen_agentchat` library, aimed specifically at multi-agent collaborations.\n\n## Class Structure\n\n### MagenticOneGroupChatConfig\n\nThis is a configuration model defined using Pydantic's `BaseModel`. It encapsulates various parameters required for setting up a `MagenticOneGroupChat`. The parameters include:\n\n- **name**: An optional name for the group chat.\n- **description**: An optional description of the group chat's purpose.\n- **participants**: A list of agents (participants) in the group chat, represented as `ComponentModel`.\n- **model_client**: The `ComponentModel` for the language model used in generating responses.\n- **termination_condition**: An optional condition that determines when the group chat will end.\n- **max_turns**: Optional maximum number of turns before the chat stops (default is 20).\n- **max_stalls**: Maximum number of stalls allowed before re-planning (required).\n- **final_answer_prompt**: The prompt used to generate the final answer from the conversation\u2019s transcript.\n- **emit_team_events**: An option to emit team events during the chat.\n\n### MagenticOneGroupChat\n\nThe `MagenticOneGroupChat` class inherits from `BaseGroupChat` and `Component`. It manages the group chat activities and orchestrates the interaction among agents. The constructor takes the following parameters:\n\n- **participants**: A list of `ChatAgent` instances that participate in the chat.\n- **model_client**: The `ChatCompletionClient` instance for response generation.\n- Additional parameters include the optional name, description, termination condition, maximum turns, maximum stalls, custom message types, and whether to emit events.\n\n#### Initialization\n\nDuring initialization, the class validates that the provided participants are indeed instances of `ChatAgent`. It also sets up various properties related to the chat, such as the maximum stalls and final answer prompt. If no participants are provided, a `ValueError` is raised.\n\n## Orchestrator Interaction\n\nThe primary goal of the `MagenticOneGroupChat` class is to facilitate efficient conversation management through the `MagenticOneOrchestrator`. This orchestrator ensures that conversations flow smoothly among agents, effectively handling their interactions and any tasks they need to complete together.\n\n### Factory Method for Orchestrator\n\nThe `_create_group_chat_manager_factory` method defines how to instantiate the `MagenticOneOrchestrator`. It creates a closure that can produce an orchestrator instance using the provided parameters related to the group chat, including names, types, and output handling. This provides the flexibility needed for varying chat setups.\n\n## Configuration and Serialization\n\n### _to_config Method\n\nThe `_to_config` method is responsible for serializing the instance into its configuration model `MagenticOneGroupChatConfig`. This process involves extracting current attributes and converting participants and conditions into their compact representations, allowing for easier configuration management.\n\n### _from_config Class Method\n\nConversely, the `_from_config` method is a class method that allows for creating a `MagenticOneGroupChat` instance from a configuration object. It deserializes the configuration back into a fully functional group chat, reconstructing participants and clients as needed.\n\n## Event Logging\n\nThroughout the class, logging functionality is incorporated for tracing and event management. The `trace_logger` and `event_logger` are set up to help in debugging and understanding the operational flow of the group chat interactions. These loggers capture significant events and can help developers trace the logic as agents engage in conversations.\n\n## Usage Example\n\nA common scenario for using the `MagenticOneGroupChat` can be seen in the provided example. A simple setup involves initializing a model client (like OpenAI's GPT-4), creating an assistant agent, and running the group chat through a console interface. This showcases how easy it is to set up and initiate a group chat for collaborative problem-solving.\n\n### Example Code\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    assistant = AssistantAgent(\"Assistant\", model_client=model_client)\n    team = MagenticOneGroupChat([assistant], model_client=model_client)\n    await Console(team.run_stream(task=\"Provide a different proof to Fermat's last theorem\"))\n\nasyncio.run(main())\n```\n\n## Conclusion\n\nThe `MagenticOneGroupChat` class serves a critical role in enabling multi-agent conversations through structured orchestration. With built-in event management and flexibility in configuration, it stands as a powerful tool for developing conversational agents that can collaboratively tackle complex challenges efficiently. For more detailed configurations and capabilities, users are encouraged to refer to relevant literature and API documentation.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_group_chat.py"
  },
  {
    "code": false,
    "content": "# MagenticOneOrchestrator Documentation\n\n## Overview\n\nThe `MagenticOneOrchestrator` class is a specialized component that manages group chats with an emphasis on ledger-based orchestration. This class extends the `BaseGroupChatManager` and utilizes a chat completion model to facilitate communication among participants, gather facts, create plans, and manage task executions in a collaborative environment. It integrates asynchronous operations to allow real-time interaction and leverages various messaging events to handle the flow of conversation effectively.\n\n## Class Initialization\n\n### Constructor\n\nThe constructor initializes several attributes essential for group chat orchestration:\n\n```python\ndef __init__(self, name: str, group_topic_type: str, output_topic_type: str, ...):\n```\n\n- **Parameters:**\n  - `name`: Identifier for the orchestrator.\n  - `group_topic_type`: Type for the group chat topic.\n  - `output_topic_type`: Type for the output messages.\n  - `participant_topic_types`: Types associated with each participant.\n  - `participant_names`: List of participant names.\n  - `participant_descriptions`: List of descriptions corresponding to participants.\n  - `max_turns`: Maximum number of turns before completion.\n  - `message_factory`: Instance of `MessageFactory` to create various chat messages.\n  - `model_client`: Instance of `ChatCompletionClient` for generating model responses.\n  - `max_stalls`: The number of times the chat can stall before triggering a recheck.\n  - `final_answer_prompt`: Prompt template for generating the final answer.\n  - `output_message_queue`: An asyncio queue for managing output messages.\n  - `termination_condition`: Condition to determine if the task should terminate.\n  - `emit_team_events`: Flag to emit team events or not.\n\n- **Attributes Initialized:**\n  - Internal structures to store the current task, facts, and planning within the orchestrator and generate a formatted team description.\n\n## Task and Orchestration\n\n### Task Management\n\nThe class provides various methods to facilitate task orchestration, including ledger management, planning, and progress evaluation.\n\n#### Ledger Prompts\n\nThe orchestrator generates prompts based on the task at hand. These include:\n\n- `_get_task_ledger_facts_prompt`: Forms the prompt to gather facts for a specified task.\n- `_get_task_ledger_plan_prompt`: Generates the prompt for planning, incorporating team descriptions.\n- `_get_task_ledger_full_prompt`: Constructs a prompt summarizing the task, team, facts, and plan.\n- Methods to generate updates for facts and plans.\n\nThese prompts are essential for interacting with the model client, which uses them to generate coherent responses relevant to the ongoing task.\n\n### Managing Conversations\n\n#### Handling Start of Task\n\nThe `handle_start` method manages the onset of a task:\n\n```python\n@rpc\nasync def handle_start(self, message: GroupChatStart, ctx: MessageContext) -> None:\n```\n\n- This method verifies if the task can begin, validates the group state based on incoming messages, and constructs the initial task based on user inputs.\n- It then gathers relevant facts and creates a plan based on the task and team information. The orchestration process begins with a call to `_reenter_outer_loop`.\n\n#### Event Handling\n\nThe orchestrator listens for responses and messages through the `handle_agent_response` method:\n\n```python\n@event\nasync def handle_agent_response(self, message: GroupChatAgentResponse | GroupChatTeamResponse, ctx: MessageContext) -> None:\n```\n\n- This method processes incoming messages from agents, updates the message thread, checks for termination conditions, and orchestrates the next steps based on the current context and agent responses.\n\n### Orchestration Loop\n\n#### Outer and Inner Loops\n\nThe orchestrator employs two primary loops for task management:\n\n1. **Outer Loop**:\n   - Initiates when a task begins and resets participants' states.\n   - Prepares and disseminates the task ledger.\n\n2. **Inner Loop**:\n   - Continuously assesses progress, checks for task completion, evaluates stalling, and determines the next speaker through `_orchestrate_step`.\n\n### Managing Stalls\n\nThe orchestrator keeps track of how many stalling conditions occur through the `_n_stalls` counter. If excessive stalls are detected, it triggers a re-evaluation of the task with `_update_task_ledger`.\n\n## State Management\n\nThe `MagenticOneOrchestrator` includes mechanisms for maintaining its state, enabling it to save and load its status seamlessly:\n\n### Saving and Loading State\n\n#### Save State\n\nThe `save_state` method returns a structured mapping of the current state of the orchestrator, including its message thread, current turn, task, facts, plan, and stall count. \n\n#### Load State\n\nConversely, `load_state` sets up the orchestrator's state based on a previously saved state, ensuring that all parameters are properly re-initialized.\n\n## Utilities\n\n### Logging\n\nThe class includes a method for logging messages:\n\n```python\nasync def _log_message(self, log_message: str) -> None:\n```\n\n- This is useful for debugging and tracking the orchestration process by maintaining a traceable record of significant events and actions.\n\n### Context Management\n\nSeveral methods convert messages to a context suitable for the model, such as `_thread_to_context` and `_get_compatible_context`. These ensure that the information passed to the model remains valid and coherent.\n\n## Conclusion\n\nIn summary, the `MagenticOneOrchestrator` effectively orchestrates group communication for collaborative tasks through the meticulous management of tasks, facts, and participant interactions. Its asynchronous design and structured state handling enable it to operate in real-time environments efficiently, making it integral for orchestrating group chats centered around complex tasks.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_magentic_one/_magentic_one_orchestrator.py"
  },
  {
    "code": false,
    "content": "# Orchestrator System Overview\n\nThis code provides a framework for managing orchestration tasks and their associated workflows. The code utilizes the `pydantic` library for data modeling, defining specific messages and prompts used in the task orchestration process. Below, we will break down various components, including predefined messages, prompts, and class definitions.\n\n## Constants Definition\n\n### SYSTEM MESSAGE AND PROMPTS\n\nThe code initializes several string constants that serve as templates for the coordination of task-related interactions:\n\n1. `ORCHESTRATOR_SYSTEM_MESSAGE`: This constant is currently initialized as an empty string, suggesting it could be utilized for future system messages or configurations.\n\n2. **TASK PROMPTS**:\n   - `ORCHESTRATOR_TASK_LEDGER_FACTS_PROMPT`: This prompt requests users to categorize information related to a given task in a structured format. It invites thorough thinking about what is known, what needs research, and what can be deduced or guessed. This helps ensure comprehensive consideration of the task's requirements.\n   \n   - `ORCHESTRATOR_TASK_LEDGER_PLAN_PROMPT`: This prompt is designed to collect input on how to address tasks based on team composition. The response should be a brief bullet-point plan that effectively uses team members' expertise.\n\n   - **FULL PROMPT**: The `ORCHESTRATOR_TASK_LEDGER_FULL_PROMPT` combines a user task, team information, a fact sheet, and a task plan into one cohesive prompt. It sets the stage for a comprehensive review of the task and the resources allocated to it.\n\n3. **PROGRESS EVALUATION PROMPT**: \n   - `ORCHESTRATOR_PROGRESS_LEDGER_PROMPT`: This prompt facilitates progress checks on tasks. It includes questions regarding satisfaction, loops, and next steps, and mandates the output in a strict JSON format to maintain clarity and consistency.\n\n## Data Models\n\n### Pydantic Base Models\n\nThe code defines several Pydantic models which help to validate and structure the data:\n\n1. **LedgerEntryBooleanAnswer**: This class models a boolean response with a reason explaining the outcome. It's used to assess binary inquiries like completion status and progression.\n\n   ```python\n   class LedgerEntryBooleanAnswer(BaseModel):\n       reason: str\n       answer: bool\n   ```\n\n2. **LedgerEntryStringAnswer**: Similar to the boolean model, this class encapsulates string responses with reasoning. It is particularly used for responses that involve identifying the next speaker and instructions.\n\n   ```python\n   class LedgerEntryStringAnswer(BaseModel):\n       reason: str\n       answer: str\n   ```\n\n3. **LedgerEntry**: This is the main class that consolidates all responses into a single entry format. It includes assessments of the request's satisfaction, loop status, progress, the next speaker, and any instructions or questions.\n\n   ```python\n   class LedgerEntry(BaseModel):\n       is_request_satisfied: LedgerEntryBooleanAnswer\n       is_in_loop: LedgerEntryBooleanAnswer\n       is_progress_being_made: LedgerEntryBooleanAnswer\n       next_speaker: LedgerEntryStringAnswer\n       instruction_or_question: LedgerEntryStringAnswer\n   ```\n\n## Task Management Prompts\n\n### FACT UPDATE AND PLAN UPDATE PROMPTS\n\nThe framework also provides templates for updating facts and plans based on task progression:\n\n1. **Fact Update Prompt**: The `ORCHESTRATOR_TASK_LEDGER_FACTS_UPDATE_PROMPT` invites users to revise existing facts about the task, integrating new insights gleaned from progress made. This enhances adaptability in the orchestration process.\n\n2. **Plan Update Prompt**: The `ORCHESTRATOR_TASK_LEDGER_PLAN_UPDATE_PROMPT` encourages reflection on previous failures, pushing for a revised plan that avoids repeating mistakes. It calls for concise, actionable steps derived from prior experience and the existing team\u2019s capabilities.\n\n## Final Response Prompt\n\n### FINAL ANSWER PROMPT\n\nThe `ORCHESTRATOR_FINAL_ANSWER_PROMPT` marks the conclusion of a task. It summarizes the interactions undertaken to complete the task, culminating in a final answer directed to the user. This structured closure reinforces transparency and provides clarity about the operational workflow followed during the task's ownership.\n\n## Conclusion\n\nThe provided code establishes a structured orchestration workflow aimed at effectively managing complex tasks. Through clearly defined prompts, data models, and progress evaluation metrics, it facilitates collaborative task management while fostering adaptability and clarity in responses. These components collectively contribute to a rich environment fostering effective communication and logical deduction within task-oriented teams.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_magentic_one/_prompts.py"
  },
  {
    "code": false,
    "content": "# RoundRobinGroupChat Module Documentation\n\nThis module implements a group chat system that facilitates interaction among multiple participants (agents) by allowing them to take turns speaking in a round-robin manner. It manages how messages are exchanged and ensures every participant has an opportunity to contribute.\n\n## Overview of Key Classes\n\n### RoundRobinGroupChatManager\n\nThe `RoundRobinGroupChatManager` is responsible for managing the state of the group chat and determining which participant gets to speak next according to the round-robin selection process. \n\n#### Attributes\n\n- **`_next_speaker_index`**: Tracks the index of the next speaker in the list of participants.\n\n#### Methods\n\n- **`__init__`**: Initializes the manager with various configuration parameters including participant details, message queue, termination conditions, etc.\n  \n- **`validate_group_state`**: An asynchronous placeholder method to validate the state of the group (currently unimplemented).\n  \n- **`reset`**: Resets the state of the chat manager, including clearing the message thread and resetting the speaker index.\n  \n- **`save_state`**: Captures the current state of the group chat, which includes the messages exchanged, the current turn number, and the next speaker index.\n  \n- **`load_state`**: Restores the group chat to a previous state based on provided state data.\n  \n- **`select_speaker`**: Selects the next speaker in the group, ensuring that each participant has an opportunity to speak in a cyclic manner.\n\n### RoundRobinGroupChatConfig\n\nThe `RoundRobinGroupChatConfig` class, derived from `BaseModel`, encapsulates the configuration settings for a round-robin group chat.\n\n#### Attributes\n\n- **`name`**: An optional field specifying the name of the chat.\n- **`description`**: An optional description of the chat.\n- **`participants`**: A list of participant models.\n- **`termination_condition`**: An optional condition defining when the group chat ends.\n- **`max_turns`**: An optional limit on the number of turns allowed in the chat.\n- **`emit_team_events`**: A boolean to determine if team events should be emitted.\n\n### RoundRobinGroupChat\n\nThe `RoundRobinGroupChat` class integrates the chat manager with configuration and provides a structured way to manage chat interactions. It is a type of `BaseGroupChat` and accepts a list of participants.\n\n#### Attributes and Methods\n\n- **`__init__`**: Initializes an instance of the group chat with provided participants and optional parameters such as a termination condition or message configurations.\n  \n- **`_create_group_chat_manager_factory`**: A factory method that returns an instance of `RoundRobinGroupChatManager` configured with the necessary parameters.\n  \n- **`_to_config`**: Converts the current group chat state into an instance of `RoundRobinGroupChatConfig`.\n  \n- **`_from_config`**: A class method that allows the creation of a `RoundRobinGroupChat` instance from a configuration object.\n\n## Functionality Explained\n\n1. **Initialization and Configuration**:\n   The chat manager, participants, and all necessary configurations are initialized through the constructors provided in the `RoundRobinGroupChat` and `RoundRobinGroupChatManager` classes.\n\n2. **Round-Robin Speaker Selection**:\n   In a chat, when the `select_speaker` method is invoked, it selects the next speaker in a cyclic manner, ensuring that each participant in the group gets an opportunity to contribute.\n\n3. **State Management**:\n   The group chat manager provides `save_state` and `load_state` methods to serialize and deserialize the chat state, enabling persistence across sessions or interactions.\n\n4. **Resetting Chats**:\n   The `reset` method clears the chat history and resets the index of the next speaker, facilitating a fresh start for the chat.\n\n5. **Chat Interaction**:\n   The group chat manages how messages are exchanged based on participant types, whether they are agents or teams, and handles the internal message queue concurrent through asynchronous programming.\n\n6. **Configurations and Conditions**:\n   The chat can be customized with various configurations, such as termination conditions that define when the chat should end. For example, a condition might exist to stop the chat based on the number of turns taken or specific messages received.\n\n7. **Example Usages**:\n   The documentation includes practical examples demonstrating different scenarios of group chats with various agents and termination conditions, which assists developers in understanding how to implement and utilize the system in their projects.\n\n## Conclusion\n\nThe `RoundRobinGroupChat` and its associated manager provide a robust framework for creating interactive chat sessions where multiple participants can communicate in a structured way. This approach is especially suitable for developing collaborative systems where participation needs to be evenly distributed among agents, whether they're AI or user proxies. The modularity with customizable configurations and termination conditions adds flexibility for various use cases in chat applications.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_round_robin_group_chat.py"
  },
  {
    "code": false,
    "content": "# Selector Group Chat Documentation\n\n## Overview\nThe `SelectorGroupChat` module manages a group chat environment where multiple participants can take turns to communicate. It utilizes a ChatCompletion model to select the next speaker each time a message is exchanged. This module is suitable for creating interactive conversations among agents or teams, employing either predefined or dynamic selection strategies to determine who speaks next. It allows customization through various functions and configuration options, making it versatile for numerous applications.\n\n## Key Classes\n\n### SelectorGroupChatManager\nThis class is a subclass of `BaseGroupChatManager`, specifically designed to select the next speaker using a ChatCompletion model and a custom selector function.\n- **Initialization Parameters:** \n  - It receives parameters like `name`, `group_topic`, `participant_names`, and message queue among others, to set up the chat environment.\n  - It establishes a model client, selector prompt, and termination conditions to customize the chat flow.\n\n### SelectorGroupChat\nThis is the primary class for managing the group chat. Inherits from `BaseGroupChat` and implements the core chat functionality.\n- **Initialization Parameters:**\n  - It takes a list of `participants`, a `model_client`, and various optional parameters, including a `termination_condition`, `selector_prompt`, and `allow_repeated_speaker`.\n  - The `selector_func` can be supplied to dictate custom logic for voice selection.\n- **Functionalities:** Handles participant messaging, speaker selection, and related events.\n\n## Speaker Selection Process\n\n### Selecting a Speaker\nThe `select_speaker` method selects the next participant to speak in the group based on:\n\n1. **Custom Selector Function:** \n   - If a `selector_func` is provided, it gets evaluated first to determine the speaker.\n   - The selector function can be either synchronous or asynchronous, and it must return a valid participant name.\n\n2. **Candidate Filtering:**\n   - If no speaker is selected via the selector function, a `candidate_func` can be applied to filter the list of potential speakers.\n   - If neither function is set, the selection defaults to all participants, skipping the last speaker if `allow_repeated_speaker` is false.\n\n3. **Chat Completion Model:**\n   - If no valid speaker is determined, the ChatCompletion model will be employed to select from the available participants based on conversation context.\n\n### Message Management\nThe `update_message_thread` method serves to enrich the conversation history in the chat. It stores incoming messages and integrates them into the model context for future reference during speaker selection.\n\n## Conversation History and Context Management\nThe class includes mechanisms to track the message history, leveraging methods like `construct_message_history`, which formats the conversation into a structured string for better context understanding during selection.\n\n## Validation and State Management\n### Resetting State\nThe `reset` method clears the current chat state, including message threads and previous speaker records. This is crucial for restarting conversations properly.\n\n### State Saving and Loading\nState management functionality allows the chat's current state to be saved (`save_state`) or restored (`load_state`). This is beneficial for use cases where chat sessions need to be paused and resumed later.\n\n## Logging and Debugging\nLogging, utilizing the `trace_logger`, helps monitor the selection process, troubleshooting issues such as multiple selections or invalid names returned by the selector function.\n\n## Example Use Cases\n### Basic Chat Setup\n```python\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    agent1 = AssistantAgent(\"Agent1\", model_client)\n    agent2 = AssistantAgent(\"Agent2\", model_client)\n    \n    group_chat = SelectorGroupChat(\n        participants=[agent1, agent2],\n        model_client=model_client,\n    )\n    \n    await Console(group_chat.run_stream(task=\"Discuss trip planning.\"))\n```\nThis basic example sets up a simple two-agent chat using OpenAI's model.\n\n### Custom Selection Logic\n```python\ndef custom_selector(messages: Sequence[BaseChatMessage]) -> str | None:\n    # Custom logic for selecting the next speaker based on message content.\n    return \"Agent1\" if messages[-1].author == \"Agent2\" else \"Agent2\"\n```\nThis snippet illustrates how a developer can define a custom selection function that shapes how agents interact based on specific conversation patterns.\n\n## Conclusion\nThe `SelectorGroupChat` module is an intricate system for dynamic group interactions. Through its customizable architecture, it allows for effective communication among AI agents, leveraging advanced selection techniques to manage conversational flow. This can be transformative for applications such as collaborative decision-making, storytelling, and role-playing scenarios.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_selector_group_chat.py"
  },
  {
    "code": false,
    "content": "# Documentation for `sequential_routed_agent.py`\n\n## Overview\n\nThis Python code defines classes for managing asynchronous message processing in a way that ensures certain types of messages are handled sequentially. Utilizing Python's `asyncio` library, it provides a custom locking mechanism (`FIFOLock`) and a specialized message processing agent (`SequentialRoutedAgent`) that extends from a base class (`RoutedAgent`) in the `autogen_core` module.\n\n## Key Components\n\n### FIFOLock Class\n\nThe `FIFOLock` class ensures that when multiple coroutines try to acquire the lock, they do so in a First-In-First-Out (FIFO) manner. This is crucial for scenarios where the order of operations matters, especially in asynchronous programming.\n\n#### Properties\n- **_queue**: An `asyncio.Queue` that holds events corresponding to each waiting coroutine.\n- **_locked**: A boolean flag indicating whether the lock is currently held.\n\n#### Methods\n- **`__init__`**: Initializes the FIFO lock with an empty queue and sets `_locked` to `False`.\n- **`acquire`**: Asynchronously attempts to acquire the lock. If already held, it places an event in the queue and waits for it to be set.\n- **`release`**: Releases the lock. If there are other coroutines waiting in the queue, it sets the next event to allow those coroutines to proceed. If no events are waiting, it simply releases the lock.\n\n### SequentialRoutedAgent Class\n\nThe `SequentialRoutedAgent` class is a subclass of `RoutedAgent` designed to process certain types of messages sequentially, using the `FIFOLock` class to manage concurrency.\n\n#### Properties\n- **_fifo_lock**: An instance of `FIFOLock` used to ensure FIFO processing of specific message types.\n- **_sequential_message_types**: A sequence of message types (class types) that are to be processed sequentially.\n\n#### Constructor\n- **`__init__`**: Accepts a description string and a sequence of message types. It initializes the base class and sets up the FIFO lock and the types of messages to be processed sequentially.\n\n#### Methods\n- **`on_message_impl`**: This asynchronous method is called when a message is received. It determines whether the message type falls within the `sequential_message_types`. If it does, it acquires the FIFO lock to ensure the message is processed in order:\n  1. It checks each message against the specified sequential types.\n  2. If a match is found, it acquires the FIFO lock.\n  3. It calls the superclass\u2019s message handler, ensuring the message is processed.\n  4. Finally, it releases the lock, allowing the next message to be processed.\n  5. If the message type does not require sequential processing, it proceeds with standard message handling.\n\n## Purpose of the Code\n\nThis code is particularly useful in messaging applications where maintaining the order of certain critical messages is essential. For instance, in group chats or collaborative tools, some messages may need to be processed in a specific order to preserve context or user experience. \n\n## Use Cases\n\n- In scenarios where messages can be of different types, and some of these types (e.g., urgent messages) must be processed at a higher priority while maintaining their order.\n- Enabling asynchronous agents that interact in a controlled manner, preventing race conditions due to out-of-order processing of critical messages.\n\n## Conclusion\n\nOverall, this code establishes a robust framework for handling messages in an asynchronous environment while ensuring that specific messages are processed sequentially. The integration of `FIFOLock` allows for a scalable way to manage locking without blocking other parts of the system, thus enabling efficient concurrent message processing.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py"
  },
  {
    "code": false,
    "content": "# Swarm Group Chat Documentation\n\n## Overview\n\nThe code defines a system for managing a group chat through a class-based architecture, which allows for dynamic handoff of speaking roles among participants. It utilizes asynchronous programming with Python's `asyncio` library to facilitate real-time communication in a non-blocking manner. The primary components include the `SwarmGroupChatManager`, responsible for managing participants and their speaking turns based on handoff messages, and the `Swarm` class, which integrates agents into a collaborative chat environment.\n\n## SwarmGroupChatManager Class\n\n### Purpose\n\nThe `SwarmGroupChatManager` inherits from `BaseGroupChatManager` and is specifically designed to manage group chats by selecting the next speaker based on handoff messages. It initializes with parameters that define the group chat's characteristics, including participant names, descriptions, and termination conditions.\n\n### Initialization\n\nThe constructor takes various parameters, including:\n\n- **name**: Identifier for the group chat.\n- **group_topic_type**: Defines the topic type for the group.\n- **output_topic_type**: Specifies the output message topic type.\n- **participant_topic_types**: A list that outlines the topic types for each participant.\n- **participant_names**: List of names for the participants.\n- **participant_descriptions**: Descriptions for each participant.\n- **output_message_queue**: An asyncio queue for managing chat messages and events.\n- **termination_condition**: Defines when the chat should end, if applicable.\n- **max_turns**: The maximum number of turns before termination.\n- **message_factory**: Factory for creating messages.\n- **emit_team_events**: Boolean indicating whether to emit events.\n\nThe manager starts with the first participant as the current speaker.\n\n### Methods\n\n#### `validate_group_state`\n\nThis asynchronous method verifies the state of the group chat at startup. It checks if any of the given messages include a handoff signal targeting valid participants. If invalid handoff messages are found, it raises a `ValueError`.\n\n#### `reset`\n\nResets the group chat state, clearing the message thread, resetting the current turn, and returning the current speaker to the first participant. It also resets the termination condition if it exists.\n\n#### `select_speaker`\n\nThis method identifies the next speaker based on the most recent handoff message in the conversation thread. If there are no handoff messages, the current speaker remains unchanged. Notably, it always returns a single speaker.\n\n#### `save_state`\n\nSaves the current state of the chat, including the message thread, current turn, and current speaker, returning this information in a dictionary format.\n\n#### `load_state`\n\nRestores the chat state from a provided dictionary, reconstructing the message thread and resetting the current turn and current speaker.\n\n## Swarm Class\n\n### Purpose\n\nThe `Swarm` class extends `BaseGroupChat` and `Component`, forming the core of the group chat system with unique functionalities for selecting speakers based on handoff messages. This class sets the foundation for agent behavior in the chat environment.\n\n### Initialization\n\nIn the constructor, the participants are validated to ensure they are instances of `ChatAgent`. Several optional parameters are available, allowing for customization of the group chat's characteristics, including:\n\n- **participants**: List of `ChatAgent` instances, where the first one is designated as the initial speaker.\n- **name**: Optional name for the chat.\n- **description**: Optional description for the chat.\n- **termination_condition**: Optional condition for when the chat should end.\n- **max_turns**: Maximum number of turns before stopping the chat.\n\n### Component Configuration\n\nThe `Swarm` class also accommodates a schema (`SwarmConfig`) that defines the configuration for the swarm setup, including participant details and termination conditions. The configuration is crucial for initializing the system in a structured manner.\n\n### Methods\n\n#### `_create_group_chat_manager_factory`\n\nThis private method provides a factory for creating instances of `SwarmGroupChatManager`, ensuring that each chat instance is properly initialized with all necessary parameters.\n\n#### `_to_config`\n\nGenerates configuration data from the current state of the swarm instance. This method is essential for serialization or for creating a configuration schema suitable for saving and loading.\n\n#### `_from_config`\n\nActs as a class method for instantiating a `Swarm` object from a configuration input, allowing for re-creation of the state from stored configurations.\n\n## Example Usage\n\nThe provided code snippets demonstrate how to create and manage a group chat using the `Swarm` class. Participants can be defined as instances of `AssistantAgent`, and termination conditions can be implemented to stop the chat after a specific number of turns or under certain circumstances. \n\n### Simple Chat Example\n\nAn example is included to create a simple interaction between two agents, including a usage of a maximum message termination condition:\n\n```python\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    \n    agent1 = AssistantAgent(\"Alice\", model_client=model_client, ...)\n    agent2 = AssistantAgent(\"Bob\", model_client=model_client, ...)\n    \n    termination = MaxMessageTermination(3)\n    team = Swarm([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"What is bob's birthday?\")\n    async for message in stream:\n        print(message)\n\nasyncio.run(main())\n```\n\n### Human-in-the-Loop Handoff\n\nAnother example illustrates how to use a human-in-the-loop approach with handoff messages:\n\n```python\nasync def main() -> None:\n    ...\n    termination = HandoffTermination(target=\"user\") | MaxMessageTermination(3)\n    ...\n    await Console(team.run_stream(task=\"What is bob's birthday?\"))\n    await Console(team.run_stream(task=HandoffMessage(source=\"user\", target=\"Alice\", content=\"Bob's birthday is on 1st January.\")))\n    \nasyncio.run(main())\n```\n\n## Conclusion\n\nOverall, the code provides a robust framework for managing dynamic group chats, allowing for flexible speaker selection and integration of various chat agents. The design promotes resilience and adaptability in communication through strategic handoff messages and configurable components.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_swarm_group_chat.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface to expose specific tools from its internal implementation. It imports two classes, `AgentTool` and `TeamTool`, from their respective modules (`._agent` and `._team`). This allows users to access these tools directly from this module.\n\n## Imports\n\n### AgentTool\n\n- **Location**: Imported from the module `._agent`.\n- **Purpose**: The `AgentTool` likely encapsulates functionalities related to an agent's operations or utilities. The specifics of its implementation are not provided in this module but can be inferred to assist in agent-related actions or interactions.\n\n### TeamTool\n\n- **Location**: Imported from the module `._team`.\n- **Purpose**: The `TeamTool` presumably offers functionalities related to team management or operations. Similar to `AgentTool`, the details of its internal workings are not specified, but it can be inferred that it facilitates team-related tasks.\n\n## Export Mechanism\n\n### `__all__`\n\n- The `__all__` variable is a convention used in Python to define a public API for a module. \n- In this case, it explicitly states that both `AgentTool` and `TeamTool` are intended to be publicly accessible when this module is imported. \n- This prevents other internal classes or functions from being exposed to the module's users, promoting better encapsulation and controlled access to its API.\n\n## Usage\n\nThis module can be imported in other Python files where the `AgentTool` and `TeamTool` are needed. By using the syntax:\n\n```python\nfrom your_module_name import *  # This will import both AgentTool and TeamTool\n```\n\nor\n\n```python\nfrom your_module_name import AgentTool, TeamTool  # This will import the specific tools directly\n```\n\n### Summary\n\nOverall, this module is a simple setup that provides access to two tool classes: `AgentTool` and `TeamTool`. These tools encapsulate specific functionalities that are utilized within a larger framework or application dealing with agents and teams. The strategic use of `__all__` ensures that only these tools are visible to the module's consumers, maintaining a cleaner namespace and clearer API.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/tools/__init__.py"
  },
  {
    "code": false,
    "content": "# AgentTool Documentation\n\n## Overview\n\nThe `AgentTool` class represents a specialized tool for executing tasks using a chat agent from the `autogen_agentchat` library. It is designed to facilitate task execution while ensuring that results are returned in a cohesive format. The tool is built upon the `TaskRunnerTool` class and inherits from the generic `Component` class, configured with an `AgentToolConfig`.\n\n## Components \n\n### AgentToolConfig\n\nThe `AgentToolConfig` class is a configuration model defined using Pydantic. It has the following attributes:\n\n- **agent**: This attribute holds a reference to a `ComponentModel`, representing the agent to be used for the task execution.\n  \n- **return_value_as_last_message**: A boolean flag indicating whether the result of the task execution should be returned as the content of the last message. If `True`, it returns only the last message. If `False`, it returns all messages concatenated together in a formatted string.\n\nThis configuration class ensures that the `AgentTool` functions with the necessary settings, making its instantiation clearer and more consistent.\n\n### AgentTool Class\n\nThe `AgentTool` class serves as the main functionality holder, facilitating task execution through a specified agent. Key attributes and methods include:\n\n- **__init__ Method**: This method initializes the `AgentTool` instance. It takes an `agent` of type `BaseChatAgent` and a `return_value_as_last_message` flag. It initializes its internal agent and invokes the parent class's initialization method.\n\n- **_to_config Method**: This method converts the current instance into an instance of `AgentToolConfig`, effectively allowing the tool's configuration to be output in a structured manner.\n\n- **_from_config Class Method**: This factory method creates an instance of `AgentTool` from a provided `AgentToolConfig`. It loads the agent from the configuration and sets the return value behavior accordingly. This allows for easy reconstruction of the tool\u2019s state from stored configurations.\n\n### Important Considerations\n\nA critical note when using the `AgentTool` is the requirement to disable parallel execution of tool calls. This ensures that the agent maintains its internal state throughout the task execution, preventing concurrency issues that could arise from simultaneously running agents.\n\n## Usage Example\n\nAn example provided in the class docstring demonstrates how to set up and use the `AgentTool`. Here\u2019s a high-level description of the example:\n\n1. An `OpenAIChatCompletionClient` instance is created to interact with the OpenAI model (e.g., GPT-4.1).\n  \n2. An `AssistantAgent` named \"writer\" is instantiated with a specified name, description, model client, and a system message to guide its behavior.\n\n3. An `AgentTool` is then instantiated using the \"writer\" agent.\n\n4. Another `OpenAIChatCompletionClient` is created with parallel tool calls disabled for the main agent interaction.\n\n5. A second `AssistantAgent` named \"assistant\" is created, which incorporates the `writer_tool`, and is configured to assist users effectively.\n\n6. Finally, the `Console` is utilized to stream the interaction with the `assistant`, running a specific task \u2014 in this case, asking it to write a poem about the sea.\n\n### Asynchronous Execution\n\nThe example is encapsulated within an asynchronous `main` function, showcasing the async/await pattern commonly used for non-blocking execution in Python. The operation culminates in executing the story generation request as part of an async workflow with `asyncio.run(main())`.\n\n## Conclusion\n\nIn summary, the `AgentTool` class, powered by `AgentToolConfig`, provides a structured and efficient method for executing tasks through chat agents. By ensuring that internal agent states are preserved through single-threaded execution and delivering results in either a single or concatenated form, it plays a critical role in the functionality of agent-based task management and interactions.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/tools/_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for TaskRunnerTool Module\n\n## Overview\n\nThe `TaskRunnerTool` is designed to facilitate the execution of tasks utilizing either a chat agent or group chat context. This module extends basic functionality defined for tools within an asynchronous environment. It integrates various core components such as task management, event messaging, and state persistence in a structured way, leveraging Pydantic for structured input handling.\n\n## Definitions and Imports\n\nThe code imports several modules and classes which are essential for its functionality:\n\n- **ABC**: Base class for defining abstract base classes.\n- **Typed Annotations**: Facilitates type hints, enhancing the code\u2019s clarity regarding expected argument types.\n- **CancellationToken**: Manages cancellation signals for ongoing tasks.\n- **BaseStreamTool**: A generic base class for tools operating in a streaming manner.\n- **Pydantic BaseModel**: Used for creating data models with validation features.\n- **Agents and Messaging Components**: These are custom components presumably part of a larger framework, allowing for complex chat interactions and task management.\n\n\n## Class: TaskRunnerToolArgs\n\n### Purpose\n\nThis class is a data model created using Pydantic. It serves as the structured input to the `TaskRunnerTool`. It defines a single attribute:\n\n- `task`: A string that represents the task to be executed.\n\n### Role\n\nIt ensures that any instance of `TaskRunnerToolArgs` contains the necessary 'task' attribute, providing a means to validate input seamlessly.\n\n## Class: TaskRunnerTool\n\n### Purpose\n\n`TaskRunnerTool` is an abstract base class positioned within the framework for executing tasks. It is designed to work with agents or group chats to perform asynchronous operations based on provided arguments.\n\n### Attributes\n\n- `component_type`: A static string that identifies the type of the component as \"tool\".\n- `task_runner`: An instance of `BaseGroupChat` or `BaseChatAgent` responsible for executing the task.\n- `return_value_as_last_message`: A boolean that specifies whether the last message of the task result should be returned.\n\n### Initialization Method\n\n```python\ndef __init__(self, task_runner, name, description, return_value_as_last_message):\n```\n\n- Initializes the component with the task runner and necessary metadata.\n- Sets up the superclass with type information and validation requirements.\n\n## Method: run\n\n```python\nasync def run(self, args: TaskRunnerToolArgs, cancellation_token: CancellationToken) -> TaskResult:\n```\n\n### Purpose\n\nThis asynchronous method is responsible for executing the task defined in `args`. It utilizes the task runner to carry out the operation and returns the result.\n\n### Role\n\n- Accepts an instance of `TaskRunnerToolArgs` and a `CancellationToken`.\n- Calls the `run` method of the `task_runner`, effectively delegating the execution.\n\n## Method: run_stream\n\n```python\nasync def run_stream(self, args: TaskRunnerToolArgs, cancellation_token: CancellationToken) -> AsyncGenerator:\n```\n\n### Purpose\n\nThis method enables the execution of tasks in a streaming fashion, yielding results as they are produced instead of waiting for the entire task to complete.\n\n### Role\n\n- It allows for real-time handling of output, which is particularly useful for long-running tasks.\n- Uses the asynchronous generator to yield each event or message produced in the course of running the task until completion, allowing for a more interactive experience.\n\n## Method: return_value_as_string\n\n```python\ndef return_value_as_string(self, value: TaskResult) -> str:\n```\n\n### Purpose\n\nThis method converts the result of a task into a string format.\n\n### Role\n\n- It formats messages into a readable string, either returning the last message if specified or compiling all relevant messages.\n- Checks the source of messages to ensure only appropriate ones are included (e.g., excluding user messages).\n\n## Method: save_state_json\n\n```python\nasync def save_state_json(self) -> Mapping[str, Any]:\n```\n\n### Purpose\n\nThis asynchronous method saves the current state of the task runner in a JSON-compatible format.\n\n### Role\n\n- It delegates the state-saving functionality to the `task_runner`, ensuring that state persistence is consistent.\n\n## Method: load_state_json\n\n```python\nasync def load_state_json(self, state: Mapping[str, Any]) -> None:\n```\n\n### Purpose\n\nThis asynchronous method restores a task runner to a previous state based on a JSON-compatible structure.\n\n### Role\n\n- It allows the task runner to reload its state, enabling resumption of tasks or preservation of context across operations.\n\n## Summary\n\nThe `TaskRunnerTool` is an abstract tool that provides structured interfaces for executing tasks within a chat context. By leveraging asynchronous programming, the tool supports both standard task execution and streaming outputs. Its integration with state management allows for a robust framework aimed at enhancing task execution in interactive environments.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/tools/_task_runner_tool.py"
  },
  {
    "code": false,
    "content": "# TeamTool Documentation\n\n## Overview\n\nThe provided code defines a class `TeamTool` which serves as a specialized tool for executing tasks using a team of agents within a chat interface. This tool, part of a larger system that appears to implement collaborative task management, is built to facilitate synchronous communication between team members represented by agents. The main objective is to execute predefined tasks while allowing structured interactions among agents.\n\n## Dependencies and Imports\n\nThe code imports several essential modules:\n\n- **`Component`, `ComponentModel`**: These are likely part of a framework for component-based architecture, providing the infrastructure for defining components and their configurations.\n- **`BaseModel`**: This is from the Pydantic library and provides data validation and settings management using Python type annotations.\n- **`BaseGroupChat`**: This appears to represent a chat group, integral for managing a set of agents working collaboratively.\n- **`TaskRunnerTool`**: A base class that likely provides core functionalities for running tasks.\n\n\n## Class: TeamToolConfig\n\n### Purpose\n`TeamToolConfig` is a configuration class that outlines the necessary parameters for initializing a `TeamTool`. It inherits from `BaseModel`, allowing it to leverage Pydantic's features for type validation.\n\n### Attributes\n- **`name`**: A string attribute representing the name of the tool.\n- **`description`**: A string attribute giving a brief description of the tool's functionalities.\n- **`team`**: An instance of `ComponentModel`, which specifies the team utilized for task execution.\n- **`return_value_as_last_message`**: A boolean flag indicating whether to return the final message of the task as the primary result of the tool.\n\n## Class: TeamTool\n\n### Purpose\n`TeamTool` extends the capabilities of `TaskRunnerTool` while also being a component defined with a configuration schema of `TeamToolConfig`. This tool is specifically designed for executing tasks with team members and managing state effectively within a chat setting.\n\n### Important Notes\nThe documentation stresses that parallel tool calls must be disabled to prevent concurrency issues since the agents maintain internal states that could conflict when called simultaneously. This configuration is particularly relevant for clients like `OpenAIChatCompletionClient`.\n\n### Constructor\nThe constructor initializes a `TeamTool` instance with the following parameters:\n- **`team` (BaseGroupChat)**: Specifies which group of agents will collaborate on the task.\n- **`name` (str)**: Name of the tool.\n- **`description` (str)**: Description of the tool.\n- **`return_value_as_last_message` (bool)**: A flag to indicate the method of returning results.\n\n### Configuration Methods\n- **`_to_config` method**: Returns a `TeamToolConfig` instance populated with the current attributes of the `TeamTool`.\n  \n- **`_from_config` class method**: Constructs a `TeamTool` instance using the provided `TeamToolConfig`, ensuring it respects the predefined configuration schema.\n\n## Example Usage\n\nThe commented-out example demonstrates the practical implementation of `TeamTool` within an asynchronous context. Here\u2019s an overview of the key steps involved:\n\n- **Model Client Initialization**: An instance of `OpenAIChatCompletionClient` is initialized, ensuring that parallel tool calls are disabled for proper synchronous interactions.\n  \n- **Agent Creation**: Three agents (`writer`, `reviewer`, and `summarizer`) are instantiated, each configured with unique system messages that define their roles in the task.\n\n- **Team Setup**: The agents are combined into a `RoundRobinGroupChat`, which facilitates their interactions, adhering to a termination condition that triggers when the summarizer agent completes its job.\n\n- **Tool Initialization**: The `TeamTool` is created using the defined team, with parameters set to manage task execution and result handling.\n\n- **Main Agent Definition**: A primary agent that can utilize the `TeamTool` is created, embedding the tool to facilitate task execution.\n\n- **Execution Handling**: The script contains an async for-loop that can handle messages as they are generated, and an instance of `Console` is used to output these messages in a readable format.\n\n## Conclusion\n\nThe `TeamTool` serves as a robust framework for managing collaborative tasks involving multiple agents. With thoughtful configuration and strict adherence to synchronous operation, it effectively leverages the capabilities of team-based interactions within a conversational context. This tool has the potential to be applied across a variety of collaborative scenarios where multiple perspectives and contributions are desired to enhance task outcomes.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/tools/_team.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: Agent Message Formatting Utilities\n\n## Overview\nThis module provides utility classes designed to facilitate the formatting and printing of messages related to agents. It serves as a part of a larger system that likely deals with agent-based functionalities, ensuring that the messages exchanged are presented in a clear and structured manner.\n\n## Imported Components\nThe module imports two classes:\n- `Console` \n- `UserInputManager`\n\nThese classes are expected to be defined in a sibling module called `_console`, suggesting that they might handle different aspects of console output and user interaction.\n\n## Public Interface\nThe module specifies an `__all__` list, which explicitly declares what is available for import when the module is used. This is particularly useful for maintaining a clean namespace and allowing users to know which classes are intended for public use:\n\n- **`Console`** - Presumably responsible for managing console output, formatting messages for clarity and readability.\n- **`UserInputManager`** - Likely designed for handling user input, possibly facilitating interaction with the agent messages or commands through the console.\n\n## Purpose and Functionality\n\n### Console Class\nThe `Console` class is likely intended for formatting and presenting messages to the user in a visually appealing way. This may include features such as:\n- Color formatting for different types of messages (e.g., information, warnings, errors).\n- Structured output to ensure that messages are easily readable.\n- Methods for printing messages to standard output or perhaps logging them to a file.\n\n### UserInputManager Class\nThe `UserInputManager` class is probably focused on capturing and processing user inputs. Its functionality might include:\n- Prompting users for input within the console environment.\n- Validating the input provided to ensure it meets certain criteria.\n- Managing the flow of user interactions, potentially handling various commands or queries related to message processing.\n\n## Conclusion\nBy encapsulating message formatting and user input handling within dedicated classes, this module aims to enhance the user experience during agent interactions. It abstracts the details of output formatting and input management, enabling other parts of the application to focus on functionality while relying on these utilities for effective communication. \n\nOverall, this module forms a critical part of the agent system\u2019s user interface, providing essential tools for presenting information and facilitating interaction.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/ui/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Asynchronous User Input Management and Console Message Handling\n\nThis document provides a detailed overview of a Python module designed for handling asynchronous user input and streaming output to a console, particularly for chat-based applications. It is structured into sections that describe the overall purpose of the module, auxiliary functions, classes, and the main asynchronous console messaging function.\n\n## Overview\n\nThe code facilitates the management of user input and the rendering of messages from a stream, such as responses from a chatbot. It employs `asyncio` for concurrent operations, allowing it to handle multiple inputs and outputs without blocking. The code is particularly useful in scenarios requiring real-time interaction where chat messages and user prompts are processed asynchronously.\n\n### Key Libraries Used\n\n1. **asyncio**: This is the main library for managing asynchronous operations, enabling the function to await on events and execute multiple coroutines concurrently.\n2. **os** and **sys**: These libraries are used for environment checks, specifically to determine the type of terminal and the output capabilities.\n3. **autogen_core**: This module provides necessary types and functionalities, including cancellation tokens and request usage tracking.\n\n## Auxiliary Functions\n\n### Output Checks\n\n- **_is_running_in_iterm()**: This function checks if the application is running in the iTerm terminal, returning a boolean value based on the terminal environment variable.\n  \n- **_is_output_a_tty()**: This function checks if the script's output is directed to a terminal that supports interactive output via `isatty()`.\n\n### Input Functions Definition\n\nInput functions are defined to handle both synchronous and asynchronous user inputs using Python type hints:\n- **SyncInputFunc**: A callable type for synchronous input.\n- **AsyncInputFunc**: A callable type for asynchronous input that can also take an optional cancellation token.\n\n## UserInputManager Class\n\n### Purpose\n\nThe `UserInputManager` class is designed to manage user input events effectively. It keeps track of user input requests and ensures that the appropriate callback is executed when a response is awaited.\n\n### Attributes\n\n- **input_events**: A dictionary mapping request IDs to asyncio events, allowing the program to wait for user inputs and signal when they are available.\n- **callback**: A user-defined function that can be either synchronous or asynchronous, invoked to process user input.\n\n### Methods\n\n- **get_wrapped_callback()**: This method wraps the input callback, handling both synchronous and asynchronous cases. It checks for existing input events and creates them if they do not already exist.\n  \n- **notify_event_received(request_id: str)**: This method sets the event for a given request ID, signaling that user input has been received.\n\n## Asynchronous Printing Function\n\n### aprint Function\n\nThe `aprint(output: str, end: str = \"\\n\", flush: bool = False)` function allows asynchronous printing. It utilizes `asyncio.to_thread()` to print in a non-blocking manner, ensuring that the console output does not halt other asynchronous operations.\n\n## Console Function\n\n### Purpose\n\nThe `Console` function serves as the main entry point for handling and displaying messages from a stream. It processes a continuous flow of messages (such as chat or task results) and updates the console accordingly.\n\n### Parameters\n\n- **stream**: An asynchronous generator that yields a stream of messages from task runners or chat agents.\n- **no_inline_images**: A boolean that determines whether to render images inline in iTerm, helpful for terminal compatibility.\n- **output_stats**: An optional boolean to enable summary statistics output.\n- **user_input_manager**: An optional instance of `UserInputManager` for managing user input events.\n\n### Logic Overview\n\n1. **Initialization**: The function checks the terminal type, sets up tracking for usage stats, and initializes a timer.\n  \n2. **Message Processing Loop**: It enters an asynchronous loop where it awaits messages from the stream:\n   - **TaskResult Handling**: When a `TaskResult` message is received, it potentially outputs a summary of processing statistics if `output_stats` is enabled.\n   - **Response Handling**: For `Response` messages, it formats and prints the message content and any associated usage stats.\n   - **User Input Event**: The function listens for a `UserInputRequestedEvent`, notifying the `UserInputManager` to allow user input to be processed.\n   - **Message Display**: Non-user input messages are printed based on their types, handling streaming chunks and multimodal messages appropriately.\n\n3. **Completion Check**: After processing all messages, if no `TaskResult` or `Response` was found, it raises a ValueError indicating that no relevant messages were processed.\n\n4. **Return**: Finally, the last processed message (either a `TaskResult` or `Response`) is returned.\n\n## Summary\n\nIn essence, this module provides a robust framework for asynchronous user input handling and console output management suitable for real-time applications such as chatbots. Through effective event management and the use of asynchronous programming paradigms, it is able to maintain responsiveness while processing potentially numerous messages and user interactions in parallel.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/ui/_console.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: AgentChat Utilities\n\n## Overview\n\nThis module serves as a utility library specifically designed for use by AgentChat agents and teams. It includes common functions that streamline processes related to content management within the framework of AgentChat.\n\n## Imports\n\nTwo functions are imported from a sibling module named `_utils`:\n\n1. **`content_to_str`**: This utility function is likely used to convert content\u2014potentially in various formats\u2014into a string representation. This is particularly useful for ensuring that content can be easily processed or displayed.\n\n2. **`remove_images`**: This utility function presumably serves to remove any image content from input data. This can be important for scenarios where text-only processing is required, or where reducing content complexity is desired.\n\n## Exported Functions\n\nThe `__all__` variable is explicitly defined to indicate which components of this module should be accessible when the module is imported. The following functions are exported:\n\n- **`content_to_str`**\n- **`remove_images`**\n\nBy using `__all__`, the module controls its public interface, ensuring that only these utilities are exposed to users. This is a common practice in Python to promote clean and manageable code design.\n\n## Purpose and Role\n\n### Utility for AgentChat Agents\n\nThe primary purpose of this module is to provide shared functionalities that make development easier for agents and teams engaged in the AgentChat framework. By encapsulating common utility functions, developers can avoid redundancy and promote code reuse.\n\n### Content Handling\n\nEfficient handling of diverse content types is essential for chat applications. The `content_to_str` function allows agents to convert input data or messages into a normalized string format, enabling consistent processing and interaction with various types of content.\n\n### Image Management\n\nThe presence of the `remove_images` function reflects a design consideration for text-based interactions. By stripping away image content, agents can focus on textual data, which is often paramount in chat applications. This aids in maintaining clarity and can help in creating user-friendly outputs.\n\n## Conclusion\n\nIn summary, this module plays a critical role in the AgentChat ecosystem by providing essential utilities that enhance content processing capabilities. Through the shared functions of converting content to string format and removing image data, it supports developers in building more efficient and effective AgentChat agents and teams. The explicit inclusion of an interface through `__all__` emphasizes good practices in module design, promoting a clean separation of public and internal components.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/utils/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided source code is designed to manipulate and format messages exchanged in a language model (LLM) interaction. It defines two primary functions: `content_to_str` and `remove_images`. Both functions leverage type aliasing for clarity and employ data structures such as `BaseModel` from the Pydantic library, which is commonly used for data validation and settings management in Python.\n\n## Type Aliases\n\nThe code defines several type aliases to improve code readability and maintainability:\n\n- `_StructuredContent`: This alias is a type for any Pydantic `BaseModel`.\n- `_UserContent`: This alias describes a type that can either be a string or a list containing strings or `Image` objects.\n- `_AssistantContent`: This represents a type that can be a string or a list of `FunctionCall` objects.\n- `_FunctionExecutionContent`: This is defined as a list of `FunctionExecutionResult` objects.\n- `_SystemContent`: A string type to represent system messages.\n\nThese aliases streamline the type annotations throughout the code, making it easier to understand what kinds of content the functions are expected to process.\n\n## Function: `content_to_str`\n\n### Purpose\n\nThe `content_to_str` function is responsible for converting various content types typically found in messages from a language model interaction into a single string format. This is particularly useful for logging, rendering, or further processing of the message content.\n\n### Parameters\n\n- `content`: Accepts content of various types, including user messages, assistant responses, execution results, or system messages.\n\n### Functionality\n\n1. **String Handling**: If the content is a string, the function returns it directly.\n2. **BaseModel Serialization**: If the content is an instance of `BaseModel`, it uses the `model_dump_json()` method to convert the model into a JSON string representation.\n3. **List Handling**: If the content is a list (either of strings or a mix of strings and `Image` objects):\n   - It initializes an empty list, `result`.\n   - It iterates through each item in the list, converting each string to itself, each `Image` to the placeholder \"<image>\", or any other object to its string representation using the built-in `str()` function.\n4. **Return Value**: Finally, the function joins all elements in the `result` list into a single string, separated by newline characters.\n\n## Function: `remove_images`\n\n### Purpose\n\nThe `remove_images` function processes a list of messages (`LLMMessage` objects) to remove `Image` content from user messages while retaining the structure of the original messages. This could be useful in scenarios where image content is not needed, such as text-only processing.\n\n### Parameters\n\n- `messages`: A list of `LLMMessage` objects, which may contain user messages, system messages, or assistant responses.\n\n### Functionality\n\n1. **Initialization**: Creates an empty list `str_messages` to store processed messages.\n2. **Iterate Through Messages**: The function iterates over each message in the `messages` list.\n   - If the message is a `UserMessage` and its content is a list:\n     - It transforms this content into a string format using `content_to_str`.\n     - It appends a new `UserMessage` (with the content as a string) to `str_messages`.\n   - If the message is not a user message or does not contain list content, it appends the original message to `str_messages`.\n3. **Return Value**: The function returns the newly created list of messages, now devoid of image content in user messages.\n\n## Conclusion\n\nThe provided code efficiently processes content from language model interactions by transforming various types of message content into a string representation and filtering user messages to remove images. The use of type aliases enhances clarity, while the implementation of Pydantic's features allows for robust handling of structured data. Overall, this code serves as a utility for text manipulation in the context of LLM applications.",
    "filename": "python/packages/autogen-agentchat/src/autogen_agentchat/utils/_utils.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code is a set of unit tests for verifying the serialization and deserialization functionality of different agent types in a chat system. It utilizes the `pytest` framework to perform tests against several model context classes utilized by different types of agents, namely `AssistantAgent`, `SocietyOfMindAgent`, and `CodeExecutorAgent`. The main focus is to ensure that the state of the agent (particularly their model context) is preserved during serialization and deserialization processes.\n\n## Imports\n\nThe code begins by importing necessary components from various modules:\n\n- **Agents**: Specifically, three agent types: `AssistantAgent`, `CodeExecutorAgent`, and `SocietyOfMindAgent`.\n- **Teams**: The `RoundRobinGroupChat` class which facilitates group interactions among agents.\n- **Model Contexts**: Several context classes for managing chat completions, which include:\n  - `BufferedChatCompletionContext`\n  - `ChatCompletionContext`\n  - `HeadAndTailChatCompletionContext`\n  - `TokenLimitedChatCompletionContext`\n  - `UnboundedChatCompletionContext`\n- **Code Executors**: The local command line code executor (`LocalCommandLineCodeExecutor`) for executing code snippets.\n- **Replay Clients**: The `ReplayChatCompletionClient` for simulating chat completions during tests.\n\n## Test Parameterization\n\nThe tests are parameterized to run multiple instances with different model context classes. Each context class is designed to manage message completion in distinct ways (e.g., bounded by tokens, buffered, or unbounded). This flexibility allows for broad testing coverage of how context affects the serialization and deserialization processes.\n\n## Test Function: `test_serialize_and_deserialize_model_context_on_assistant_agent`\n\n### Purpose\n\nThis function tests the serialization and deserialization of the `model_context` on the `AssistantAgent`.\n\n### Steps\n\n1. An instance of `AssistantAgent` is created, initialized with a name, a replay chat completion client, a description, and a specific model context class.\n2. The agent\u2019s state is serialized into a format suitable for storage or transmission via the `dump_component()` method.\n3. A new `AssistantAgent` instance is created by loading this serialized data using the `load_component()` method.\n4. Assertions are made to ensure that both the original and deserialized agents maintain the same type of model context and that their contexts are equal.\n\n## Test Function: `test_serialize_and_deserialize_model_context_on_society_of_mind_agent`\n\n### Purpose\n\nThis function tests the serialization and deserialization of the `model_context` on the `SocietyOfMindAgent`.\n\n### Steps\n\n1. Two instances of `AssistantAgent` are created to serve as participants in a team.\n2. A `RoundRobinGroupChat` is instantiated with the two agents as participants.\n3. An instance of `SocietyOfMindAgent` is created, using the team and a specified model context class.\n4. Similar to the previous test, the agent\u2019s state is serialized, and then a new instance is created by deserializing this data.\n5. Assertions check the integrity of agent state between the original and deserialized instances as in the previous case.\n\n## Test Function: `test_serialize_and_deserialize_model_context_on_code_executor_agent`\n\n### Purpose\n\nThis function tests the serialization and deserialization of the `model_context` on the `CodeExecutorAgent`.\n\n### Steps\n\n1. An instance of `CodeExecutorAgent` is created with a name, a local command line code executor, a description, and a model context class.\n2. The agent is serialized using the `dump_component()` method.\n3. A new instance of `CodeExecutorAgent` is created by loading the serialized data.\n4. Assertions confirm that both the original and deserialized agents maintain identical contexts and types.\n\n## Conclusion\n\nThis code effectively validates the functionality of serialization and deserialization for multiple agent types within the chat system. By parameterizing the tests across different model contexts, it ensures thorough testing of potentially diverse behaviors in context handling. Each test highlights the importance of maintaining data integrity across instances, confirming that agents can be accurately recreated after serialization. This capability is crucial for applications that may need to save and restore agent states dynamically.",
    "filename": "python/packages/autogen-agentchat/tests/test_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for the AssistantAgent Testing Suite\n\nThis document provides a high-level description and an overview of the testing suite designed to assess the functionality of the `AssistantAgent` class in the `autogen_agentchat` package. The tests implemented within this suite focus on various capabilities, including complex interactions, tool execution, memory management, and response validation.\n\n## Overview of the Testing Environment\n\nThe tests utilize standard libraries such as `asyncio`, `unittest.mock`, and `pytest` to facilitate async testing and mocking of components. The suite leverages several first-party imports from the `autogen_agentchat` package, including the main `AssistantAgent`, tools, and message classes. \n\nMock implementations, including `MockMemory` and several custom function mocks, are created to simulate the behavior of components without needing to interact with actual external systems, which ensures isolation of tests.\n\n## Mock Functions and Classes\n\n### Mock Memory Implementation\nThe `MockMemory` class simulates the memory component used by the `AssistantAgent`. It supports memory operations like adding content, querying, and clearing memory. Each method has been designed to operate asynchronously, enabling tests to accurately mimic the agent's memory behavior in real usage scenarios.\n\n### Mock Tools\nSeveral mock functions are defined, including:\n- `mock_tool_function`: A basic synchronous tool simulating tool functionality.\n- `async_mock_tool_function`: An asynchronous variant of the tool.\n- Utility functions like `_pass_function` and `_echo_function` further assist in testing different scenarios by performing simple operations.\n\n## AssistantAgent Tests\n\n### Test Initialization\nThe suite extensively covers different initialization scenarios through `TestAssistantAgentInitialization`. It verifies that:\n- The agent initializes correctly with minimal required parameters.\n- Tools and memory are registered and integrated properly.\n- Handoffs are acknowledged during initialization.\n\n### Test Execution Flow\nVarious scenarios are tested under `TestAssistantAgentToolCallLoop`, including:\n- **Tool Call Loop Behavior**: Tests verify that the tool execution process adheres to logic rules, including limits on the number of iterations (`max_tool_iterations`).\n- **Handoff Behavior**: The agent's transitions between itself and other agents, characterized by handoff messages, are validated.\n\n### Test Message Handling\nIn `TestAssistantAgentMessageContext`, the proper handling of different types of messages (e.g., text, structured, handoff) is explored. The importance of maintaining context and ensuring message integrity through updates is pronounced.\n\n### Test Memory Management\n`TestAssistantAgentMemoryIntegration` checks for:\n- Memory content updates and their impact on context responses.\n- Persistence of memory data between different agent sessions.\n\n### Test Environment and Validations\nComprehensive validation tests are in place (`TestAssistantAgentValidation`), ensuring unique tool and handoff names and handling unexpected or conflicting configurations. Testing setups simulate errors to observe agent behavior under erroneous conditions.\n\n## Advanced Scenarios and External Integrations\n\n### Complex Workflows\nTests under `TestAssistantAgentComplexIntegration` explore complex interaction scenarios that involve agent states, memory management, and reflective responses to user inputs. The comprehensive evaluation confirms that the agent can handle intricate sequences of operations.\n\n### API Integration\nCertain tests evaluate the integration with external models, specifically with the Anthropic API. Tests ensure:\n- Correct functionality when tool calls are made.\n- Validation of basic text responses and usage tracking.\n\n## Summary\nThe test suite meticulously covers a breadth of functionalities provided by the `AssistantAgent`, ensuring thorough evaluation through mock implementations and asynchronous testing. Each class and method is systematically designed to focus on key functionalities, and the expected outcomes are rigorously asserted. This not only validates the current implementation but also lays the groundwork for testing future enhancements and integrations.",
    "filename": "python/packages/autogen-agentchat/tests/test_assistant_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation of Code Execution Tests\n\nThis document provides a high-level overview of the code contained in the provided script, which is focused on testing a code execution agent using Python. The agent is responsible for generating, executing, and processing code snippets sent as messages. The tests utilize the `pytest` framework, and several components are tested using asynchronous operations.\n\n## Overview of Code Execution Agent\n\nThe core component of the script is the `CodeExecutorAgent`, which is designed to execute code snippets provided in various formats. The tests ascertain the agent's behavior in various scenarios, including successful execution, error handling, and code approval mechanisms. The agent employs a local command-line executor (`LocalCommandLineCodeExecutor`) to run the code.\n\n### Import Statements and Setup\n\nVarious modules and classes are imported, including:\n\n- **AsyncIO**: Used for asynchronous programming in Python.\n- **Pytest**: Framework for writing test cases.\n- **CodeExecutorAgent**: The main class under test, responsible for executing the provided code snippets.\n- **Message Classes**: Such as `TextMessage`, used to wrap the content sent to the agent.\n- **ReplayChatCompletionClient**: Simulates responses from a model client, which the agent may use for code generation.\n- **Approval Function Classes**: These functions are utilized to handle approval for code execution based on predefined criteria.\n\n## Test Definitions\n\n### Test Basic Code Execution\n\n```python\n@pytest.mark.asyncio\nasync def test_basic_code_execution() -> None:\n    ...\n```\n\nThis test ensures that the `CodeExecutorAgent` can successfully execute a basic Python script that calculates the square root of a number. It verifies the output and the source of the response message to ensure it matches expectations. \n\n### Test Code Generation and Execution with Model Client\n\n```python\n@pytest.mark.asyncio\nasync def test_code_generation_and_execution_with_model_client() -> None:\n    ...\n```\n\nThis test evaluates the entire pipeline from code generation to execution. A model client simulates interaction with a larger language model that generates code based on an input prompt. It captures the generated code and validates both the execution results and the messages received from the agent.\n\n### Test No Code Response with Model Client\n\n```python\n@pytest.mark.asyncio\nasync def test_no_code_response_with_model_client() -> None:\n    ...\n```\n\nIn this test, the agent is asked a question that does not require code execution. The response is validated to ensure that non-code content is processed correctly by the agent.\n\n### Test Self Debugging Loop\n\n```python\n@pytest.mark.asyncio\nasync def test_self_debugging_loop() -> None:\n    ...\n```\n\nThis test assesses the agent's ability to detect and correct a coding error. It provides an intentionally incorrect Python script, verifies that it prompts a retry, and expects a corrected version of the code to be generated and executed successfully.\n\n## Error Handling Tests\n\n### Test Code Execution Error\n\n```python\n@pytest.mark.asyncio\nasync def test_code_execution_error() -> None:\n    ...\n```\n\nThis test validates the agent's response when executing code that causes an error, such as attempting to calculate the square root of a negative number. It checks the error message returned to ensure appropriate error handling.\n\n### Test Code Execution No Output\n\n```python\n@pytest.mark.asyncio\nasync def test_code_execution_no_output() -> None:\n    ...\n```\n\nThis test examines how the agent responds when a code snippet runs successfully but produces no output. It ensures that the agent provides a sufficient response indicating that no output was generated.\n\n### Test Code Execution No Block\n\n```python\n@pytest.mark.asyncio\nasync def test_code_execution_no_block() -> None:\n    ...\n```\n\nThis test verifies the agent's behavior when no code blocks are provided in the input. It confirms that the agent responds with a message indicating the lack of executable code.\n\n### Test Code Execution Multiple Blocks\n\n```python\n@pytest.mark.asyncio\nasync def test_code_execution_multiple_blocks() -> None:\n    ...\n```\n\nIn this test, the agent processes multiple code snippets, including both valid and invalid code blocks. It checks that the output reflects each execution and captures any errors for invalid code.\n\n## Approval Functionality Tests\n\n### Custom Approval Functions\n\nThe script defines various approval functions that the agent can use to determine whether to allow code execution based on specific criteria. These functions are designed to be flexible, offering both synchronous and asynchronous approval processes. \n\nFor example:\n```python\ndef approval_function_deny_dangerous(request: ApprovalRequest) -> ApprovalResponse:\n    ...\n```\nThis function denies execution for code containing unsafe keywords.\n\n### Testing the Approval Functionality\n\nMultiple tests assess how the agent behaves when approval functions are applied.\n\n- **Test Approval Functionality No Approval**\n  \n```python\n@pytest.mark.asyncio\nasync def test_approval_functionality_no_approval() -> None:\n    ...\n```\n\n- **Test Approval Functionality Synchronous**\n  \n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(...)\nasync def test_approval_functionality_sync(...) -> None:\n    ...\n```\n\nThese tests determine whether the agent functions correctly under various approval conditions while ensuring appropriate feedback is given based on the approval outcome.\n\n### Conclusion\n\nThis test suite rigorously examines the `CodeExecutorAgent`'s capabilities in generating, executing, and approving code, covering a wide range of scenarios. The use of asynchronous programming and modular test design ensures comprehensive coverage of the agent's functionality.",
    "filename": "python/packages/autogen-agentchat/tests/test_code_executor_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level overview of the provided Python source code, which primarily focuses on testing the functionality of termination conditions and chat completion contexts in the context of an asynchronous testing framework using `pytest`.\n\n## Overview\n\nThe source code contains two main test functions that utilize the `pytest` framework to verify the correct serialization, deserialization, and composition of various termination conditions and chat completion contexts. These conditions and contexts are part of a broader system designed for managing agent-client interactions, specifically in chat applications.\n\n### Import Statements\n\nThe code imports several modules and classes necessary for its operations:\n\n- **pytest**: The testing framework used to create asynchronous test cases.\n- **Termination Conditions**: Classes such as `MaxMessageTermination`, `StopMessageTermination`, etc., that encapsulate various rules for terminating chat interactions.\n- **Model Contexts**: Classes like `BufferedChatCompletionContext`, `HeadAndTailChatCompletionContext`, which facilitate different strategies for managing chat state in conversations.\n- **OpenAI Client**: The `OpenAIChatCompletionClient` class that likely interacts with OpenAI's API for chat completion tasks.\n\n## Test Function: `test_termination_declarative`\n\n### Purpose\n\nThe `test_termination_declarative` function tests various termination conditions by asserting their serialization and deserialization properties.\n\n### Steps\n\n1. **Creation of Termination Conditions**: \n   - Several termination conditions are instantiated, including `MaxMessageTermination`, `TextMentionTermination`, `TokenUsageTermination`, among others.\n\n2. **Serialization Testing**: \n   - Each termination condition is serialized using the `.dump_component()` method. The test asserts that:\n     - The resulting object is an instance of `ComponentModel`.\n     - The configuration matches expected values (e.g., `max_messages`, `text`, `token limits`).\n\n3. **Deserialization Testing**: \n   - The test checks that serialized conditions can be restored to their original class types using `ComponentLoader.load_component()`. Assertions ensure the loaded objects are of the expected types.\n\n4. **Composite Termination Conditions**: \n   - A composite termination condition is created using logical operators (AND, OR) to combine different conditions, and it is tested for correct configuration and loading.\n\n### Conclusion\n\nThis test ensures that termination conditions function as expected, laying down clear rules for how an agent should terminate conversations based on various specified parameters.\n\n## Test Function: `test_chat_completion_context_declarative`\n\n### Purpose\n\nThe `test_chat_completion_context_declarative` function tests the various chat completion contexts to confirm their serialization and deserialization capabilities.\n\n### Steps\n\n1. **Creation of Chat Completion Contexts**: \n   - Different contexts, such as `UnboundedChatCompletionContext`, `BufferedChatCompletionContext`, etc., are instantiated. \n\n2. **Serialization Testing**: \n   - Each context is serialized, and assertions are made to confirm that:\n     - The serialization returns an object with the correct provider type.\n     - The configuration contains expected parameters (e.g., `buffer_size`, `head_size`, `tail_size`, `token_limit`).\n\n3. **Deserialization Testing**: \n   - Each serialized context is deserialized using `ComponentLoader.load_component()`, with assertions ensuring that the loaded objects match the original context classes.\n\n### Conclusion\n\nThis test ensures that chat completion contexts maintain data integrity during serialization processes, allowing the system to correctly store and retrieve configuration states for different chat setups.\n\n## Overall Structure\n\n- **Termination Conditions**: Focus on managing when interactions should end based on specific criteria. Each condition can be serialized for storage and later reconstructed, ensuring flexibility and adaptability in managing agent behavior.\n  \n- **Chat Contexts**: Focus on maintaining the current state of conversations. Different strategies are employed to manage conversation history, which is crucial for understanding and responding appropriately in chat applications.\n\n## Testing Philosophy\n\nBoth test functions exemplify a declarative testing approach, focusing on asserting outcomes based on defined conditions and configurations, thereby ensuring robustness and reliability. The use of asynchronous testing indicates an emphasis on performance, which is crucial for chat-based applications where response time is paramount.\n\n## Conclusion\n\nThe provided code is a well-structured test suite that thoroughly checks the implementations of termination conditions and chat completion contexts. By verifying both serialization and deserialization, alongside the handling of composite conditions, it assures the correctness of the underlying models in managing conversation logic effectively.",
    "filename": "python/packages/autogen-agentchat/tests/test_declarative_components.py"
  },
  {
    "code": false,
    "content": "# Group Chat Message Preservation Tests\n\nThis document describes a set of unit tests designed to verify that various group chat message types can correctly preserve specific fields from their subclass messages during serialization. The tests focus on ensuring that data created with subclass fields is accurately represented when serialized to JSON format.\n\n## Overview\n\nThe tests utilize classes from a messaging framework defined in the `autogen_agentchat` module, notably focusing on `TextMessage`, `GroupChatMessage`, `GroupChatStart`, `GroupChatAgentResponse`, and `GroupChatTeamResponse`. Each of these classes has specific properties that are being validated to ensure they remain intact through the serialization process.\n\n## Dependencies\n\nThe tests require:\n- `json`: A built-in module for parsing JSON data.\n- Classes from the `autogen_agentchat` library, including `Response`, `TaskResult`, `TextMessage`, and various group chat event classes.\n\n## Test Cases\n\n### 1. GroupChatMessage Serialization\n\n#### Purpose\nThe `test_group_chat_message_preserves_subclass_data` function ensures that when a `TextMessage` is wrapped inside a `GroupChatMessage`, its subclass fields are preserved during JSON serialization.\n\n#### Implementation Steps\n- A `TextMessage` is created with specific content and source attributes.\n- This message is then wrapped in a `GroupChatMessage`.\n- The resulting message is serialized to JSON.\n- Assertions are made to check if the `content` and `type` attributes are preserved in the parsed JSON.\n\n### 2. GroupChatStart Message List Serialization\n\n#### Purpose\nThe `test_group_chat_start_preserves_message_list_data` function checks that multiple `TextMessage` instances included in a `GroupChatStart` object maintain their data correctly during serialization.\n\n#### Implementation Steps\n- Two `TextMessage` instances are created with different content and sources.\n- These messages are passed into a `GroupChatStart` instance.\n- The object is serialized to JSON, followed by parsing.\n- Assertions validate that both messages' `content` fields are preserved.\n\n### 3. GroupChatAgentResponse Serialization\n\n#### Purpose\nIn the `test_group_chat_agent_response_preserves_dataclass_fields` function, the test verifies that a `GroupChatAgentResponse` object retains its inner `Response` dataclass fields, particularly when they include subclassed `TextMessage` instances.\n\n#### Implementation Steps\n- A main `TextMessage` and an inner `TextMessage` are created.\n- These messages are encapsulated in a `Response` object.\n- This response is then used to create a `GroupChatAgentResponse`.\n- Serialization is performed, and assertions check that both the `chat_message` content and inner messages are preserved accurately.\n\n### 4. GroupChatTeamResponse Nested Data Serialization\n\n#### Purpose\nThe `test_group_chat_team_response_preserves_nested_data` function aims to validate that nested data within a `GroupChatTeamResponse` is preserved through serialization.\n\n#### Implementation Steps\n- A single `TextMessage` is created and added to a `TaskResult`.\n- This `TaskResult` is then used within a `GroupChatTeamResponse`.\n- The response is serialized to JSON, and assertions confirm that the nested message's `content` is accurately preserved.\n\n## Summary\n\nThrough these four tests, the code successfully verifies the integrity of subclass data within various group chat message types during the serialization process. Each test focuses on different aspects of the messaging structure, ensuring that critical information is consistently maintained when converting to and from JSON. The use of assertions guarantees that any changes in structure or data loss can be quickly identified, contributing to robust software behavior.",
    "filename": "python/packages/autogen-agentchat/tests/test_events.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Chat Agent Framework Code\n\nThis documentation provides a high-level overview of a Python codebase that implements a chat framework using asynchronous agents. The framework supports multiple agent designs, group chats, message handling, and termination conditions.\n\n## Overview\n\nThe code employs Python's `asyncio` library to enable asynchronous programming and uses various classes and functions to define agents, messages, termination conditions, and group chat mechanisms. It is structured into multiple components, each responsible for a different aspect of the chat framework, such as message generation, agent interactions, and task management.\n\n### Logging Setup\n\nThe code initializes a logger to track the operation of the framework, storing logs in a file called \"test_group_chat.log\".\n\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(FileLogHandler(\"test_group_chat.log\"))\n```\n\n## Agent Definitions\n\n### Echo Agent\n\nThe `_EchoAgent` class extends `BaseChatAgent`, functioning as a basic agent that echoes back the received messages. \n\n- **State Management**: It keeps track of the last message received and the total number of messages processed.\n- **Message Handling**: If a message is received, it returns the content back. If no message is received, it returns the last known message.\n\n### Flaky Agent\n\nThe `_FlakyAgent` class also extends `BaseChatAgent` but is designed to randomly raise exceptions for testing purposes.\n\n- **Fault Simulation**: It raises a `ValueError` when processing messages, demonstrating how the system reacts to flaky agents.\n\n### Structured Message Agent\n\nThe `_StructuredAgent` class sends structured messages defined by a specification. It returns a `StructuredMessage` when it receives a task.\n\n## Message Types\n\n### Base Chat Messages\n\nSeveral message types are defined, including `TextMessage`, `StopMessage`, and more structured message types that enrich the chat interface. Messages are designed to be easily processed by agents and can carry various payloads depending on the message type.\n\n### Custom Message Handling\n\nSome messages can prompt special handling, such as `_UnknownMessageType`, which serves as a placeholder when the message type is not recognized.\n\n## Group Chat Mechanics\n\n### Group Chat Types\n\nSeveral group chat configurations are implemented:\n- **Round Robin Group Chat**: Agents take turns processing messages. This setup is managed by the `RoundRobinGroupChat` class.\n- **Selector Group Chat**: A specific agent is selected based on the last agent that spoke. This is managed by the `SelectorGroupChat` class.\n- **Swarm**: A dynamically configured team chat where agents can hand off tasks based on conditions.\n\n### Termination Conditions\n\nThe framework includes different termination conditions that define when a chat session should end, including:\n- **Handoff Termination**: Ends the session when a specific agent is mentioned.\n- **Max Message Termination**: Ends after a specified number of messages.\n\n## Testing Framework\n\nThe code uses the `pytest` framework for unit testing, with several defined tests to ensure functionality across various scenarios:\n- Testing round-robin group chats with message input.\n- Ensuring proper state management and recovery.\n- Handling exceptions raised by agents or termination conditions.\n- The ability to validate streamed messages and their order.\n\n### Sample Tests\n\nTests are typically asynchronous, employing the `@pytest.mark.asyncio` decorator to handle async execution. Each test assesses various behaviors of agents and group configurations under distinct conditions.\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_group_chat(runtime: AgentRuntime | None) -> None:\n    ...\n```\n\n## Conclusion\n\nThis codebase substantiates a dynamic and extensible chat agent framework. Through its design, it illustrates the use of agent-based programming patterns, asynchronous operations, and interactive messaging protocols, ideal for developing conversational agents in various applications. Each component is modular, allowing for easy expansion and modification to create tailored chat experiences.",
    "filename": "python/packages/autogen-agentchat/tests/test_group_chat.py"
  },
  {
    "code": false,
    "content": "# Documentation: Group Chat Selector Testing\n\n## Overview\nThis script serves as a testing suite for a selector group chat functionality, leveraging asynchronous programming and multiple agents that interact in a simulated environment. The main focus is on creating intelligent agent interactions to fulfill specific tasks, and it uses the Pytest framework for unit testing. Agents are specialized roles tailored for different tasks, and JavaScript-like native functionality (async/await) ensures non-blocking behavior.\n\n---\n\n## Dependencies\nThe script imports several essential modules:\n- **os**: For accessing environment variables (e.g., API keys).\n- **List, Sequence**: Type annotations from `typing` for better code clarity.\n- **pytest**: A testing framework for Python, supporting asynchronous testing.\n- **Agent Classes**: Various classes such as `AssistantAgent`, `TaskResult`, and chat message classes from `autogen_agentchat` to facilitate chat functionality.\n- **ChatCompletionClient**: A model that interfaces with chat completion APIs.\n\n---\n\n## Key Functions and Testing Cases\n\n### Asynchronous Chat Testing\nTwo main asynchronous functions are defined for testing group chat functionalities in different contexts:\n\n1. **`_test_selector_group_chat`**:\n   - It sets up a simple team of two agents, an \"assistant\" and a \"critic.\"\n   - The agents are instantiated with descriptions and a system message that guides their behavior.\n   - The task for the team is to draft a short email about organizing a New Year holiday party.\n   - The chat is executed with the `run_stream` method, and messages are directed to a console interface, likely for output visualization.\n\n   This tests basic interactions within a small team that generates a creative written task.\n\n2. **`_test_selector_group_chat_with_candidate_func`**:\n   - This function showcases a more complex interaction, integrating a \"candidate function\" that filters participants.\n   - It includes three agents: a developer, a tester, and a project manager, each representing different roles in the software development life cycle.\n   - The `dummy_candidate_func` is designed to only allow the developer and tester to communicate for a specific task regarding dark mode implementation in a React app.\n   - The function monitors the messages and ensures that only the filtered participants are involved in the chat conversation.\n\n---\n\n## Test Cases\nTwo test cases are defined under the `pytest.mark.asyncio` decorator, enabling asynchronous testing:\n\n1. **`test_selector_group_chat_gemini`**:\n   - It retrieves the Gemini API key from environment variables. If not set, the test is skipped.\n   - Instantiates an `OpenAIChatCompletionClient` with the Gemini model and runs both previously defined test functions: `_test_selector_group_chat` and `_test_selector_group_chat_with_candidate_func`.\n\n2. **`test_selector_group_chat_openai`**:\n   - Similar to the first test, it retrieves the OpenAI API key and skips the test if the key is unavailable.\n   - Uses the GPT-4.1 model and also executes the two preceding test functions.\n\n---\n\n## Conclusion\nThis script is structured to evaluate the selector group chat's capabilities through various defined agent roles, testing their ability to collaborate on tasks while ensuring that participant filtering mechanisms work correctly. It leverages asynchronous operations to enhance responsiveness and employs environment variable management for handling sensitive configurations (API keys). The use of Pytest ensures that each test is properly managed under asynchronous contexts, validating the expected behavior of the system.",
    "filename": "python/packages/autogen-agentchat/tests/test_group_chat_endpoint.py"
  },
  {
    "code": false,
    "content": "# Documentation of DiGraph and GroupChat Functionality\n\nThis document provides an overview of the functionality contained in the provided code, which focuses on creating, manipulating, and running directed graphs (DiGraphs) that represent complex workflows in a chat-based system involving agents. It heavily utilizes asynchronous programming to manage the flow of messages among agents and includes multiple test cases to ensure correctness.\n\n## Overview of Components\n\n### Directed Graph (DiGraph)\nThe `DiGraph` class is designed to manage a collection of nodes and edges, where each node represents an agent or a specific action, and edges represent the possible transitions between these nodes depending on specific conditions. The core functionalities of the `DiGraph` include:\n- **Creating Graphs**: The ability to instantiate a directed graph using nodes and edges.\n- **Parent and Leaf Relationships**: Methods to determine parent relationships of nodes and to retrieve start and leaf nodes.\n- **Cycle Detection**: Functions to validate the existence ofcycles, ensuring that paths can be traversed without causing infinite loops.\n\n### Agents\nAgents are fundamental components of the workflow. In this implementation, agents can process messages and respond accordingly. They include:\n- **BaseChatAgent**: The foundation class for all agents, requiring a name and description. It handles incoming messages and allows the agent to respond.\n- **_EchoAgent**: A simple implementation of a chat agent that echoes back the received messages and counts the total number of messages processed.\n\n### Message Handling\nVarious message types are defined, such as:\n- **TextMessage**: Represents a simple text message exchanged between the agents.\n- **StopMessage** and **ChatMessage**: Other message types that may be used depending on the flow requirements or termination conditions.\n\n### GraphFlow\nThe `GraphFlow` class is responsible for orchestrating the execution of the directed graph. Key features include:\n- **Managing Participants**: It takes agents as participants and establishes a flow based on the defined graph.\n- **Termination Conditions**: The ability to establish conditions, like max number of messages or specific message sources, under which the flow will terminate.\n\n## Functionality and Flow\n\n### Graph Creation\nThe tests provide mechanisms to create various types of graphs:\n1. **Creating Nodes and Edges**: Simple directed graphs can be constructed using methods to add nodes and edges, including conditional edges.\n2. **Testing Graph Properties**: Functions exist for checking permissions, such as the presence of starting and leaf nodes, as well as cycle detection.\n\n### Message Filtering\nThe `MessageFilterAgent` is a pivotal component that manages how messages are filtered based on sources or conditions before being sent to a wrapped agent. Key characteristics include:\n- **Configurations**: The ability to apply configurations that determine how many messages from specific sources should be passed through, tailored to suit various workflows.\n- **Behavior Testing**: Tests validate that message filtering applies correctly based on the configured rules.\n\n### Asynchronous Execution\nThe code heavily utilizes asyncio to facilitate the asynchronous execution of chat workflows. Noteworthy features include:\n- **Parallel Execution**: Agents can be executed in parallel when conditions allow, enabling a fan-out execution model.\n- **Sequential Execution**: The framework supports sequential flows where one agent\u2019s output directly triggers the next agent\u2019s input.\n\n## Test Cases\n\nThe implementation includes a comprehensive suite of test cases organized around several key activities:\n\n### Node and Edge Tests\n- **Creating a Simple Graph**: Tests are included to ensure the successful creation of basic graphs and validation of node edges.\n- **Cycle Detection Tests**: Various configurations are tested to confirm both valid and invalid cyclic conditions and the correct error handling.\n\n### Execution Tests\n- **Sequential Execution**: Tests validate that agents execute in the appropriate order based on the graph structure.\n- **Parallel Execution**: Scenarios are given for test cases to ensure parallel execution works and that the program handles multiple agents triggering the same nodes smoothly.\n\n### Conditional Logic Tests\n- **Conditional Edges**: Test cases confirm that edges can transition based on conditions specified in messages (e.g., words trigger different paths).\n- **Message Filtering**: Tests verify the filtering mechanism's efficiency to ensure only the relevant messages are processed.\n\n## Conclusion\n\nThe provided code represents a robust framework for managing agent-oriented workloads within a directed graph format. By implementing various testing strategies and asynchronous handling, this framework is designed to facilitate complex, interactive chat experiences efficiently. The tests contained within ensure that the implementation\u2019s functionality is consistent and error-free, paving the way for future developments and enhancements in chat-based agent workflows.",
    "filename": "python/packages/autogen-agentchat/tests/test_group_chat_graph.py"
  },
  {
    "code": false,
    "content": "# Documentation for Group Chat Nested Teams Tests\n\n## Overview\n\nThis code defines a suite of tests for agent-based group chat functionality using two main chat frameworks: `RoundRobinGroupChat` and `SelectorGroupChat`. The tests are executed using the `pytest` framework, with asynchronous handling enabled through `pytest_asyncio`. The primary goal is to validate the behavior of nested teams within the group chat agents, covering various operations such as running tasks, state management, and component serialization.\n\n## Logging and Test Setup\n\n### Logging Configuration\n\nA logging configuration is established to track the events occurring during the test execution:\n\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(FileLogHandler(\"test_group_chat_nested.log\"))\n```\n\n### Fixture for Runtime Environment\n\nA pytest fixture named `runtime` is defined, which provides different runtime environments (either `SingleThreadedAgentRuntime` or `None` for embedded tests):\n\n```python\n@pytest_asyncio.fixture(params=[\"single_threaded\", \"embedded\"])\nasync def runtime(request: pytest.FixtureRequest) -> AsyncGenerator[AgentRuntime | None, None]:\n```\n\nThis fixture ensures tests can be run in both single-threaded and embedded modes, yielding the appropriate runtime configuration for each test case.\n\n## Test Cases\n\n### 1. Round Robin Group Chat Execution\n\n#### `test_round_robin_group_chat_nested_teams_run`\n\nThis test aims to verify the execution of tasks within a `RoundRobinGroupChat` that comprises a nested inner team consisting of an assistant and code executor:\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_group_chat_nested_teams_run(runtime: AgentRuntime | None) -> None:\n```\n\nA model client is used to simulate responses, leading to the running of the task \"Write a program that prints 'Hello, world!'\". Assertions ensure that the result contains the expected messages and termination reason.\n\n#### `test_round_robin_group_chat_nested_teams_run_stream`\n\nSimilar functionality is tested using streaming. This test evaluates the ability to process messages asynchronously while running tasks in a nested round-robin environment:\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_group_chat_nested_teams_run_stream(runtime: AgentRuntime | None) -> None:\n```\n\n### 2. Saving and Loading Components\n\n#### `test_round_robin_group_chat_nested_teams_dump_load_component`\n\nIn this test, the focus is on serializing and deserializing chat components to verify their integrity:\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_group_chat_nested_teams_dump_load_component(runtime: AgentRuntime | None) -> None:\n```\n\nThe test checks both the configuration of the `outer_team` after serialization and the validity of the loaded configurations.\n\n### 3. State Management\n\n#### `test_round_robin_group_chat_nested_teams_save_load_state`\n\nThis test evaluates the capability to save and restore the state of a nested round-robin group chat:\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_group_chat_nested_teams_save_load_state(runtime: AgentRuntime | None) -> None:\n```\n\nIt involves running a task, saving the current state, and ensuring that a newly instantiated group can be restored to its previous state.\n\n### 4. Mixed Group Chat Tests\n\n#### `test_selector_group_chat_nested_teams_run`\n\nThis defines a test to assess the functionality of `SelectorGroupChat`, running similar nested structures with a focus on selecting agents and actions:\n\n```python\n@pytest.mark.asyncio\nasync def test_selector_group_chat_nested_teams_run(runtime: AgentRuntime | None) -> None:\n```\n\n### 5. Deeply Nested Teams\n\n#### `test_round_robin_deeply_nested_teams`\n\nHere, the focus shifts to conducting a test with up to three levels of nesting within a round-robin chat setup:\n\n```python\n@pytest.mark.asyncio\nasync def test_round_robin_deeply_nested_teams(runtime: AgentRuntime | None) -> None:\n```\n\nIt rigorously checks whether messages flow correctly across multiple nested levels.\n\n#### `test_selector_deeply_nested_teams`\n\nA similar structure to the deeply nested round-robin test but utilizing the selector model is evaluated:\n\n```python\n@pytest.mark.asyncio\nasync def test_selector_deeply_nested_teams(runtime: AgentRuntime | None) -> None:\n```\n\n## Error Handling\n\n### `test_swarm_doesnt_support_nested_teams`\n\nThis test explicitly validates that `Swarm` does not allow nested teams as participants, ensuring robust error handling by expecting a `TypeError`:\n\n```python\n@pytest.mark.asyncio\nasync def test_swarm_doesnt_support_nested_teams() -> None:\n```\n\n## Summary\n\nThe above tests cover a comprehensive set of scenarios focusing on the interactions between agents in both round-robin and selector chat environments. By validating task execution, state management, and proper serialization/deserialization of components, the test suite aims to ensure that nested agent interactions are reliable and meet the expected functionality. The logging mechanism provides insight into the tests' execution and any potential issues encountered, supporting ongoing development and quality assurance efforts.",
    "filename": "python/packages/autogen-agentchat/tests/test_group_chat_nested.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview \n\nThe provided code is a test suite designed for an asynchronous chat agent, utilizing Python's `asyncio` library and the `pytest` framework. The key components include the definition of a `TestAgent` class that simulates a chat agent, and a pytest asynchronous test function that verifies the behavior of the agent with respect to pausing and resuming its operations.\n\n## TestAgent Class\n\n### Description\n\nThe `TestAgent` class inherits from the `BaseChatAgent` and serves as a mock implementation of a chat agent that can pause, resume, and accumulate a counter simulating task completion. It includes several methods to handle incoming messages and reset its state.\n\n### Constructor \n```python\n    def __init__(self, name: str, description: str) -> None:\n```\nThe constructor initializes the agent with a name and description, and sets up internal state variables such as `_is_paused`, a list of tasks, and a `counter` that tracks the number of simulated operations performed by the agent.\n\n### Properties\n\n```python\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n```\nThis property defines the types of messages the agent can produce, in this case, it returns a list with `TextMessage`. This is important for determining compatibility during message exchanges.\n\n### Message Handling\n\n```python\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n```\nThis method handles received messages and creates a task for processing. The processing simulates a long-running asynchronous task that increments the `counter` and pauses if `_is_paused` is `True`. If the task is cancelled, it catches the `CancelledError`.\n\n### State Management\n\n#### Resetting the Agent\n```python\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n```\nResets the `counter` to zero when invoked.\n\n#### Pausing the Agent\n```python\n    async def on_pause(self, cancellation_token: CancellationToken) -> None:\n```\nSets `_is_paused` to `True`, effectively freezing the agent's counter increments.\n\n#### Resuming the Agent\n```python\n    async def on_resume(self, cancellation_token: CancellationToken) -> None:\n```\nSets `_is_paused` to `False`, allowing the agent to continue incrementing the counter.\n\n#### Closing the Agent\n```python\n    async def close(self) -> None:\n```\nCleans up any ongoing tasks by cancelling them and ensuring they finish. This is crucial for freeing resources and avoiding memory leaks.\n\n## Fixture for Runtime\n\n### Description\n```python\n@pytest_asyncio.fixture(params=[\"single_threaded\", \"embedded\"])\n```\nThis fixture sets up the testing environment for different agent run conditions: single-threaded or embedded. It uses `pytest` to provide these parameters automatically.\n\n### Runtime Setup\nThe fixture initializes the `SingleThreadedAgentRuntime` when the \"single_threaded\" parameter is requested and starts it. For \"embedded\", the fixture yields `None`.\n\n## Test Function\n\n### Test for Pause and Resume\n```python\n@pytest.mark.asyncio\nasync def test_group_chat_pause_resume(runtime: AgentRuntime | None) -> None:\n```\nThis test verifies the agent's functionality when it is paused and resumed in a group chat scenario.\n\n### Test Flow\n\n1. **Agent Initialization**: \n    - Creates an instance of `TestAgent`, providing it with a name and description.\n    - Initializes a `RoundRobinGroupChat` team with the agent, limited to one interaction turn.\n\n2. **Running the Team**:\n    - Starts the team in an asynchronous task, allowing it to process messages.\n\n3. **Counter Check before Pause**:\n    - Records the current value of the agent's `counter` after allowing some time for operations to complete. \n    - Asserts that the counter has increased, confirming that the agent is operational.\n\n4. **Pausing the Agent**:\n    - Calls the `pause` method on the team.\n    - Waits to ensure the agent processes the pause command. The counter should remain unchanged during this period.\n\n5. **Counter Check during Pause**:\n    - Records the current counter to verify that it has not increased while the agent is paused.\n    - Asserts that the counter value is the same, confirming the pause functionality.\n\n6. **Resuming the Agent**:\n    - Calls the `resume` method on the team.\n    - Waits to ensure the agent can resume its operations and increment the counter again.\n\n7. **Counter Check after Resume**:\n    - Records the current counter again to observe the effects of resuming.\n    - Performs a final assert to ensure that the counter increased, verifying that resume functionality is operating correctly.\n\n8. **Cleanup**:\n    - Invokes the `close` method on the agent, ensuring all tasks are cancelled to avoid resource leaks.\n    - Waits for the team task to complete, ensuring a clean shutdown of the testing environment.\n\n## Summary\n\nThis code establishes a framework for testing the state management of a chat agent. The `TestAgent` class simulates asynchronous processing, allowing for extensive control features such as pause and resume. The test function validates these features under specific conditions, ensuring that the agent behaves correctly under various states. The use of a pytest fixture allows flexibility in testing with different runtime environments, adding robustness to the overall test suite.",
    "filename": "python/packages/autogen-agentchat/tests/test_group_chat_pause_resume.py"
  },
  {
    "code": false,
    "content": "# High-Level Documentation of the Code\n\nThis Python script defines a testing framework for a chat agent system, which includes functionalities to manage a group chat of agents, execute tasks, and handle cancellations and stalling scenarios. The primary components include agent definitions, testing functions, and the setup for a testing runtime environment using `pytest` and `pytest_asyncio`.\n\n## Imports and Logger Configuration\n\nThe script begins with a series of imports from various modules, including:\n\n- **asyncio**: For managing asynchronous operations.\n- **json**: For handling JSON data.\n- **logging**: For logging events and debugging.\n- **pytest** and **pytest_asyncio**: For setting up and running tests asynchronously.\n- Various classes related to the chat agents and message handling.\n\nA logger is initialized with a specific name and a log file handler to log events to `test_magentic_one_group_chat.log`. The logger level is set to DEBUG to capture detailed logs for testing purposes.\n\n## Chat Agent Definition\n\nThe script defines an internal class `_EchoAgent` that inherits from `BaseChatAgent`. This class serves as a modified chat agent with the following functionalities:\n\n- **Initialization**: Maintains state variables such as the last message received and the total count of messages processed.\n- **Message Handling**: \n  - The `on_messages` method processes incoming messages, updates the last message if one is received and counts total messages. It returns a response based on the last message or repeats the last message if none is provided.\n  - The `on_reset` method resets the last message state.\n- The `produced_message_types` property indicates that this agent can produce messages of type `TextMessage`.\n\n## Runtime Fixture\n\nThe script includes a `runtime` fixture decorated with `pytest_asyncio.fixture`. This fixture sets up a testing environment that can operate in two modes: `single_threaded` and `embedded`. \n\n- In the single-threaded mode, it initializes `SingleThreadedAgentRuntime`, starts it, and stops it after the test.\n- In embedded mode, it yields `None`, implying that no specific runtime environment is needed.\n\nThis fixture allows the tests to run with varying configurations of the agent runtime.\n\n## Group Chat Cancellation Test\n\nThe first test defined is `test_magentic_one_group_chat_cancellation`, which verifies the cancellation mechanism in a chat environment:\n\n1. **Agent Initialization**: Four instances of `_EchoAgent` are created as participants in the chat.\n2. **Model Client Setup**: A `ReplayChatCompletionClient` instance is instantiated, which simulates responses for the chat agents.\n3. **Team Formation**: A `MagenticOneGroupChat` is formed with the agents and the model client.\n4. **Task Execution**: The agents are instructed to perform a task to write a program that prints \"Hello, world!\" while handling a cancellation token.\n5. **Cancellation and Validation**: The cancellation token is triggered and the test checks that an `asyncio.CancelledError` is raised.\n\n## Basic Group Chat Test\n\nThe second test, `test_magentic_one_group_chat_basic`, evaluates a basic interaction among the agents:\n\n1. **Agent Initialization**: Similar to the previous test, four `_EchoAgent` instances are created.\n2. **Model Client Responses**: The model client is set up with a series of expected responses.\n3. **Group Chat Execution**: The group chat is initiated with the same task as before.\n4. **Assertions**: The test asserts the total number of messages exchanged, specific content in the messages, and checks a stopping reason, ensuring that the task processing works as intended.\n5. **State Management**: The state of the group chat is saved, and a new team instance is created to verify that the state can be accurately loaded and validated against the original instance's state.\n\n## Handling Stalls in Group Chat\n\nThe third test, `test_magentic_one_group_chat_with_stalls`, addresses scenarios where the group chat may stall while processing user requests:\n\n1. **Agent Initialization**: The same four agents are initialized.\n2. **Stalling Model Client Responses**: The responses are designed to induce stalling conditions so that agents get stuck for a while before making progress.\n3. **Team Setup with Stalls**: Upon setting up the group chat, a maximum number of stalls is defined.\n4. **Execution and Assertions**: The task execution is carried out with asserts to check the number of messages, capturing the stalling responses, and validating the stop reason.\n\n## Conclusion\n\nOverall, this test suite forms a comprehensive framework for validating the behavior of chat agents in a group setting. It confirms the ability of agents to process messages, handles task cancellations, manages state, and can recover from stalling situations, ensuring robustness in a multi-agent chat environment. The design leverages asynchronous programming and efficient test management provided by `pytest` and its asyncio extension.",
    "filename": "python/packages/autogen-agentchat/tests/test_magentic_one_group_chat.py"
  },
  {
    "code": false,
    "content": "# Test Suite Overview\n\nThis document provides an overview of a test suite for message-related functionality within a chat application. The suite is implemented in Python and utilizes the `pytest` testing framework. The core functionality revolves around structured messages, their serialization, and various message types.\n\n## Test Content Model\n\n### `TestContent`\nThe `TestContent` class is a Pydantic model that encapsulates the structure of a message's content. The class contains two fields:\n- `field1`: A string representing some textual data.\n- `field2`: An integer that can represent numeric data.\n\nThis model serves as a foundation for creating structured messages in the tests.\n\n## Structured Message Tests\n\n### `test_structured_message`\nThis test verifies the creation and properties of a structured message using the `TestContent` model. It ensures the message type is correctly identified, the content type matches the expected `TestContent`, and the values of the individual fields are correctly set. The test also checks that the `model_dump` method works as intended, ensuring the proper serialization of the message.\n\n### `test_structured_message_component`\nThis test expands on the previous one by using the `StructuredMessageFactory` to dynamically create and validate structured messages. It checks the instantiation of a structured message using a format string and verifies the message properties similar to the previous test. The test also confirms correct serialization and deserialization behavior for the structured message content.\n\n## Message Factory Tests\n\n### `test_message_factory`\nThe `test_message_factory` function tests the `MessageFactory`, which creates different types of messages, including `TextMessage`, `HandoffMessage`, and `StructuredMessage`. The test:\n- Confirms the successful creation and proper property validation of each message type.\n- Demonstrates the error handling when attempting to create a structured message without having it registered. \n- Finally, it shows dynamic registration of message types and validation of the factory's message creations.\n\n## Container and Union Types Tests\n\n### `TestContainer`\nThis class models a container that holds lists of chat messages and agent events. The `test_union_types` function tests the correctness of creating multiple message objects, including various message types, and validates their serialization and deserialization through the container.\n\n## Message ID Field Tests\n\n### `test_message_id_field`\nThis test ensures that each created message and event automatically generates a unique identifier (ID). The function confirms that:\n- The IDs exist and are unique across different instances.\n- They conform to the UUID format, ensuring proper uniqueness.\n\n### `test_custom_message_id`\nThis test allows for custom IDs to be provided when creating messages and events, ensuring that the specified IDs are retained correctly.\n\n### `test_streaming_chunk_full_message_id`\nThe function validates the behavior of the `full_message_id` in streaming chunk events. It checks that:\n- The `full_message_id` can be set and queried independently from the standard message ID.\n- The generated IDs conform to the UUID format.\n\n## Message Serialization Tests\n\n### `test_message_serialization_with_id`\nThis test validates that messages can be serialized and deserialized correctly, maintaining their unique ID attributes throughout the process. It includes tests for both standard messages and streaming chunk events, ensuring that the integrity of data is preserved.\n\n### `test_datetime_serialization_in_messages`\nThis test addresses the serialization of `datetime` objects in messages, ensuring that they are converted to a JSON-compatible format (ISO 8601). It:\n- Verifies that `datetime` fields are serialized correctly in messages and events.\n- Ensures accurate round-trip serialization, confirming that data remains intact during the dump and load operations.\n\n## Summary\n\nOverall, this test suite thoroughly validates the messaging system's ability to handle structured message creation, serialization, unique identifier management, and compatibility with various message types. The tests confirm the expected behaviors and robustness of the message handling components within the application, ensuring that they conform to specified requirements and types.",
    "filename": "python/packages/autogen-agentchat/tests/test_messages.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Agent Message Processing System\n\nThis document describes the source code that implements a sequential routed agent using the `autogen_agentchat` framework, along with an associated test case utilizing `pytest`. The code establishes an agent that processes messages in sequence and verifies message handling through asynchronous testing.\n\n## Overview\n\nThe code primarily consists of a test setup for a `_TestAgent`, which extends the functionality of the `SequentialRoutedAgent` class. The agent is designed to handle messages of a specific type (`Message`) and store them for later verification. The test case validates that the agent processes a series of messages in the correct order. The system employs asynchronous programming principles using the `asyncio` library.\n\n## Imports and Dependencies\n\nThe code imports several modules and libraries crucial for its execution:\n\n- `asyncio`: Provides support for asynchronous programming.\n- `random`: Used to simulate random delays in message processing.\n- `dataclass`: Facilitates the creation of simple classes for storing data.\n- `List`: Type hint to specify a list of messages.\n- `pytest`: A testing framework for Python.\n- Other custom imports from `autogen_agentchat` and `autogen_core` that provide the foundational classes and functions for building agents and handling messages.\n\n## Message Data Class\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\nThe `Message` data class serves as a simple structure to encapsulate message content, represented as a string. This class is used throughout the code as the primary message type processed by the `_TestAgent`.\n\n## The _TestAgent Class\n\n```python\n@default_subscription\nclass _TestAgent(SequentialRoutedAgent):\n```\n\n### Class Definition\n\nThe `_TestAgent` class is defined as a specialized type of `SequentialRoutedAgent`. It is decorated with `@default_subscription`, indicating it has a default subscription for message handling.\n\n### Initialization Method\n\n```python\ndef __init__(self, description: str) -> None:\n```\n\nThe constructor initializes the agent with a description, setting up a list to collect messages processed by the agent. \n\n```python\nsuper().__init__(description=description, sequential_message_types=[Message])\nself.messages: List[Message] = []\n```\n\nIt calls the parent class constructor, specifying that it will handle messages of type `Message`.\n\n### Message Handler Method\n\n```python\n@message_handler\nasync def handle_content_publish(self, message: Message, ctx: MessageContext) -> None:\n```\n\nThis method defines how the agent processes incoming messages. Using the `@message_handler` decorator, it asynchronously waits for a random interval before appending the received message to `self.messages`.\n\n```python\nawait asyncio.sleep(random.random() / 100)\nself.messages.append(message)\n```\n\nThis simulates a processing time, which allows testing the agent's ability to handle messages in a realistic manner.\n\n## Test Case: `test_sequential_routed_agent`\n\n```python\n@pytest.mark.asyncio\nasync def test_sequential_routed_agent() -> None:\n```\n\nThis function defines an asynchronous test case for the `_TestAgent`. It employs the `pytest` framework's asyncio support for executing asynchronous tests.\n\n### Runtime Initialization\n\n```python\nruntime = SingleThreadedAgentRuntime()\nruntime.start()\n```\n\nA `SingleThreadedAgentRuntime` is initiated to manage the agent's lifecycle and message processing.\n\n### Agent Registration\n\n```python\nawait _TestAgent.register(runtime, type=\"test_agent\", factory=lambda: _TestAgent(description=\"Test Agent\"))\n```\n\nThe test agent is registered to the runtime environment, which allows it to listen for messages published during the test.\n\n### Message Publishing Loop\n\n```python\nfor i in range(100):\n    await runtime.publish_message(Message(content=f\"{i}\"), topic_id=DefaultTopicId())\n```\n\nA loop publishes 100 messages with incrementing content (\"0\" to \"99\") to a default topic. This mimics a scenario where multiple messages are sent simultaneously.\n\n### Stopping the Runtime\n\n```python\nawait runtime.stop_when_idle()\n```\n\nThe runtime is instructed to stop when there are no longer messages being processed, ensuring that all messages have been handled before concluding the test.\n\n### Message Verification\n\n```python\ntest_agent = await runtime.try_get_underlying_agent_instance(test_agent_id, _TestAgent)\n```\n\nThe test retrieves an instance of the `_TestAgent` to validate its message processing.\n\n```python\nfor i in range(100):\n    assert test_agent.messages[i].content == f\"{i}\"\n```\n\nFinally, assertions are made to check that each stored message's content matches the expected sequence. This confirms that the agent processed messages in the correct order.\n\n## Conclusion\n\nThe code successfully implements and tests a sequential agent that reliably processes and stores messages while employing asynchronous programming techniques. This setup showcases how agents can be structured and tested effectively using the specified agent framework.",
    "filename": "python/packages/autogen-agentchat/tests/test_sequential_routed_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Society of Mind Agent Tests\n\nThis document provides a high-level overview of the provided code, which includes a series of asynchronous tests for the `SocietyOfMindAgent` and related classes. The tests focus on the functionality and behavior of an advanced conversational agent framework using various agents, termination conditions, and message handling.\n\n## Overview\n\nThe code primarily utilizes the `pytest` and `pytest_asyncio` libraries to create asynchronous testing scenarios for the `SocietyOfMindAgent`. This agent orchestrates interactions between multiple `AssistantAgent` instances within a conversational structure, ensuring task handling, message management, and termination conditions are met effectively.\n\n### Imports and Dependencies\n\nThe following imports are utilized:\n\n- **Testing Frameworks**: `pytest` and `pytest_asyncio` for structuring and running tests.\n- **Agent Framework**: Various classes (e.g., `AssistantAgent`, `SocietyOfMindAgent`, `RoundRobinGroupChat`) from the `autogen_agentchat` package that facilitate the agent's behavior.\n- **Core Functionality**: Entities from `autogen_core` such as `AgentRuntime` and `CreateResult` for managing operational contexts and results.\n- **Mocking and Streaming**: The `ReplayChatCompletionClient` simulates responses from a language model for controlled testing.\n\n## `runtime` Fixture\n\n### Purpose\n\nThe `runtime` fixture is defined to initialize and manage the agent runtime environment for the tests. It abstracts two modes of operation:\n\n- **Single-threaded Mode**: An instance of `SingleThreadedAgentRuntime` is created and started. This mode is suitable for simpler tasks.\n- **Embedded Mode**: It allows tests without an active runtime, catering to various testing scenarios.\n\n### Usage\n\nThis fixture is invoked in each test function where an `AgentRuntime` or `None` is required, ensuring code reusability and cleaner test setups. The respective runtime is yielded to the test functions and subsequently stopped after the tests conclude.\n\n## Testing Society of Mind Agent Behavior\n\n### `test_society_of_mind_agent`\n\n#### Description\n\nThis test assesses the overall functionality of the `SocietyOfMindAgent`. It verifies that the agent can perform a task (count to 10) and manages interactions between two instruction-based assistant agents.\n\n#### Assertions\n\n1. Confirms the length of the response messages.\n2. Checks the source of the messages to ensure they come from the expected parties.\n3. Tests the state persistence and whether it can be saved and loaded correctly.\n4. Verifies serialization and deserialization capabilities of the `SocietyOfMindAgent`.\n\n### `test_society_of_mind_agent_output_task_messages_parameter`\n\n#### Description\n\nThis test focuses on verifying the `output_task_messages` parameter impacting how task messages are handled in the conversation stream.\n\n#### Assertions\n\n1. Confirms output behavior when `output_task_messages` is set to `True` versus `False`.\n2. Validates that the `SocietyOfMindAgent` appropriately handles task messages in nested teams by only including task messages from the outer level.\n\n### `test_society_of_mind_agent_empty_messages`\n\n#### Description\n\nThis test evaluates the behavior of the `SocietyOfMindAgent` when no functional responses are generated. The agent should still produce an appropriate message indicating the lack of responses.\n\n#### Assertions\n\n1. Asserts that the agent produces exactly one message despite no content from inner agents.\n\n### `test_society_of_mind_agent_no_response`\n\n#### Description\n\nThis test verifies that the agent's output when inner agents are restricted to produce no responses works correctly, resulting in a fallback message being generated.\n\n#### Assertions\n\n1. Ensures the response flow includes expected messages from the agent and user, particularly confirming the fallback text.\n\n### `test_society_of_mind_agent_multiple_rounds`\n\n#### Description\n\nThis test checks how the `SocietyOfMindAgent` handles consecutive runs to ensure it properly carries over discussions and interactions between calls.\n\n#### Assertions\n\n1. Verifies message counts and the expected sources after multiple sequential calls to the agent.\n\n## Advanced Behavior Testing\n\n### `test_society_of_mind_agent_no_multiple_system_messages` and `test_society_of_mind_agent_yes_multiple_system_messages`\n\n#### Description\n\nThese tests examine edge cases regarding how the `SocietyOfMindAgent` manages system messages. They define whether system messages are sent to inner agents based on specific criteria.\n\n#### Assertions\n\n1. Checks that no system messages are included or enforced for retrieval based on the setup.\n2. Confirms that when permitted, system messages are indeed included in communications.\n\n### `test_default_output_task_messages_behavior`\n\n#### Description\n\nThis final test focuses on ensuring backward compatibility in message handling, confirming that task messages are included in the conversation stream by default.\n\n#### Assertions\n\n1. Proves that task messages appear in both the message stream and final task results.\n2. Validates content structure with user-initiated tasks followed by responses from the agents.\n\n## Conclusion\n\nThe provided code meticulously tests the functionality, messaging behavior, and state management of a `SocietyOfMindAgent` using various scenarios. By leveraging fixtures and async patterns in `pytest`, the tests ensure robust performance while enabling detailed validation of complex interactions among agents and their responses in a conversational framework.",
    "filename": "python/packages/autogen-agentchat/tests/test_society_of_mind_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation for Streaming Message Test\n\nThis code is a test suite that validates the behavior of streaming message events in the context of a chat-based assistant utilizing an AI model. The objective is to ensure that the message IDs correlate correctly between streaming events and final messages received from the model. The code utilizes the `pytest` framework for asynchronous testing.\n\n## Imports and Dependencies\n\n```python\nfrom typing import List, Optional\n\nimport pytest\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.messages import ModelClientStreamingChunkEvent, TextMessage\nfrom autogen_core import FunctionCall\nfrom autogen_core.models import CreateResult, ModelFamily, RequestUsage\nfrom autogen_ext.models.replay import ReplayChatCompletionClient\n```\n\nThe script imports several essential modules:\n- **Typing**: Utilized for type annotations (`List`, `Optional`).\n- **pytest**: A framework for writing simple and scalable test cases.\n- **Custom Libraries**: These include components from `autogen_agentchat`, `autogen_core`, and `autogen_ext` which appear to provide functionality for chat agents, task results, message handling, and replay clients.\n\n## Function Definitions\n\n### 1. `_echo_function`\n\n```python\nasync def _echo_function(input: str) -> str:\n    return input\n```\n\nThis is an asynchronous function that simply returns the input string it receives. Its primary role in this test context is to act as a mock function that the chat agent can call during its operation.\n\n## Test Case: Streaming Message ID Correlation\n\n### 2. `test_streaming_message_id_correlation`\n\n```python\n@pytest.mark.asyncio\nasync def test_streaming_message_id_correlation() -> None:\n    ...\n```\n\nThis is the main test function decorated with `@pytest.mark.asyncio`, allowing it to run asynchronous code. The focus of this test is to ensure that the `full_message_id` of each streaming chunk matches the ID of the final message result from the chat agent.\n\n### Test Setup\n\n```python\nmock_client = ReplayChatCompletionClient(\n    [\n        \"Response to message\",\n    ]\n)\nagent = AssistantAgent(\n    \"test_agent\",\n    model_client=mock_client,\n    model_client_stream=True,\n)\n```\n\n1. **ReplayChatCompletionClient**: A mock client simulating responses from an AI model. In this case, it is set up to respond with a simple message.\n2. **AssistantAgent**: An instance of the agent that will interact with the mock client, configured for streaming.\n\n### Collecting Messages\n\nThe test collects both streaming chunks and the final message using asynchronous iteration.\n\n```python\nasync for message in agent.run_stream(task=\"task\"):\n    if isinstance(message, TaskResult):\n        assert len(message.messages) == 2\n        assert isinstance(message.messages[0], TextMessage)\n        assert isinstance(message.messages[1], TextMessage)\n        final_message = message.messages[1]\n    elif isinstance(message, ModelClientStreamingChunkEvent):\n        chunks.append(message)\n```\n\n- **TaskResult Events**: When a `TaskResult` is received, the code checks if there are exactly two messages, ensuring that the final message is of type `TextMessage`.\n- **Streaming Chunk Events**: Each chunk received is collected into a list for later verification.\n\n### Assertions\n\nAfter collecting messages, several assertions ensure the integrity of the stream:\n\n```python\nassert len(chunks) > 0\nassert final_message is not None\n```\n\nThese assertions check that chunks and the final message were received. Additional assertions verify that each chunk's `full_message_id` matches the final message's ID.\n\n## Reflection on Tool Use\n\n### Mock Client with Function Calling\n\nThe test also simulates a scenario where the agent calls a function and reflects on its use.\n\n```python\nmock_client = ReplayChatCompletionClient(\n    [\n        CreateResult(\n            ...\n        ),\n        \"Example reflection response\",\n    ],\n    model_info={\n        ...\n    },\n)\n```\n\nHere, a new mock client is created specifically for testing the reflection capabilities of the `AssistantAgent` class. It is set to simulate a function call, with metadata about the model being used.\n\n### Agent Initialization\n\nSimilar to the previous agent setup, this instance is configured to reflect on tool usage:\n\n```python\nagent = AssistantAgent(\n    \"test_agent\",\n    model_client=mock_client,\n    model_client_stream=True,\n    reflect_on_tool_use=True,\n    tools=[_echo_function],\n)\n```\n\nThe agent is prepared to handle reflective calls from tools, with `_echo_function` being designated as a callable function.\n\n### Collecting Reflection Messages\n\nSimilar to previous message collection, this part of the test collects both reflection chunks and a final reflection message:\n\n```python\nasync for message in agent.run_stream(task=\"task\"):\n    if isinstance(message, TaskResult):\n        ...\n    elif isinstance(message, ModelClientStreamingChunkEvent):\n        reflection_chunks.append(message)\n```\n\nAssertions after this block mirror those of the primary message collection, verifying chunk integrity and ID correlation with the final message.\n\n## Conclusion\n\nThis test script serves to validate crucial aspects of an asynchronous chat agent's streaming capabilities. It ensures that all streaming chunks are properly identified and linked to a final message, maintaining integrity through ID correlation. Additionally, it tests the functionality of reflecting on tool usage, further confirming the agent's operational robustness in handling complex interactions.",
    "filename": "python/packages/autogen-agentchat/tests/test_streaming_message_id_correlation.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Testing Suite for `AgentTool` and `TeamTool`\n\nThis document provides a high-level overview of a series of tests implemented using the `pytest` framework to evaluate the functionality and performance of `AgentTool` and `TeamTool`. These classes are part of an automated agent framework designed for handling tasks, interactions, and state management within agent-based systems. Below, we outline the purpose of each test, the components involved, and the expected outcomes.\n\n## Imports and Setup\n\nThe code begins with a series of imports necessary for the functionality of the tests. This includes multiple classes for managing agents, messages, tools, and conditions from the `autogen_agentchat` and `autogen_core` libraries. \n\nKey imports include:\n- **AssistantAgent**: Represents an agent that can perform tasks.\n- **AgentTool**: A tool that allows agents to execute tasks and manage their state.\n- **TeamTool**: A tool that orchestrates interaction among multiple agents as a team.\n- **MaxMessageTermination**: A condition to terminate group chats after a set number of messages.\n- Mathematical functions and testing utilities such as `pytest`.\n\n## Test for Running a Basic Task with `AgentTool`\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_tool_run() -> None:\n    \"\"\"Test running a task with AgentTool.\"\"\"\n    mock_chat_agent = _EchoAgent(\"Mock_Agent\", \"A mock agent for testing\")\n    tool = AgentTool(agent=mock_chat_agent)\n    task_result = await tool.run_json({\"task\": \"Test task\"}, cancellation_token=CancellationToken())\n    assert task_result.messages[1].content == \"Test task\"\n```\n\nThis test checks the basic functionality of `AgentTool`. It initiates a mock agent and submits a task to be executed. \n- An `EchoAgent` is instantiated to simulate an agent's behavior.\n- The `AgentTool` runs a task and checks if the result aligns with the expected output.\n\n## Test for Saving State in `AgentTool`\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_tool_state() -> None:\n    \"\"\"Test saving state of AgentTool.\"\"\"\n    mock_chat_agent = _EchoAgent(\"Mock_Agent\", \"A mock agent for testing\")\n    tool = AgentTool(agent=mock_chat_agent)\n    ...\n```\n\nIn this test, the focus is on verifying that the state of the `AgentTool` can be accurately saved and restored. \n- States are established before and after task execution to verify if the updates occur as expected.\n- The ability to load a saved state into a new instance of `AgentTool` is also validated.\n\n## Test for Serialization of `AgentTool` Configuration\n\n```python\ndef test_agent_tool_component() -> None:\n    \"\"\"Test serialization of AgentTool to config.\"\"\"\n    model_client = ReplayChatCompletionClient([\"test\"])\n    ...\n```\n\nThis test checks the configuration serialization and deserialization of the `AgentTool`. \n- `dump_component()` is called to serialize the tool\u2019s current state into a configurational format.\n- The test subsequently loads this configuration back into a new instance of `AgentTool` and checks for integrity between original and the loaded object.\n\n## Test for Running Tasks with `TeamTool`\n\n```python\n@pytest.mark.asyncio\nasync def test_team_tool() -> None:\n    \"\"\"Test running a task with TeamTool.\"\"\"\n    agent1 = _EchoAgent(\"Agent1\", \"An agent for testing\")\n    ...\n```\n\nThis section focuses on testing the `TeamTool`, which allows multiple agents to collaborate. \n- Two `EchoAgent` instances are created and added to a `RoundRobinGroupChat`.\n- The tool runs a test task and checks the messages returned by each agent to ensure they can handle incoming tasks as expected.\n\n## Test for Serialization of `TeamTool` Configuration\n\n```python\n@pytest.mark.asyncio\nasync def test_team_tool_component() -> None:\n    \"\"\"Test serialization of TeamTool to config.\"\"\"\n    model_client = ReplayChatCompletionClient([\"test\"])\n    ...\n```\n\nSimilar to the previous serialization test for `AgentTool`, this test evaluates the `TeamTool`'s ability to serialize and deserialize its state correctly.\n- It checks if the reinstantiated team tool retains its properties after being loaded from a configuration.\n\n## Test for Streaming Tasks with `AgentTool`\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_tool_stream() -> None:\n    \"\"\"Test running a task with AgentTool in streaming mode.\"\"\"\n    ...\n```\n\nThe focus of this test is on streaming capabilities within the `AgentTool`. \n- It simulates a task that generates outputs over time rather than all at once.\n- Assertions are made on the sequence and content of messages to ensure the streaming is functioning correctly.\n\n## Test for Streaming Tasks with `TeamTool`\n\n```python\n@pytest.mark.asyncio\nasync def test_team_tool_stream() -> None:\n    \"\"\"Test running a task with TeamTool in streaming mode.\"\"\"\n    ...\n```\n\nThis final test mirrors the streaming logic used in the `AgentTool`, but for the `TeamTool`.\n- It validates that the team of agents can collectively handle streaming tasks and return consistent outputs.\n- The test asserts both the content and the originating agent for messages, ensuring that multiple agents can respond to the same task appropriately.\n\nIn summary, the entire suite of tests provides a comprehensive examination of the functionalities offered by the `AgentTool` and `TeamTool`, including task execution, state management, and serialization in both synchronous and asynchronous contexts. This ensures the robustness and reliability of these tools in automated agent systems.",
    "filename": "python/packages/autogen-agentchat/tests/test_task_runner_tool.py"
  },
  {
    "code": false,
    "content": "# Documentation of Termination Tests in Async Messaging Framework\n\nThis document outlines the functionality and structure of a series of termination tests implemented using `pytest` and `asyncio`. The tests check various termination conditions represented through different classes, ensuring they behave as expected when processing sequences of chat messages.\n\n## Overview\n\nThis code contains multiple asynchronous test functions for various termination conditions in a messaging framework. Each test evaluates a specific termination strategy by asserting the expected behavior within a sequence of inputs. The framework aims to evaluate message processing in an asynchronous context, allowing for nuanced interactions between user messages and system responses.\n\n## Imports and Dependencies\n\nThe code begins by importing necessary libraries and modules:\n\n- **asyncio**: For managing asynchronous operations.\n- **pytest**: For writing and running tests; particularly, it uses the `mark.asyncio` decorator for asynchronous test cases.\n- **Termination Classes**: Various termination classes (like `HandoffTermination`, `TextMessageTermination`, etc.) are imported from the `autogen_agentchat.conditions` module. These classes define specific termination conditions based on the messages processed.\n- **Message Classes**: Message-related classes (like `TextMessage`, `StopMessage`, etc.) are imported from `autogen_agentchat.messages`. These represent the content and structure of messages exchanged.\n- **Core Models**: `FunctionExecutionResult` and `RequestUsage` are imported from the `autogen_core.models` module to handle structured data regarding function calls and usage.\n- **Pydantic Model**: The `BaseModel` from `pydantic` is used for creating data models.\n\n## Test Cases for Different Termination Conditions\n\n### Handoff Termination\n\nThe `test_handoff_termination` function tests conditions under which a `HandoffTermination` is triggered. This occurs when specific handoff messages are received:\n\n1. Initially, an empty list should not terminate.\n2. Single messages do not trigger termination, but receiving a specific `HandoffMessage` will.\n3. The function verifies the termination state across various scenarios, including multiple messages in the input list.\n\n### Stop Message Termination\n\nThe `test_stop_message_termination` function evaluates the `StopMessageTermination` class:\n\n1. An empty list doesn't trigger termination.\n2. The introduction of a `StopMessage` should end the process.\n3. The test also validates the response to multiple sequential messages to observe termination states.\n\n### Text Message Termination\n\nThe `test_text_message_termination` checks the behavior of `TextMessageTermination` under different source contexts:\n\n1. Termination is triggered by a `TextMessage` from the specified source.\n2. It validates responses to different message combinations and ensures that subsequent messages can raise a `TerminatedException`.\n\n### Max Message Termination\n\nIn the `test_max_message_termination`, the `MaxMessageTermination` class is explored:\n\n1. Termination is based on a count of messages processed.\n2. Different configurations (with or without agency events) are tested to verify message counts leading to termination.\n\n### Mention Termination\n\nThe `test_mention_termination` function tests the behavior of `TextMentionTermination` with specific mention words:\n\n1. It ensures that mentions from various sources can lead to termination.\n2. The test verifies detection of target phrases and combinations.\n\n### Token Usage Termination\n\nThe `test_token_usage_termination` function assesses `TokenUsageTermination` based on token limits:\n\n1. Several test cases validate termination based on total tokens used across messages.\n2. The function checks both exceeding limits and scenarios that do not reach the threshold.\n\n### Contextual Compositions: AND and OR Terminations\n\nTwo test functions, `test_and_termination` and `test_or_termination`, explore logical compositions of terminators:\n\n- **AND**: Both conditions must be met for termination.\n- **OR**: Meeting either condition results in termination.\n\nThese tests validate combinations of message inputs to observe how the termination states change based on logical operators.\n\n### Timeout and External Termination Tests\n\nThe `test_timeout_termination` and `test_external_termination` functions examine specific external factors that trigger terminations:\n\n1. The timeout termination implements delays to simulate timeout conditions.\n2. The external termination checks the ability to trigger termination through external signals.\n\n### Source Match and Functional Termination\n\nThe `test_source_match_termination` ensures specific sources (like \"Assistant\") lead to termination under given message patterns, while `test_function_call_termination` assesses terminations that occur on function call executions.\n\n### Final Functional Termination Test\n\nLastly, `test_functional_termination` explores termination based on custom functional logic, demonstrating flexibility in defining termination conditions through user-defined asynchronous or synchronous functions.\n\n## Conclusion\n\nThese tests offer thorough coverage of termination conditions in an asynchronous messaging environment. Each termination class is put to the test under various scenarios, ensuring robust handling of user inputs and conditions that define when to cease interactions. By utilizing `pytest` and `asyncio`, the framework ensures efficiency and handles asynchronous operations elegantly. The documentation encapsulates the intricacies of termination logic and provides insight into potential extensions or modifications to the existing tests.",
    "filename": "python/packages/autogen-agentchat/tests/test_termination_condition.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Testing Module\n\nThis documentation provides a detailed overview of the asynchronous testing module designed for the `UserProxyAgent` in the `autogen_agentchat` package. The tests focus on verifying the various functionalities and error handling of the `UserProxyAgent` when processing chat messages with different input methods.\n\n## Overview\n\nThe tests are organized using the `pytest` framework and utilize the `asyncio` library to handle asynchronous operations. Each test is marked with `@pytest.mark.asyncio`, indicating that they are asynchronous tests. A `CancellationToken` is used to manage cancellation requests, allowing for simulation of real-world scenarios where message handling may need to be interrupted.\n\n## Test Cases\n\n### 1. `test_basic_input`\n\nThis test verifies the basic input handling capabilities of the `UserProxyAgent`.\n\n- **Setup**: A custom synchronous input function (`custom_input`) is defined that returns a predefined response when called with a prompt.\n- **Operation**: The agent receives a `TextMessage` asking for the height of the Eiffel Tower and processes it using the `on_messages` method.\n- **Assertions**:\n  - The response must be an instance of `Response`.\n  - The message in the response must be a `TextMessage` with the correct content matching the predefined response.\n  - The source of the response must match the agent's name (\"test_user\").\n\n### 2. `test_async_input`\n\nThis test focuses on the handling of asynchronous input functions by the `UserProxyAgent`.\n\n- **Setup**: An asynchronous input function (`async_input`) simulates a delayed response of a string \"async response\".\n- **Operation**: The agent processes a `TextMessage` prompt using the `on_messages` method.\n- **Assertions**:\n  - The response must contain a `TextMessage` with content equal to \"async response\".\n  - The source of the response should correctly reflect the agent's name.\n\n### 3. `test_handoff_handling`\n\nThis test examines how the `UserProxyAgent` deals with handoff messages, which may require confirmation from the user.\n\n- **Setup**: A custom input function is defined to return a response when handling a handoff situation. The test sets up multiple messages including an initial `TextMessage` followed by a `HandoffMessage`.\n- **Operation**: The agent processes a sequence of messages containing both text and a handoff notification.\n- **Assertions**:\n  - The response should reflect the content of the `HandoffMessage`, indicating handling has occurred.\n  - It checks that a `RuntimeError` is raised when a handoff message is addressed to a different agent.\n  - The test confirms no handoff occurs if the last message is not directed to this agent.\n\n### 4. `test_cancellation`\n\nThis test assesses how the agent behaves when the handling of messages is interrupted by a cancellation request.\n\n- **Setup**: A cancellable input function (`cancellable_input`) simulates a delay and raises a `CancelledError` if the cancellation token is activated. A cancellation routine is also defined to trigger cancellation after a brief pause.\n- **Operation**: The agent processes a text message while being monitored for cancellation.\n- **Assertions**:\n  - The test ensures that an `asyncio.CancelledError` is raised if cancellation occurs before the message handling completes.\n\n### 5. `test_error_handling`\n\nThis test verifies the error handling capabilities of the `UserProxyAgent` when the input function fails.\n\n- **Setup**: An input function (`failing_input`) is defined to raise a `ValueError`, simulating a failure in retrieving user input.\n- **Operation**: The agent processes a simple text message.\n- **Assertions**:\n  - The test raises a `RuntimeError` which is validated to contain a specific error message indicating that the input function did not succeed.\n\n## Conclusion\n\nThe outlined tests serve to validate the core functionalities of the `UserProxyAgent`, ensuring it can handle various types of inputs, cancellations, and errors effectively. By employing both synchronous and asynchronous input functions, along with handoff messages, the tests cover a broad spectrum of possible agent interactions, mitigating the risk of runtime failures and maintaining robust user interaction handling in the chat system.",
    "filename": "python/packages/autogen-agentchat/tests/test_userproxy_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation for `test_remove_images`\n\n## Overview\n\nThe provided code is a test function, written using the `pytest` framework, which tests the functionality of the `remove_images` utility function from the `autogen_agentchat.utils` module. This function is intended to process a list of messages, specifically to remove images while preserving the rest of the message content. The test validates that the structure and content of messages are maintained after image removal.\n\n## Imports\n\nThe test function imports several modules:\n- **List** from `typing` is imported for type hinting.\n- **pytest** is imported to leverage the testing capabilities provided by the pytest framework.\n- Functions and classes from **autogen_agentchat.utils** and **autogen_core**, including `remove_images`, `Image`, `AssistantMessage`, `LLMMessage`, `SystemMessage`, and `UserMessage`, are used to manipulate messages and images.\n\n## Test Function: `test_remove_images`\n\nThis function is defined as an asynchronous test using the `@pytest.mark.asyncio` decorator, which allows for testing asynchronous code. The purpose of this test is to verify that the `remove_images` function correctly processes a list of messages with an embedded image.\n\n### Step 1: Setting Up Test Data\n\nThe test begins by defining a base64 encoded string representing an image (`img_base64`). This string simulates a simple image that will be included in a message. A list of messages (`messages`) is constructed, which includes:\n- A `SystemMessage`.\n- A `UserMessage` containing a text and an image created from the base64 string.\n- An `AssistantMessage`.\n- Another `UserMessage`.\n\n```python\nimg_base64 = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAADElEQVR4nGP4//8/AAX+Av4N70a4AAAAAElFTkSuQmCC\"\nmessages: List[LLMMessage] = [\n    SystemMessage(content=\"System.1\"),\n    UserMessage(content=[\"User.1\", Image.from_base64(img_base64)], source=\"user.1\"),\n    AssistantMessage(content=\"Assistant.1\", source=\"assistant.1\"),\n    UserMessage(content=\"User.2\", source=\"assistant.2\"),\n]\n```\n\n### Step 2: Invoking `remove_images` Function\n\nNext, the `remove_images` function is called with the list of messages as its argument. The result is stored in the variable `result`. This function is expected to return a new list of messages where any messages containing images have those images removed.\n\n```python\nresult = remove_images(messages)\n```\n\n### Step 3: Validating Results\n\nThe test asserts multiple conditions to ensure that the results are as expected after invoking `remove_images`.\n\n#### Count Check\nFirst, it checks that the length of the result is equal to the original message count, which is 4. This verifies that no messages were lost during processing.\n\n```python\nassert len(result) == 4\n```\n\n#### Type Checks\nThe next assertions validate the types of each message in the result, ensuring that each message is of the expected class type, such as `SystemMessage`, `UserMessage`, and `AssistantMessage`.\n\n```python\nassert isinstance(result[0], SystemMessage)\nassert isinstance(result[1], UserMessage)\nassert isinstance(result[2], AssistantMessage)\nassert isinstance(result[3], UserMessage)\n```\n\n#### Content Verification\nFurther assertions confirm that the content of the messages, specifically for the `SystemMessage`, `AssistantMessage`, and the second `UserMessage`, remain unchanged. \n\n```python\nassert result[0].content == messages[0].content\nassert result[2].content == messages[2].content\nassert result[3].content == messages[3].content\n```\n\n#### Source Verification\nThe sources of the `AssistantMessage` and the last `UserMessage` are also checked to ensure they remain consistent with the original data.\n\n```python\nassert result[2].source == messages[2].source\nassert result[3].source == messages[3].source\n```\n\n### Step 4: Image Removal Check\n\nFinally, the test confirms that the content of the second `UserMessage` in the result does not include the original content plus the image. Instead, it checks that the output indicates an image placeholder.\n\n```python\nassert result[1].content == \"User.1\\n<image>\"\n```\n\n## Conclusion\n\nThis unit test effectively tests the `remove_images` function by ensuring the integrity of the message list while validating that image content is appropriately replaced with a placeholder. The structured assertions cover both the count and content of the messages, confirming that the function operates as intended. By employing asynchronous testing with `pytest`, the code is robust and can handle potential future extensions.",
    "filename": "python/packages/autogen-agentchat/tests/test_utils.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe provided code is designed for logging and comparing messages within an application that likely handles chat or agent events. It incorporates logging functionality that allows for both console output and file-based logging, while also providing utility functions to compare different message types and task results.\n\n## Imports\nThe code begins with importing essential libraries and modules:\n- **json**: For serializing messages into JSON format.\n- **logging**: To handle logging actions.\n- **sys**: To output logs to the console.\n- **datetime**: To manage timestamps of log messages.\n- **Sequence**: For type hinting, to represent an ordered collection of items.\n- **TaskResult and message classes**: These classes from `autogen_agentchat` are likely used for representing task outcomes and chat messages in a structured way.\n- **BaseModel**: Pydantic\u2019s base class for data validation and settings management.\n\n## Custom Log Handlers\n### FileLogHandler\nThe `FileLogHandler` class extends `logging.Handler` to log messages to a specified file. \n\n- **Initialization**: The constructor takes a `filename` parameter for the log file and initializes a file handler.\n- **emit method**: This method processes each log record for enhanced structure:\n    - It formats the timestamp of the record.\n    - If the message is a Pydantic `BaseModel`, it converts the message to JSON format, including the timestamp and type of the message before sending it to the file.\n\n### ConsoleLogHandler\nThe `ConsoleLogHandler` class also extends `logging.Handler` but logs messages directly to the console output.\n\n- **emit method**: Similar to `FileLogHandler`, it formats the log messages in a JSON structure with timestamps if the message is a `BaseModel`, but outputs it to `stdout` instead of a file.\n\n## Message Comparison Functions\n### compare_messages\nThis function compares two messages of various types: `BaseAgentEvent`, `BaseChatMessage`, or `BaseTextChatMessage`.\n\n- It specifically checks for equality in content if both messages are instances of `BaseTextChatMessage`.\n- It also verifies that the properties `source`, `models_usage`, and `metadata` of both messages are the same for them to be considered equal.\n\n### compare_message_lists\nThe `compare_message_lists` function accepts two sequences of messages and determines if they are equal.\n\n- It first checks if the lengths are the same; if not, it returns `False`.\n- Then it iterates through each message pair, calling `compare_messages` to ensure every corresponding message is equivalent.\n\n### compare_task_results\nThis function compares two `TaskResult` objects.\n\n- It compares the `stop_reason` property of both results directly.\n- It utilizes `compare_message_lists` to check if the lists of messages in both task results are equivalent.\n\n## Usage Context\nThe code appears to be designed for an application in the domain of chatbots or automated agents. The logging functionality allows developers to trace events and conduct diagnostics by storing structured log messages either in a file or displaying them in the console. The comparison functions serve to test dynamic components of the application, ensuring that messages and task results are consistent across processes or invocations, thereby maintaining integrity in interactions.\n\n## Conclusion\nOverall, this code establishes a foundation for logging and message comparison in an agent-based chat system. The combination of structured logging and the ability to compare results and messages enhances the robustness of the application, providing clear insights into operational behavior and facilitating easier debugging.",
    "filename": "python/packages/autogen-agentchat/tests/utils.py"
  },
  {
    "content": "# AutoGen Core\n\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)\n\nAutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.",
    "filename": "python/packages/autogen-core/README.md"
  },
  {
    "code": false,
    "content": "# Overview of `autogen_core`\n\nThe `autogen_core` module serves as the foundation for a system centered around agents, interoperability, and component management. This module facilitates the creation and management of agents, provides tools for data serialization, defines message handling protocols, and manages logging functionalities. By importing various submodules and defining global constants and classes, it offers an organized layout for the internal structure of the agent management system.\n\n## Versioning Information\n\nThe module starts by retrieving its version using the `importlib.metadata` package. This is a common practice to enable tracking of the module\u2019s version which helps in ensuring compatibility and debugging:\n\n```python\n__version__ = importlib.metadata.version(\"autogen_core\")\n```\n\n## Core Agent Functionality\n\nThe module defines several core classes representing different aspects of agent functionality including:\n\n- **Agent**: Central class representing an agent that encapsulates core behaviors and properties.\n- **AgentId**: A class that likely manages unique identifiers for agents, which is essential for tracking and communication.\n- **AgentInstantiationContext**: Manages context information during the instantiation of agents.\n- **AgentMetadata**: Stores metadata related to agents such as attributes or configurations.\n\nThese components contribute to defining how agents operate within this framework, maintaining unique states and handling interactions.\n\n## Agent Management and Runtime\n\nThe module includes a range of classes designed to handle the runtime behavior of agents:\n\n- **AgentRuntime**: Likely manages the lifecycle of agents, including their execution and state management.\n- **SingleThreadedAgentRuntime**: A special runtime for executing agents in a single-threaded context, ensuring sequential processing without concurrency issues.\n\nAdditionally, **AgentProxy** allows for interfacing with agents from potentially different execution contexts or systems, facilitating remote communication.\n\n## Component System\n\nA significant portion of the module revolves around component management:\n\n- **Component** and its related classes (e.g., **ComponentBase**, **ComponentLoader**) allow the system to dynamically load and configure components at runtime. \n- This includes defining schemas (**ComponentSchemaType**) and potentially handling configurations through **ComponentFromConfig** and **ComponentToConfig**.\n\nThese components likely enable extensibility and modularity, where new features can be integrated without significant modifications to the core system.\n\n## Messaging and Interventions\n\nThe module also defines mechanisms for handling messages and interventions:\n\n- **MessageContext** and **MessageHandlerContext** encapsulate information necessary for message processing and handler states.\n- **InterventionHandler** and **DefaultInterventionHandler** are utilized to manage disruptions or changes in message flows, which help in error handling or providing alternate routes for message processing.\n\nThis aids in maintaining the reliability of communication in agent interactions.\n\n## Subscription Mechanism\n\nTo support an event-driven architecture, the module provides several classes and functions related to subscriptions:\n\n- **Subscription**: Central class for managing subscriptions to events or message types.\n- **TypeSubscription** and **TypePrefixSubscription** allow for more specific subscription behaviors based on message types, enabling agents to respond to targeted messages.\n\nThis mechanism enhances the responsiveness of agents to specific events, promoting efficient processing.\n\n## Serialization Handling\n\nData serialization is crucial in this context, and the module defines elements for handling various data formats:\n\n- **MessageSerializer**: Implements serialization logic, allowing messages to be converted to and from different formats.\n- Constants like **JSON_DATA_CONTENT_TYPE** and **PROTOBUF_DATA_CONTENT_TYPE** define the expected data types for serialization.\n\nBy supporting multiple formats, the system can handle a broader range of data interactions, making it versatile for different use cases.\n\n## Logging and Telemetry\n\nThe module includes robust logging capabilities:\n\n- **EVENT_LOGGER_NAME**, **ROOT_LOGGER_NAME**, and **TRACE_LOGGER_NAME** provide structured logging support, enabling tracking and debugging of agent activities.\n- Functions like **trace_create_agent_span** and **trace_invoke_agent_span** help in monitoring performance and analyzing agent behavior over time.\n\nThis focus on logging ensures that users can gather insights into system operations, which is essential for maintenance and troubleshooting.\n\n## Exposed API \n\nLastly, the module exports a well-defined set of public interfaces with the `__all__` variable, outlining which classes and functions are accessible for use in other modules. This approach aids in defining a clear API for users of the `autogen_core`, ensuring encapsulation and reducing the risk of interference with internal workings. Each of these exported components plays a distinct role in agent management, message processing, or system integration, enabling a comprehensive ecosystem of functionalities tailored for developing agent-based applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for `Agent` Protocol\n\n## Overview\nThe provided code defines a high-level interface in the form of a `Protocol` named `Agent`. This protocol includes various properties and asynchronous methods that any agent-like class should implement. The `Agent` protocol is intended for use in environments where agents communicate and manage their states, typically in a concurrent or asynchronous system like agent-based architectures.\n\n## Type Checking and Dependencies\nThe module imports features from the `typing` library to enable type checking and annotations. It includes specific types such as `Any`, `Mapping`, `Protocol`, and `runtime_checkable`. Additionally, it imports three components:\n- `AgentId`: Presumably encapsulates a unique identifier for each agent.\n- `AgentMetadata`: Likely holds various metadata attributes about the agent.\n- `MessageContext`: Provides context for messages sent to agents.\n\nA conditional import for `AgentRuntime` is provided but only for type checking purposes (e.g., during static analysis) and is not necessary at runtime.\n\n## The `Agent` Protocol\n### Properties\nThe `Agent` Protocol defines two properties:\n1. **`metadata`**: This property should return an `AgentMetadata` instance, providing descriptive attributes about the agent. It serves to encapsulate agent-specific information, which may include its capabilities, version, or other relevant data.\n   \n2. **`id`**: This property returns an `AgentId`, representing a unique identifier for the agent. This ID is crucial in systems where agents interact, as it allows for specific identification during communication.\n\n### Asynchronous Methods\nThe `Agent` Protocol includes several asynchronous methods that describe the agent\u2019s behavior and state management.\n\n#### `bind_id_and_runtime`\n- **Purpose**: This method is designed to link an instance of the `Agent` with a specific `AgentRuntime` and bind it to an agent ID.\n- **Arguments**:\n  - `id` (of type `AgentId`): The unique identifier for the agent.\n  - `runtime` (of type `AgentRuntime`): The runtime environment in which the agent operates.\n\nThis binding process is essential for ensuring the agent can correctly respond to messages and manage its lifecycle within the runtime context.\n\n#### `on_message`\n- **Purpose**: This method serves as the primary handler for incoming messages directed to the agent. It is expected to be invoked only by the `AgentRuntime`, not other agents.\n- **Arguments**:\n  - `message` (of type `Any`): Represents any received message and can vary in type.\n  - `ctx` (of type `MessageContext`): Provides context for the incoming message, which can include sender information or additional metadata.\n- **Returns**: The method may return a response of type `Any`, which might be `None` in some cases.\n- **Exception Handling**: Can raise `asyncio.CancelledError` if the operation is canceled and `CantHandleException` if it cannot process the message.\n\nThis method is crucial for implementing the agent's communication logic and message handling capabilities.\n\n#### `save_state`\n- **Purpose**: This method is used to persist the current state of the agent. It should return a serializable mapping that can be stored as JSON.\n- **Returns**: A `Mapping[str, Any]` that represents the current state, ensuring that it can be easily saved for later retrieval.\n\nSaving state is important for agent continuity and recovery, especially in long-running applications.\n\n#### `load_state`\n- **Purpose**: Complements the `save_state` method by allowing the agent to restore its state from a previously saved mapping.\n- **Arguments**:\n  - `state` (of type `Mapping[str, Any]`): This input should represent a state that was generated by `save_state`, also needing to be JSON serializable.\n\nImplementing this method allows the agent to maintain continuity across invocations, which is significant in environments with multiple execution cycles.\n\n#### `close`\n- **Purpose**: This method is called when the `AgentRuntime` is about to close. It allows the agent to perform any necessary cleanup operations, ensuring that resources are released properly.\n- **Returns**: This method does not return any value.\n\n### Conclusion\nThe `Agent` Protocol serves as a framework for defining the essential behavior and characteristics that agent-based components must implement. By adhering to this protocol, agents can achieve modularity and interoperability within an agent-based system, facilitating efficient communication, state management, and lifecycle handling.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent.py"
  },
  {
    "code": false,
    "content": "# AgentId Module Documentation\n\nThis document describes the functionality and structure of the `AgentId` class defined in the source code. It includes validation functions, constructors, properties, and the overall purpose of the code.\n\n## Overview\n\nThe primary purpose of this module is to define the `AgentId` class, which serves as a unique identifier for agent instances. An agent is a conceptual entity that can receive messages within a distributed runtime environment. The `AgentId` ensures that each agent can be distinctively referenced using a combination of a \"type\" and a \"key\". \n\n## Validation Function\n\n### `is_valid_agent_type(value: str) -> bool`\n\nThis function checks if the given string `value` adheres to specific formatting rules. The function uses a regular expression to validate that the string only contains word characters (alphanumeric and underscores), hyphens, or periods and matches the complete string (`\\Z` denotes the end of the string). If the string matches the pattern, it returns `True`; otherwise, it returns `False`.\n\n## AgentId Class\n\n### Class Definition\n\n```python\nclass AgentId:\n    ...\n```\n\nThe `AgentId` class provides methods and properties to create, manage, and validate the unique identifier for agents. An instance of `AgentId` consists of two components: `type` and `key`. The class has several methods and properties to facilitate handling agent identifiers.\n\n### Constructor\n\n#### `__init__(self, type: str | AgentType, key: str) -> None`\n\nThis constructor initializes an `AgentId` instance.\n\n- **Parameters**:\n  - `type`: Can either be a string or an `AgentType`. If it's an `AgentType`, the actual type string is extracted for storage.\n  - `key`: A string that serves as the unique key for the agent instance.\n\n- **Validation**: Upon instantiation, it checks if the provided `type` is valid using the `is_valid_agent_type` function. If invalid, it raises a `ValueError`.\n\n### Hashing and String Representation\n\n#### `__hash__(self) -> int`\n\nThis method returns a hash value for the `AgentId` instance. It generates a hash based on both the `type` and `key`, allowing `AgentId` instances to be used in hashed collections like sets and dictionaries.\n\n#### `__str__(self) -> str`\n\nThis method returns a user-friendly string representation of the `AgentId`, formatted as `type/key`. This is useful for displaying the identifier in logs or user interfaces.\n\n#### `__repr__(self) -> str`\n\nThis method provides a more detailed string representation suitable for debugging, showing the class name along with the `type` and `key`.\n\n### Equality Comparison\n\n#### `__eq__(self, value: object) -> bool`\n\nThis method checks for equality between two `AgentId` instances. It returns `True` if both instances have the same `type` and `key`; otherwise, it returns `False`.\n\n### Class Method for String Conversion\n\n#### `from_str(cls, agent_id: str) -> Self`\n\nThis class method converts a string formatted as `type/key` back into an `AgentId` instance.\n\n- **Parameters**:\n  - `agent_id`: A string representation of the agent identifier.\n\n- **Validation**: If the string does not correctly split into two parts, a `ValueError` is raised.\n\n### Properties\n\n#### `type`\n\n- This property returns the `type` of the agent. It is validated to be composed only of alphanumeric letters and underscores.\n\n#### `key`\n\n- This property returns the `key` associated with the agent instance. It also adheres to the same validation rules as the `type`.\n\n## Summary\n\nIn summary, the `AgentId` class is designed to provide a structured and validated way to identify agents within a distributed environment. It ensures that each agent can be uniquely referenced, supports operational features such as hashing and equality checks, and provides mechanisms to construct identifiers from strings. The validation ensures that identifiers remain compliant with the required naming conventions, enhancing reliability within the runtime system.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_id.py"
  },
  {
    "code": false,
    "content": "# Agent Instantiation Context Class Documentation\n\n## Overview\n\nThe `AgentInstantiationContext` class is designed to provide a context for agent instantiation within the framework of an agent-based architecture. It plays a crucial role in managing the current runtime and agent ID, which are essential during the creation and operation of agents. This class operates as a static entity, essentially serving as a utility for accessing information relevant to the agent instantiation process.\n\nThe class includes several key components, such as methods to retrieve the current runtime and agent ID, and a mechanism to manage context using Python's `contextvars`. Context management is particularly important in asynchronous programming to maintain state across different execution contexts.\n\n## Class Description\n\n### Static Class Nature\n\nThe `AgentInstantiationContext` is defined as a static class, as indicated by the absence of an instance initializer (`__init__`). Attempting to instantiate this class raises a `RuntimeError`, which clearly communicates its intended use as a utility class rather than a typical object that can be instantiated. This design choice ensures that users cannot create instances of the class, emphasizing that all interaction should be handled through its class methods.\n\n### Context Variable Declaration\n\nA class variable, `_AGENT_INSTANTIATION_CONTEXT_VAR`, is declared using `ContextVar`. This variable is crucial to retaining information about the current agent runtime and agent ID during the instantiation process. By leveraging context variables, which are designed to hold state information in asynchronous execution, this class can ensure that context is preserved correctly across different coroutine calls.\n\n## Methods Overview\n\n### populate_context\n\n```python\n@classmethod\n@contextmanager\ndef populate_context(cls, ctx: tuple[AgentRuntime, AgentId]) -> Generator[None, Any, None]:\n```\n\nThe `populate_context` method utilizes a context manager to set the current context for agent instantiation. This method takes a tuple containing the current `AgentRuntime` and `AgentId`. It sets this tuple in the context variable using `set()`, then allows the execution of any block of code that follows within this context. When the block completes, it resets the context variable to its prior state. This functionality is essential when creating agents, as it maintains context during their instantiation.\n\n### current_runtime\n\n```python\n@classmethod\ndef current_runtime(cls) -> AgentRuntime:\n```\n\nThe `current_runtime` method retrieves the current `AgentRuntime` from the context variable. If the method is called outside of a valid instantiation context, it raises a `RuntimeError`. This method is critical for agent instantiation as agents may need to know the context in which they are being created, especially for accessing relevant resources and functionalities provided by the runtime.\n\n### current_agent_id\n\n```python\n@classmethod\ndef current_agent_id(cls) -> AgentId:\n```\n\nSimilar to `current_runtime`, the `current_agent_id` method retrieves the current `AgentId` from the context variable. It also raises a `RuntimeError` if invoked outside an appropriate context. This method enables agents to identify themselves accurately, which is crucial for managing interactions within the agent system.\n\n### is_in_factory_call\n\n```python\n@classmethod\ndef is_in_factory_call(cls) -> bool:\n```\n\nThe `is_in_factory_call` method checks whether the current code is executing within a factory call context. It does this by attempting to access the context variable and returning a boolean value. This method aids in conditionally executing code based on the context, which can be useful for debugging or managing logic that behaves differently depending on the state of the agent creation process.\n\n## Example Usage\n\n### Integrating with Agents and Runtime\n\nThe included example illustrates how the `AgentInstantiationContext` can be utilized within an agent-based architecture. In the example, a `TestAgent` class demonstrates how to retrieve the current runtime and agent ID both from its constructor and within a factory function. \n\nThe `main` function instantiates a `SingleThreadedAgentRuntime`, starts it, registers the `TestAgent` factory, and sends messages to the agent. The agent then can receive and process messages, utilizing the context to maintain relevant state.\n\n### Error Handling\n\nIt is essential to note that the class raises `RuntimeError` in situations where methods are accessed outside the expected context. This design helps prevent misuse of the class and ensures correctness by enforcing proper instantiation patterns, suggesting that the agent must be created using the runtime rather than directly.\n\n## Summary\n\nOverall, the `AgentInstantiationContext` class is foundational for managing the state of agents during their instantiation. Its use of context management, along with its methods to retrieve runtime information, provides a robust framework for building agent-based applications. By ensuring that agents operate within the correct context, the class enhances the reliability and maintainability of the codebase, contributing to a well-structured agent architecture.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_instantiation.py"
  },
  {
    "code": false,
    "content": "## Overview\n\nThe provided code snippet defines a structure for holding metadata related to some \"Agent\" entity using a type-safe approach. It utilizes the `TypedDict` feature from the `typing` module in Python, allowing for the creation of dictionaries with a fixed set of keys, each with an associated type.\n\n## Key Components\n\n### 1. TypedDict Definition\n\nThe primary element in the code is the `AgentMetadata` class, which extends `TypedDict`. This class is set up to enforce a specific structure for dictionaries that represent agent metadata. Here\u2019s a breakdown of its components:\n\n- **`type: str`**: This key is expected to hold a string value that indicates the category or function of the agent. It may refer to a predefined set of types that classify agents in a system.\n  \n- **`key: str`**: This key is meant to represent a unique identifier for the agent. It serves as a reference point when managing multiple agents, ensuring that each can be distinguished from the others.\n  \n- **`description: str`**: This key provides a textual description of the agent, offering insights into its functionality, purpose, or any other relevant details.\n\n### 2. Purpose of `TypedDict`\n\nThe use of `TypedDict` enhances type safety in Python by allowing dictionaries to have specific field names and types. This helps developers ensure that the data structure conforms to expected formats, reducing the likelihood of runtime errors due to type mismatches or missing keys.\n\n### 3. Use Cases\n\nThe `AgentMetadata` structure can be employed in various scenarios where metadata handling is necessary. Examples include:\n\n- **Configuration Management**: When loading configurations for different agents in a system, the metadata could contain information on how to process or utilize each agent.\n  \n- **Dynamic Systems**: In systems where agents are created or modified at runtime, maintaining a consistent metadata structure helps in managing interactions and the lifecycle of these agents.\n\n### 4. Integration with Other Components\n\nWhile the snippet itself only declares a data structure, `AgentMetadata` can be integrated into larger applications. For instance, it could be part of a class responsible for agent management, where this metadata is utilized in functions that register, deregister, or update agents.\n\n### 5. Example of Usage\n\nIn practice, a developer might create an instance of `AgentMetadata` to store information about a specific agent like so:\n\n```python\nagent_info = AgentMetadata(\n    type='robot',\n    key='agent_01',\n    description='A robotic agent for automating tasks.'\n)\n```\n\nThis instance would then be used in the application to reference the agent\u2019s properties across various functionalities.\n\n### 6. Benefits of TypedDict\n\nBy using `TypedDict`, developers gain several advantages:\n\n- **Clarity in Code**: As each key in the metadata dictionary is associated with a specific type, the intent and structure of the data are clear to anyone reading the code.\n  \n- **IDE Support**: Many Integrated Development Environments (IDEs) can provide autocompletion and type hinting, enhancing developer productivity when working with the `AgentMetadata` structure.\n\n- **Error Checking**: At development time, type checkers can validate the use of `AgentMetadata`, helping catch errors before runtime.\n\n### 7. Limitations\n\nDespite the advantages, there are some limitations to consider:\n\n- **Immutability**: Once defined, the keys and their associated types cannot be changed. This might result in a lack of flexibility if the structure needs to evolve.\n  \n- **Support**: As of certain versions of Python, `TypedDict` was introduced under PEP 589 which means that earlier versions may not support it.\n\n### 8. Conclusion\n\nIn summary, the code snippet provides a formalized and type-safe way to manage agent metadata within Python applications. It highlights the importance of structured data representation and offers a foundation for more complex functionalities, making it a valuable tool for developers working on systems that involve agent management.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_metadata.py"
  },
  {
    "code": false,
    "content": "# AgentProxy Class Documentation\n\n## Overview\nThe `AgentProxy` class serves as a facilitator for interactions with an associated `Agent` while utilizing its `AgentId`. This design allows for a simplified interface to communicate with agents in a manner that abstracts away some of the complexity of managing agent states and messaging. By employing this proxy pattern, the code provides a clean separation of concerns, allowing for the management and manipulation of agent instances without directly interacting with their underlying implementations.\n\n## Dependencies\nThe class utilizes several dependencies:\n- **`AgentId`**: Represents the unique identifier for an agent.\n- **`AgentMetadata`**: Encapsulates metadata associated with an agent.\n- **`CancellationToken`**: Handles cancellation for asynchronous operations.\n- **`AgentRuntime`**: A runtime environment managing agent actions and state.\n\nThese dependencies are essential for the operation of the `AgentProxy`, enabling it to send messages, retrieve metadata, and manage the agent's state. The `TYPE_CHECKING` import is used to enable type hints for classes that may not need to be imported at runtime, ensuring a cleaner, more efficient design.\n\n## Class Initialization\n### `__init__(self, agent: AgentId, runtime: AgentRuntime)`\nThe constructor initializes the `AgentProxy` class with two primary parameters:\n- **`agent`**: An instance of `AgentId` that identifies the agent.\n- **`runtime`**: An instance of `AgentRuntime` that manages the execution and state of the agent.\n\nThese parameters are stored as private attributes, enabling the proxy to utilize the `AgentId` when sending messages and managing the agent's state.\n\n## Properties\n### `id`\n- **Type**: `AgentId`\n- **Description**: This property returns the target agent associated with the proxy. It allows the user to access the underlying `AgentId` object directly.\n\n### `metadata`\n- **Type**: `Awaitable[AgentMetadata]`\n- **Description**: This property fetches and returns the metadata for the associated agent asynchronously. It calls the `agent_metadata` method of the `AgentRuntime`, allowing the caller to gather relevant information about the agent's configuration, capabilities, and state.\n\n## Asynchronous Methods\n### `send_message(self, message: Any, *, sender: AgentId, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`\nThis method is responsible for sending messages to the associated agent. It takes the following parameters:\n- **`message`**: The content of the message to be sent.\n- **`sender`**: An instance of `AgentId` representing the source of the message.\n- **`cancellation_token`**: An optional token that can signal the cancellation of the operation.\n- **`message_id`**: An optional unique identifier for the message.\n\nThe method asynchronously calls the `send_message` method of the `AgentRuntime`, passing along the necessary parameters. This function facilitates communication between agents in a structured way, ensuring messages are routed properly.\n\n### `save_state(self) -> Mapping[str, Any]`\nThis method allows the state of the associated agent to be saved asynchronously. It returns a mapping that must be JSON serializable, representing the current state of the agent. The method invokes `agent_save_state` on the `AgentRuntime`, ensuring that the current state is persisted and can later be restored if needed.\n\n### `load_state(self, state: Mapping[str, Any]) -> None`\nThe `load_state` method accepts a mapping representing the agent's state and asynchronously loads this state into the agent. The state should be previously obtained from a `save_state` call, maintaining compatibility and ensuring a seamless transition back to a previously saved state. The method internally calls `agent_load_state` of the `AgentRuntime`, applying the provided state to the agent.\n\n## Summary\nThe `AgentProxy` class provides a robust interface for managing agents within a runtime environment. By using this proxy, developers can easily send messages, retrieve metadata, and save or load the state of agents without directly interacting with the more complex underlying implementations of these agents. The versatility and asynchronous capability of this class enable efficient and organized management of agent operations in a distributed or concurrent system.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_proxy.py"
  },
  {
    "code": false,
    "content": "# AgentRuntime Documentation\n\n## Overview\n\nThe `AgentRuntime` class defines a protocol for managing and interacting with agents in an asynchronous environment. Agents are individual entities that can send and receive messages, maintain state, and respond to events. This protocol includes methods for sending and publishing messages, registering agent factories and instances, saving and loading states, and managing subscriptions and message serialization.\n\n## Type Variables and Protocol Setup\n\n### Type Variable\n\nThe code utilizes a generic type variable `T`, which is bounded by the `Agent` class. This allows the methods defined in the protocol to be flexible in terms of the specific agent types they can work with.\n\n```python\nT = TypeVar(\"T\", bound=Agent)\n```\n\n### AgentRuntime Protocol\n\n`AgentRuntime` is defined as a protocol using the `Protocol` class from the `typing` module, making it possible to define the expected methods without providing specific implementations.\n\n```python\n@runtime_checkable\nclass AgentRuntime(Protocol):\n    ...\n```\n\nThis class serves as a blueprint for any runtime component that manages agents, ensuring that it supports the specified asynchronous methods.\n\n## Core Methods of the AgentRuntime Protocol\n\n### Sending and Publishing Messages\n\n#### send_message\n\nThe `send_message` method facilitates communication between agents by allowing one agent to send a message to another. The method is asynchronous and returns a response from the recipient agent.\n\n```python\nasync def send_message(...):\n    ...\n```\n\n- **Arguments**: Accepts the message to be sent, recipient agent ID, optional sender ID, cancellation token, and message ID.\n- **Returns**: Any response from the recipient or raises exceptions if the message cannot be handled or delivered.\n\n#### publish_message\n\nThe `publish_message` method allows an agent to broadcast a message to all agents subscribed to a specific topic.\n\n```python\nasync def publish_message(...):\n    ...\n```\n\n- **Arguments**: Similar to `send_message`, it takes a message and a topic ID, with optional sender details.\n- **Returns**: Nothing, as this method does not expect a response. May raise an `UndeliverableException` for delivery issues.\n\n## Agent Registration and Management\n\n### Registering Factories and Instances\n\n#### register_factory\n\nThe `register_factory` method enables the registration of a factory function that creates agents of a specific type. This method is primarily for advanced use cases.\n\n```python\nasync def register_factory(...):\n    ...\n```\n\n- **Arguments**: Accepts a string/type representing the agent type, a factory function, and an optional expected class for validation.\n- **Returns**: An `AgentType` object representing the registered agent type.\n\n#### register_agent_instance\n\nThe `register_agent_instance` method registers an already created agent instance with the runtime. Each agent must have a unique ID.\n\n```python\nasync def register_agent_instance(...):\n    ...\n```\n\n- **Arguments**: Requires an instance of an `Agent` and its corresponding `AgentId`.\n- **Returns**: The ID of the registered agent.\n\n### Retrieving Agent Information\n\n#### get\n\nThe `get` method allows fetching an agent's ID using either its `AgentId` or by type and key.\n\n```python\nasync def get(...):\n    ...\n```\n\n- **Arguments**: Accepts an `AgentId`, `AgentType`, or string for the type, along with an optional key and lazy loading flag.\n- **Returns**: The `AgentId` of the requested agent.\n\n## State Management\n\n### Saving and Loading States\n\n#### save_state\n\nThe `save_state` method captures the entire runtime state, which includes all agent states. This state can be restored later.\n\n```python\nasync def save_state(...):\n    ...\n```\n\n- **Returns**: A mapping representing the state, which is JSON serializable.\n\n#### load_state\n\nThe `load_state` method restores the runtime to a previously saved state.\n\n```python\nasync def load_state(...):\n    ...\n```\n\n- **Arguments**: Accepts a mapping representing the saved state.\n- **Returns**: Nothing; it's a void function intended for state restoration.\n\n## Subscriptions and Message Serialization\n\n### Managing Subscriptions\n\n#### add_subscription\n\nThis method allows the addition of a subscription, which can trigger actions when specific messages are received.\n\n```python\nasync def add_subscription(...):\n    ...\n```\n\n- **Arguments**: Requires a `Subscription` object describing the subscription parameters.\n\n#### remove_subscription\n\nThe `remove_subscription` method allows for the removal of an existing subscription based on its unique ID.\n\n```python\nasync def remove_subscription(...):\n    ...\n```\n\n- **Arguments**: Takes the ID of the subscription to be removed.\n- **Returns**: May raise a `LookupError` if the subscription does not exist.\n\n### Serializers\n\n#### add_message_serializer\n\nThis method allows adding message serializers to the runtime, which are used to handle the serialization and deserialization of message types.\n\n```python\ndef add_message_serializer(...):\n    ...\n```\n\n- **Arguments**: Accepts one or more `MessageSerializer` objects.\n- **Returns**: Nothing; it directly modifies the runtime's configuration.\n\n## Conclusion\n\nThe `AgentRuntime` protocol is crucial for defining the functionalities required for managing and interacting with agents in a collaborative, asynchronous environment. It encapsulates a wide array of methods that facilitate agent-to-agent communication, registration, state management, and subscription handling, all while ensuring that implementations adhere to a defined structure and behavior through the use of typing protocols.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_runtime.py"
  },
  {
    "code": false,
    "content": "# Documentation for `AgentType` Class\n\n## Overview\nThe provided code defines a class `AgentType` using Python's `dataclasses` module. This class is designed to represent a model for an agent type with a single attribute, `type`, which provides a string representation of the agent. The class is immutable and comparable due to its specific configuration.\n\n## Imports\nThe code begins with an import statement:\n```python\nfrom dataclasses import dataclass\n```\nThis statement imports the `dataclass` decorator from the `dataclasses` module, which simplifies the creation of classes by automatically generating special methods such as `__init__`, `__repr__`, and `__eq__`.\n\n## Class Definition\n### `AgentType` Class\nThe core of the code is the `AgentType` class, defined as follows:\n```python\n@dataclass(eq=True, frozen=True)\nclass AgentType:\n    type: str\n    \"\"\"String representation of this agent type.\"\"\"\n```\n\n- **Decorator**: The `@dataclass(eq=True, frozen=True)` decorator indicates that:\n  - `eq=True`: The class automatically implements a method to compare instances for equality based on the values of their fields.\n  - `frozen=True`: Instances of this class are immutable. Once an instance is created, its attributes cannot be modified.\n\n### Attributes\n- **type**: The class has a single attribute, `type`, which is a string.\n  - **Purpose**: This attribute is intended to hold a meaningful description of the agent's type, serving as its identifier or category.\n\n## Usage\n### Instantiation\nTo create an instance of the `AgentType`, one would use the class constructor with a string value. For example:\n```python\nagent = AgentType(type=\"Human\")\n```\nIn this instance, `agent` represents a type of agent categorized as \"Human\".\n\n### Immutability\nDue to the `frozen=True` setting, attempting to alter the `type` after instantiation will raise an error. This ensures that instances of `AgentType` maintain a consistent and defined state.\n\n## Equality\nThe `eq=True` parameter allows for direct comparison between instances of `AgentType`. Two instances will be considered equal if their `type` attributes are the same:\n```python\nagent1 = AgentType(type=\"AI\")\nagent2 = AgentType(type=\"AI\")\nassert agent1 == agent2  # This will be True\n```\n\n## Documentation\nA docstring is provided for the `type` attribute:\n```python\n\"\"\"String representation of this agent type.\"\"\"\n```\nThis serves as inline documentation, clarifying the purpose of the `type` attribute.\n\n## Summary\nThe `AgentType` class is a straightforward, immutable data model for representing different types of agents in a system. Its use of `dataclasses` provides convenience features for object management while ensuring that each agent type is unique and cannot be altered once defined. This model would be particularly useful in scenarios that involve categorizing agents for simulations, games, or any system requiring distinct agent types.",
    "filename": "python/packages/autogen-core/src/autogen_core/_agent_type.py"
  },
  {
    "code": false,
    "content": "# Documentation for Agent Base Class\n\nThis documentation provides a high-level overview of a base class used for defining agents in a messaging or event-driven architecture. The class is designed to facilitate message handling, subscription management, and interaction with a runtime environment.\n\n## Imports and Type Definitions\n\nThe code starts by importing necessary components and modules from the Python standard library and other internal modules. Key components include:\n\n- **`inspect` and `warnings`**: For runtime introspection and warning management.\n- **`ABC` and `abstractmethod`**: For defining abstract base classes.\n- **Type hints** from `typing` and `typing_extensions`, which are used throughout the code to specify type constraints on functions, methods, and classes.\n\nThe code defines type variables such as `T` and `BaseAgentType` for flexibility and strong typing of the agent classes.\n\n## Subscription Factory and Handlers\n\nThe code defines two decorators:\n\n1. **`subscription_factory`**: This decorator allows agents to register unbound subscriptions. An unbound subscription is added to the `internal_unbound_subscriptions_list` of the agent class to ensure proper management of event listeners.\n\n2. **`handles`**: This decorator is used to associate message types with serializers in the agent class. It checks whether serializations are available for a specific message type and raises an error if none are found, ensuring that the agent is prepared to handle specific message formats.\n\n## The `BaseAgent` Class\n\nThe primary class of interest is `BaseAgent`, which extends both `ABC` and `Agent`. This class serves as a foundational framework for creating specialized agents with common functionality.\n\n### Class Variables and Initialization\n\n- **`internal_unbound_subscriptions_list`** and **`internal_extra_handles_types`** are class-level attributes that are assigned for each subclass during instantiation. These lists are used to store subscriptions and message types that the agent can handle.\n\n- The **`__init_subclass__` method** ensures that each subclass has its own instance of these lists to prevent shared state across subclasses.\n\n### Agent Metadata and ID Management\n\nThe `metadata` property provides a structured object containing identification details about the agent, such as its key, type, and a descriptive string.\n\n- The constructor initializes the agent with a description and manages the agent's runtime context and ID through the `AgentInstantiationContext`, allowing flexibility in agent creation.\n\n### Binding, Sending, and Publishing Messages\n\nThe class provides several core asynchronous methods:\n\n- **`bind_id_and_runtime`**: This method binds the agent to a specific `AgentId` and `AgentRuntime`, ensuring that the agent does not switch contexts once set.\n\n- **`send_message`**: Facilitates the sending of messages to other agents. It checks for a cancellation token and invokes the runtime's `send_message` method.\n\n- **`publish_message`**: Similar to `send_message`, but designed to publish messages to a specific topic rather than targeting a single recipient.\n\n### State Management\n\nThe class contains placeholder methods for saving and loading the agent's state (`save_state` and `load_state`). These methods currently raise warnings that the implementation needs to be completed, indicating areas for further development.\n\n### Subscription Registration and Class Register Function\n\n- **`register_instance`**: This method registers the instance of the agent with the runtime. It handles subscriptions and manages special direct message subscriptions. It allows for a streamlined way to attach the agent's functionality to an underlying messaging system.\n\n- **`register`**: This is a class method that registers an agent factory. It allows for the creation of new agent instances dynamically while managing subscriptions and message handling.\n\n### Abstract Message Handling Method\n\nThe class defines an abstract method, **`on_message_impl`**, that must be overridden in subclasses. This method is crucial for handling incoming messages and implementing specific logic related to those messages. It acts as a placeholder for subclass developers to provide their custom behavior.\n\n## Conclusion\n\nThe `BaseAgent` class provides a robust framework for creating agents in a messaging or event-driven architecture. With its focus on type safety, subscription management, and messaging capabilities, it serves as an essential component for building reactive systems. Developers extending this base class can leverage its functionalities to create agents that interact seamlessly within their intended ecosystems.",
    "filename": "python/packages/autogen-core/src/autogen_core/_base_agent.py"
  },
  {
    "code": false,
    "content": "# Cache Store Implementation Documentation\n\nThis document provides an overview of the cache store implementation written in Python. The code defines an abstract base class for cache stores and an in-memory implementation that adheres to the specified interface.\n\n## Overview of CacheStore\n\n### Abstract Base Class\n\nThe `CacheStore` class is an abstract base class that defines a protocol for cache operations. It is designed to facilitate the creation of various cache implementations while ensuring a consistent interface for interacting with different types of caches.\n\n```python\nclass CacheStore(ABC, Generic[T], ComponentBase[BaseModel]):\n    \"\"\"\n    This protocol defines the basic interface for store/cache operations.\n    \n    Sub-classes should handle the lifecycle of underlying storage.\n    \"\"\"\n```\n\n### Core Operations\n\n`CacheStore` includes two essential abstract methods:\n\n1. **get(key: str, default: Optional[T] = None) -> Optional[T]**: This method is intended to retrieve an item from the cache using the provided key. If the key does not exist in the cache, it returns a specified default value.\n\n2. **set(key: str, value: T) -> None**: This method stores an item in the cache, associating it with a specified key.\n\nBoth methods are defined as abstract, meaning that any subclass must implement them to provide specific functionality.\n\n## InMemoryStore Implementation\n\n### Class Definition\n\nThe `InMemoryStore` class extends `CacheStore` to provide a concrete implementation for in-memory storage. It inherits from `Component`, indicating that it may have additional properties or functionalities related to components.\n\n```python\nclass InMemoryStore(CacheStore[T], Component[InMemoryStoreConfig]):\n    ...\n```\n\n### Configuration\n\n`InMemoryStore` utilizes a configuration class, `InMemoryStoreConfig`, which is defined as an empty subclass of `BaseModel`. This design allows for potential future extensions or configurations to the in-memory store. The class includes:\n\n- **component_provider_override**: A string indicating where the store's configuration can be found.\n- **component_config_schema**: Specifies the configuration schema the store conforms to.\n\n### Constructor\n\n```python\ndef __init__(self) -> None:\n    self.store: Dict[str, T] = {}\n```\n\nThe constructor initializes an empty dictionary (`self.store`) to hold key-value pairs. This dictionary serves as the storage mechanism for the cache.\n\n## Method Implementations\n\n### Get Method\n\n```python\ndef get(self, key: str, default: Optional[T] = None) -> Optional[T]:\n    return self.store.get(key, default)\n```\n\nThe `get` method utilizes Python's built-in dictionary `get` method to retrieve the value associated with the specified key. If the key does not exist, it returns the `default` value (if provided) or `None`.\n\n### Set Method\n\n```python\ndef set(self, key: str, value: T) -> None:\n    self.store[key] = value\n```\n\nThe `set` method adds a new key-value pair to the storage dictionary. If the key already exists, it updates the value associated with that key.\n\n## Configuration Methods\n\n### Serialization\n\n```python\ndef _to_config(self) -> InMemoryStoreConfig:\n    return InMemoryStoreConfig()\n```\n\nThe `_to_config` method returns an instance of `InMemoryStoreConfig`. It can be useful for serialization or extracting the configuration of the store at runtime.\n\n### Deserialization\n\n```python\n@classmethod\ndef _from_config(cls, config: InMemoryStoreConfig) -> Self:\n    return cls()\n```\n\nThe `_from_config` class method creates a new instance of `InMemoryStore` from the provided configuration. This method currently does not utilize any data from `InMemoryStoreConfig`, but it is set up for potential future use.\n\n## Summary\n\nThe code defines a flexible cache store architecture through the abstract `CacheStore` class and provides a specific in-memory implementation through `InMemoryStore`. This design allows for easy extension and modification as caching needs evolve while ensuring a consistent interface for users of the cache system. Overall, it serves as a foundational component for applications that require caching capabilities.",
    "filename": "python/packages/autogen-core/src/autogen_core/_cache_store.py"
  },
  {
    "code": false,
    "content": "# Documentation for CancellationToken Class\n\n## Overview\nThe `CancellationToken` class provides a mechanism for managing the cancellation of asynchronous operations. It allows users to cancel pending tasks and register callbacks that will execute when a cancellation occurs. This is particularly useful in concurrent programming where operations may need to be aborted in response to certain conditions.\n\n## Class Definition\n\n### `__init__(self) -> None`\nThis is the constructor method that initializes an instance of the `CancellationToken` class. \n\n- **Attributes**:\n  - `_cancelled`: A boolean flag that indicates whether the token is in a cancelled state. It is initialized to `False`.\n  - `_lock`: A thread-safe lock that ensures that operations on shared state are synchronized across threads.\n  - `_callbacks`: A list that stores callback functions that should run when the token is cancelled.\n\n## Methods\n\n### `cancel(self) -> None`\nThis method is responsible for cancelling any pending asynchronous operations associated with the cancellation token.\n\n- **Functionality**:\n  - Acquires the lock to safely update the `_cancelled` attribute.\n  - If the token hasn't already been cancelled, it sets `_cancelled` to `True`.\n  - Iterates through any registered callbacks and executes them.\n\n### `is_cancelled(self) -> bool`\nThis method checks whether the cancellation token has been cancelled.\n\n- **Returns**: A boolean indicating the cancellation state of the token.\n- **Functionality**:\n  - Acquires the lock and returns the value of `_cancelled`.\n\n### `add_callback(self, callback: Callable[[], None]) -> None`\nThis method allows the user to register a callback function that will be executed upon cancellation of the token.\n\n- **Parameters**: \n  - `callback`: A function with no arguments that will be invoked when the token is cancelled.\n- **Functionality**: \n  - Acquires the lock to ensure thread-safe access.\n  - If the token is already cancelled, the callback is invoked immediately. Otherwise, it appends the callback to `_callbacks`.\n\n### `link_future(self, future: Future[Any]) -> Future[Any]`\nThis method binds an asynchronous `Future` object to the cancellation token, allowing for the future's cancellation when the token is cancelled.\n\n- **Parameters**: \n  - `future`: An instance of `Future`, representing a pending async operation.\n- **Returns**: The same `Future` object that was passed in, now linked to the cancellation token.\n- **Functionality**: \n  - Acquires the lock to check the cancellation state.\n  - If the token is already cancelled, the future is immediately cancelled.\n  - Otherwise, it defines a cancel function that calls `future.cancel()` and appends this function to `_callbacks`.\n\n## Use Cases\nThe `CancellationToken` class can be employed in various scenarios where async operations need to be managed, including:\n\n1. **Task Management**: In a multi-threaded or asynchronous application, tasks may need to be cancelled based on user input or other conditions. This class facilitates that.\n2. **Resource Cleanup**: By linking callbacks to the cancellation token, developers can ensure that necessary cleanup occurs whenever an operation is cancelled.\n3. **Responsive Interfaces**: For applications with GUI or user interaction, cancellation tokens enable a more responsive design by allowing long-running tasks to be aborted based on user actions.\n\n## Thread Safety\nThe class is designed to be thread-safe. The use of a lock (`_lock`) ensures that operations modifying the cancellation state and the callbacks list are safely executed in multi-threaded contexts. This is crucial to prevent race conditions that could lead to inconsistent states.\n\n## Conclusion\nThe `CancellationToken` class encapsulates a robust mechanism for the cancellation of asynchronous tasks and the handling of callback functions. By providing an easy-to-use interface for managing cancellation, it enhances the control developers have over asynchronous behavior in their applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/_cancellation_token.py"
  },
  {
    "code": false,
    "content": "# Closure Agent Documentation\n\nThis documentation provides an overview of the `ClosureAgent` class and related components defined in the source code. The `ClosureAgent` is designed to facilitate the creation of agents that handle messages using closures, allowing for a flexible and functional programming style within an agent-oriented framework.\n\n## Overview\n\nThe primary responsibility of the `ClosureAgent` is to encapsulate behavior defined in a closure (a callable function) that processes incoming messages. The agent framework leverages asyncio for asynchronous programming. The `ClosureAgent` defines methods for message handling and state management while allowing the customization of how different message types are handled.\n\n### Key Imports and Dependencies\n\nThe code begins by importing necessary modules and types, including:\n\n- `inspect`: For inspecting callable parameters.\n- `warnings`: To issue warnings when message types do not match expectations.\n- Type hints for robust type checking, enhancing the clarity and reliability of the code.\n- Various internal components from the same module, like `AgentId`, `BaseAgent`, `CancellationToken`, etc., which are used to define the core functionalities of the agents.\n\n## Type Handling\n\n### `get_handled_types_from_closure`\n\nThis utility function analyzes a closure to extract the types of messages it is designed to handle. It checks the signature of the closure to ensure it has the correct number of parameters and retrieves type hints for the message parameter and the return type. If the signature does not meet expectations, appropriate assertions raise errors.\n\n### Closure Context Protocol\n\nA `ClosureContext` protocol is defined that the closure must conform to. It guarantees that any implementing context will provide methods for sending and publishing messages, thereby providing structural integrity to the handling of messages within agents.\n\n## ClosureAgent Class\n\n### Initialization and Properties\n\n#### `__init__`\n\nThe constructor for the `ClosureAgent` initializes the agent with a description, a closure for message handling, and an optional policy for handling unknown message types. It retrieves the current agent runtime and ID from the `AgentInstantiationContext`. It also validates the closure's expected message types using `get_handled_types_from_closure`.\n\n- **`metadata`**: Provides descriptive metadata about the agent, including unique identifiers.\n- **`id`**: Returns the unique `AgentId` for the agent.\n- **`runtime`**: Returns the `AgentRuntime` instance within which the agent operates.\n\n### Message Handling\n\n#### `on_message_impl`\n\nThis method processes incoming messages. It checks whether the message type is included in the expected types. Depending on the configured `unknown_type_policy`, it either issues a warning, raises an exception, or ignores unrecognized message types. If recognized, it invokes the closure, passing itself, the message, and the context for further processing.\n\n### State Management\n\n#### `save_state` and `load_state`\n\nAs `ClosureAgent`s do not maintain internal state, both methods are implemented to reflect this. They return empty states and effectively do nothing when attempting to load state.\n\n## Agent Registration\n\n### `register_closure`\n\nThe `register_closure` class method allows users to register a `ClosureAgent` in a specified runtime. This method abstracts the complexity of closure registration under the hood, including subscriptions and message serialization.\n\n- Allows for setting an `unknown_type_policy`.\n- Defines whether to skip direct message subscriptions.\n- Accepts a description and subscriptions that the agent will use to handle messages.\n\n### Example Usage\n\nAn example usage of the `ClosureAgent` is provided in the comments, showing how to create an asynchronous message handling routine using a closure. This showcases the agent's capability to publish messages under specific conditions and retrieve them in a structured fashion.\n\n## Summary\n\nThe `ClosureAgent` class and its associated methods provide a flexible mechanism for creating message-handling agents using closures. It enhances the existing agent framework by enabling easier integration of functional programming patterns while ensuring strict type safety and clarity in message processing. This design promotes the efficient creation of agents that can easily adapt to various messaging scenarios, conforming to the agent-oriented architecture principles.",
    "filename": "python/packages/autogen-core/src/autogen_core/_closure_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Component Management System\n\n## Overview\n\nThis code defines a framework for managing components in a software system, utilizing `Pydantic` for validation and configuration. The primary purpose of this framework is to create, load, and handle configurations for various components, such as models and tools, in a structured and type-safe manner.\n\n## ComponentModel Class\n\nThe core of the framework revolves around the `ComponentModel` class, which is a `Pydantic` model representing the essential attributes of a component:\n\n- **provider**: A string indicating how the component can be instantiated.\n- **component_type**: An optional logical type of the component, which defaults to the provider's type if not specified.\n- **version**: The version of the component specification, defaulting to the library's current version if not provided.\n- **component_version**: The component\u2019s specific version, similarly defaulting when not specified.\n- **description**: A human-readable description of the component.\n- **label**: A human-readable label for the component that defaults to the provider's class name if omitted.\n- **config**: A dictionary holding the configuration parameters validated by `Pydantic`.\n\nThis model serves as a foundation for serializing and deserializing component configurations.\n\n## Well-Known Providers\n\nThe code defines a constant dictionary, `WELL_KNOWN_PROVIDERS`, mapping user-friendly names to their corresponding fully qualified class names. It facilitates the seamless instantiation of components using simple identifiers rather than complex module paths.\n\n## ComponentFromConfig Class\n\n`ComponentFromConfig` is a generic class designed for components that can be instantiated from configuration. Its key methods include:\n\n- **_from_config**: Instantiates a new component from a configuration object. It raises a `NotImplementedError`, indicating subclasses must implement this method.\n- **_from_config_past_version**: Similar to `_from_config`, but intended for older configurations. It checks the version and raises a `NotImplementedError` if not implemented for the specific component.\n\n## ComponentToConfig Class\n\nIn contrast, `ComponentToConfig` provides an interface for classes that can generate a configuration from their state. It has the following notable attributes and methods:\n\n- **component_type**: Class variable defining the logical type of the component.\n- **component_version**: Indicates the component's version, which should be updated when schema changes occur.\n- **_to_config**: Dumps the configuration required to create a component instance. This method also raises a `NotImplementedError` for implementation in subclasses.\n- **dump_component**: Converts the component to a `ComponentModel`, validating its state against the provided class variables and configuration.\n\nThis dual interface allows for both creating components from configurations and generating configurations from component instances.\n\n## ComponentLoader Class\n\nThe `ComponentLoader` class is responsible for loading components from a `ComponentModel`. Its main feature is the `load_component` method, which handles different types of input (both `ComponentModel` and dictionaries). This method performs several tasks:\n\n1. Importing the specified class based on the provider string.\n2. Validating the loaded component against `WELL_KNOWN_PROVIDERS`.\n3. Managing version compatibility and ensuring schemas are valid.\n4. Creating instances of components based on validated configurations.\n\nThe method enforces type safety, ensuring the loaded instance matches any expected types when necessary.\n\n## ComponentSchemaType Class\n\n`ComponentSchemaType` is an abstract class defining a required class variable, `component_config_schema`, which specifies a `Pydantic` model representing the component's configuration. It enforces conformity among subclasses, issuing warnings if the required variables are not defined.\n\n## ComponentBase and Component Class\n\nThe `ComponentBase` class extends basic functionalities from `ComponentToConfig`, `ComponentLoader`, and serves as a theoretical base for components.\n\nThe `Component` class combines multiple functionalities by inheriting from `ComponentFromConfig`, `ComponentSchemaType`, and others. It serves as the primary class from which concrete component implementations should be derived. Concrete implementations are expected to define their specific `component_config_schema` and `component_type`. Additionally, subclasses are required to implement the `_to_config` and `_from_config` methods.\n\n## Utility Functions\n\nTwo type-checking utility functions, `is_component_instance` and `is_component_class`, are defined to validate if an object is a component instance or if a class is a component class, respectively. These functions leverage `TypeGuard`, providing safe checks on component types at runtime.\n\n## Conclusion\n\nThis code provides a robust and flexible framework for defining, managing, and validating components within a software application. By utilizing Pydantic for configuration management and type safety, it ensures all component interactions are well-defined and maintainable. Developers can extend this framework by creating specific components that adhere to the defined structure and requirements, promoting modularity and reusability in their systems.",
    "filename": "python/packages/autogen-core/src/autogen_core/_component_config.py"
  },
  {
    "code": false,
    "content": "# Documentation for Logging Configuration\n\nThis section provides a high-level description of the logging configuration used in the code snippet. It defines various logger names utilized throughout the application, which are essential for tracking and debugging purposes.\n\n## Purpose of Logger Names\n\nThe defined logger names help categorize logging messages into different contexts. This segmentation is crucial for both developers and users to easily interpret log outputs, understanding where the messages originated from. Each logger serves a specific purpose, and proper logging practices can greatly enhance the maintainability of the code.\n\n## Root Logger\n\n```python\nROOT_LOGGER_NAME = \"autogen_core\"\n```\n\n### Description\nThe root logger serves as the primary logging point in the application. It aggregates all log messages generated within the core framework. Having a dedicated name for the root logger allows for consistent logging across various modules of the application under a unified namespace.\n\n### Usage\nBy naming this logger as \"autogen_core\", developers can configure multiple handlers and formatters under this root logger. This is particularly useful for setting logging levels or output destinations (e.g., console, files) for all log messages originating from the core functionalities of the application.\n\n## Event Logger\n\n```python\nEVENT_LOGGER_NAME = \"autogen_core.events\"\n```\n\n### Description\nThe event logger is specifically designed for structured event logging. This format is typically used to record significant application events, such as user actions or system events, allowing easier tracking of application behavior.\n\n### Usage\nUsing a distinct logger name like \"autogen_core.events\" enables filtering of logs related to events separately from other types of logs. It makes it easier to manage and analyze application behavior by allowing developers to focus on event-driven logging without mixing it with general runtime logs.\n\n## Trace Logger\n\n```python\nTRACE_LOGGER_NAME = \"autogen_core.trace\"\n```\n\n### Description\nThe trace logger is intended for more detailed, developer-oriented tracing of application execution. Logs generated by this logger do not need to conform to a structured format and are mainly aimed at developers when debugging.\n\n### Usage\nBy using \"autogen_core.trace\" as the logger name, developers can track specific operations or states within the application without introducing a structured logging format. This logger should be utilized for messages that are not expected to contribute to the logging output for end-users, but are crucial for debugging purposes.\n\n## Conclusion\n\nThis simple yet effective logging configuration allows for enhanced clarity and structure in logging practices. By defining specific logger names, the codebase facilitates better monitoring and debugging, ultimately leading to more maintainable and robust software. Each logger serves its own purpose, ensuring that log messages are appropriate for their intended audiences, whether that be end-users, administrators, or developers.",
    "filename": "python/packages/autogen-core/src/autogen_core/_constants.py"
  },
  {
    "code": false,
    "content": "# Documentation for Subscription Management Code\n\n## Overview\n\nThis module provides a subscription management system designed for interacting with agents in a context of event-driven or message-based architectures. The core functionality revolves around creating and managing subscriptions to certain \"topics\", allowing agents to perform tasks based on events related to these topics. The `DefaultSubscription` class acts as the primary means of handling subscriptions, while a couple of utility functions facilitate creating subscriptions effectively.\n\n## DefaultSubscription Class\n\n### Purpose\n\nThe `DefaultSubscription` class is a specialized type of subscription that is intended to simplify subscription management. It is particularly designed for applications that require a global scope for agents, meaning all agents can access the same topics without explicit bindings to specific instances.\n\n### Initialization\n\nThe class constructor accepts two optional parameters:\n- `topic_type` (default is \"default\"): The specific topic to which the agents will subscribe.\n- `agent_type`: An optional parameter that can either be a string representing the agent type or an instance of `AgentType`. If not provided, the method attempts to determine the agent type from the current `SubscriptionInstantiationContext`. If it cannot identify the agent type, it raises a `CantHandleException`.\n\n#### Code Snippet\n```python\ndef __init__(self, topic_type: str = \"default\", agent_type: str | AgentType | None = None):\n```\n\n### Exception Handling\n\nThe `DefaultSubscription` class is designed to raise a `CantHandleException` if the `agent_type` is not defined and the class is not instantiated correctly within a proper context. This highlights the importance of context during subscription instantiation.\n\n## Type Variables\n\n### BaseAgentType\n\nA generic type variable, `BaseAgentType`, is defined using `TypeVar`. It is bound to the `BaseAgent` class and is used to specify the type of agents that can be associated with a subscription. This enhances type safety and allows for flexible integration with different agent implementations.\n\n#### Code Snippet\n```python\nBaseAgentType = TypeVar(\"BaseAgentType\", bound=\"BaseAgent\")\n```\n\n## Overloaded Functions\n\n### default_subscription Function\n\nThe `default_subscription` function is overloaded to allow two different usages:\n1. Without parameters, it returns a callable that when invoked, produces a new `DefaultSubscription`.\n2. With a class parameter, it returns a subscription factory that constructs a `DefaultSubscription` for the specified agent class.\n\n### Function Implementation\n\nThe function uses `subscription_factory` to create a callable that can generate subscriptions. When called with a class, it utilizes the provided class to return a properly configured subscription.\n\n#### Code Snippet\n```python\ndef default_subscription(cls: Type[BaseAgentType] | None = None) -> ...\n```\n\n## Subscription Factory\n\n### Role of subscription_factory\n\nThe `subscription_factory` function is called within the `default_subscription` function to produce instances of the `DefaultSubscription`. This design encapsulates the logic for creating new subscriptions and abstracts it away from the user, making the API easier to work with.\n\n### Implementation\n\nThe factory method relies on a lambda function to call the `DefaultSubscription` constructor, providing it with the necessary parameters.\n\n#### Code Snippet\n```python\nreturn subscription_factory(lambda: [DefaultSubscription()])(cls)\n```\n\n## Creating Type-Specific Subscriptions\n\n### type_subscription Function\n\nThe `type_subscription` function enables users to create subscriptions that are specialized for a given `topic_type`. It returns a callable that produces a `DefaultSubscription` for the specified topic.\n\n### Usage Example\n\nThis function can be particularly useful for applications needing to handle various topics with similar agent behaviors, without having to redefine the subscription process for each topic.\n\n#### Code Snippet\n```python\ndef type_subscription(topic_type: str) -> Callable[[Type[BaseAgentType]], Type[BaseAgentType]]:\n```\n\n## Summary\n\nThis code encapsulates a structured and flexible approach to managing subscriptions in an agent-based architecture. The `DefaultSubscription` class serves as the foundation for creating subscriptions, while utility functions like `default_subscription` and `type_subscription` streamline the process of associating agents with events or messages. Proper error handling ensures that users are alerted to misconfigurations, maintaining the robustness of the subscription system. This modular design allows for easy integration and usage, making it user-friendly for developers working with event-driven patterns.",
    "filename": "python/packages/autogen-core/src/autogen_core/_default_subscription.py"
  },
  {
    "code": false,
    "content": "# Documentation for `DefaultTopicId` Class\n\n## Overview\n\nThe `DefaultTopicId` class is part of a messaging framework, extending the capabilities of the `TopicId` base class. It aims to simplify the process of assigning values to the `topic_id` and `source` fields of a message by providing sensible defaults. Specifically, the class is designed to operate within the context of a message handling system, offering dynamic value assignment depending on the execution context.\n\n## Class Definition\n\n### `DefaultTopicId`\n\nThis class serves to create a topic identifier with programmable options tailored for messaging tasks. When instantiated, the class sets defaults for the `type` and `source`, providing a more convenient setup for message publishing.\n\n#### Attributes\n\n- **type (str)**: A string that indicates the type of the topic where messages will be published. It defaults to `\"default\"` if not specified, making it suitable for general use cases.\n  \n- **source (str | None)**: This optional attribute indicates the source of the message being published. If not provided, the class attempts to extract the `agent_id` from the `MessageHandlerContext`, resulting in a contextually relevant source. If the class is instantiated outside of a message handling context, the source defaults to `\"default\"`.\n\n## Constructor\n\n### `__init__(self, type: str = \"default\", source: str | None = None) -> None`\n\nThe constructor initializes the `DefaultTopicId` instance and performs the following operations:\n\n1. **Type Parameter**: It receives an optional argument `type` which defaults to `\"default\"`. This determines the category of the messages intended for the topic.\n\n2. **Source Parameter**: It takes an optional `source` argument, which can be either a string or `None`. If it is passed as `None`, the constructor attempts to fetch the `agent_id` from `MessageHandlerContext`.\n\n3. **Context Handling**: The constructor includes a try-except block to manage the context:\n   - If the `MessageHandlerContext.agent_id()` call succeeds, it retrieves the agent's key to assign as a source.\n   - In cases where the current context does not support message handlers (i.e., when a `RuntimeError` occurs), it defaults the source to `\"default\"`.\n\n4. **Superclass Initialization**: After determining appropriate values for `type` and `source`, it calls the superclass (`TopicId`) constructor to initialize these attributes within the broader messaging framework.\n\n## Error Handling\n\nThe class incorporates basic error handling via a try-except block. It specifically addresses situations in which the instantiation occurs outside of a message handler's context by defaulting to the string `\"default\"`.\n\n## Purpose and Role\n\nThe `DefaultTopicId` class is essential in a messaging infrastructure where different agents may communicate through various topics. By centralizing default assignment and context-aware source determination, it simplifies the developer's workload. Developers can leverage this class to ensure that messages are correctly categorized and attributed without requiring extensive configuration during each publishing action.\n\n## Use Cases\n\n1. **Standard Messaging**: When building services that routinely publish messages to a defined topic, `DefaultTopicId` reduces boilerplate code associated with setting standard topic attributes.\n\n2. **Dynamic Contexts**: Applications that involve multiple message handlers can utilize this class to ensure that messages retain contextual information that varies based on the originating agent.\n\n3. **Fallback Mechanism**: In scenarios where proper context may be lost or inaccessible, the default values provide a fallback, ensuring the system remains robust and minimizes message delivery errors.\n\n## Summary\n\nIn summary, the `DefaultTopicId` class is a well-structured utility for managing topic identifiers in a messaging system, bridging dynamic contextual needs with practical defaults. Its design leverages Python's object-oriented features while providing thoughtful error handling and user-friendly defaults, making it a vital component in any agent-based communication strategy.",
    "filename": "python/packages/autogen-core/src/autogen_core/_default_topic.py"
  },
  {
    "code": false,
    "content": "# Documentation for `function_utils.py`\n\n## Overview\n\nThe `function_utils.py` module provides utility functions and data structures to facilitate the introspection and schema generation of callable functions in Python. It is especially useful for compatibility with the OpenAI API, allowing developers to define functions that can be used as tools with structured parameters and return types, leveraging Pydantic for validation and schema definition.\n\nThe module includes functions to extract type annotations, parameter details, and generate JSON schemas for functions. Additionally, it incorporates data models that encapsulate function properties in a format compatible with the OpenAI API.\n\n## Imports and Dependencies\n\nThe module relies on several standard and third-party libraries to deliver its functionality:\n\n- **Standard Libraries**:\n  - `inspect`: For inspecting live objects, retrieving signatures of callable objects.\n  - `typing` and `typing_extensions`: For type hints and type checking.\n\n- **Third-party Libraries**:\n  - `pydantic`: For data validation and settings management using Python type annotations.\n  - `logging`: For logging warnings and errors.\n\nThe module also creates several type aliases and utilities, which streamline type handling throughout the code.\n\n## Functions\n\n### `get_typed_signature`\n\n```python\ndef get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n```\n\n**Purpose**: Retrieves the signature of a callable with proper type annotations.\n\n**Arguments**:\n- `call`: The function for which the signature is retrieved.\n\n**Returns**: An `inspect.Signature` object that includes parameter names, types, and default values.\n\n### `get_typed_return_annotation`\n\n```python\ndef get_typed_return_annotation(call: Callable[..., Any]) -> Any:\n```\n\n**Purpose**: Obtains the return type annotation of a given callable.\n\n**Arguments**:\n- `call`: The function for which the return annotation is requested.\n\n**Returns**: The return annotation or `None` if not present.\n\n### `get_param_annotations`\n\n```python\ndef get_param_annotations(typed_signature: inspect.Signature) -> Dict[str, Union[Annotated[Type[Any], str], Type[Any]]]:\n```\n\n**Purpose**: Extracts the parameter type annotations from the provided function signature.\n\n**Arguments**:\n- `typed_signature`: The function's inspected signature.\n\n**Returns**: A dictionary mapping parameter names to their respective type annotations.\n\n### `get_required_params`\n\n```python\ndef get_required_params(typed_signature: inspect.Signature) -> List[str]:\n```\n\n**Purpose**: Identifies the required parameters of a function (those without default values).\n\n**Arguments**:\n- `typed_signature`: The signature of the inspected function.\n\n**Returns**: A list of required parameter names.\n\n### `get_default_values`\n\n```python\ndef get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]:\n```\n\n**Purpose**: Collects the default values for function parameters.\n\n**Arguments**:\n- `typed_signature`: The signature of the inspected function.\n\n**Returns**: A dictionary of parameter names mapped to their default values.\n\n## Data Models\n\n### `Parameters`\n\n```python\nclass Parameters(BaseModel):\n```\n\n**Description**: Represents the parameters of a function as defined by the OpenAI API, including their types and whether they are required.\n\n**Attributes**:\n- `type`: Fixed value \"object.\"\n- `properties`: A dictionary of properties defining parameter configurations.\n- `required`: A list indicating which parameters are required.\n\n### `Function`\n\n```python\nclass Function(BaseModel):\n```\n\n**Description**: Represents a function with its description, name, and parameters conforming to OpenAI API specifications.\n\n**Attributes**:\n- `description`: A description of the function.\n- `name`: The name of the function.\n- `parameters`: An instance of `Parameters`.\n\n### `ToolFunction`\n\n```python\nclass ToolFunction(BaseModel):\n```\n\n**Description**: Encompasses a function under tools per the OpenAI API, including its type.\n\n**Attributes**:\n- `type`: Fixed value \"function.\"\n- `function`: An instance of the `Function` class.\n\n## Schema Generation Functions\n\n### `get_function_schema`\n\n```python\ndef get_function_schema(f: Callable[..., Any], *, name: Optional[str] = None, description: str) -> Dict[str, Any]:\n```\n\n**Purpose**: Generates a JSON schema for a function aligned with OpenAI API standards, including validation of function annotations.\n\n**Arguments**:\n- `f`: The target function from which the schema is generated.\n- `name`: Optional parameter for explicitly naming the function.\n- `description`: A descriptive string for the function.\n\n**Returns**: A dictionary representing the JSON schema for the function, raising exceptions if required annotations are missing.\n\n### `get_parameter_json_schema`\n\n```python\ndef get_parameter_json_schema(k: str, v: Any, default_values: Dict[str, Any]) -> Dict[str, Any]:\n```\n\n**Purpose**: Constructs a JSON schema for a parameter of the function, incorporating its type and default value.\n\n**Arguments**:\n- `k`: The name of the parameter.\n- `v`: The type of the parameter.\n- `default_values`: A mapping of default parameter values.\n\n**Returns**: A dictionary representing the JSON schema of the parameter.\n\n## Additional Utilities\n\n### `normalize_annotated_type`\n\n```python\ndef normalize_annotated_type(type_hint: Type[Any]) -> Type[Any]:\n```\n\n**Purpose**: Normalizes types wrapped in `Annotated` to extract the inner type.\n\n**Arguments**:\n- `type_hint`: A type that may be an `Annotated`.\n\n**Returns**: The underlying type, without the `Annotated` wrapper.\n\n### `args_base_model_from_signature`\n\n```python\ndef args_base_model_from_signature(name: str, sig: inspect.Signature) -> Type[BaseModel]:\n```\n\n**Purpose**: Generates a Pydantic model from a function's signature, facilitating structured parameter validation.\n\n**Arguments**:\n- `name`: The desired name for the resulting model.\n- `sig`: The signature of the inspected function.\n\n**Returns**: A Pydantic `BaseModel` type representing the parameters of the function.\n\n## Conclusion\n\nThe `function_utils.py` module serves as a robust toolkit for introspecting and handling function signatures, particularly for use within the OpenAI API context. By utilizing the utility functions and data models defined within, developers can streamline the process of defining and validating callable interfaces in Python, making codebases cleaner and more maintainable. The integration of Pydantic ensures that the resulting schemas are consistently validated against defined types and rules.",
    "filename": "python/packages/autogen-core/src/autogen_core/_function_utils.py"
  },
  {
    "code": false,
    "content": "# Image Class Documentation\n\n## Overview\n\nThe `Image` class provides functionality to handle and manipulate images using the Python Imaging Library (PIL). It allows for the creation of image objects from various sources such as raw images, image files, and base64 encoded strings. Additionally, it supports conversion to base64 and OpenAI specific formats for seamless integration in web and chat applications.\n\n## Class Definition\n\n### Image Class\n\nThe `Image` class encapsulates an image and provides methods for image handling.\n\n#### Constructor\n\n- **`__init__(self, image: PILImage.Image)`**\n  - Initializes an instance of the `Image` class.\n  - Converts the input image to RGB format if it is not already in that format.\n  - **Parameters:**\n    - `image`: An instance of `PILImage.Image`.\n\n### Class Methods\n\n#### from_pil\n\n- **`from_pil(cls, pil_image: PILImage.Image) -> Image`**\n  - Creates an `Image` object from a given PIL image.\n  - **Parameters:**\n    - `pil_image`: An instance of `PILImage.Image`.\n  - **Returns:** An instance of `Image`.\n\n#### from_uri\n\n- **`from_uri(cls, uri: str) -> Image`**\n  - Constructs an `Image` object from a base64 encoded image URI.\n  - Validates the format of the URI before processing.\n  - **Parameters:**\n    - `uri`: A string representing the base64 encoded image.\n  - **Returns:** An instance of `Image`.\n  - **Raises:** `ValueError` if the URI format is incorrect.\n\n#### from_base64\n\n- **`from_base64(cls, base64_str: str) -> Image`**\n  - Constructs an `Image` object from a base64 string.\n  - **Parameters:**\n    - `base64_str`: A string containing base64 encoded image data.\n  - **Returns:** An instance of `Image`.\n\n#### from_file\n\n- **`from_file(cls, file_path: Path) -> Image`**\n  - Creates an `Image` object from a specified image file.\n  - **Parameters:**\n    - `file_path`: A `Path` object representing the image file path.\n  - **Returns:** An instance of `Image`.\n\n### Instance Methods\n\n#### to_base64\n\n- **`to_base64(self) -> str`**\n  - Converts the instance image to a base64 encoded string.\n  - **Returns:** A base64 encoded string of the image.\n\n#### _repr_html_\n\n- **`_repr_html_(self) -> str`**\n  - Provides an HTML representation of the image for display in Jupyter notebooks.\n  - **Returns:** An HTML string displaying the image.\n\n#### data_uri\n\n- **`data_uri(self) -> str`**\n  - A property that returns the base64 encoded image as a data URI for web use.\n  - **Returns:** A string in data URI format.\n\n#### to_openai_format\n\n- **`to_openai_format(self, detail: Literal[\"auto\", \"low\", \"high\"] = \"auto\") -> Dict[str, Any]`**\n  - Formats the image data into a structure compatible with OpenAI's API requirements.\n  - **Parameters:**\n    - `detail`: Specifies the quality of the image (\"auto\", \"low\", or \"high\").\n  - **Returns:** A dictionary containing the image information.\n\n### Pydantic Core Schema Integration\n\n#### __get_pydantic_core_schema__\n\n- **`__get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema`**\n  - Defines custom validation and serialization logic for integrating `Image` with Pydantic\u2019s core schema.\n  - **Returns:** A `core_schema.CoreSchema` object that handles validation and serialization.\n  - **Details:**\n    - **Validation Logic:** Validates input data, ensuring it is either a dictionary containing base64 data or an instance of `Image`.\n    - **Serialization Logic:** Converts an `Image` instance back to a dictionary including its base64 encoded data.\n\n## Helper Functions\n\n### _convert_base64_to_data_uri\n\n- **`_convert_base64_to_data_uri(base64_image: str) -> str`**\n  - Converts a base64 string into a data URI format, determining the MIME type based on the image content.\n  - **Parameters:**\n    - `base64_image`: A base64 encoded image string.\n  - **Returns:** A string formatted as a data URI.\n  - **Details:**\n    - Determines the MIME type by checking the initial bytes of the decoded image data, catering to JPEG, PNG, GIF, and WEBP formats. If unknown, it defaults to \"image/jpeg\".\n\n## Summary\n\nOverall, the `Image` class serves as a comprehensive solution for various image handling needs. It enables loading images from different sources, converting them to formats suitable for web use and APIs, and provides seamless integration with Pydantic for structured data validation and serialization. The additional functionalities such as converting images to base64 and constructing data URIs enhance its applicability in modern web applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/_image.py"
  },
  {
    "code": false,
    "content": "# Documentation for Intervention Handlers\n\n## Overview\n\nThis module provides a structure for handling messages within an agent-based architecture. It allows the implementation of intervention handlers that can modify, log, or drop messages processed by a runtime environment. It's designed to seamlessly integrate with the `SingleThreadedAgentRuntime` component of the `autogen_core.base` module.\n\nThe primary components of this module are:\n- `DropMessage`: A marker type for signaling that a message should not be processed further.\n- `InterventionHandler`: A protocol defining methods that intervention handlers must implement.\n- `DefaultInterventionHandler`: A class that provides a default no-op implementation of the `InterventionHandler`.\n\n## DropMessage Class\n\n```python\n@final\nclass DropMessage:\n    \"\"\"Marker type for signalling that a message should be dropped by an intervention handler. The type itself should be returned from the handler.\"\"\"\n    ...\n```\n\nThe `DropMessage` class serves as a unique type that indicates a message should be dropped during processing. It acts as a return type for methods in intervention handlers to signal that no further action should be taken on the message. By defining it as a marker type, it helps to clearly differentiate this behavior from other return values.\n\n## The InterventionHandler Protocol\n\n```python\nclass InterventionHandler(Protocol):\n    \"\"\"An intervention handler is a class that can be used to modify, log or drop messages that are being processed by the :class:`autogen_core.base.AgentRuntime`.\"\"\"\n    ...\n```\n\nThe `InterventionHandler` is a protocol that outlines the expected behavior of any class implementing it. This includes three specific asynchronous methods that must be defined:\n\n1. **on_send**:\n    - Triggered when a message is sent to an agent through the runtime's `send_message` method.\n  \n2. **on_publish**:\n    - Invoked when a message is published through the runtime's `publish_message` method.\n\n3. **on_response**:\n    - Called when a response is received from an agent's message handler.\n\nEach of these methods allows for the potential modification of the message being processed. Importantly, if an intervention handler returns `None`, it will emit a warning and not affect the message. Returning `DropMessage` is the appropriate way to indicate a message should be discarded.\n\n## DefaultInterventionHandler Class\n\n```python\nclass DefaultInterventionHandler(InterventionHandler):\n    \"\"\"Simple class that provides a default implementation for all intervention\n    handler methods, that simply returns the message unchanged. Allows for easy\n    subclassing to override only the desired methods.\"\"\"\n    ...\n```\n\nThe `DefaultInterventionHandler` class provides a straightforward implementation of the `InterventionHandler` protocol. It serves as a base class that simply returns the message unchanged for all intervention methods. This allows developers to extend this class and only override specific methods as needed, promoting code reuse and simplifying the creation of custom intervention handlers.\n\n### Methods in DefaultInterventionHandler\n\n1. **on_send**:\n    - Simply returns the message as-is.\n\n2. **on_publish**:\n    - Similar to `on_send`, it returns the published message without any modification.\n\n3. **on_response**:\n    - Returns the response message unaltered.\n\nBy inheriting from `DefaultInterventionHandler`, developers can create specialized handlers that implement only the logic they need, while the default behavior is handled for the methods they do not override.\n\n## Example Usage\n\nThe documentation includes an example of how to create a custom intervention handler by extending `DefaultInterventionHandler`.\n\n```python\n@dataclass\nclass MyMessage:\n    content: str\n\nclass MyInterventionHandler(DefaultInterventionHandler):\n    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:\n        if isinstance(message, MyMessage):\n            message.content = message.content.upper()\n        return message\n```\n\nIn this example, a `MyMessage` dataclass is defined, and `MyInterventionHandler` modifies the content of `MyMessage` instances to uppercase when sending. This showcases the flexibility of the framework, allowing for specific manipulations while retaining a base functionality.\n\n## Conclusion\n\nThis module provides a robust framework for message handling within the agent runtime environment, allowing for flexible customization of message processing through intervention handlers. By defining a clear protocol and a default implementation, it assists developers in easily creating and managing message flows within their applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/_intervention.py"
  },
  {
    "code": false,
    "content": "# Documentation of the MessageContext Class\n\n## Overview\n\nThe given code defines a class called `MessageContext` using Python's `dataclasses` module. This class serves to encapsulate the context information associated with a message in a communication system. The use of dataclasses simplifies the creation of classes by automatically generating special methods such as `__init__()` and `__repr__()`.\n\n## Class Definition: `MessageContext`\n\n### Attributes\n\nThe `MessageContext` class contains several attributes that define the properties of the message context. These attributes are as follows:\n\n- **sender** (`AgentId | None`):  \n  This attribute represents the identity of the sender of the message. It is of type `AgentId`, which is presumably defined elsewhere in the codebase. In scenarios where the sender is unknown or not applicable, this attribute can be set to `None`.\n\n- **topic_id** (`TopicId | None`):  \n  This attribute holds the identifier for the topic associated with the message. Similar to `sender`, it is of type `TopicId`, allowing for specific categorization of messages based on topics. It can also be `None` if the message does not pertain to any specific topic.\n\n- **is_rpc** (`bool`):  \n  This boolean attribute indicates whether the message is part of a Remote Procedure Call (RPC) operation. If `is_rpc` is `True`, it suggests that the message is being used in a context where a function call is made over a network, typically involving a request and a response.\n\n- **cancellation_token** (`CancellationToken`):  \n  This attribute is an instance of `CancellationToken`, which likely provides a mechanism to cancel asynchronous operations or message handling processes. This is useful in scenarios where the message processing needs to be aborted based on external conditions.\n\n- **message_id** (`str`):  \n  The `message_id` attribute is a unique identifier for the message itself. This string ensures messages can be tracked individually, which is helpful for logging, tracing, and debugging purposes.\n\n### Example Scenario\n\nThe `MessageContext` class could be utilized in a message-driven architecture where various agents communicate asynchronously. For instance, when an agent sends a request to another agent via a messaging system, an instance of `MessageContext` could be created to encapsulate all relevant details:\n\n1. The `sender` attribute would hold the identity of the requesting agent.\n2. The `topic_id` might characterize the type of request being sent.\n3. The `is_rpc` flag would be `True`, indicating that the request expects a response.\n4. A `CancellationToken` could be provided to allow the operation to be halted if needed.\n5. The `message_id` would uniquely identify the request for tracking.\n\n## Conclusion\n\nThe `MessageContext` class is integral in managing the flow and properties of messages within a messaging framework. By utilizing typed attributes, the class ensures that relevant metadata is encapsulated, enhancing both the clarity and functionality of the code. This class is a building block for more complex interactions between agents, particularly in systems implementing RPC or requiring cancellation mechanisms.",
    "filename": "python/packages/autogen-core/src/autogen_core/_message_context.py"
  },
  {
    "code": false,
    "content": "# Documentation for `MessageHandlerContext` Class\n\n## Overview\n\nThe `MessageHandlerContext` class is designed to manage the context for message handling in a thread-safe manner using Python's `contextvars`. It provides mechanisms for setting and retrieving the current agent context, ensuring that the agent ID is properly maintained during message processing. This class is static and cannot be instantiated directly.\n\n## Purpose\n\nThe primary role of the `MessageHandlerContext` class is to facilitate the management of a context variable related to an `AgentId`. This context variable allows various parts of the code to access the current agent ID during message handling operations without passing the agent ID explicitly through function parameters.\n\n## Class Properties\n\n- **Static Instantiation Block**: The constructor (`__init__`) raises a `RuntimeError`, indicating that the class is intended to be used statically. This serves as a protective measure to prevent its instantiation.\n\n- **Context Variable**: \n  - `_MESSAGE_HANDLER_CONTEXT`: This is a class-level variable of type `ContextVar`, used specifically for storing the current agent ID (of type `AgentId`) for the message handling context. ContextVars allow for maintaining values in an asynchronous environment, making it suitable for concurrent message processing.\n\n## Context Management\n\n### `populate_context` Method\n\n- **Signature**: `@classmethod @contextmanager def populate_context(cls, ctx: AgentId) -> Generator[None, Any, None]`\n  \n- **Purpose**: This method is a context manager that sets the current agent ID for the duration of a context block. It uses the `contextmanager` decorator from the `contextlib` module to manage the context.\n\n- **Functionality**:\n  1. It receives an `AgentId` instance (`ctx`) as an argument.\n  2. Sets the context variable `_MESSAGE_HANDLER_CONTEXT` to the provided `AgentId`.\n  3. It yields control back to the calling code, allowing the execution of the block within the context.\n  4. Upon exiting the block (in the `finally` clause), it resets the context variable to its previous state, ensuring that the context does not leak to other parts of the application.\n\n### Example Usage\n\nWhile the code itself does not provide an explicit example, users of the `MessageHandlerContext` class would typically wrap their message handling logic within the `populate_context` context manager to ensure that the agent's context is correctly set.\n\n```python\nwith MessageHandlerContext.populate_context(agent_id):\n    # Logic to handle messages, where agent_id is accessible\n```\n\n## Retrieval of Agent ID\n\n### `agent_id` Method\n\n- **Signature**: `@classmethod def agent_id(cls) -> AgentId`\n  \n- **Purpose**: This method retrieves the `AgentId` currently stored in the context.\n\n- **Functionality**:\n  1. It attempts to access the agent ID from `_MESSAGE_HANDLER_CONTEXT`.\n  2. If called outside of a properly set context (i.e., not within the `populate_context` block), it raises a `RuntimeError`, indicating that this call must occur within a message handler\u2019s context.\n\n## Error Handling\n\nThe class uses exception handling to ensure that attempts to access the agent ID outside of the intended context produce informative errors. This encourages correct usage patterns and reduces the likelihood of bugs related to context mismanagement.\n\nIn summary, the `MessageHandlerContext` class is an essential utility for managing agent context in asynchronous or multi-threaded message handling scenarios, providing a safe and structured way to handle contexts associated with different parts of an application.",
    "filename": "python/packages/autogen-core/src/autogen_core/_message_handler_context.py"
  },
  {
    "code": false,
    "content": "# Queue Implementation with Shutdown Support\n\nThis document provides a high-level overview of a custom queue implementation in Python, inspired by the asyncio queue module. It adds support for shutting down the queue, making it usable in versions prior to Python 3.13. Below, the structure and functionality of the code are detailed, including class definitions and methods.\n\n## Overview\n\nThe primary purpose of this code is to create a thread-safe asynchronous queue that allows for item storage and retrieval, along with controlled shutdown behavior. The queue is designed to work with asyncio's event loop while providing additional features like fault tolerance during shutdown. \n\nThe core components include:\n\n- **Class Definitions**: Specifically, the `Queue` class which encapsulates the queue behavior, and `_LoopBoundMixin` to ensure the queue is bound to the appropriate asyncio event loop.\n- **Error Handling**: Custom exceptions are raised under certain conditions, such as attempting operations on a shut-down queue.\n\n## Class: `_LoopBoundMixin`\n\n### Purpose\n\nThe `_LoopBoundMixin` class ensures that any instance of a subclass is associated with the correct asyncio event loop. This is crucial for maintaining consistency and correctness during async operations.\n\n### Key Method\n\n- **`_get_loop`**: This method retrieves the current running event loop. It ensures that the queue instance remains tied to the same loop throughout its lifetime. If an inconsistency is detected (i.e., binding to a different loop), it raises a `RuntimeError`.\n\n## Class: `Queue`\n\n### Purpose\n\nThe `Queue` class implements the main functionality of the queue, allowing for the asynchronous addition and removal of items. It also manages the state of the queue regarding whether it is shut down or has items still to process.\n\n### Attributes\n\n- **`_maxsize`**: Defines the maximum number of items allowed in the queue. A maxsize of `0` indicates an unlimited queue.\n- **`_getters` and `_putters`**: Two deques that manage waiting tasks for getting items from and putting items into the queue.\n- **`_unfinished_tasks`**: A counter for tracking the number of unfinished tasks linked to items in the queue.\n- **`_finished`**: An asyncio event used to signal when all tasks have been processed.\n- **`_queue`**: The core data structure (deque) that stores the actual items.\n- **`_is_shutdown`**: A boolean flag indicating whether the queue has been shut down.\n\n### Key Methods\n\n#### Initialization\n\n- **`__init__`**: Initializes a new queue, setting up its size, data structures, and a flag for shutdown status.\n\n#### Queue Operations\n\n- **`put`**: Asynchronously adds an item to the queue. If the queue is full, it waits until space is available, raising `QueueShutDown` if shut down during this wait.\n- **`put_nowait`**: Adds an item without blocking. Raises `QueueFull` if there is no space and raises `QueueShutDown` if the queue is shut down.\n\n- **`get`**: Asynchronously removes and returns an item. If the queue is empty and shut down, it raises `QueueShutDown`.\n- **`get_nowait`**: Works similarly to `get` but does not wait, raising `QueueEmpty` or `QueueShutDown` as needed.\n\n#### Utility Methods\n\n- **`qsize`**: Returns the number of items currently in the queue.\n- **`empty`**: Checks whether the queue is empty.\n- **`full`**: Determines if the queue has reached its max size.\n\n#### Task Management\n\n- **`task_done`**: Decrements the unfinished tasks counter. When all tasks are completed, it sets the finished event.\n- **`join`**: Asynchronously waits until all items have been processed (when unfinished tasks drop to zero).\n\n#### Shutdown Mechanism\n\n- **`shutdown`**: Safely shuts down the queue, setting the shutdown flag and waking up waiting tasks. If `immediate` is `True`, it immediately processes remaining items and marks them as done.\n\n## Error Handling\n\nThe `Queue` class raises specific exceptions such as `QueueShutDown`, `QueueFull`, and `QueueEmpty` under defined conditions. This ensures that calling code can appropriately handle errors related to the queue's state, which is especially critical when using it in asynchronous environments.\n\n## Final Remarks\n\nThis queue implementation serves as a robust structure for managing asynchronous tasks in a Python application. With particular emphasis on graceful shutdowns and error handling, it ensures that developers can utilize a queue effectively even when working with complex async tasks or when using features from older Python versions that lack built-in support for these functionalities.",
    "filename": "python/packages/autogen-core/src/autogen_core/_queue.py"
  },
  {
    "code": false,
    "content": "# Documentation for Message Handling Framework\n\n## Overview\n\nThis module provides a framework for handling messages in an asynchronous context using decorators. It comprises a `RoutedAgent` class and supports message routing based on type and specific matching criteria. The framework allows the registration of event and RPC message handlers, ensuring that messages are processed efficiently.\n\n## Key Components\n\n### MessageHandler Protocol\n\n```python\n@runtime_checkable\nclass MessageHandler(Protocol[AgentT, ReceivesT, ProducesT]):\n```\n\nThe `MessageHandler` protocol defines a contract for message handling functions. It specifies:\n- `target_types`: The types of messages that this handler can process.\n- `produces_types`: The types of messages that it can produce as a response.\n- `router`: A callable that determines whether this handler should process a given message.\n- A static `__call__` method that takes in an agent instance, a message, and a context, returning a response.\n\n### Decorators\n\n#### 1. message_handler\n\n```python\ndef message_handler(\n    func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None,\n    *,\n    strict: bool = True,\n    match: None | Callable[[ReceivesT, MessageContext], bool] = None\n) -> Callable[...]:\n```\n\n- **Purpose**: This decorator is designed to mark methods in a `RoutedAgent` class that handle both event and RPC messages.\n- **Parameters**:\n  - `func`: The function to decorate.\n  - `strict`: A flag indicating whether strict type checking should be enforced.\n  - `match`: Optional callable for secondary routing.\n- **Functionality**: Validates the method's signature, including the message and return types, and wraps the function to include message passing logic.\n\n#### 2. event\n\n```python\ndef event(\n    func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]] = None,\n    *,\n    strict: bool = True,\n    match: None | Callable[[ReceivesT, MessageContext], bool] = None\n) -> Callable[...]:\n```\n\n- **Purpose**: This decorator is specifically for methods that handle event messages.\n- **Parameters**: Similar to `message_handler`, but assumes that the handler returns `None`.\n- **Functionality**: Enforces that the decorated method adheres to the signature required for handling event messages.\n\n#### 3. rpc\n\n```python\ndef rpc(\n    func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None,\n    *,\n    strict: bool = True,\n    match: None | Callable[[ReceivesT, MessageContext], bool] = None\n) -> Callable[...]:\n```\n\n- **Purpose**: A decorator for methods dealing with RPC messages.\n- **Parameters**: Same as `event` and `message_handler`, but allows for a variety of return types.\n- **Functionality**: The logic is similar to `message_handler`, with a focus on request-response communication.\n\n### RoutedAgent Class\n\n```python\nclass RoutedAgent(BaseAgent):\n```\n\nThe `RoutedAgent` class extends `BaseAgent` and serves as the core for routing messages to their respective handlers.\n\n#### Constructor\n\n```python\ndef __init__(self, description: str) -> None:\n```\n\n- **Description**: Initializes the agent and discovers its message handlers. It maps target message types to registered handlers.\n\n#### Message Handling\n\n```python\nasync def on_message_impl(self, message: Any, ctx: MessageContext) -> Any | None:\n```\n\n- **Purpose**: Handles incoming messages by routing them to the appropriate handler based on the message type.\n- **Functionality**: It checks for the existence of handlers for the message type and invokes the first one that matches based on the given conditions.\n\n#### Unhandled Messages\n\n```python\nasync def on_unhandled_message(self, message: Any, ctx: MessageContext) -> None:\n```\n\n- **Purpose**: Handles cases where no appropriate handler is found for a received message. It logs a message to inform that the message was unhandled.\n\n#### Discover Handlers\n\n```python\n@classmethod\ndef _discover_handlers(cls) -> Sequence[MessageHandler[Any, Any, Any]]:\n```\n\n- **Purpose**: Scans the class for methods decorated as message handlers, returning a list of them.\n\n#### Handling Types\n\n```python\n@classmethod\ndef _handles_types(cls) -> List[Tuple[Type[Any], List[MessageSerializer[Any]]]]:\n```\n\n- **Purpose**: Gathers types of messages the class can handle and their associated serializers.\n- **Functionality**: Raises an error if any type lacks a defined serializer, ensuring robust serialization capabilities.\n\n## Conclusion\n\nThe framework implemented in this module provides an efficient and type-safe way to handle multiple types of messages in an asynchronous manner. It encourages good design practices through the use of decorators, type annotations, and structured message handling. The `RoutedAgent` class effectively manages the routing of messages to their corresponding handlers while logging appropriate warnings for unhandled cases or type mismatches.",
    "filename": "python/packages/autogen-core/src/autogen_core/_routed_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation for the SubscriptionManager Module\n\nThis module is responsible for managing subscriptions to various topics in a system where agents can subscribe to events or data. It includes functionality for adding, removing, and querying subscriptions, as well as handling notifications to subscribed agents.\n\n## Overview\n\nThe module defines an asynchronous function, `get_impl`, and a class, `SubscriptionManager`. The function is used for retrieving an agent's ID based on its type or identifier, while the class encapsulates the logic for managing subscriptions and their associated recipients.\n\n## Function: `get_impl`\n\n### Purpose\n`get_impl` is an asynchronous function designed to obtain an `AgentId` based on the input, which can be an `AgentId`, `AgentType`, or a string identifier. It also allows searching for the agent's instance via an `instance_getter` function.\n\n### Parameters\n- `id_or_type`: The identifier of the agent that can be of type `AgentId`, `AgentType`, or `str`.\n- `key`: A string representing a key associated with the agent.\n- `lazy`: A boolean indicating whether to lazily fetch the agent instance.\n- `instance_getter`: A callable that takes an `AgentId` and returns an `Awaitable` of type `Agent`.\n\n### Behavior\n1. If the `id_or_type` input is an instance of `AgentId`, it optionally retrieves the agent instance using `instance_getter` if `lazy` is `False`.\n2. If `id_or_type` is a string or an `AgentType`, it creates an `AgentId` using this information and retrieves the agent accordingly if not lazy.\n3. It returns the `AgentId`.\n\n## Class: `SubscriptionManager`\n\n### Overview\nThe `SubscriptionManager` class manages the subscriptions to various topics by maintaining a list of subscriptions and a mapping of topics to agents subscribed to them.\n\n### Attributes\n- `_subscriptions`: A list storing all active subscriptions.\n- `_seen_topics`: A set containing unique topic identifiers to ensure that topics are processed only once.\n- `_subscribed_recipients`: A default dictionary that maps `TopicId` to lists of `AgentId` representing agents subscribed to each topic.\n\n### Property: `subscriptions`\nThe `subscriptions` property returns the current list of subscriptions. It provides an interface to access subscription information without exposing the internal data structure directly.\n\n## Methods\n\n### `add_subscription`\n#### Purpose\nAdds a new subscription to the list of subscriptions.\n\n#### Parameters\n- `subscription`: An instance of `Subscription` to be added.\n\n#### Behavior\n- Checks if the subscription already exists and raises a `ValueError` if it does.\n- Appends the new subscription to `_subscriptions`.\n- Calls `_rebuild_subscriptions` to update `_subscribed_recipients`.\n\n### `remove_subscription`\n#### Purpose\nRemoves an existing subscription based on its identifier.\n\n#### Parameters\n- `id`: A string identifier for the subscription to be removed.\n\n#### Behavior\n- Checks if the subscription exists; raises `ValueError` if it doesn't.\n- Uses a filter to construct a new list of subscriptions excluding the removed one.\n- Calls `_rebuild_subscriptions` to refresh the recipients' mapping.\n\n### `get_subscribed_recipients`\n#### Purpose\nRetrieves the list of agents subscribed to a specific topic.\n\n#### Parameters\n- `topic`: A `TopicId` representing the topic of interest.\n\n#### Behavior\n- Checks if the topic is already in `_seen_topics`. If not, it builds for the new topic.\n- Returns a list of `AgentId` associated with that topic.\n\n### Private Method: `_rebuild_subscriptions`\n#### Purpose\nRebuilds the mapping of subscribed recipients based on current subscriptions.\n\n#### Parameters\n- `topics`: A set of `TopicId` that has been seen.\n\n#### Behavior\n- Clears the current mapping of `_subscribed_recipients`.\n- Iterates over the provided topics and builds the mapping for new and existing subscriptions.\n\n### Private Method: `_build_for_new_topic`\n#### Purpose\nBuilds the mapping of recipients for a new topic based on current subscriptions.\n\n#### Parameters\n- `topic`: A `TopicId` representing the new topic.\n\n#### Behavior\n- Adds the topic to `_seen_topics`.\n- Iterates through the subscriptions to find matches and maps them to agents using the `map_to_agent` method from the `Subscription` class.\n\n## Summary\nThis module effectively manages subscriptions, ensuring that agents can subscribe to relevant topics while providing an efficient mechanism to update and query these subscriptions. The use of asynchronous programming allows it to handle potentially I/O-bound operations without blocking execution. The `get_impl` function serves as a utility for managing agent identifiers, making it easier to fetch agents based on their IDs or types.",
    "filename": "python/packages/autogen-core/src/autogen_core/_runtime_impl_helpers.py"
  },
  {
    "code": false,
    "content": "# Serialization Framework Documentation\n\nThis documentation provides a high-level overview of a serialization framework designed for handling different data formats, specifically JSON and Protocol Buffers, using data classes and Pydantic models in Python.\n\n## Overview\n\nThe primary purpose of this code is to define a set of serializers that can convert Python data structures, represented as either Python data classes or Pydantic models, into JSON or Protocol Buffers (protobuf) formats, and vice versa. It leverages Python's type hints, dataclass functionality, and Pydantic validation to ensure that the serialization and deserialization processes are reliable and type-safe.\n\n## Protocol Definitions\n\n### MessageSerializer Protocol\n\nThe `MessageSerializer` protocol defines a framework for creating specific serializers for different types of messages:\n\n- **Properties**:\n  - `data_content_type`: Returns the MIME type for the serialized data.\n  - `type_name`: Returns the type name of the message that the serializer handles.\n\n- **Methods**:\n  - `deserialize(self, payload: bytes) -> T`: Converts the byte payload into an instance of type `T`.\n  - `serialize(self, message: T) -> bytes`: Converts an instance of type `T` into a byte payload.\n\nThese methods lay the groundwork for any serializer that needs to process messages in a consistent manner.\n\n### IsDataclass Protocol\n\nThe `IsDataclass` protocol is used to check if a given class is a dataclass. It relies on the presence of the `__dataclass_fields__` attribute, which is unique to dataclasses in Python.\n\n## Type Checking Functions\n\n### is_dataclass\n\nThis function checks if a class is a dataclass based on the existence of the `__dataclass_fields__` attribute. This function is utilized by other type-checking functions to ensure compliance with the serialization constraints.\n\n### has_nested_dataclass\n\nDetermines whether a dataclass contains any nested dataclass fields. If such fields are found, the frame will raise an error, as nested data structures are not supported by the current serializer design.\n\n### contains_a_union\n\nThis function checks whether a dataclass has any fields with union types. The framework does not support union types directly, preferring Pydantic models instead.\n\n### has_nested_base_model\n\nThe function checks if a dataclass or its fields contain any nested Pydantic models. This ensures that the serializer adheres to the guidelines regarding supported data structures.\n\n## Serializer Implementations \n\n### DataclassJsonMessageSerializer\n\nThis serializer handles dataclasses, transforming them to and from JSON:\n\n- **Deserialization**: Converts a byte payload into a dataclass instance by decoding the bytes and using `json.loads`.\n- **Serialization**: Converts a dataclass instance into bytes via `json.dumps` and `asdict`.\n\nThe serializer raises exceptions if the dataclass has union types or nested dataclasses.\n\n### PydanticJsonMessageSerializer\n\nThis serializer is specifically designed for Pydantic models:\n\n- **Deserialization**: Validates JSON strings against the model using `model_validate_json`.\n- **Serialization**: Serializes Pydantic models to JSON strings using `model_dump_json`.\n\nThis pattern allows extensive validation and offers advanced features provided by Pydantic.\n\n### ProtobufMessageSerializer\n\nThis serializer is built for Google Protocol Buffers, allowing for serialization into the `google.protobuf.Any` type:\n\n- **Deserialization**: Uses Protobuf's unpacking mechanism to decode and validate incoming byte payloads.\n- **Serialization**: Packs messages into `Any` and converts them into byte strings.\n\nThis serializer allows seamless integration with systems that utilize Protocol Buffers for message passing.\n\n## UnknownPayload Dataclass\n\nThe `UnknownPayload` class is a simple container for storing messages that couldn't be mapped to any known serializer. It captures the type name, data content type, and raw payload to support debugging and error-handling scenarios.\n\n## Type Name Resolution\n\nThe function `_type_name` resolves the name of a class or Protobuf message to ensure that the correct identifiers are used when communicating type information during serialization and deserialization.\n\n## Serialization Registry\n\n### SerializationRegistry Class\n\nThe core of the serializer management is encapsulated in the `SerializationRegistry` class:\n\n- Stores a mapping of type names and content types to their associated serializers.\n- Allows addition of serializers for different message types.\n- Provides methods for serialization and deserialization by looking up the appropriate serializer.\n- Includes checks for whether a type is registered, ensuring that operations are safe and effective.\n  \nUpon encountering an unknown type, the registry returns an `UnknownPayload`.\n\n## Conclusion\n\nThis serialization framework offers a robust, type-safe solution for converting between Python classes and various serialized formats, primarily focusing on JSON and Protocol Buffers. By ensuring strict adherence to type definitions and encapsulating serialization strategies within dedicated classes, the framework promotes clear separation of concerns and extensibility.",
    "filename": "python/packages/autogen-core/src/autogen_core/_serialization.py"
  },
  {
    "code": false,
    "content": "# Documentation for `SingleThreadedAgentRuntime`\n\n## Overview\nThe `SingleThreadedAgentRuntime` class provides a runtime environment for managing agents within an asynchronous Python application. It handles message processing using a single asyncio queue and processes each message concurrently by creating individual async tasks. This implementation is suitable for development or standalone applications but is not optimal for high-throughput scenarios.\n\n## Key Components\n### Data Classes\n1. **PublishMessageEnvelope**\n   - Represents a message meant for publishing to all agents registered for a specific topic. It includes details such as the message content, sender, cancellation token, and metadata.\n   \n2. **SendMessageEnvelope**\n   - Represents a message sent to a specific agent. Contains the message content, sender, recipient, and related metadata.\n\n3. **ResponseMessageEnvelope**\n   - Used to encapsulate responses sent back from one agent to another. It includes details about the original sender, intended recipient, and the message content.\n\n### Class: `RunContext`\n- Manages the lifecycle of the runtime, facilitating starting and stopping the message processing loop.\n- Ensures running tasks are properly handled and allows the runtime to stop based on specific conditions.\n\n## Main Class: `SingleThreadedAgentRuntime`\n### Constructor\n- Initializes various components such as message queues, agent factories, and intervention handlers.\n- Accepts optional parameters for intervention handlers and tracer providers to manage telemetry.\n\n### Properties\n- **unprocessed_messages_count**: Returns the count of messages currently in the processing queue.\n- **_known_agent_names**: Provides a set of known agent types registered within the runtime.\n\n### Message Processing Methods\n1. **send_message**: Sends a direct message to a specified recipient. Utilizes a future to handle the asynchronous nature of the response.\n   \n2. **publish_message**: Publishes a message to all agents subscribed to a specific topic. Supports intervention handlers to modify messages before they are sent out.\n\n3. **process_next**: Processes the next message in the queue. This function handles sending, publishing, and responding according to the type of message envelope.\n\n4. **_process_send**, **_process_publish**, and **_process_response**: Private methods designed to handle each respective message type's processing logic.\n\n### Lifecycle Management\n- **start**: Begins the message processing loop as a background task.\n- **stop**, **stop_when_idle**, **stop_when**: Methods for stopping the runtime either immediately or when certain conditions are met.\n- **close**: Ensures all instantiated agents are closed correctly when stopping the runtime.\n\n## Agent Management\n### Agent Registration\n- **register_factory**: Allows the registration of agent factories to create instances of agents. \n- **register_agent_instance**: Directly registers instantiated agent objects to the runtime.\n\n### State Management\n- **save_state**: Saves the current state of all instantiated agents as a mapping.\n- **load_state**: Loads the state of agents from a provided state mapping, allowing for persistence of agent data across runtime instances.\n\n## Telemetry and Debugging\n### Tracing\n- Integrated with OpenTelemetry for tracing message interactions, including sending, publishing, and handling responses.\n- Utilizes logging for monitoring message flows and handling exceptions during processing.\n\n### Exception Handling\nThe runtime employs careful exception handling. If errors occur during message processing, they are logged, and specific actions can be taken based on unhandled exceptions.\n\n## Usage Examples\nThe documentation provides two examples showcasing the binding of agents to the runtime and sending and publishing messages. Both examples highlight the simplicity of utilizing the `SingleThreadedAgentRuntime` for message-driven interactions in an asyncio context.\n\n### Example 1: Sending a Message\n```python\nasync def main() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    await MyAgent.register(runtime, \"my_agent\", lambda: MyAgent(\"My agent\"))\n    \n    runtime.start()\n    await runtime.send_message(MyMessage(\"Hello, world!\"), recipient=AgentId(\"my_agent\", \"default\"))\n    await runtime.stop()\n\nasyncio.run(main())\n```\n\n### Example 2: Publishing a Message\n```python\nasync def main() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    await MyAgent.register(runtime, \"my_agent\", lambda: MyAgent(\"My agent\"))\n\n    runtime.start()\n    await runtime.publish_message(MyMessage(\"Hello, world!\"), DefaultTopicId())\n    await runtime.stop_when_idle()\n\nasyncio.run(main())\n```\n\n## Conclusion\nThe `SingleThreadedAgentRuntime` class serves as a foundational building block for orchestrating message interactions among agents in an asynchronous environment. Its straightforward architecture facilitates easy integration and testing, making it a suitable choice for developers working in agent-based applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py"
  },
  {
    "code": false,
    "content": "# Documentation for Subscription Protocol\n\n## Overview\n\nThe provided code snippet defines a `Subscription` protocol, which is used to represent subscriptions in a messaging or event-driven system. Each subscription indicates a specific topic or set of topics that an agent (a component or entity in the system) is interested in receiving messages or notifications about. The `Subscription` interface outlines the expected behavior and properties that any concrete implementation must adhere to.\n\n## Protocol Definition\n\nThe `Subscription` is defined using a `Protocol` from the `typing` module, allowing for structural subtyping. This means that any class implementing the required methods and properties of the `Subscription` protocol will be recognized as a valid subscription, regardless of its inheritance.\n\n### Properties and Methods\n\n#### ID Property\n\n```python\n@property\ndef id(self) -> str:\n```\n\n- **Purpose**: This property is expected to return a unique identifier (ID) for each subscription.\n- **Implementation**: Implementations should typically use a UUID (Universally Unique Identifier) format for the ID.\n- **Returns**: A string that uniquely identifies the subscription.\n\n#### Equality Method\n\n```python\ndef __eq__(self, other: object) -> bool:\n```\n\n- **Purpose**: This method checks if two subscription instances are equal.\n- **Arguments**: Takes one argument, `other`, which represents another subscription instance for comparison.\n- **Returns**: A boolean value indicating whether the subscriptions are considered equal. It returns `True` if both subscriptions have the same ID and `False` otherwise.\n\n#### Match Method\n\n```python\ndef is_match(self, topic_id: TopicId) -> bool:\n```\n\n- **Purpose**: This method determines if the given `topic_id` matches the subscription.\n- **Arguments**: Accepts a `TopicId` object as an argument to check against the subscription's criteria.\n- **Returns**: A boolean indicating whether the `topic_id` is applicable for this subscription.\n\n#### Mapping Method\n\n```python\ndef map_to_agent(self, topic_id: TopicId) -> AgentId:\n```\n\n- **Purpose**: This method maps a `topic_id` to a specific agent ID, which indicates the agent designated to handle the messages related to that topic.\n- **Arguments**: Takes a `TopicId` object as input, which should have matched with the subscription.\n- **Returns**: An `AgentId` object representing the ID of the agent responsible for handling the topic.\n- **Exceptions**: Raises a `CantHandleException` if the subscription is unable to handle the provided `topic_id`.\n\n## Helper Aliases\n\n```python\nUnboundSubscription = Callable[[], list[Subscription] | Awaitable[list[Subscription]]]\n```\n\n- **Purpose**: Defines a helper type alias called `UnboundSubscription` which represents a callable (such as a function or lambda) that, when invoked, returns a list of `Subscription` objects or an `Awaitable` that resolves to such a list. This flexibility allows for both synchronous and asynchronous contexts when retrieving subscriptions.\n\n## Use Case\n\nThis `Subscription` protocol is likely part of a broader event-driven architecture. Agents may register their interests in various topics, and the protocol ensures a consistent way of handling these subscriptions. The methods outlined in the protocol allow agents to react appropriately when messages related to their subscribed topics are published, creating a decoupled and scalable messaging system.\n\n## Summary\n\n- **Subscription Protocol**: Defines an interface for managing subscriptions to topics.\n- **ID Property**: Provides a unique identifier for a subscription.\n- **Equality Method**: Facilitates the comparison of subscriptions based on their IDs.\n- **Match Method**: Checks compatibility between a subscription and a given topic.\n- **Mapping to Agents**: Connects topics to the corresponding handlers.\n- **Helper Type Alias**: Supports flexible subscription retrieval, accommodating both synchronous and asynchronous operations.\n\nThis design encourages modularity and interoperability in systems where agents need to exchange messages based on predefined topics, catering to various applications including chat systems, event processing, and notification services.",
    "filename": "python/packages/autogen-core/src/autogen_core/_subscription.py"
  },
  {
    "code": false,
    "content": "# Documentation for `SubscriptionInstantiationContext`\n\nThe `SubscriptionInstantiationContext` class is designed to manage the context in which subscriptions are instantiated in an agent-based system. It prevents direct instantiation and provides a structured way to maintain the relevant state using context variables. Below is a high-level overview of its components and functionality.\n\n## Class Overview\n\n### `SubscriptionInstantiationContext`\n\nThis class is static and cannot be instantiated. It serves primarily as a utility for managing context during subscription operations related to agents.\n\n### Purpose\n\nThe main purpose of `SubscriptionInstantiationContext` is to ensure that certain operations involving agents occur within a controlled context. This could be crucial for maintaining state or managing resources effectively, particularly in systems that handle multiple agents.\n\n## Core Components\n\n### 1. Constructor\n\n```python\ndef __init__(self) -> None:\n    raise RuntimeError(...)\n```\n\n- **Description**: The constructor of `SubscriptionInstantiationContext` prevents instantiation by raising a `RuntimeError`. This enforces the static nature of the class.\n- **Purpose**: Ensures that this class is used solely for its context management methods without allowing the creation of class instances.\n\n### 2. Class Variable: `_SUBSCRIPTION_CONTEXT_VAR`\n\n```python\n_SUBSCRIPTION_CONTEXT_VAR: ClassVar[ContextVar[AgentType]]\n```\n\n- **Description**: This is a `ContextVar` that holds the context for the subscription. `ContextVar` allows for maintaining context across asynchronous tasks or different threads without interference.\n- **Purpose**: To enable the storage and management of the current agent type in a thread-safe manner, which is essential during subscription operations.\n\n### 3. Method: `populate_context`\n\n```python\n@classmethod\n@contextmanager\ndef populate_context(cls, ctx: AgentType) -> Generator[None, Any, None]:\n    ...\n```\n\n- **Description**: This class method is a context manager that sets the `_SUBSCRIPTION_CONTEXT_VAR` with the provided `ctx` of type `AgentType`. It yields control back to the caller, allowing code to be executed within the defined context.\n- **Purpose**: To ensure that the context variable is correctly set before executing operations requiring that context and to clean up (reset) once the operations are complete.\n\n### 4. Method: `agent_type`\n\n```python\n@classmethod\ndef agent_type(cls) -> AgentType:\n    ...\n```\n\n- **Description**: This class method retrieves the currently set agent type from `_SUBSCRIPTION_CONTEXT_VAR`. If no context is set, it raises a `RuntimeError`.\n- **Purpose**: To access the agent type within the context that was previously populated. The exception handling ensures that the developer is informed if they attempt to access the agent type outside of a valid context.\n\n## Error Handling\n\nThe code includes error handling in two primary areas:\n\n1. **Constructor**: Prevents instantiation with a clear error message.\n2. **`agent_type` Method**: Raises a runtime error if an attempt to access the agent type is made outside the appropriate context, encouraging the use of the context manager for proper setup.\n\n## Usage and Context\n\nIn a broader application, `SubscriptionInstantiationContext` would likely be used in conjunction with an `AgentRuntime` class. The `populate_context` method would be invoked prior to creating or manipulating the agent instances, ensuring the correct agent type context is always available when needed.\n\n### Significance in Agent Frameworks\n\nThe design directly addresses complexities that come with managing state in concurrent or asynchronous environments, particularly important in agent-based systems that may spawn multiple agents dynamically. By centralizing context management, this class helps maintain a consistent state and prevents common errors related to context misuse.\n\n## Conclusion\n\nOverall, the `SubscriptionInstantiationContext` class offers a robust framework for managing subscription instantiation contexts in agent systems. Its enforced static nature, combined with careful context management through `ContextVar`, makes it an essential utility for developers working with agents in dynamic environments.",
    "filename": "python/packages/autogen-core/src/autogen_core/_subscription_context.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code is a module that imports various functionalities from other modules within the same package. It primarily focuses on telemetry and tracing aspects, likely related to monitoring or debugging within a software system. The use of `__all__` indicates that this module is designed to define a public API for what can be exported when the module is imported elsewhere.\n\n## Imports\n\nThe first section of the code consists of three import statements that bring in specific functions and classes from other modules: \n\n1. **Tracing Functions**:\n   - `trace_create_agent_span`\n   - `trace_invoke_agent_span`\n   - `trace_tool_span`\n\n   These functions likely help in establishing and managing tracing spans when an agent or tool is invoked, which aids in performance monitoring and debugging by capturing context-specific information.\n\n2. **Propagation and Metadata**:\n   - `EnvelopeMetadata`\n   - `TelemetryMetadataContainer`\n   - `get_telemetry_envelope_metadata`\n   - `get_telemetry_grpc_metadata`\n\n   This group is concerned with metadata propagation. `EnvelopeMetadata` and `TelemetryMetadataContainer` likely encapsulate metadata structures for telemetry data, while the `get_...` functions are probably utility functions to retrieve this metadata for telemetry purposes, including gRPC (a high-performance RPC framework).\n\n3. **Tracing Utilities**:\n   - `TraceHelper`\n   - `MessageRuntimeTracingConfig`\n\n   The `TraceHelper` class or function may contain utility methods to assist in setting up or managing traces. `MessageRuntimeTracingConfig` presumably holds configuration details related to tracing, allowing for customization of tracing behavior during message runtime.\n\n## Public API Definition\n\nThe `__all__` list is defined to specify the public API of this module. This is essential when using the `from module import *` syntax, as it dictates what is exported. The included elements are:\n\n- `EnvelopeMetadata`\n- `get_telemetry_envelope_metadata`\n- `get_telemetry_grpc_metadata`\n- `TelemetryMetadataContainer`\n- `TraceHelper`\n- `MessageRuntimeTracingConfig`\n- `trace_create_agent_span`\n- `trace_invoke_agent_span`\n- `trace_tool_span`\n\nThese components collectively demonstrate the module\u2019s focus on telemetry tracking and tracing capabilities.\n\n## Conclusion\n\nIn summary, this code serves as an integration point for telemetry and tracing functionalities within a larger software system. By importing specific utilities and metadata handlers and explicitly defining a public API, it allows for better monitoring and debugging of agent and tool interactions in the application, enhancing observability and performance analysis. The presence of classes and functions suggests that the module is designed to work seamlessly with other parts of a tracing infrastructure, potentially supporting complex telemetry operations.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe code defines a constant variable `NAMESPACE` with the value `\"autogen\"`. This is a simple yet essential line of code that serves to standardize a naming or organizational convention within a larger program.\n\n## Purpose of NAMESPACE\nThe constant `NAMESPACE` can be used throughout the application to group and identify related functionalities, classes, or variables under a common namespace. This is particularly useful in larger codebases where different modules or packages may define similar names. By using a distinctive namespace, developers can avoid naming conflicts and allow for more organized code.\n\n## Potential Use Cases\n1. **Module Identification**: The namespace can be utilized to clearly delineate which module or component of the application a certain function, class, or variable belongs to.\n  \n2. **Code Organization**: By prefixing function names or class names with the namespace (e.g., `autogen_functionName`), developers can quickly understand the context in which a particular piece of code operates.\n\n3. **Dynamic Loading**: In frameworks that support dynamic module loading, the namespace could help dynamically identify and load the relevant components.\n\n4. **Configuration Management**: The namespace could also aid in managing configurations that are specific to autogenerated features, especially when working with tools that manipulate code or generate content.\n\n## Example Scenarios\n- If developing a web application, possible classes could be named as `autogen_User`, `autogen_DBHandler`, etc., ensuring that their purpose is clear and preventing clashes with similarly named entities in other parts of the code.\n  \n- In testing setups, you could implement tests that specifically target autogenerated components by referring to them through the defined namespace.\n\n## Enhance Readability\nUsing the `NAMESPACE` throughout the code enhances readability and maintainability. New developers or contributors can immediately see that certain elements belong to the autogenerated category without needing to dive deeply into context or documentation.\n\n## Future Modifications\nIf the application evolves, it may be necessary to modify or extend the `NAMESPACE` value. For instance, if multiple autogenerated features or modules are introduced, redefining or expanding the naming conventions can provide even clearer delineation.\n\n## Conclusion\nIn summary, the `NAMESPACE` constant serves a crucial organizational role in the code. By encapsulating the term `\"autogen\"`, it aids in reducing name collisions, enhancing readability, and improving maintainability. Although this line may appear simple, its implications for larger software architecture can be significant, particularly in collaborative environments where many components interact.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/_constants.py"
  },
  {
    "code": false,
    "content": "# Documentation for Generative AI Tracing\n\nThis documentation provides an overview of the source code which implements tracing for generative AI operations using OpenTelemetry. The code defines several context managers that facilitate tracing various operations related to generative AI, such as tool execution and agent creation/invocation.\n\n## Overview\n\nThe main purpose of this code is to instrument generative AI operations by creating tracing spans that adhere to OpenTelemetry's semantic conventions. This instrumentation allows developers to monitor the performance, identify bottlenecks, and capture error information related to generative AI tasks. The code supports tracing for three distinct operations: executing a tool, creating an agent, and invoking an agent.\n\n### Import Statements\n\nThe code begins by importing necessary modules and classes, including:\n\n- `Generator` from `collections.abc`, utilized for type hinting.\n- `contextmanager` from `contextlib`, which helps create context managers.\n- `Enum` from `enum` to define enumerated constants.\n- `trace` and associated types from `opentelemetry`, which enable tracing functionalities.\n- An `AgentInstantiationContext` from a local module, used to retrieve the currently instantiated agent ID.\n\n### OpenTelemetry Semantic Conventions Constants\n\nThe code defines a series of constants that represent semantic conventions for generative AI operations. These constants are categorized into three groups: \n- **GenAI Agent Attributes**: Attributes related to agents such as ID, name, and description. \n- **GenAI Operation Attributes**: Attributes for operations, specifically their names and systems.\n- **GenAI Tool Attributes**: Attributes relating to tools, including call IDs, names, and descriptions.\n- **Error Attributes**: Specifically used to capture details about errors, such as error type.\n\n## Enum Definition: `GenAiOperationNameValues`\n\nThe `GenAiOperationNameValues` class is an enumeration that defines various operation names for generative AI tasks. Each operation corresponds to a specific action that can be traced:\n- `CHAT`\n- `CREATE_AGENT`\n- `EMBEDDINGS`\n- `EXECUTE_TOOL`\n- `GENERATE_CONTENT`\n- `INVOKE_AGENT`\n- `TEXT_COMPLETION`\n\n## Context Managers\n\nThe code includes three primary context managers that facilitate tracing:\n\n### 1. `trace_tool_span`\n\n#### Purpose\nThe `trace_tool_span` context manager is designed for tracking the execution of a tool. It creates a span, capturing metadata and exception details if any issues arise during the tool's execution.\n\n#### Parameters\n- `tool_name`: Specifies the name of the tool being executed.\n- `tracer`: Allows an optional tracer instance; defaults to a predefined tracer if not provided.\n- `parent`: Optional span to link to the current execution context.\n- `tool_description`: Description of the tool being called.\n- `tool_call_id`: Unique identifier for the tool call.\n\n#### Functionality\nWhen the context manager is used, it creates a span with relevant attributes and begins monitoring the tool execution. If an exception occurs, it records the error details in the span.\n\n### 2. `trace_create_agent_span`\n\n#### Purpose\nThe `trace_create_agent_span` context manager serves to trace the process of agent creation. A span is created for this operation, logging essential details.\n\n#### Parameters\n- `agent_name`: The name of the agent being created.\n- `tracer`: The tracer for the span (optional).\n- `parent`: Optional parent span for context linking.\n- `agent_id`: Unique identifier for the agent.\n- `agent_description`: A description of the agent being created.\n\n#### Functionality\nThis context manager initializes a span and captures agent details while monitoring the creation process. It also accounts for errors by recording exceptions and relevant information.\n\n### 3. `trace_invoke_agent_span`\n\n#### Purpose\nThe `trace_invoke_agent_span` context manager tracks the invocation of an agent, creating a span that captures important metadata and error information if necessary.\n\n#### Parameters\n- `agent_name`: The name of the agent being invoked.\n- `tracer`: Optional tracer for span creation.\n- `parent`: An optional parent span for linking context.\n- `agent_id`: Unique identifier for the agent (optional).\n- `agent_description`: A description of the agent.\n\n#### Functionality\nJust like the previous context managers, this one starts a span and ensures monitoring of the agent invocation. It captures any exceptions that occur, recording them in the span for later analysis.\n\n## Conclusion\n\nThe provided code implements a robust tracing mechanism for generative AI operations, enhancing observability through structured logging and monitoring. By utilizing context managers, it offers a clear way to manage spans associated with various generative AI tasks while adhering to OpenTelemetry conventions. This approach facilitates the diagnosis of issues and the assessment of performance, ultimately contributing to better system robustness and reliability.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/_genai.py"
  },
  {
    "code": false,
    "content": "# Telemetry Metadata and Context Management\n\nThis module provides functionality for managing telemetry context and metadata, primarily using the OpenTelemetry library. The focus of the code is to handle tracing information that can be utilized in distributed systems to monitor and trace requests across different services.\n\n## Data Class Definition: `EnvelopeMetadata`\n\nThe `EnvelopeMetadata` class is defined using the `@dataclass` decorator, which simplifies the construction of classes by automatically generating special methods. This class captures key tracing-related metadata:\n\n- **traceparent**: An optional string that represents the parent trace context.\n- **tracestate**: An optional string containing state information from the trace.\n- **links**: An optional sequence of `Link` objects which can represent relationships to other spans.\n\n### Example Usage of `EnvelopeMetadata`\n\nThis class primarily serves to encapsulate metadata used in telemetry envelopes. It is constructed with key tracing details that can be injected into or extracted from headers in distributed systems.\n\n## Function: `_get_carrier_for_envelope_metadata`\n\nThis private function generates a carrier dictionary specifically for an `EnvelopeMetadata` instance. It maps the `traceparent` and `tracestate` attributes to their respective keys in a dictionary format.\n\n```python\ndef _get_carrier_for_envelope_metadata(envelope_metadata: EnvelopeMetadata) -> Dict[str, str]:\n```\n\n### Purpose\n\nThe primary role of this function is to prepare the metadata for transport as part of the OpenTelemetry tracing protocol. If the attributes are present, they are added to the resulting carrier dictionary.\n\n## Function: `get_telemetry_envelope_metadata`\n\nThis public function retrieves telemetry envelope metadata, specifically fetching trace information using the OpenTelemetry `inject` method.\n\n```python\ndef get_telemetry_envelope_metadata() -> EnvelopeMetadata:\n```\n\n### Step-by-Step Overview\n\n1. An empty dictionary called `carrier` is initialized.\n2. The `TraceContextTextMapPropagator` is used to inject trace context into the `carrier`.\n3. The function returns an `EnvelopeMetadata` instance populated with the trace values (`traceparent` and `tracestate`) extracted from the `carrier`.\n\n### Return Value\n\nReturns an `EnvelopeMetadata` instance containing tracing metadata that can be used for monitoring purposes.\n\n## Function: `_get_carrier_for_remote_call_metadata`\n\nThis function is similar to `_get_carrier_for_envelope_metadata` but specifically processes remote call-related metadata.\n\n```python\ndef _get_carrier_for_remote_call_metadata(remote_call_metadata: Mapping[str, str]) -> Dict[str, str]:\n```\n\n### Purpose\n\nIt transforms a mapping containing `traceparent` and `tracestate` keys into a carrier dictionary. This is useful for preparing telemetry data associated with remote calls to be sent across service boundaries.\n\n## Function: `get_telemetry_grpc_metadata`\n\nThis function retrieves telemetry metadata formatted specifically for gRPC calls.\n\n```python\ndef get_telemetry_grpc_metadata(existingMetadata: Optional[Mapping[str, str]] = None) -> Dict[str, str]:\n```\n\n### Parameters\n\n- `existingMetadata`: Optional metadata that may already exist, which will be preserved in the returned gRPC metadata.\n\n### Step-by-Step Overview\n\n1. Retrieves tracing context and prepares a `carrier`.\n2. Adds any existing metadata provided in the `existingMetadata` argument.\n3. Returns a dictionary that includes both existing and newly generated trace metadata (`traceparent` and `tracestate`).\n\n### Return Value\n\nReturns a dictionary suitable for gRPC metadata transmission, ensuring traceability for service calls.\n\n## Type Definition: `TelemetryMetadataContainer`\n\n```python\nTelemetryMetadataContainer = Optional[EnvelopeMetadata] | Mapping[str, str]\n```\n\nThis type alias indicates that telemetry metadata can either be an `EnvelopeMetadata` instance or a generic mapping (e.g., dictionary). It allows for flexible handling of incoming metadata.\n\n## Function: `get_telemetry_context`\n\nThis function extracts a telemetry context from the provided metadata.\n\n```python\ndef get_telemetry_context(metadata: TelemetryMetadataContainer) -> Context:\n```\n\n### Parameters\n\n- `metadata`: The optional telemetry metadata from which to extract context.\n\n### Logic\n\n1. Checks if the metadata is `None`, returning an empty context in that case.\n2. If `metadata` is an instance of `EnvelopeMetadata`, the function prepares a carrier and extracts the context.\n3. If it is a mapping, a carrier for remote call metadata is created and extracted.\n4. If neither condition is satisfied, an error is raised.\n\n### Return Value\n\nReturns a valid OpenTelemetry `Context` object that can be used further in telemetry handling.\n\n## Function: `get_telemetry_links`\n\nThis function retrieves links from the provided telemetry metadata context.\n\n```python\ndef get_telemetry_links(metadata: TelemetryMetadataContainer) -> Optional[Sequence[Link]]:\n```\n\n### Purpose\n\nIt helps in extracting telemetry links, which can represent relationships between different tracing spans.\n\n### Logic\n\n1. The function checks if metadata is valid (not `None` and of an appropriate type).\n2. It extracts the context from the metadata.\n3. Uses the current span to get its context and constructs a list of `Link` objects.\n\n### Return Value\n\nReturns a sequence of `Link` objects if found; otherwise, returns `None`. This can be used to track relationships in distributed tracing.\n\n## Conclusion\n\nThrough these functions and structures, the module provides comprehensive support for managing telemetry data in a distributed system. It ensures that tracing information is accurately propagated and utilized, facilitating better monitoring and debugging of service interactions.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/_propagation.py"
  },
  {
    "code": false,
    "content": "# TraceHelper Documentation\n\n## Overview\n\nThe `TraceHelper` class is a utility designed to facilitate tracing operations in applications using OpenTelemetry, a popular framework for monitoring and observability in software systems. The class allows developers to create and manage spans\u2014representations of operations in distributed tracing\u2014enabling better insights into performance and system behavior.\n\nBy leveraging OpenTelemetry standards, `TraceHelper` supports nested spans and ensures that tracing aligns with semantic conventions. This document outlines the purpose of the class, its methods, and details about its operation.\n\n## Class `TraceHelper`\n\n### Purpose\n\n`TraceHelper` acts as a wrapper around OpenTelemetry's tracing capabilities, providing a structured and context-aware way to create spans for various operations. The class is generic, accepting types for `Operation`, `Destination`, and `ExtraAttributes`, which facilitates flexible usage based on the application requirements.\n\n### Constructor\n\n```python\ndef __init__(\n    self,\n    tracer_provider: TracerProvider | None,\n    instrumentation_builder_config: TracingConfig[Operation, Destination, ExtraAttributes],\n) -> None:\n```\n\n#### Parameters:\n- `tracer_provider`: An optional instance of `TracerProvider`. This allows users to specify a custom tracing provider for their application.\n- `instrumentation_builder_config`: An instance of `TracingConfig` which defines how spans will be constructed, including settings for attributes and other metadata.\n\n#### Functionality:\n1. The constructor checks if runtime tracing is disabled through an environment variable. If so, it initializes a no-operation tracer.\n2. If tracing is enabled, it attempts to use the provided `tracer_provider`. If none is provided, it retrieves a global tracer provider or defaults to a no-operation provider.\n3. Initializes the instance variable `tracer` which will be used to create spans.\n\n## Context Manager: `trace_block`\n\n### Purpose\n\nThe `trace_block` method serves as a context manager that simplifies span creation and management. This allows users to wrap blocks of code with tracing semantics, automatically handling span lifecycle events.\n\n### Method Definition\n\n```python\n@contextlib.contextmanager\ndef trace_block(\n    operation: Operation,\n    destination: Destination,\n    parent: Optional[TelemetryMetadataContainer],\n    *,\n    extraAttributes: ExtraAttributes | None = None,\n    kind: Optional[SpanKind] = None,\n    attributes: Optional[types.Attributes] = None,\n    start_time: Optional[int] = None,\n    record_exception: bool = True,\n    set_status_on_exception: bool = True,\n    end_on_exit: bool = True,\n) -> Iterator[Span]:\n```\n\n#### Parameters:\n- `operation`: The specific operation being traced.\n- `destination`: The target destination relevant to the tracing.\n- `parent`: An optional `TelemetryMetadataContainer`, which provides context for nested spans.\n- `extraAttributes`: Optional additional attributes for the span.\n- `kind`: The type of span being created, which affects how it is interpreted in the tracing context. Defaults to producer or consumer based on the operation.\n- `attributes`: Additional custom non-defined span attributes.\n- `start_time`: Optional to specify the start time of the span.\n- `record_exception`: Specifies whether to capture exceptions within the span.\n- `set_status_on_exception`: Decides if the span should have its status set to an error on exceptions.\n- `end_on_exit`: Indicates if the span should automatically close when exiting the context manager.\n\n### Functionality\n1. **Span Initialization**: The `trace_block` method constructs a span with parameters obtained from the `instrumentation_builder_config` and the provided arguments.\n2. **Attributes Handling**: It aggregates defined attributes along with any extra attributes specified by the user to create a comprehensive set of attributes for the span.\n3. **Span Management**: Once the span is created, the context manager yields control for the code block wrapped within it. The span is automatically closed when the block execution completes, adhering to the span lifecycle.\n4. **Exception Handling**: If exceptions occur within the code block, those can be recorded depending on the `record_exception` and `set_status_on_exception` flags.\n\n## Usage Example\n\nWhile the snippet does not include an explicit usage example, a typical usage of `TraceHelper` and its `trace_block` method may look as follows:\n\n```python\ntrace_helper = TraceHelper(tracer_provider, config)\n\nwith trace_helper.trace_block(operation=\"operation_name\", destination=\"destination_name\", parent=None) as span:\n    # Perform the operation\n```\n\nIn this usage, the `trace_block` method is invoked, creating a span that will capture the performance and any errors for the operation being conducted within the `with` block.\n\n## Conclusion\n\nThe `TraceHelper` class is a crucial component of applications seeking to implement distributed tracing using OpenTelemetry. By providing a clear API for managing spans, it simplifies the implementation of observability in applications, allowing developers to focus on the core logic while gaining insights into performance and operational issues through telemetry data.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/_tracing.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Message Runtime Tracing Configuration Module\n\n## Overview\n\nThis module defines a configuration setup for tracing instrumentation in messaging operations using the OpenTelemetry framework. It provides a structured way to configure tracing for various messaging operations and is designed for extensibility through the use of abstraction. The key classes here include `TracingConfig`, `MessageRuntimeTracingConfig`, and related type definitions for operations and destinations.\n\n## Key Classes and Protocols\n\n### TracingConfig\n\n```python\nclass TracingConfig(ABC, Generic[Operation, Destination, ExtraAttributes]):\n```\n\nThe `TracingConfig` is an abstract base class that outlines the necessary configuration requirements for instrumentation in messaging operations. It uses Python's generic types to define its parameters and requires implementing classes to define specific methods and properties related to tracing.\n\n#### Properties and Methods\n\n- **name**: This abstract property must return the name of the module being instrumented.\n- **build_attributes**: An abstract method that builds and returns a dictionary of attributes based on the messaging operation, destination, and any extra attributes.\n- **get_span_name**: An abstract method that returns the name of the span tied to a specific messaging operation and destination.\n- **get_span_kind**: An abstract method that returns the span kind based on the type of messaging operation.\n\n### ExtraMessageRuntimeAttributes\n\n```python\nclass ExtraMessageRuntimeAttributes(TypedDict):\n```\n\n`ExtraMessageRuntimeAttributes` is a `TypedDict` that defines additional attributes related to message runtime, which can include:\n\n- **message_size**: An optional integer representing the size of the message.\n- **message_type**: An optional string representing the type of the message.\n\n### MessageRuntimeTracingConfig\n\n```python\nclass MessageRuntimeTracingConfig(TracingConfig[MessagingOperation, MessagingDestination, ExtraMessageRuntimeAttributes]):\n```\n\nThis class implements the `TracingConfig` protocol and is specifically designed for message runtime tracing. \n\n#### Constructor\n\n- **__init__**: Initializes the class with a `runtime_name`, defining the name of the module to be traced.\n\n#### Properties and Methods\n\n- **name**: Returns the previously set module name.\n  \n- **build_attributes**: Constructs a dictionary of attributes relevant to the instrumentation configuration. It utilizes private methods to determine the operation type and destination string representation. If `extraAttributes` are provided, it adds their components into the dictionary.\n\n- **get_span_name**: Creates a formatted span name based on the operation type and destination. It follows OpenTelemetry's semantic conventions.\n\n- **get_span_kind**: Determines and returns the appropriate `SpanKind` based on the passed messaging operation. Operations are classified into producers and consumers, with a fallback for clients.\n\n## Helper Methods\n\n### _get_destination_str\n\n```python\ndef _get_destination_str(self, destination: MessagingDestination) -> str:\n```\n\nThis private method generates a string representation of the destination, which could be an `AgentId`, `TopicId`, string, or None. It formats the output to be consistent with naming conventions for different destination types.\n\n### _get_operation_type\n\n```python\ndef _get_operation_type(self, operation: MessagingOperation) -> str:\n```\n\nAnother private method that maps messaging operations to a standardized operation type string. For instance, it translates \"send\" or \"publish\" to \"publish,\" and \"create\" to \"create,\" providing a uniform output for further processing.\n\n## Logging \n\nThe module initializes two loggers:\n\n```python\nlogger = logging.getLogger(\"autogen_core\")\nevent_logger = logging.getLogger(\"autogen_core.events\")\n```\n\nThese loggers allow tracking of runtime events and errors that may occur within the tracing configurations, aiding in monitoring and debugging efforts.\n\n## Types and Constants\n\nThe module utilizes several type definitions to ensure type safety and improve code readability:\n\n- **MessagingDestination**: A type that can be an `AgentId`, `TopicId`, string, or None.\n- **MessagingOperation**: A `Literal` type that defines permissible messaging operations.\n\nAdditionally, a constant `NAMESPACE` is imported, which is used to standardize naming conventions within spans.\n\n## Summary\n\nOverall, this module effectively abstracts the complexity of configuring tracing instrumentation for messaging operations, making it easier to integrate and monitor such operations in applications using OpenTelemetry. Its design promotes extensibility, enabling future improvements or adaptations based on emerging requirements in tracing practices.",
    "filename": "python/packages/autogen-core/src/autogen_core/_telemetry/_tracing_config.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis Python code defines a mechanism for managing topics in a publish-subscribe messaging system, focusing on the definition and validation of a topic identifier conforming to the CloudEvents specification. The code uses a combination of regular expressions, data classes, and type hinting to ensure type safety and clear organization of topic data.\n\n## Functions\n\n### `is_valid_topic_type(value: str) -> bool`\nThis function takes a string as input and checks if it matches a specific regex pattern that validates topic types. The regex pattern `^[\\w\\-\\.\\:\\=]+\\Z` allows a range of characters commonly used in topic identifiers, enforcing standards for valid topic types. It returns `True` if the input string matches the pattern and `False` otherwise.\n\n## Data Class: `TopicId`\n\n### Class Definition\nThe `TopicId` class is defined using the `@dataclass` decorator with parameters `eq=True` and `frozen=True`. This means instances of `TopicId` can be compared for equality and are immutable once created. \n\n### Attributes\n- **type (str)**: Represents the type of the event contained in the topic ID, adhering to the CloudEvents specification.\n- **source (str)**: Identifies the origin or context of the event, also following the CloudEvents specification.\n\n### Post Initialization\nThe `__post_init__` method is automatically invoked after the dataclass initialization. It validates the `type` attribute using the `is_valid_topic_type` function. If the validation fails, a `ValueError` is raised, ensuring that only valid topic types can be assigned.\n\n### String Representation\nThe `__str__` method provides a string representation of the `TopicId` instance, formatted as `type/source`. This is useful for debugging and logging, providing an easily recognizable format for the topic identifier.\n\n### Class Method: `from_str`\nThis class method allows for creating a `TopicId` instance from a string formatted as `type/source`. It splits the input string at the first slash `/`. If the resulting list does not contain exactly two elements, a `ValueError` is raised. Otherwise, it creates and returns a new `TopicId` instance using the extracted type and source values.\n\n## Summary of Functionality\n\n1. **Validation of Topic Types**: The code provides a mechanism to validate string inputs against a predefined pattern for valid topic types using the `is_valid_topic_type` function.\n\n2. **Structured Topic Definition**: The `TopicId` class encapsulates the concept of a topic identifier in a structured way, making it easy to create and manage topic identifiers in a publish-subscribe system.\n\n3. **Immutable Data**: Instances of `TopicId` are immutable after creation, ensuring that the topic identifiers remain constant throughout their lifecycle.\n\n4. **Error Handling**: Appropriate checks are in place to ensure that only valid topic types and correctly formatted string representations are processed, enhancing the robustness of the system.\n\n5. **Ease of Conversion**: The `from_str` class method makes it convenient to create `TopicId` instances from strings, facilitating the integration of topic identifiers from external sources or configurations.\n\n6. **Compliance with Standards**: By adhering to the CloudEvents specification, the code aligns with industry standards for event-driven architectures, promoting consistency and interoperability across different systems. \n\nIn conclusion, this code serves as a foundational part of a messaging system, efficiently managing and validating topic identifiers, which are critical for event-driven programming models.",
    "filename": "python/packages/autogen-core/src/autogen_core/_topic.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script provides utility functions for type checking and type manipulation in Python, particularly focused on generic types and type unions. It utilizes the `typing` module and the `collections.abc` module to evaluate and extract information about types. Specifically, it identifies whether types are unions or optionals, and it extracts the component types from these constructs.\n\n## Dependencies\nThis script imports several components from Python\u2019s standard library:\n- **`Sequence`** from `collections.abc`: A generic abstract base class that represents a sequence.\n- **`NoneType`, `UnionType`** from `types`: Types for representing `None` and union types, respectively.\n- **`Any`, `Optional`, `Type`, `Union`, `get_args`, `get_origin`** from `typing`: Various utilities for type hinting and introspection in Python.\n\n## Functions Overview\n\n### `is_union(t: object) -> bool`\nThis function checks if the provided type `t` is a union type. It uses `get_origin` to retrieve the base type of `t`. The function returns `True` if `t` is an instance of `Union` or `UnionType`, otherwise it returns `False`.\n\n#### Parameters\n- `t`: An object representing a type to check.\n\n#### Returns\n- `bool`: True if `t` is a union type; False otherwise.\n\n### `is_optional(t: object) -> bool`\nSimilar to `is_union`, this function determines if the given type `t` is an optional type. An optional type is essentially a union of a type and `None`, and it can be checked using `get_origin`.\n\n#### Parameters\n- `t`: An object representing a type to check.\n\n#### Returns\n- `bool`: True if `t` is an optional type; False otherwise.\n\n### `AnyType`\nThis is a special class defined to handle compatibility between different Python versions (3.10 and 3.11+). It serves as a substitute for `typing.Any` in contexts where type checking needs to be more robust across versions.\n\n### `get_types(t: object) -> Sequence[Type[Any]] | None`\nThis is the most comprehensive function in the code. It examines the type `t` and returns a sequence of types depending on its characteristics. \n\n#### Logic:\n1. **Union Types**: If `t` is a union, it retrieves the constituent types using `get_args`.\n2. **Optional Types**: If `t` is optional, it processes `t` to return the types combined with `NoneType`, making it clear that the type can be `None`.\n3. **Type Any**: If `t` is `Any`, it returns a tuple containing the `AnyType` instance.\n4. **Single Type**: If `t` is a standard type, it returns a tuple containing `t`.\n5. **NoneType**: If `t` is `NoneType`, it returns a tuple with `NoneType`.\n6. **Default Case**: If none of the above conditions are met, the function returns `None`.\n\n#### Parameters\n- `t`: An object representing a type.\n\n#### Returns\n- `Sequence[Type[Any]] | None`: A sequence of types based on the input type; returns None if the type isn't recognized.\n\n## Summary\nIn summary, the code provides tools to work with Python\u2019s type system, particularly useful for developers who need to perform type inspections or manipulations. It carefully distinguishes between types, particularly the nuances of union and optional types and handles cases where the `Any` type is involved, ensuring compatibility across Python versions.",
    "filename": "python/packages/autogen-core/src/autogen_core/_type_helpers.py"
  },
  {
    "code": false,
    "content": "# TypePrefixSubscription Class Documentation\n\n## Overview\n\nThe `TypePrefixSubscription` class is a specialized subscription model that links topics with agents based on the prefix of the topic type. This allows for a structured way of handling topics where each source of the topic is associated with an individual agent instance. This behavior supports dynamic and flexible topic handling in systems that utilize a Pub/Sub architecture. \n\n### Key Features:\n- Matches topics based on a specified prefix of their type.\n- Maps each topic's source to its own agent instance.\n- Handles multiple subscriptions under the same agent type but different sources.\n\n## Class Definition\n\n### `TypePrefixSubscription`\n\nThe `TypePrefixSubscription` class is a derived class from the `Subscription` base class. Its purpose is to establish a subscription configuration that facilitates topic management based on prefixes. \n\n### Constructor\n\n```python\ndef __init__(self, topic_type_prefix: str, agent_type: str | AgentType, id: str | None = None):\n```\n\n#### Parameters\n- **topic_type_prefix** (`str`): A string representing the prefix to match against the type of incoming topics.\n- **agent_type** (`str` or `AgentType`): Specifies the type of agent that will handle the subscription.\n- **id** (`str` or `None`): An optional unique identifier for the subscription. If not provided, a new UUID is generated.\n\n#### Functionality\n- Initializes the instance by storing the provided prefix and agent type.\n- Generates a unique ID using the `uuid` library if one is not supplied.\n\n## Properties\n\n### `id`\n```python\n@property\ndef id(self) -> str:\n```\nReturns the unique identifier for the subscription.\n\n### `topic_type_prefix`\n```python\n@property\ndef topic_type_prefix(self) -> str:\n```\nReturns the prefix used for matching topic types.\n\n### `agent_type`\n```python\n@property\ndef agent_type(self) -> str:\n```\nReturns the type of agent configured for this subscription.\n\n## Methods\n\n### `is_match`\n```python\ndef is_match(self, topic_id: TopicId) -> bool:\n```\n#### Parameters\n- **topic_id** (`TopicId`): The identifier of the incoming topic to check against the subscription.\n\n#### Returns\n- **bool**: True if the `topic_id` type starts with the defined `topic_type_prefix`, otherwise False.\n\n#### Purpose\nDetermines whether a given topic ID is relevant to this subscription by checking if its type matches the specified prefix.\n\n### `map_to_agent`\n```python\ndef map_to_agent(self, topic_id: TopicId) -> AgentId:\n```\n#### Parameters\n- **topic_id** (`TopicId`): The identifier of the topic to map.\n\n#### Returns\n- **AgentId**: Returns an `AgentId` instance that represents the agent responsible for handling the topic.\n\n#### Exceptions\n- Raises a `CantHandleException` if the topic ID does not match the subscription.\n\n#### Purpose\nMaps a topic to the corresponding agent based on the source of the topic. It first checks if the topic is relevant and then creates an `AgentId` using the agent type and topic source.\n\n### `__eq__`\n```python\ndef __eq__(self, other: object) -> bool:\n```\n#### Parameters\n- **other** (`object`): An object to compare against.\n\n#### Returns\n- **bool**: True if the provided object is equivalent to the current instance based on ID or matching properties.\n\n#### Purpose\nOverrides the equality operator to compare subscription instances either by their unique ID or by their properties (agent type and topic type prefix).\n\n## Practical Example\n\nHere is a quick example of how to use the `TypePrefixSubscription` class:\n\n```python\nfrom autogen_core import TypePrefixSubscription\n\nsubscription = TypePrefixSubscription(topic_type_prefix=\"t1\", agent_type=\"a1\")\n```\n\n### Explanation:\n- In this example, a subscription is created to handle topics of type starting with `t1` and routed to agent type `a1`.\n- Each distinct source (`s1`, `s2`, etc.) will have its own agent instance for processing the matched topics.\n\n## Summary\n\nThe `TypePrefixSubscription` class is a robust way to manage mappings between topics and agents in a system that relies on dynamic topic handling based on prefixes. By defining properties and methods for matching and mapping topics, it facilitates organized and efficient agent management. This pattern is particularly useful in event-driven programming where topic subscriptions must be both flexible and powerful.",
    "filename": "python/packages/autogen-core/src/autogen_core/_type_prefix_subscription.py"
  },
  {
    "code": false,
    "content": "# TypeSubscription Class Documentation\n\nThe `TypeSubscription` class is a key component of a subscription mechanism designed to map topics to agents based on specific criteria. This class facilitates subscriptions that are defined by the type of messages they receive, and it associates these messages with agents identified by their source. Below is a structured overview of its functionalities and roles.\n\n## Overview\n\nThe `TypeSubscription` class extends from the general `Subscription` class. It provides a mechanism to create subscriptions that are tied to a specific topic type, while also defining how agents will handle those topics. Each source related to a topic is represented as an instantiation of an agent, allowing for clear routing of tasks or messages based on defined criteria.\n\n### Key Features:\n- **Topic and Agent Mapping**: Each topic based on its type will have a corresponding agent determined by its source.\n- **Dynamic Agent Creation**: This structure leads to potentially multiple agent instances for differing sources of the same topic type.\n- **UUID-based Identification**: Each instance has a unique ID that can be utilized for comparison and identification.\n\n## Constructor\n\n### `__init__(self, topic_type: str, agent_type: str | AgentType, id: str | None = None)`\n\n- **Parameters**:\n  - `topic_type`: A string indicating the type of the topic this subscription will listen to.\n  - `agent_type`: A string or an `AgentType` instance that defines the type of agent responsible for handling this subscription.\n  - `id`: An optional string that can be provided to uniquely identify the subscription. If not supplied, a new UUID is generated.\n\n#### Purpose\nThe constructor initializes the subscription instance and validates the parameters, ensuring that the appropriate topic type and agent type are assigned. The unique ID is generated automatically if not provided.\n\n## Properties\n\n### `id`\n\n- **Type**: `str`\n- **Description**: Returns the unique identifier for the subscription instance.\n\n### `topic_type`\n\n- **Type**: `str`\n- **Description**: Gives the type of the topic associated with this subscription.\n\n### `agent_type`\n\n- **Type**: `str`\n- **Description**: Indicates the type of agent that will handle topics matching this subscription.\n\n## Methods\n\n### `is_match(self, topic_id: TopicId) -> bool`\n\n- **Parameters**: \n  - `topic_id`: An instance of `TopicId` to check against the subscription's `topic_type`.\n  \n- **Returns**: `bool` indicating whether the provided `topic_id` matches the subscription's `topic_type`.\n\n#### Purpose\nThis method checks if the incoming topic identifier is relevant to the current subscription based on its type. It simplifies the decision-making related to whether a particular topic should be processed under this subscription.\n\n### `map_to_agent(self, topic_id: TopicId) -> AgentId`\n\n- **Parameters**:\n  - `topic_id`: An instance of `TopicId` that needs to be mapped to an agent.\n\n- **Returns**: An `AgentId` instance that represents the agent responsible for handling the given topic.\n\n#### Purpose\nThis method provides the logic to convert a topic identifier into the corresponding agent identifier. It first checks for a match; if there is none, it raises a `CantHandleException`. This ensures that invalid topic IDs do not proceed to agent mapping.\n\n## Equality Comparison\n\n### `__eq__(self, other: object) -> bool`\n\n- **Parameters**: \n  - `other`: Another object to compare against.\n\n- **Returns**: `bool` indicating if the current instance is considered equal to the other object.\n\n#### Purpose\nThis method allows for comparison between two `TypeSubscription` instances. They are considered equal if they share the same ID or if they have matching `agent_type` and `topic_type`. This is crucial for operations that require uniqueness or duplication checks of subscription instances.\n\n## Conclusion\n\nThe `TypeSubscription` class serves an essential role in managing subscriptions in a pub/sub system, enabling granular control over message handling based on topic types and agent types. It encapsulates the logic needed to match topics to agents efficiently, ensuring a robust framework for dynamic message processing in applications that require such functionalities.",
    "filename": "python/packages/autogen-core/src/autogen_core/_type_subscription.py"
  },
  {
    "code": false,
    "content": "# Code Analysis and Documentation\n\n## Overview\n\nThis code snippet defines a simple data structure using Python's `dataclasses` module. The primary purpose of this structure is to encapsulate the details of a function call, including its identifier, the arguments being passed to the function, and the name of the function itself.\n\n## Imports\n\n### `from __future__ import annotations`\n\nThis import statement is used to enable postponed evaluation of type annotations. It allows type hints to be represented as strings, which can be beneficial in avoiding certain circular import issues or improving performance.\n\n### `from dataclasses import dataclass`\n\nThis import brings in the `dataclass` decorator from the `dataclasses` module, which streamlines the creation of classes that are primarily used to store data. It reduces boilerplate code significantly by automatically generating methods such as `__init__()`, `__repr__()`, and others based on the class attributes.\n\n## Class Definition: `FunctionCall`\n\n### Purpose\n\nThe `FunctionCall` class serves as a blueprint for creating objects that store the necessary information to represent a single function call in a structured manner. This is particularly useful in contexts such as logging, serialization, or managing tasks where functions with specific parameters need to be executed or tracked.\n\n### Attributes\n\n- **`id: str`**  \n  - This attribute holds a unique identifier for the function call. It can be used to reference or log the specific instance of the function call accurately.\n\n- **`arguments: str`**  \n  - This attribute is meant to contain the arguments for the function call in a JSON-compatible string format. This design allows easy serialization and deserialization of the function's parameters.\n\n- **`name: str`**  \n  - This attribute specifies the name of the function that is to be called. It is essential to identify which function is being referenced when the `FunctionCall` instance is invoked or processed.\n\n## Use Cases\n\nThe `FunctionCall` class can be used in various scenarios, including:\n\n- **Task Scheduling**: In applications that involve scheduling tasks, this class can encapsulate the necessary details of what needs to be executed at a certain time.\n\n- **Remote Procedure Call (RPC)**: By structuring the function calls this way, they can easily be serialized and transported over a network for execution on a remote server.\n\n- **Event Handling**: Implementing an event-driven architecture can benefit from using this class to define actions triggered by specific events.\n\n- **Code Generation**: In scenarios where dynamic code execution is required, such as in frameworks or libraries, this structure can help represent and conduct function calls.\n\n## Example Usage\n\nWhile the provided code does not include an example usage, one could visualize creating an instance of `FunctionCall` as follows:\n\n```python\ncall = FunctionCall(id=\"1\", arguments='{\"param1\": \"value1\", \"param2\": \"value2\"}', name=\"my_function\")\n```\n\nThis example demonstrates how a `FunctionCall` object could be created, encapsulating a function named `my_function` with specific JSON arguments.\n\n## Summary\n\nIn summary, this code snippet effectively sets up a simple yet flexible infrastructure to represent function calls in Python. By leveraging data classes, it promotes cleaner and more maintainable code by automating the typical patterns involved in defining data structures.",
    "filename": "python/packages/autogen-core/src/autogen_core/_types.py"
  },
  {
    "code": false,
    "content": "# Module Documentation\n\nThis module imports and defines a set of classes, functions, and utilities designed for code execution and management, particularly within the context of requirements. \n\n## Imports and Dependencies\n\nThe module begins with import statements from two submodules: \n\n- `._base`: This submodule provides three core components:\n  - `CodeBlock`: A class likely intended to encapsulate a block of code.\n  - `CodeExecutor`: A class for executing code blocks, potentially handling execution contexts or environments.\n  - `CodeResult`: A class designed to encapsulate the results returned from executing code blocks.\n\n- `._func_with_reqs`: This submodule brings in various functionalities related to managing requirements in the execution of functions or code:\n  - `Alias`: Possibly a utility for creating function aliases or managing names.\n  - `FunctionWithRequirements`: A class that represents a function tied to specific requirements.\n  - `FunctionWithRequirementsStr`: Likely a string representation of a function with its requirements.\n  - `Import`: A function or class for importing modules or specific functions from modules.\n  - `ImportFromModule`: A specialized import function that may handle imports differently from standard methods.\n  - `with_requirements`: A decorator or utility function that likely manages function execution in relation to specified requirements.\n\n## Public API\n\nThe `__all__` variable defines the public interface of the module. This ensures that when the module is imported using `from module import *`, only the specified names will be accessible. The public API includes:\n\n- **Classes for Code Execution**:\n  - `CodeBlock`\n  - `CodeExecutor`\n  - `CodeResult`\n\n- **Utilities for Function Management with Requirements**:\n  - `Alias`\n  - `Import`\n  - `ImportFromModule`\n  - `FunctionWithRequirements`\n  - `FunctionWithRequirementsStr`\n  - `with_requirements`\n\nThis module structure indicates a design focused on enhancing the capability of running and managing code, especially scenarios where dependencies or requirements are a factor.\n\n## Purpose and Roles\n\n### CodeBlock\nThe `CodeBlock` class is presumably fundamental for encapsulating code that can be executed. It allows for the management of code snippets as discrete entities which can be run within a controlled context.\n\n### CodeExecutor\nThe `CodeExecutor` class is expected to handle the process of executing the code provided by `CodeBlock`. This could involve setting up the environment in which the code runs, managing exceptions, and returning results.\n\n### CodeResult\nThe `CodeResult` class likely serves as a wrapper for the outcomes of executing a code block. This may include the returned values, error messages, or execution status\u2014providing a structured way to relay the results back to the caller.\n\n### Function Management Classes\n- **Alias**: This may allow users to create alternate names for functions, simplifying usability and enhancing readability within the larger scope of the module\u2019s functionality.\n\n- **FunctionWithRequirements**: This class is crucial for functions that carry specific operational prerequisites, indicating that certain conditions or dependencies must be met before execution.\n\n- **FunctionWithRequirementsStr**: As a string representation, this serves to convey the requirements of functions in a human-readable format, which can be useful for documentation and debugging.\n\n### Import Utilities\n- **Import & ImportFromModule**: These classes/functions streamline the process of fetching and utilizing functionality from external modules. They likely handle complexities inherent in Python's import system, ensuring that necessary dependencies are loaded appropriately.\n\n### with_requirements\nThe `with_requirements` utility likely functions as a decorator or context manager that wraps function calls to enforce checking or management of specified requirements. This ensures that the execution logic adheres to the necessary constraints outlined during the function definition.\n\n## Conclusion\n\nIn summary, this module serves to encapsulate and manage code execution, especially in contexts where dependencies play a critical role. Through its classes and utilities, it provides a structured and accessible framework for handling code snippets, execution results, and functional requirements. The careful organization and clarity of the public API indicated by `__all__` allow for intuitive use and extendability in larger applications or systems that depend on dynamic code execution.",
    "filename": "python/packages/autogen-core/src/autogen_core/code_executor/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Code Execution Framework\n\nThis documentation provides a high-level overview of a code execution framework defined in the `base.py` file. The framework is structured around executing code blocks in various environments with proper resource management. Below are detailed sections highlighting the main components and structures present in the code.\n\n## Overview\n\nThe code facilitates the execution of code blocks\u2014pieces of code that may be extracted from agent messages\u2014and returns results in a structured format. The framework is designed to be extensible, allowing different implementations depending on the environment in which code needs to be executed. This could include environments such as a local machine, Docker containers, or cloud environments.\n\n## Key Components\n\nThe code includes several key components:\n\n### `CodeBlock`\n\n```python\n@dataclass\nclass CodeBlock:\n    \"\"\"A code block extracted from an agent message.\"\"\"\n\n    code: str\n    language: str\n```\n\n- **Purpose**: Represents a block of code to be executed and the programming language it is written in. \n- **Attributes**:\n  - `code`: A string containing the actual code.\n  - `language`: A string denoting the language of the code (e.g., Python, JavaScript).\n\n### `CodeResult`\n\n```python\n@dataclass\nclass CodeResult:\n    \"\"\"Result of a code execution.\"\"\"\n\n    exit_code: int\n    output: str\n```\n\n- **Purpose**: Encapsulates the result of executing a code block.\n- **Attributes**:\n  - `exit_code`: An integer representing the exit status of the code execution (typically `0` for success).\n  - `output`: A string containing any output produced during the execution of the code (e.g., console output, errors).\n\n## `CodeExecutor` Class\n\n### Description\n\n```python\nclass CodeExecutor(ABC, ComponentBase[BaseModel]):\n    \"\"\"Executes code blocks and returns the result.\"\"\"\n```\n\n- **Purpose**: This is an abstract base class that needs to be extended by concrete implementations of code execution environments.\n- **Components**:\n  - Inherits from `ABC` for defining abstract methods and `ComponentBase` for component behavior.\n  - `component_type`: A class attribute set to \"code_executor\" to identify the type of component.\n\n### Abstract Methods\n\nThe class defines several abstract methods that concrete implementations must provide:\n\n1. **`execute_code_blocks`**:\n   - **Purpose**: To execute given code blocks asynchronously and return the result.\n   - **Parameters**:\n     - `code_blocks`: A list of `CodeBlock` instances.\n     - `cancellation_token`: An instance of `CancellationToken` that can be used to cancel execution.\n   - **Returns**: An instance of `CodeResult`.\n   - **Exceptions**: May raise `ValueError`, `asyncio.TimeoutError`, or `asyncio.CancelledError`.\n\n2. **`start`**:\n   - **Purpose**: To initialize the code executor before executing code blocks.\n\n3. **`stop`**:\n   - **Purpose**: To release any resources held by the code executor after execution.\n\n4. **`restart`**:\n   - **Purpose**: To reset the state of the code executor. It is called when the agent is reset.\n\n### Context Management\n\nThe class provides asynchronous context management capabilities by implementing the `__aenter__` and `__aexit__` methods:\n\n- **`__aenter__`**:\n  - Calls the `start` method to prepare the executor when entering the context.\n\n- **`__aexit__`**:\n  - Calls the `stop` method when exiting the context, ensuring that resources are cleaned up.\n\n## Usage Guidance\n\nDevelopers looking to implement a specific code execution environment should subclass `CodeExecutor` and provide concrete implementations for the abstract methods. It is strongly recommended to use the implemented subclass as an asynchronous context manager to ensure that resources are properly initialized and cleaned up.\n\n## Summary\n\nThis framework establishes a clean and extensible way to manage code execution in diverse environments, effectively managing lifecycle events through context management and providing structured input and output handling. As a result, developers can create tailored code execution solutions while adhering to a consistent interface.",
    "filename": "python/packages/autogen-core/src/autogen_core/code_executor/_base.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for `func_with_reqs.py`\n\nThis module provides utilities to facilitate the management of function dependencies and requirements in Python code execution environments. It defines several classes and a decorator to handle functions that may have specific import dependencies and Python package requirements. Below is a breakdown of its components.\n\n## Overview\n\n### Main Purpose\n\nThe primary purpose of this module is to allow users to wrap Python functions, ensuring that any required packages or imports are tracked and can be automatically resolved when the function is executed in a different context. This is particularly useful in environments where code is dynamically executed, allowing for greater flexibility and control over dependencies.\n\n## Function and Class Definitions\n\n### Helper Functions\n\n- **`_to_code`**:\n  This function extracts the source code of a function or a string representation of a function, handling decorators by removing them from the source code.\n\n- **`_import_to_str`**:\n  Converts import statements represented by `Import` objects into string format suitable for Python import statements.\n\n### Data Classes\n\n- **`Alias`**:\n  Represents an alias for an import, encapsulating a module name and its alias, allowing for ease of representation in import statements.\n\n- **`ImportFromModule`**:\n  Represents multiple imports from a module. It maintains backward compatibility by allowing both tuples and lists of imports.\n\n- **`FunctionWithRequirementsStr`**:\n  This class is used to represent a function defined by a string. Upon initialization, it compiles the string into an executable function object and stores its name and any import requirements. This class is not callable directly.\n\n- **`FunctionWithRequirements`**:\n  A generic class that wraps a callable Python function while tracking its required Python packages and global imports. This enables easy serialization and reference of the function along with its requirements.\n\n### Decorator\n\n- **`with_requirements`**:\n  This decorator allows functions to be annotated with their package and import requirements. When applied, it wraps the target function within a `FunctionWithRequirements` instance, making it easy to manage dependencies.\n\n### Utility Functions\n\n- **`build_python_functions_file`**:\n  Compiles a sequence of functions (including those with requirements) into a complete Python file format with necessary import statements. It collects all imports and prepares the code for execution.\n\n- **`to_stub`**:\n  Generates a stub representation of a function, which can be useful for documentation or placeholders. The generated stub includes the function name, signature, and optional docstring.\n\n- **`to_code`**:\n  A straightforward wrapper for `_to_code`, which converts a function or string representation into its source code form.\n\n- **`import_to_str`**:\n  A wrapper around `_import_to_str` that provides an external interface to convert various import representations to string format.\n\n## Class and Method Details\n\n### `FunctionWithRequirementsStr`\n\n#### Attributes:\n- `func`: The string representation of the function.\n- `compiled_func`: The actual callable function compiled from the string.\n- `python_packages`: A list of required Python packages.\n- `global_imports`: A list of import requirements.\n\n#### Behavior:\nOn initialization, attempts to compile the string into a function and stores necessary metadata. It performs checks to ensure exactly one function is defined in the passed string.\n\n### `FunctionWithRequirements`\n\n#### Attributes:\n- `func`: The actual Python callable function.\n- `python_packages`: List of required Python packages.\n- `global_imports`: List of import requirements.\n\n#### Class Method:\n- **`from_callable`**: \n  Factory method to create an instance from an existing callable function.\n  \n#### Static Method:\n- **`from_str`**: \n  Creates a `FunctionWithRequirementsStr` instance from a string representation of a function.\n\n#### Call Method:\nThe instance can be called like a standard function, executing the stored callable with provided arguments.\n\n### Decorator Implementation\n\n#### `with_requirements`\n\n- **Parameters**:\n  - `python_packages`: Specifies necessary Python packages.\n  - `global_imports`: Lists import statements required by the function.\n\n#### Returns:\nA decorator that wraps a function, retaining its original behavior while also linking any stated dependencies.\n\n## Use Case Example\n\nAn illustrative example is provided demonstrating how to use `with_requirements`. A function `load_data()` is decorated with the required package information. This wrapped function can then be dynamically executed from a string context, pulling in necessary dependencies automatically.\n\n```python\n@with_requirements(python_packages=[\"pandas\"])\ndef load_data() -> pandas.DataFrame:\n    \"\"\"Load some sample data.\"\"\"\n    ...\n```\n\nWorking with dynamically executed code and ensuring that all dependencies are resolved appropriately helps in environments such as data processing, web applications, or automated code generation tools.\n\n## Conclusion\n\nThe `func_with_reqs.py` module is designed to effectively manage function dependencies within dynamic Python execution contexts, providing a clear structure for defining and using functions with specific import and package requirements. This can significantly streamline workflows in various programming environments.",
    "filename": "python/packages/autogen-core/src/autogen_core/code_executor/_func_with_reqs.py"
  },
  {
    "code": false,
    "content": "# Exception Classes Documentation\n\nThis document provides an overview of a module that defines several custom exception classes used in a messaging or handling system. Each class extends Python's built-in `Exception` class, allowing for specialized error handling that improves clarity and maintainability in the codebase.\n\n## Overview of Custom Exceptions\n\nThe code defines four custom exceptions:\n\n1. **CantHandleException**\n2. **UndeliverableException**\n3. **MessageDroppedException**\n4. **NotAccessibleError**\n\nThese exceptions are used to signify different error conditions that may occur within a system, especially one that handles messages or interactions between different components. By having specific exceptions, developers can manage error cases more effectively.\n\n## CantHandleException Class\n\n```python\nclass CantHandleException(Exception):\n    \"\"\"Raised when a handler can't handle the exception.\"\"\"\n```\n\nThe `CantHandleException` class is intended to be raised when a particular handler cannot process a given exception. This could occur in a generic error handling mechanism where specific handlers are designated to manage certain types of exceptions. When an exception occurs that a handler cannot handle, this custom exception provides a clear signaling mechanism to indicate that the handler's capabilities have been exceeded.\n\n## UndeliverableException Class\n\n```python\nclass UndeliverableException(Exception):\n    \"\"\"Raised when a message can't be delivered.\"\"\"\n```\n\nThe `UndeliverableException` class signifies failure in delivering a message. This could be applicable in scenarios such as networking or messaging systems where messages may fail to reach their intended recipients due to various reasons like network failure, unavailability of the recipient, or routing issues. By raising this exception, a developer can handle these delivery failures separately from other types of errors, allowing for more nuanced responses such as retries or alerts.\n\n## MessageDroppedException Class\n\n```python\nclass MessageDroppedException(Exception):\n    \"\"\"Raised when a message is dropped.\"\"\"\n```\n\nThe `MessageDroppedException` is raised in cases where messages are intentionally or unintentionally discarded. This could happen in systems that employ buffering, rate limiting, or other message management strategies that involve ignoring excess messages. Signaling this condition through a dedicated exception enables developers to differentiate between a lost message and other types of failures, allowing for appropriate logging or corrective actions.\n\n## NotAccessibleError Class\n\n```python\nclass NotAccessibleError(Exception):\n    \"\"\"Tried to access a value that is not accessible. For example if it is remote cannot be accessed locally.\"\"\"\n```\n\nThe `NotAccessibleError` class indicates an attempt to access a value that cannot be reached. This situation is common in distributed systems, where certain resources or data points may only be available under specific conditions (e.g., only accessible from certain network locations). Handling this through a specialized exception allows systems to manage access-related failures more effectively by providing context to the reason behind the inaccessibility.\n\n## Summary\n\nIn summary, this module provides a foundation for robust error management by defining specific exception classes that cater to unique error conditions. This specialization aids in making the error handling processes clearer and more manageable across different scenarios, improving the overall reliability and clarity of the codebase. Each exception serves as a purposeful indicator of different problem states within a messaging or handling framework, enhancing the developer's ability to implement targeted error recovery and logging strategies.",
    "filename": "python/packages/autogen-core/src/autogen_core/exceptions.py"
  },
  {
    "code": false,
    "content": "# Documentation for Logging Events in the LLM Framework\n\nThis module defines a series of event classes intended to log various interactions related to Large Language Models (LLMs) within a framework. Each class encapsulates specific types of events, allowing for structured logging of activities such as calls to LLMs, tool executions, message transmissions, and exceptions encountered during processing.\n\n## LLM Call Events\n\n### LLMCallEvent\n\nThe `LLMCallEvent` class is designed for logging an entire call made to a Language Model (LLM). It captures vital information related to the request and the response received.\n\n#### Parameters:\n- **messages**: A list of dictionaries representing the messages sent during the call, which should be JSON serializable.\n- **response**: A dictionary capturing the response from the LLM, also JSON serializable.\n- **prompt_tokens**: An integer representing the number of tokens used in the user's prompt.\n- **completion_tokens**: An integer indicating the number of tokens used in the model's response.\n\n#### Properties:\n- **prompt_tokens**: Returns the number of tokens used in the prompt.\n- **completion_tokens**: Returns the number of tokens used in the completion.\n\nThis class includes a `__str__` method that outputs the encapsulated data in a JSON serializable format, making it easy to log in a structured manner.\n\n### LLMStreamStartEvent\n\nThe `LLMStreamStartEvent` class logs the initiation of a streaming call. This is important for real-time applications where data is sent and received in a stream.\n\n#### Parameters:\n- **messages**: Similar to `LLMCallEvent`, this parameter captures the messages involved in the stream. They also need to be JSON serializable.\n\nThe JSON serializable output is ensured via the `__str__` method, enabling structured logging.\n\n### LLMStreamEndEvent\n\nThe `LLMStreamEndEvent` class is used for logging the completion of a stream. It captures data similar to the `LLMCallEvent` but focuses on the completion aspect of a streaming session.\n\n#### Parameters:\n- **response**: The output received at the end of the stream, which must be JSON serializable.\n- **prompt_tokens**: Number indicating prompt token usage.\n- **completion_tokens**: Number indicating completion token usage.\n\nMuch like the previous event classes, it also allows for structured logging through its `__str__` method, which produces a JSON serializable string.\n\n## Tool Call Event\n\n### ToolCallEvent\n\nThe `ToolCallEvent` class is designed to log the execution of various tools that might be part of the process.\n\n#### Parameters:\n- **tool_name**: A string representing the name of the tool being logged.\n- **arguments**: A dictionary specifying the arguments passed to the tool, which should be JSON serializable.\n- **result**: A string that denotes the result returned by the executed tool.\n\nThis class follows the same pattern as preceding classes, focusing on providing structured logging through its `__str__` method.\n\n## Message Events\n\n### MessageEvent\n\nThe `MessageEvent` class encapsulates events related to message transmission within the system.\n\n#### Parameters:\n- **payload**: A string representing the message content.\n- **sender**: An `AgentId` or `None` to indicate who sent the message.\n- **receiver**: An `AgentId`, `TopicId`, or `None` indicating the recipient of the message.\n- **kind**: An enum indicating the type of message (e.g., direct, publish, or respond).\n- **delivery_stage**: An enum indicating at which stage of delivery the message is (e.g., sent or delivered).\n\nThis class supports structured logging like others, outputting the event in a JSON serializable format via its `__str__` method.\n\n### MessageDroppedEvent\n\nSimilar to `MessageEvent`, the `MessageDroppedEvent` class is used to log instances where messages were not delivered. \n\n#### Parameters:\n- **payload**: The message that was dropped.\n- **sender**: The sender of the message.\n- **receiver**: The intended recipient of the message.\n- **kind**: The type of the message.\n\nIt also produces a structured JSON serializable output for logging purposes.\n\n## Exception Events\n\n### MessageHandlerExceptionEvent\n\nThis class captures exceptions raised during message handling.\n\n#### Parameters:\n- **payload**: The content of the message that caused the exception.\n- **handling_agent**: The `AgentId` of the agent that was handling the message when the exception occurred.\n- **exception**: The exception object that was raised.\n\nThe JSON serializable output is managed in the same way as with previous classes.\n\n### AgentConstructionExceptionEvent\n\nThe `AgentConstructionExceptionEvent` class logs any exceptions that occur during the construction of an agent.\n\n#### Parameters:\n- **agent_id**: The `AgentId` associated with the agent construction process.\n- **exception**: The exception that was raised during construction.\n\nThe class's JSON serializable output is created through its `__str__` method, allowing for structured error logging.\n\n## Conclusion\n\nThis module encapsulates a robust logging framework specifically targeted at events within LLM interactions and agent messaging systems. By leveraging structured logging through JSON serialization, it facilitates better tracking and debugging of the system's dynamic, real-time events.",
    "filename": "python/packages/autogen-core/src/autogen_core/logging.py"
  },
  {
    "code": false,
    "content": "# Module Documentation\n\nThis module is part of a package that defines memory-related functionalities. It imports various components necessary for memory operations and exposes these components for external use. Below is a high-level description of the code and its structure.\n\n## Imports\n\nThe module begins with several import statements that load specific classes and types from other components within the package. The imported elements include:\n\n- **Memory**: Likely a class that handles memory storage and retrieval operations.\n- **MemoryContent**: Presumably a class or type that defines the content that can be stored in memory.\n- **MemoryMimeType**: This could represent various MIME types associated with the memory content, indicating the format of the data.\n- **MemoryQueryResult**: Probably a structure that encapsulates the results obtained from querying the memory.\n- **UpdateContextResult**: This might handle the context result of an update operation in memory.\n- **ListMemory**: Another class that may offer functionalities specifically related to memory operations utilizing lists.\n\n## `__all__` Declaration\n\nThe `__all__` list is defined next in the module. This list serves as an interface for the module, dictating which components are to be exposed and accessible when the module is imported using the `from module import *` syntax. The inclusion of these components suggests that they are considered essential parts of the module's public API:\n\n- `Memory`\n- `MemoryContent`\n- `MemoryQueryResult`\n- `UpdateContextResult`\n- `MemoryMimeType`\n- `ListMemory`\n\nThis structure indicates that users of this module can import these classes directly, promoting better usability and organization.\n\n## Purpose and Role of Imports\n\nEach import likely serves a specific role within the overall architecture of the memory handling system:\n\n### Memory\n\nThe `Memory` class is foundational, providing core functionalities for memory management. It may handle tasks such as saving, retrieving, and deleting data within a certain memory structure.\n\n### MemoryContent\n\n`MemoryContent` likely defines the data structure that can be stored in memory, including attributes that describe the nature of the content.\n\n### MemoryMimeType\n\n`MemoryMimeType` would be instrumental in managing the formats of data being processed. This may include types for images, texts, and other media forms, helping ensure the correct handling of different data types.\n\n### MemoryQueryResult\n\nThis class encapsulates the results from memory queries. Users could leverage this to understand what data was retrieved, whether it was found, and potentially other metadata about the query operations.\n\n### UpdateContextResult\n\n`UpdateContextResult` probably deals with the aftermath of updating actions within a memory context, indicating whether updates were successful and providing any relevant context information.\n\n### ListMemory\n\nThe `ListMemory` class may focus on memory management operations that specifically deal with lists, allowing for handling collections of memory entries efficiently and intuitively. It could provide specialized methods for manipulating lists, such as addition, removal, or querying of list items.\n\n## Conclusion\n\nIn summary, this module is designed for memory handling within an application, offering a structured way to manage various types of data. By carefully importing essential components and exposing them through the `__all__` list, it provides a clean API that facilitates the use of these memory functionalities in other parts of the application. The role of each imported class or type is critical for the coherent operation and usability of the overall memory management system.",
    "filename": "python/packages/autogen-core/src/autogen_core/memory/__init__.py"
  },
  {
    "code": false,
    "content": "# Memory Management System Documentation\n\nThis document provides a high-level overview of a memory management system defined in the provided source code. The code utilizes abstract base classes and data models to create a structured memory interface for storing and retrieving various types of content.\n\n## MemoryMimeType Enum\n\n### Overview\nThe `MemoryMimeType` enum defines the various supported MIME types for memory content. This ensures that the content stored in memory conforms to predefined formats.\n\n### Supported Types\n- **TEXT**: Corresponds to plain text content (`text/plain`).\n- **JSON**: For JSON formatted content (`application/json`).\n- **MARKDOWN**: For markdown formatted content (`text/markdown`).\n- **IMAGE**: A generic type for image content (`image/*`), allowing for flexibility in image formats.\n- **BINARY**: For binary data (`application/octet-stream`), enabling the storage of non-textual data.\n\n## MemoryContent Class\n\n### Overview\nThe `MemoryContent` class is a data model defined to represent an individual memory item. It includes the content, its MIME type, and optional metadata.\n\n### Attributes\n- **content**: This can be of type string, bytes, dictionary, or an instance of the `Image` class. It serves to hold the actual memory content.\n- **mime_type**: Specifies the type of the content, which can either be a `MemoryMimeType` instance or a string.\n- **metadata**: Optional attribute that allows the user to store additional information related to the memory item.\n\n### Serialization\nThe class includes a `field_serializer` method to ensure that the `mime_type` is consistently stored as a string, regardless of its original type.\n\n## MemoryQueryResult Class\n\n### Overview\nThe `MemoryQueryResult` class defines the structure of the result returned from querying the memory. It contains a list of memory items that match the query.\n\n### Attributes\n- **results**: A list of `MemoryContent` instances that represent the entries relevant to a particular query.\n\n## UpdateContextResult Class\n\n### Overview\nThe `UpdateContextResult` class encapsulates the result from operations that update the model context with relevant memory entries.\n\n### Attributes\n- **memories**: An instance of `MemoryQueryResult`, containing the specific memory entries retrieved to update the context.\n\n## Memory Abstract Base Class\n\n### Overview\nThe `Memory` class serves as an abstract base class, defining the interface for any memory implementation. It emphasizes the importance of flexibility in both storage and retrieval mechanisms.\n\n### Responsibilities\nThe memory implementation is responsible for:\n- Storing data using any chosen method, such as lists, databases, or file systems.\n- Retrieving data leveraging mechanisms like vector or text search.\n- Updating the model context with relevant memory based on queries.\n\n### Defined Methods\n1. **update_context**: An abstract method that accepts a `model_context` (an instance of `ChatCompletionContext`) and returns an `UpdateContextResult`. This method is meant to enrich the provided context with relevant memory content.\n\n2. **query**: Another abstract method which takes a `query` (either a string or a `MemoryContent` instance) along with an optional `cancellation_token`. It returns a `MemoryQueryResult` containing entries relevant to the query.\n\n3. **add**: This method is defined to add new `MemoryContent` to the memory. It also accepts an optional `cancellation_token`.\n\n4. **clear**: An abstract method to remove all entries from the memory, resetting its state.\n\n5. **close**: Method intended for cleaning up any resources held by the memory implementation, ensuring proper resource management.\n\n## Implementation Notes\nThe `Memory` class is expected to be extended by specific memory implementations (like `ListMemory` as mentioned). Each implementation will provide concrete functionality for the abstract methods defined in this interface. This design allows for greater modularity and extensibility for various use cases in memory management.\n\n### End of Documentation\nThis overview provides a foundational understanding of the memory management system defined in the code. Each class and method plays a crucial role in ensuring the effective storage and retrieval of memory content, paving the way for versatile applications in memory-based operations.",
    "filename": "python/packages/autogen-core/src/autogen_core/memory/_base_memory.py"
  },
  {
    "code": false,
    "content": "# ListMemory Component Documentation\n\n## Overview\n\nThe `ListMemory` component provides a straightforward implementation of memory storage in a chronological list format. It allows for storing, retrieving, and managing messages or content, enabling applications to maintain historical context. Designed to function within asynchronous environments, this memory component offers methods for adding to, querying, and updating a given context with stored information.\n\n## Class Definitions\n\n### ListMemoryConfig\n\n```python\nclass ListMemoryConfig(BaseModel):\n    ...\n```\n\n- **Purpose**: This class defines the configuration parameters for the `ListMemory` component. It is derived from Pydantic's `BaseModel`, providing data validation and settings management.\n- **Attributes**:\n  - `name`: An optional string that serves as the identifier for this memory instance.\n  - `memory_contents`: A list of `MemoryContent` objects that stores the actual data the memory component will manage.\n\n### ListMemory\n\n```python\nclass ListMemory(Memory, Component[ListMemoryConfig]):\n    ...\n```\n\n- **Purpose**: This class represents the memory storage functionality, inheriting from `Memory` and `Component`. It manages memory contents in a list format and provides context updates.\n  \n- **Key Features**:\n  - Maintains a `name` and a list of `MemoryContent` items.\n  - Provides asynchronous methods for adding new content, querying existing memories, and updating model contexts.\n\n## Initialization\n\n### Constructor\n\n```python\ndef __init__(self, name: str | None = None, memory_contents: List[MemoryContent] | None = None) -> None:\n    ...\n```\n\n- **Parameters**:\n  - `name`: An optional string for the memory instance name; defaults to \"default_list_memory\" if not provided.\n  - `memory_contents`: An optional list of `MemoryContent` to initialize the memory instance.\n  \n- **Functionality**: Initializes the `ListMemory` instance with a specified name and starts with existing memory contents if provided.\n\n## Properties\n\n### Name\n\n```python\n@property\ndef name(self) -> str:\n    ...\n```\n\n- **Returns**: The name of the memory instance.\n\n### Content\n\n```python\n@property\ndef content(self) -> List[MemoryContent]:\n    ...\n```\n\n- **Returns**: The current list of stored memory contents.\n\n### Content Setter\n\n```python\n@content.setter\ndef content(self, value: List[MemoryContent]) -> None:\n    ...\n```\n\n- **Args**: Accepts a new list of `MemoryContent` and sets the current memory contents to this new value.\n\n## Important Methods\n\n### update_context\n\n```python\nasync def update_context(\n    self,\n    model_context: ChatCompletionContext,\n) -> UpdateContextResult:\n    ...\n```\n\n- **Purpose**: This method appends all stored memory contents as `SystemMessage` to the provided `ChatCompletionContext`.\n  \n- **Args**:\n  - `model_context`: The context to be updated, which will be modified to include the memories.\n  \n- **Returns**: An `UpdateContextResult` containing the memories added to the context.\n\n### query\n\n```python\nasync def query(\n    self,\n    query: str | MemoryContent = \"\",\n    cancellation_token: CancellationToken | None = None,\n    **kwargs: Any,\n) -> MemoryQueryResult:\n    ...\n```\n\n- **Purpose**: Returns all memories without any filtering.\n  \n- **Args**: \n  - `query`: Ignored in this implementation.\n  - `cancellation_token`: Optional cancellation token.\n  \n- **Returns**: A `MemoryQueryResult` containing all stored memories.\n\n### add\n\n```python\nasync def add(self, content: MemoryContent, cancellation_token: CancellationToken | None = None) -> None:\n    ...\n```\n\n- **Purpose**: Adds a new piece of content to the memory.\n\n- **Args**:\n  - `content`: The `MemoryContent` to be stored.\n\n### clear\n\n```python\nasync def clear(self) -> None:\n    ...\n```\n\n- **Purpose**: Clears all stored memory contents.\n\n### close\n\n```python\nasync def close(self) -> None:\n    ...\n```\n\n- **Purpose**: Cleanup function for managing resources when the memory is no longer needed.\n\n## Configuration Conversion Methods\n\n### _from_config\n\n```python\n@classmethod\ndef _from_config(cls, config: ListMemoryConfig) -> Self:\n    ...\n```\n\n- **Purpose**: Creates an instance of `ListMemory` from a given `ListMemoryConfig`.\n\n### _to_config\n\n```python\ndef _to_config(self) -> ListMemoryConfig:\n    ...\n```\n\n- **Purpose**: Converts the current instance into a `ListMemoryConfig` object for configuration purposes. \n\n## Example Usage\n\nAn example in the documentation demonstrates how to use the `ListMemory` instance effectively, including initializing memory, adding content, modifying memory contents directly, updating a model context, and retrieving messages for output. This serves as a practical illustration of the component's functionalities in action.",
    "filename": "python/packages/autogen-core/src/autogen_core/memory/_list_memory.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module is part of a larger codebase, likely dealing with the context management of chat-based interactions. It imports multiple classes from different modules and defines an `__all__` list, which specifies the public symbols of the module. This approach helps in organizing code and controlling the scope of what can be accessed when the module is imported.\n\n## Imports\n\nThe module imports the following classes:\n\n1. **BufferedChatCompletionContext**:  This class is likely designed to manage chat completions in a buffered manner, storing intermediate results for efficiency or for access purposes.\n\n2. **ChatCompletionContext**: This class probably encapsulates the state and behavior related to processing chat completions, handling various scenarios that may arise during chat interactions.\n\n3. **ChatCompletionContextState**: It seems to represent the state of a `ChatCompletionContext`, allowing for management of various possible states during chat operations (e.g., initial, processing, completed).\n\n4. **HeadAndTailChatCompletionContext**: This context class likely manages chat completions with a focus on both the beginning (head) and the end (tail) of the conversation, which could be useful for understanding and generating relevant responses.\n\n5. **TokenLimitedChatCompletionContext**: This class appears to implement constraints on the size of chat completions based on token limits, facilitating resource management and efficiency in processing.\n\n6. **UnboundedChatCompletionContext**: Unlike the token-limited context, this class probably allows for an unlimited size in chat completions, which could be suitable for less constrained scenarios.\n\n## Public Interface\n\nThe `__all__` declaration is important for setting the public API of the module. It restricts the import of only the specified classes when `from module import *` is used, promoting encapsulation and keeping potentially sensitive or internal classes hidden from users. \n\nThe listed public classes are:\n\n- **ChatCompletionContext**\n- **ChatCompletionContextState**\n- **UnboundedChatCompletionContext**\n- **BufferedChatCompletionContext**\n- **TokenLimitedChatCompletionContext**\n- **HeadAndTailChatCompletionContext**\n\nThis structure indicates a well-organized class hierarchy or relationships, where each context class serves a different purpose, potentially tailored to different scenarios in chat processing.\n\n## Summary of Functionality\n\n1. **Context Management**: The module is focused on various chat completion contexts, which likely handle and process the state of chat interactions. \n\n2. **State Handling**: With the inclusion of `ChatCompletionContextState`, the module probably supports complex state transitions, allowing for robust management of the conversation lifecycle.\n\n3. **Efficiency Mechanisms**: The classes that imply buffering or token limitations suggest that the code is designed to optimize resource use and possibly enhance performance during chat interactions.\n\n4. **Flexibility**: The different context classes provide flexibility for developers to choose an appropriate context based on specific use cases, whether that be for minimal token usage or handling large amounts of data without constraints.\n\n5. **Modular Design**: Each context class appears to serve a distinct role, contributing to a more modular design that enhances maintainability and usability.\n\n6. **Exported Classes**: By exporting only certain classes, the module keeps its interface clean and manageable, allowing developers to focus on the essential components without being overwhelmed by internal workings.\n\n## Conclusion\n\nOverall, this module offers a structured approach to handling various chat contexts, employing classes that cater to specific requirements in terms of state management, efficiency, and flexibility. This design is beneficial for applications involving complex chat interactions, where the context of each conversation can dynamically shift. The explicit control over what is included in the public interface showcases thoughtful code organization and encapsulation practices.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/__init__.py"
  },
  {
    "code": false,
    "content": "# Buffered Chat Completion Context Documentation\n\n## Overview\n\nThe `BufferedChatCompletionContext` class is designed to manage a contextual chat system that retains a limited number of messages for continuity during interactions. This functionality is especially useful in applications that require tracking of conversations over time, such as chatbots or conversational agents. The class is implemented using Pydantic for configuration management and type safety.\n\n## Key Components\n\n### Imports\n\nThe code begins with several import statements:\n\n```python\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\nfrom .._component_config import Component\nfrom ..models import FunctionExecutionResultMessage, LLMMessage\nfrom ._chat_completion_context import ChatCompletionContext\n```\n\n- **List**: Imported for type hinting related to lists of messages.\n- **BaseModel**: A Pydantic model that serves as a base for creating data validation structures.\n- **Self**: Allows for type hinting of methods that return instances of the same class.\n- **Component**: A base class for components at a certain application level.\n- **FunctionExecutionResultMessage, LLMMessage**: Custom message types representing specific message structures in the chat context.\n- **ChatCompletionContext**: A parent class from which `BufferedChatCompletionContext` inherits, providing foundational chat functionality.\n\n### BufferedChatCompletionContextConfig\n\n```python\nclass BufferedChatCompletionContextConfig(BaseModel):\n    buffer_size: int\n    initial_messages: List[LLMMessage] | None = None\n```\n\n- **Purpose**: This Pydantic model is used to define the configuration schema for the chat completion context.\n- **Attributes**:\n  - `buffer_size`: An integer specifying the maximum number of messages to retain.\n  - `initial_messages`: An optional list of messages to initialize the context.\n\n## Class Definition\n\n### BufferedChatCompletionContext\n\n```python\nclass BufferedChatCompletionContext(ChatCompletionContext, Component[BufferedChatCompletionContextConfig]):\n```\n\n- **Description**: This class extends `ChatCompletionContext` and `Component`, and is designed to maintain a circular buffer of chat messages for quick access.\n- **Docstring**: Provides additional details on the parameters `buffer_size` and `initial_messages`.\n\n### Constructor\n\n```python\ndef __init__(self, buffer_size: int, initial_messages: List[LLMMessage] | None = None) -> None:\n    super().__init__(initial_messages)\n    if buffer_size <= 0:\n        raise ValueError(\"buffer_size must be greater than 0.\")\n    self._buffer_size = buffer_size\n```\n\n- **Purpose**: Initializes an instance of the `BufferedChatCompletionContext`.\n- **Validation**: The constructor checks that `buffer_size` is greater than 0 and raises a `ValueError` if it is not.\n\n## Message Management\n\n### get_messages Method\n\n```python\nasync def get_messages(self) -> List[LLMMessage]:\n    \"\"\"Get at most `buffer_size` recent messages.\"\"\"\n```\n\n- **Functionality**: This asynchronous method retrieves the most recent messages up to the specified buffer size.\n- **Handling Function Execution Messages**: It checks if the first message in the buffer is a function execution result message and removes it from the returned list if true.\n\n### Configuration Methods\n\n#### _to_config\n\n```python\ndef _to_config(self) -> BufferedChatCompletionContextConfig:\n    return BufferedChatCompletionContextConfig(\n        buffer_size=self._buffer_size, initial_messages=self._initial_messages\n    )\n```\n\n- **Purpose**: Converts the current instance configuration back into a `BufferedChatCompletionContextConfig` object for serialization or storage.\n\n#### _from_config\n\n```python\n@classmethod\ndef _from_config(cls, config: BufferedChatCompletionContextConfig) -> Self:\n    return cls(**config.model_dump())\n```\n\n- **Purpose**: A class method that constructs an instance of `BufferedChatCompletionContext` from a given configuration model, allowing for easy re-hydration of state.\n\n## Conclusion\n\nIn summary, the `BufferedChatCompletionContext` class provides an efficient way to manage chat interactions by keeping a defined number of recent messages. It leverages Pydantic for robust configuration specifications and offers utility methods for both retrieval and configuration management. This class is particularly valuable in scenarios requiring context retention for improving user experience in conversational AI applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/_buffered_chat_completion_context.py"
  },
  {
    "code": false,
    "content": "# Documentation: ChatCompletionContext Implementation\n\n## Overview\n\nThe provided code defines an abstract base class, `ChatCompletionContext`, intended for implementing a context that handles messages in chat completions using a large language model (LLM). The design is modular and allows for different implementations that can utilize various strategies for recalling messages. This abstraction can be particularly useful for applications that require contextual information to generate coherent and relevant responses.\n\n## Class: `ChatCompletionContext`\n\n### Purpose\n\nThe `ChatCompletionContext` class acts as a blueprint for creating specialized contexts that manage LLM messages in chat applications. It is part of a component architecture, suggesting it may interact with other components within a larger chat framework. The class uses Python\u2019s abstract base class framework (via `ABC`) to enforce the implementation of the `get_messages` method in derived classes.\n\n### Constructor\n\n```python\ndef __init__(self, initial_messages: List[LLMMessage] | None = None) -> None:\n```\n\nThe constructor initializes the chat completion context. It accepts an optional argument, `initial_messages`, which can be a list of `LLMMessage` objects. If provided, these messages are stored internally in the `_messages` attribute. This capability allows flexibility in populating the context with existing messages upon instantiation.\n\n### Attributes\n\n- **component_type**: Set to `\"chat_completion_context\"`, denoting the type of this component.\n- **_messages**: A private list that holds instances of `LLMMessage`, representing the chat messages stored in the context.\n- **_initial_messages**: A stored reference to the initial messages provided during initialization, allowing class implementations to maintain context as needed.\n\n## Methods\n\n### `add_message`\n\n```python\nasync def add_message(self, message: LLMMessage) -> None:\n```\n\nThis asynchronous method allows adding new `LLMMessage` objects to the current context. It appends the provided message to the `_messages` list, enabling dynamic updates to the chat context as conversations progress.\n\n### `get_messages`\n\n```python\n@abstractmethod\nasync def get_messages(self) -> List[LLMMessage]: ...\n```\n\nThis abstract method must be implemented in any subclass derived from `ChatCompletionContext`. It should return a list of `LLMMessage` objects, allowing for the retrieval of messages in a specific format or order defined by the subclass. The enforcement of this method allows for customizable recall strategies.\n\n### `clear`\n\n```python\nasync def clear(self) -> None:\n```\n\nThis asynchronous method clears the messages from the context by resetting the `_messages` list. It provides a mechanism to start fresh, which can be essential in scenarios where a new conversation context is needed, or after a session has completed.\n\n### `save_state`\n\n```python\nasync def save_state(self) -> Mapping[str, Any]:\n```\n\nThis method serializes the current state of the context into a dictionary format, encapsulating the stored messages in a `ChatCompletionContextState` model. This behavior is useful for persisting the context, allowing it to be restored later.\n\n### `load_state`\n\n```python\nasync def load_state(self, state: Mapping[str, Any]) -> None:\n```\n\nThis method restores the context from a previously saved state, mapping the provided dictionary back to the `_messages` list. It ensures that the context can resume from a specific point, which is particularly useful in complex applications involving user sessions.\n\n## Class: `ChatCompletionContextState`\n\n### Purpose\n\nThe `ChatCompletionContextState` class serves as a model for encapsulating the state of the `ChatCompletionContext`. It inherits from Pydantic's `BaseModel`, which offers built-in validation and serialization capabilities.\n\n### Attributes\n\n- **messages**: A list of `LLMMessage` objects, initialized as an empty list by default. This attribute is designated to hold messages during the serialization and deserialization processes, facilitating easy state management.\n\n## Example Usage\n\nIn the accompanying docstring of the `ChatCompletionContext` class, an example of how to extend this context is provided:\n\n```python\nclass ReasoningModelContext(UnboundedChatCompletionContext):\n    \"\"\"A model context for reasoning models.\"\"\"\n    \n    async def get_messages(self) -> List[LLMMessage]:\n        messages = await super().get_messages()\n        # Filter out thought field from AssistantMessage.\n        messages_out: List[LLMMessage] = []\n        for message in messages:\n            if isinstance(message, AssistantMessage):\n                message.thought = None\n            messages_out.append(message)\n        return messages_out\n```\n\nThis example demonstrates how a derived class can override the `get_messages` method to filter certain fields from the messages before returning them, showcasing the flexibility offered by the abstract base class design.\n\n## Conclusion\n\nThe `ChatCompletionContext` class provides a robust framework for managing chat completions with LLMs, supporting customizable implementations through inheritance. It emphasizes modularity by using abstract methods, creating a clear separation between the context management and message retrieval strategies. The inclusion of methods for state saving and loading further enhances its utility in session-based or multi-turn dialogue applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/_chat_completion_context.py"
  },
  {
    "code": false,
    "content": "# HeadAndTailChatCompletionContext Module Documentation\n\n## Overview\n\nThe `HeadAndTailChatCompletionContext` module is designed to manage chat completion contexts for a conversational AI model. It retains a specified number of initial messages (the \"head\") and the most recent messages (the \"tail\"). This is particularly useful in scenarios where chat context needs to be efficiently managed while allowing for essential contextual information to persist. \n\n## Dependencies\n\nThe code relies on several libraries and modules, specifically:\n- `pydantic`: For data validation and settings management via models.\n- `typing`: For type hints, particularly the `List` type.\n- Other custom modules which are implied to exist (like `_component_config`, `_types`, and `models`), suggesting this is part of a larger project.\n\n## Configuration Class: HeadAndTailChatCompletionContextConfig\n\n### Purpose\n`HeadAndTailChatCompletionContextConfig` serves as a configuration model. It is built using `pydantic.BaseModel`, enabling structured data representation with validation features.\n\n### Attributes\n- **head_size (int)**: Specifies how many messages from the beginning of the conversation will be kept in the context.\n- **tail_size (int)**: Indicates how many of the most recent messages will be retained in the context.\n- **initial_messages (List[LLMMessage] | None)**: Optional list of messages that can be included at the initialization of the context.\n\n## Main Class: HeadAndTailChatCompletionContext\n\n### Purpose\n`HeadAndTailChatCompletionContext` is a specialized component that manages chat message contexts, retaining a configurable number of both initial and recent messages as defined by the `head_size` and `tail_size`.\n\n### Inheritance\nIt extends `ChatCompletionContext` and implements the `Component` interface, suggesting that it can be integrated into component-based architectures.\n\n### Initialization\n- The constructor takes three parameters: \n  - `head_size`: Must be greater than zero.\n  - `tail_size`: Must also be greater than zero.\n  - `initial_messages`: An optional parameter for starting messages.\n- It raises `ValueError` if either `head_size` or `tail_size` are non-positive.\n\n### Component Configuration\n- **component_config_schema**: Specifies that the configuration schema for this component is defined by `HeadAndTailChatCompletionContextConfig`.\n- **component_provider_override**: Points to a specific provider path for the component.\n\n## Message Retrieval: get_messages Method\n\n### Purpose\nThe `get_messages` method retrieves a combination of messages from the stored context based on the defined `head_size` and `tail_size`.\n\n### Logic\n1. **Head Messages**: It collects the first `head_size` messages.\n   - It checks if the last message in the head is an `AssistantMessage` containing a list of `FunctionCall` items; if so, this message is excluded from the head.\n   \n2. **Tail Messages**: It gathers the last `tail_size` messages.\n   - It checks if the first message in the tail is a `FunctionExecutionResultMessage` and excludes it if true.\n\n3. **Message Count**: Calculates how many messages were skipped based on the total number of messages minus the expected head and tail sizes.\n   - If not enough messages exist to fulfill both head and tail sizes, it returns all available messages.\n\n4. **Placeholder Message**: If any messages are skipped, it creates a `UserMessage` to inform about the skipped messages, providing clearer context to the user.\n\n### Return Value\nThe method concatenates the `head_messages`, a possible placeholder, and the `tail_messages`, returning the resulting list.\n\n## Configuration Methods: _to_config and _from_config\n\n### Purpose\nThese methods handle the conversion between class instances and their configuration representations, facilitating easy serialization and deserialization of component states.\n\n### _to_config Method\n- Converts `HeadAndTailChatCompletionContext` instance attributes into a configuration model (`HeadAndTailChatCompletionContextConfig`), thus allowing for easy state management and retrieval.\n\n### _from_config Method\n- A class method that allows for reconstruction of a `HeadAndTailChatCompletionContext` instance from a provided configuration instance, enabling state restoration functionalities.\n\n## Conclusion\n\nThe `HeadAndTailChatCompletionContext` class provides a robust way to manage chat histories in conversational systems, balancing the need to retain context while ensuring efficiency in message handling. Its structure supports both configuration management and dynamic retrieval of message contexts, suited for advanced chatbot implementations or other AI-based messaging systems.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/_head_and_tail_chat_completion_context.py"
  },
  {
    "code": false,
    "content": "# TokenLimitedChatCompletionContext Module Documentation\n\n## Overview\n\nThe `TokenLimitedChatCompletionContext` module is part of a larger system that handles chat completion in a token-limited manner. It is designed to maintain context within a set token limit while collaborating with a specified model client to count tokens effectively. This component stands as an experimental feature, indicated by the comments within the code, and is tailored for scenarios where managing token usage is critical for performance and compliance with model constraints.\n\nThis module leverages various dependencies from the `pydantic` library for data validation, and it interacts with components such as `ChatCompletionClient` and `ToolSchema` to provide structured functionalities.\n\n## Classes\n\n### TokenLimitedChatCompletionContextConfig\n\n```python\nclass TokenLimitedChatCompletionContextConfig(BaseModel):\n    model_client: ComponentModel\n    token_limit: int | None = None\n    tool_schema: List[ToolSchema] | None = None\n    initial_messages: List[LLMMessage] | None = None\n```\n\n**Purpose:**  \nThis class serves as a configuration schema for the `TokenLimitedChatCompletionContext`. It extends `BaseModel` from Pydantic, providing validation features for its attributes.\n\n**Attributes:**\n- `model_client`: Specifies the `ChatCompletionClient` to be used for counting tokens.\n- `token_limit`: Defines the maximum number of tokens to retain in context.\n- `tool_schema`: A collection of tool definitions applicable for augmentation in the chat context.\n- `initial_messages`: An array of messages that initializes the chat context.\n\n### TokenLimitedChatCompletionContext\n\n```python\nclass TokenLimitedChatCompletionContext(ChatCompletionContext, Component[TokenLimitedChatCompletionContextConfig]):\n```\n\n**Purpose:**  \nThis is the main component that manages chat context with token limitations. It inherits from both `ChatCompletionContext` and `Component`, establishing itself as a configurable component.\n\n**Key Features:**\n- Monitors token limits while retaining relevant messages in the chat context.\n- Interfaces with the model client for token counting and checking remaining tokens.\n\n#### Initialization\n\n```python\ndef __init__(self, model_client: ChatCompletionClient, *, token_limit: int | None = None, tool_schema: List[ToolSchema] | None = None, initial_messages: List[LLMMessage] | None = None) -> None:\n```\n\n**Role:**  \nSets up a new instance of `TokenLimitedChatCompletionContext` by accepting the model client and various optional parameters.\n\n**Parameters:**\n- `model_client`: Required parameter indicating the client used for model interactions.\n- `token_limit`: Optional; if defined, it should be greater than 0.\n- `tool_schema`: Optional; defaults to an empty list if not provided.\n- `initial_messages`: Optional; serves for initializing chat history.\n\n**Validation:**  \nThe initializer checks if `token_limit` is a positive integer and raises a `ValueError` if it does not meet the criteria.\n\n#### Asynchronous Message Retrieval\n\n```python\nasync def get_messages(self) -> List[LLMMessage]:\n```\n\n**Purpose:**  \nAsynchronously retrieves messages from the chat context, limiting the retrieval based on token constraints.\n\n**Logic:**\n- Messages are reduced until the total token count does not exceed `token_limit` or remaining tokens allow further retrieval.\n- If the first message is a `FunctionExecutionResultMessage`, it excludes that message from the final list returned.\n\n**Token Management:**  \nThe counting logic interacts with the `model_client` to gauge the token usage dynamically, accommodating either the predefined `token_limit` or the available remaining tokens.\n\n#### Configuration Methods\n\n```python\ndef _to_config(self) -> TokenLimitedChatCompletionContextConfig:\n```\n\n**Role:**  \nConverts the current context instance into a `TokenLimitedChatCompletionContextConfig`, facilitating easy serialization and storage.\n\n```python\n@classmethod\ndef _from_config(cls, config: TokenLimitedChatCompletionContextConfig) -> Self:\n```\n\n**Purpose:**  \nThis class method creates a new instance of `TokenLimitedChatCompletionContext` from an existing configuration object. It helps in rehydrating the component with previously stored parameters.\n\n### Notes on Experimental Status\n\nThe component's status as experimental means that its interface and implementation may evolve. Users should be cautious when relying on its functionality in production environments, as that may affect stability and maintenance.\n\n## Conclusion\n\nThe `TokenLimitedChatCompletionContext` class is a sophisticated component designed to handle chat contexts with strict token management. By encapsulating behavior related to message handling within the constraints defined by token limits, it facilitates efficient and controlled operations for chat-based applications leveraging model clients. The integration with Pydantic adds an extra layer of robustness and validation, ensuring that service interactions adhere to the expected schemas and types.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/_token_limited_chat_completion_context.py"
  },
  {
    "code": false,
    "content": "# Documentation for UnboundedChatCompletionContext\n\n## Overview\n\nThe provided code defines an unbounded chat completion context within a conversational framework, where messages from users and language models can be stored and manipulated during chat sessions. This context allows for the management of chat history by encapsulating related operations and configurations for message handling.\n\n## Dependencies\n\nThis code relies on several external modules:\n\n- **pydantic** for model validation with the `BaseModel` class.\n- The `Component` class, which is likely part of a component architecture, suggesting that this context can be integrated as part of a larger system.\n- The `LLMMessage` data structure, which likely captures a message's content and associated metadata.\n- The `ChatCompletionContext` which provides a base for chat-related functionalities.\n\n## Class: UnboundedChatCompletionContextConfig\n\n### Purpose\n\n`UnboundedChatCompletionContextConfig` is a simple configuration class derived from `BaseModel`. It is designed to hold the initial messages for a chat session.\n\n### Attributes\n\n- **initial_messages**: A list of `LLMMessage` objects that may or may not be provided upon configuration. This acts as the starting point for the chat context, allowing for initialization with pre-existing messages.\n\n## Class: UnboundedChatCompletionContext\n\n### Purpose\n\nThe `UnboundedChatCompletionContext` class extends `ChatCompletionContext` and implements the `Component` interface, which indicates that this class functions as a component within a larger application or framework.\n\n### Attributes\n\n- **component_config_schema**: Identifies the configuration schema to be used with this component, linking it to the `UnboundedChatCompletionContextConfig` schema.\n- **component_provider_override**: A string that defines a standard component provider, which can be utilized for dependency injection or component instantiation.\n\n### Methods\n\n#### `async def get_messages(self) -> List[LLMMessage]`\n\n##### Purpose\n\nThis asynchronous method retrieves the recent messages stored in the chat context.\n\n##### Behavior\n\n- It returns the list of `LLMMessage` objects that are held in memory within the instance, thus providing access to the current state of the chat history.\n- It is implicit that the context may manage messages in a non-limited manner, hence the term \"unbounded\".\n\n#### `def _to_config(self) -> UnboundedChatCompletionContextConfig`\n\n##### Purpose\n\nThis method converts the instance of the chat context back into its corresponding configuration.\n\n##### Behavior\n\n- It returns an instance of `UnboundedChatCompletionContextConfig`, encapsulating the initial messages of the context for potential storage or further processing.\n\n#### `@classmethod def _from_config(cls, config: UnboundedChatCompletionContextConfig) -> Self`\n\n##### Purpose\n\nThis class method is used to create an instance of `UnboundedChatCompletionContext` from a given configuration.\n\n##### Behavior\n\n- It accepts a configuration object of type `UnboundedChatCompletionContextConfig` and returns a new instance of the chat context, initializing it with the provided initial messages.\n\n## Summary\n\nThe `UnboundedChatCompletionContext` class is part of a broader framework for managing conversational interactions, allowing for message continuity throughout a session. With the ability to both retrieve and configure message states, this context plays a crucial role in maintaining an unbounded dialogue, making it suitable for applications where ongoing conversation history is vital. By leveraging Pydantic for model validation and structured design principles, the code adheres to best practices in Python software development, making it robust and extensible.",
    "filename": "python/packages/autogen-core/src/autogen_core/model_context/_unbounded_chat_completion_context.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe given code is an import statement from a Python module, which outlines how various classes, types, and functions are structured and made available for use in the current module. This code is organized into several logical sections that include both model client functionalities and type definitions. It utilizes an `__all__` list to explicitly define which components should be accessible when the module is imported.\n\n## Module Imports\n\nThe code begins by importing several components from internal modules (`._model_client` and `._types`):\n\n### From `_model_client`\n\n- **ChatCompletionClient**: This likely represents a client that manages interactions with a chat model, possibly facilitating the initiation and handling of chat completions.\n- **ModelCapabilities**: This class or structure may provide information about the capabilities of different models available through the client.\n- **ModelFamily**: This could categorize different models under broader classes or groups.\n- **ModelInfo**: Would likely contain metadata or specifics about a particular model, including its characteristics and performance metrics.\n- **validate_model_info**: A function designed to validate the provided model information to ensure it meets certain specifications.\n\n### From `_types`\n\n- **AssistantMessage**: This type probably encapsulates messages generated by an AI assistant during a chat session.\n- **ChatCompletionTokenLogprob**: This type may handle the token probabilities for generated completions, useful for analyzing responses.\n- **CreateResult**: Presumably represents the outcomes of a create action concerning chat generation.\n- **FinishReasons**: Likely enumerates various conditions or reasons for the termination of a chat session or message generation.\n- **FunctionExecutionResult** and **FunctionExecutionResultMessage**: These would represent results and associated messages from executing certain functions.\n- **LLMMessage**: This type could encapsulate messages sent or received by a large language model (LLM).\n- **RequestUsage**: Probably tracks the usage metrics or statistics for requests made to the client or model.\n- **SystemMessage**: Likely categorizes messages intended for system-level interactions or commands.\n- **UserMessage**: Represents messages provided by the user interacting with the AI model.\n- **TopLogprob**: This may represent the top log probabilities associated with token outputs from the model.\n\n## `__all__` Declaration\n\nThe `__all__` list is defined, which controls what is exported when the module is imported using `from module import *`. By including the various components in this list, the developer is promoting a controlled interface. This enforcement of public API helps in maintaining a clean namespace and provides clear expectations to users of the module.\n\n### Listed Components\n\nThe components included in the `__all__` list include both classes and function references that provide the primary functionality for users. \n\n- Model-related classes and functions (`ModelInfo`, `ModelCapabilities`, etc.) are essential for interacting with the models.\n- Message types such as `UserMessage`, `AssistantMessage`, and `SystemMessage` form the core of the chat interaction protocols.\n\n## Conclusion\n\nThis module appears to be part of a larger system designed to facilitate interactions with a chat model, particularly an AI-driven system. The imports suggest an organized architecture that separates model management and message type definitions. The `__all__` declaration indicates a commitment to a logical interface design, improving usability and reducing the potential for conflicts in a larger application.\n\nUsers of this module would utilize the imported classes and types for tasks related to chat message construction, model interaction, and information handling, contributing to the development of AI-enhanced conversational applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/models/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Model Family and Chat Completion Structure\n\n## Overview\n\nThis code defines a set of classes and types related to a model framework for handling machine learning models. It includes categorization by model families, management of model capabilities, and the creation of chat completion clients. The primary aim is to provide an interface for interacting with various machine learning models, such as OpenAI, Anthropic, Google, and others, facilitating functionalities like message handling, tool usage, and output formatting.\n\n## Model Family Class\n\n### Purpose\n\nThe `ModelFamily` class serves as a namespace to define constants representing different model families. It categorizes models by their capabilities rather than by specific features, making it easier to identify and relate models that share similar characteristics. \n\n### Key Components\n\n- **Constants**: The class defines several string constants for various known models (e.g., `GPT_5`, `CLAUDE_3_HAIKU`). These constants provide a clear naming convention for referencing models throughout the code.\n- **Type Alias**: The `ANY` type alias allows for an easy reference to any model family defined within the class or as an unknown string.\n- **Static Methods**: There are multiple static methods (e.g., `is_claude`, `is_gemini`, etc.) that help determine if a given model family belongs to specific categories (like Claude or Gemini).\n\n### Instantiation Prevention\n\nThe class overrides the `__new__` method to prevent instantiation, as it is intended solely to contain constants.\n\n## Model Capabilities and Information\n\n### ModelCapabilities (Deprecated)\n\nThe `ModelCapabilities` class, marked for deprecation, defines a dictionary-like structure (using `TypedDict`) to specify a model's capabilities. It includes boolean values for properties such as vision support and function calling, serving as a standard template for model compatibility.\n\n### ModelInfo\n\nThe `ModelInfo` class also defines a dictionary structure that is expected to provide detailed model property information. It includes attributes for model capabilities (like `vision`, `json_output`) and a `family` field constrained to known model family constants. \n\n### Validation Function\n\nThe `validate_model_info` function ensures that essential fields are present in a `ModelInfo` object. It raises a `ValueError` if required fields are absent and issues warnings for fields that will be mandatory in future versions.\n\n## Chat Completion Client\n\n### Overview\n\nThe `ChatCompletionClient` class is an abstract base class that extends `ComponentBase`. This class lays out the blueprint for implementing client functionality for different models, encapsulating methods for creating responses, streaming outputs, and handling usage statistics.\n\n### Abstract Methods\n\n- **create**: An asynchronous method to generate a response based on input messages. It takes various optional arguments, like tools for enhanced interaction and settings for JSON output.\n  \n- **create_stream**: Similar to `create`, but this method returns an asynchronous generator yielding multiple result chunks. This is particularly useful for applications needing continuous interaction with a model, allowing it to process messages and provide updates incrementally.\n\n- **close**: An abstract method intended for cleanup operations, ensuring that resources are released appropriately.\n\n### Usage and Statistics Methods\n\nAdditional abstract methods provide functionalities to manage model usage metrics:\n- **actual_usage**: Returns the current usage statistics.\n- **total_usage**: Provides cumulative usage statistics.\n- **count_tokens**: Returns the token count for the given inputs, allowing management of input sizes.\n- **remaining_tokens**: Calculates the remaining token allowance based on the model limit.\n\n### Deprecated Property\n\nThe `capabilities` property is marked as deprecated and suggests using the `model_info` property instead. When accessed, it triggers a warning to encourage clients to implement `model_info` properly, ensuring future compatibility.\n\n## Conclusion\n\nThe code establishes a structured framework for interfacing with various machine learning models by defining model families and handling capabilities. Through abstract classes and typed data structures, it prepares the groundwork for various implementations of chat completion clients, promoting both clarity and extensibility for future enhancements. The use of type aliases and validation functions further bolsters code reliability, ensuring that client implementations adhere to established standards.",
    "filename": "python/packages/autogen-core/src/autogen_core/models/_model_client.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level overview and explanation of the provided code, which defines various message models and structures for communication between a user, an assistant, and a system, specifically for an AI model's interaction framework.\n\n## Overview\n\nThe code utilizes Python's `pydantic` library and `dataclasses` to create data models representing different types of messages and execution results in a conversational artificial intelligence context. These models help facilitate structured data transfer during interactions with a language model, enabling seamless communication among different entities in the system, such as the system itself, the user, and the assistant that generates responses.\n\n## Message Classes\n\n### SystemMessage\n\nThe `SystemMessage` class represents a message generated by the developer or system, providing instructions for the AI model. \n\n- **Attributes:**\n  - `content`: A string containing the actual message content.\n  - `type`: A literal indicating the message type, which always equals `\"SystemMessage\"`.\n\n**Note:** The code comments that OpenAI is transitioning to using the `\"developer\"` role instead of `\"system\"`, but both remain compatible within the API.\n\n### UserMessage\n\nThe `UserMessage` class defines messages originating from users or other agents in the system.\n\n- **Attributes:**\n  - `content`: Accepts either a string or a list that can include strings or `Image` objects, representing the user input.\n  - `source`: A string indicating the origin of the message, such as the name of the user or agent.\n  - `type`: A literal for message type, set to `\"UserMessage\"`.\n\n### AssistantMessage\n\nThe `AssistantMessage` class is used to model messages generated by the AI assistant.\n\n- **Attributes:**\n  - `content`: Accepts a string or a list of `FunctionCall` objects, representing the assistant's output.\n  - `thought`: An optional string for reasoning behind the assistant's generation or completion.\n  - `source`: A string indicating the entity that generated the message.\n  - `type`: A literal for message type, set to `\"AssistantMessage\"`.\n\n## Function Execution Results\n\n### FunctionExecutionResult\n\nThis class encapsulates the output of any function invoked during interactions.\n\n- **Attributes:**\n  - `content`: A string that holds the result of the function call.\n  - `name`: The name of the function executed.\n  - `call_id`: A unique identifier for the function call, which may be empty in some cases.\n  - `is_error`: A boolean indicating whether an error occurred during the function execution.\n\n### FunctionExecutionResultMessage\n\nThe `FunctionExecutionResultMessage` class represents multiple results coming from function calls.\n\n- **Attributes:**\n  - `content`: A list of `FunctionExecutionResult` objects, each representing the outcome of a function call.\n  - `type`: A literal identifying this message type, set to `\"FunctionExecutionResultMessage\"`.\n\n## Composite Types\n\n### LLMMessage\n\nThe `LLMMessage` is an annotated union that allows for polymorphism among the defined message classes. It can be any of the message types (`SystemMessage`, `UserMessage`, `AssistantMessage`, or `FunctionExecutionResultMessage`).\n\n### RequestUsage\n\nThis is a data class encapsulating the usage statistics regarding the tokens used in the conversation.\n\n- **Attributes:**\n  - `prompt_tokens`: The count of tokens in the user prompt.\n  - `completion_tokens`: The count of tokens in the assistant's completion.\n\n## Token Log Probability Structures\n\n### TopLogprob\n\nThis data class provides information about the log probability of certain tokens.\n\n- **Attributes:**\n  - `logprob`: A float representing the log probability value.\n  - `bytes`: An optional list of integers corresponding to byte representations of the tokens.\n\n### ChatCompletionTokenLogprob\n\nThis class models the log probabilities for tokens generated during the model's completion.\n\n- **Attributes:**\n  - `token`: The specific token string.\n  - `logprob`: The associated log probability value.\n  - `top_logprobs`: An optional list containing `TopLogprob` objects.\n  - `bytes`: An optional list of integers representing byte values.\n\n## CreateResult\n\nThe `CreateResult` class summarizes the output produced by the AI model.\n\n- **Attributes:**\n  - `finish_reason`: A literal indicating why the model stopped generating content.\n  - `content`: Holds the output generated by the model, which can include text or function calls.\n  - `usage`: An instance of `RequestUsage` providing token usage data.\n  - `cached`: A boolean denoting whether the result was sourced from a cached entry.\n  - `logprobs`: An optional list of `ChatCompletionTokenLogprob` objects detailing token probabilities.\n  - `thought`: An optional string for any additional reasoning or text retained from the model's processing.\n\n## Conclusion\n\nThis code serves as a foundational element for an interactive AI conversational system, defining essential data structures for managing messages and results effectively. By leveraging the robustness of Pydantic and data classes, it ensures data integrity and clarity in communication between users and the assistant. Each class plays a critical role in capturing the complexity of interactions, simplifying the handling of varied message types and outcomes.",
    "filename": "python/packages/autogen-core/src/autogen_core/models/_types.py"
  },
  {
    "code": false,
    "content": "# Module Documentation\n\nThis module is part of a larger package and serves as an interface to various tool-related functionalities. It imports classes and functions from other modules and specifies which components should be publicly accessible when the module is imported.\n\n## Imports and Dependencies\n\nThe module begins with import statements that bring in the necessary functionalities from other parts of the package:\n\n```python\nfrom ._caller_loop import tool_agent_caller_loop\nfrom ._tool_agent import (\n    InvalidToolArgumentsException,\n    ToolAgent,\n    ToolException,\n    ToolExecutionException,\n    ToolNotFoundException,\n)\n```\n\n- **tool_agent_caller_loop**: A function or utility that likely manages the execution flow of tool agents. The exact details of its functionality are encapsulated within the `_caller_loop` module.\n\n- **InvalidToolArgumentsException**: This exception is thrown when invalid arguments are passed to a tool.\n\n- **ToolAgent**: A core class that presumably represents an agent capable of managing and executing tools within the system.\n\n- **ToolException**: A base class for exceptions related to tools, providing a way to catch all tool-related errors.\n\n- **ToolExecutionException**: A specific exception that is raised when there is a failure during the execution of a tool.\n\n- **ToolNotFoundException**: An exception that indicates a requested tool could not be located or does not exist.\n\n## Public API\n\nThe `__all__` variable defines the public interface of the module. This is important for encapsulation and understanding which components are meant to be accessible by users of the module.\n\n```python\n__all__ = [\n    \"ToolAgent\",\n    \"ToolException\",\n    \"ToolNotFoundException\",\n    \"InvalidToolArgumentsException\",\n    \"ToolExecutionException\",\n    \"tool_agent_caller_loop\",\n]\n```\n\nThe elements included in `__all__` are:\n\n- **ToolAgent**: The main class for tool management.\n- **ToolException**: A general-purpose exception for catching any tool-related errors.\n- **ToolNotFoundException**: For signaling when a specified tool is absent.\n- **InvalidToolArgumentsException**: To handle cases of invalid inputs to tools.\n- **ToolExecutionException**: For errors occurring during tool operations.\n- **tool_agent_caller_loop**: A top-level function important for the workflow in executing tools.\n\n## Overview of Components\n\n### ToolAgent Class\n\nThe `ToolAgent` class is likely a central component that provides methods and properties to initialize, configure, and execute various tools. It serves as the main interface for users to work with tool implementations, ensuring that tools are utilized correctly and efficiently.\n\n### Exception Handling\n\nThe various exceptions defined in this module allow for structured error handling. Users can implement customized error management strategies based on the type of exception raised:\n\n- `ToolException` provides a catch-all for general tool-related errors.\n- Specific exceptions like `InvalidToolArgumentsException` and `ToolExecutionException` allow developers to pinpoint issues more accurately and handle them accordingly.\n\n### Method of Execution\n\nThe `tool_agent_caller_loop` function facilitates the execution of tool agents. Although the specifics of this function's implementation are not provided in this module, it likely handles the process by which tool agents are called, monitored, and managed throughout their operational lifecycle.\n\n## Conclusion\n\nThis module acts as an entry point for tool management within the larger system, exposing essential classes and exceptions while maintaining a clean namespace through the use of the `__all__` directive. It encapsulates both the facilitation of tool execution and structured error handling, making it a foundational component of robust tool operations.",
    "filename": "python/packages/autogen-core/src/autogen_core/tool_agent/__init__.py"
  },
  {
    "code": false,
    "content": "# Tool Agent Caller Loop Documentation\n\n## Overview\nThe `tool_agent_caller_loop` function is an asynchronous coroutine designed to facilitate interactions between a client model and a tool agent. It manages the communication by sending messages back and forth, executing any function calls that the model generates, and collecting responses until the model stops making further tool calls.\n\n## Key Components\n\n### Imports\nThe function imports several components:\n- **AgentId, AgentRuntime, BaseAgent, CancellationToken, FunctionCall**: These are likely classes or types related to managing agents and their interactions.\n- **AssistantMessage, ChatCompletionClient, FunctionExecutionResult, FunctionExecutionResultMessage, LLMMessage**: These models are utilized for message formatting and processing.\n- **Tool, ToolSchema**: Used to define the tools that the model can utilize during its operation.\n- **ToolException**: Represents exceptions raised by tool operations.\n\n### Function Definition\n#### `async def tool_agent_caller_loop(...)`\nThe function begins its definition with various inputs:\n- **caller**: An instance of `BaseAgent` or `AgentRuntime`, which will send messages to the tool agent.\n- **tool_agent_id**: The identifier for the tool agent that will receive commands.\n- **model_client**: An instance of `ChatCompletionClient` that interacts with the underlying model API, generating responses.\n- **input_messages**: A list of messages sent to the model as input.\n- **tool_schema**: Defines the tools available for the model's use, as a list of `ToolSchema` or `Tool`.\n- **cancellation_token**: An optional parameter to manage cancellation requests.\n- **caller_source**: A string indicating the source of the caller, defaulting to \"assistant\".\n\n## Function Workflow\n\n### Initial Response Generation\n1. **Initial Call**: The function first sends the `input_messages` to the `model_client` using the `create` method, incorporating the available tools. The response is awaited and stored.\n2. **Append Response**: The generated response content is encapsulated as an `AssistantMessage` and added to the `generated_messages` list.\n\n### Iterative Execution of Tool Calls\n3. **Loop Until Completion**: The function enters a `while` loop that continues as long as the response's content is a list of `FunctionCall` objects. This structure suggests that the model may issue multiple requests in one response.\n4. **Execute Tool Calls**: Within the loop, it sends each function call to the `tool_agent` through the `caller` using `asyncio.gather`. It handles exceptions through `return_exceptions=True`, allowing the collection of results regardless of success or failure.\n   \n### Handling Results and Errors\n5. **Result Processing**: The results obtained are processed:\n    - Successful results of type `FunctionExecutionResult` are collected.\n    - Any `ToolException` is transformed into a `FunctionExecutionResult` with an error message.\n    - Unexpected exceptions will raise an error, ceasing the function's execution.\n6. **Response Compilation**: A `FunctionExecutionResultMessage` is created from the processed results and appended to the `generated_messages`.\n\n### Querying the Model Again\n7. **Subsequent Model Call**: After handling the tool calls, the function issues a new query to the model using the updated list of messages (including the generated responses) and appends this new message to `generated_messages`.\n\n## Final Output\n8. **Return Generated Messages**: Once the model ceases to generate further tool calls, the function exits the loop and returns the complete list of `generated_messages`, allowing potential downstream processes to access all the messages exchanged through the tool agent and model.\n\n## Conclusion\nThe `tool_agent_caller_loop` effectively orchestrates communication with a tool agent, managing both function calls and responses while providing mechanisms to handle errors gracefully. Its design emphasizes asynchronous execution, making it suitable for environments requiring responsive and non-blocking operations, such as chatbots or automated assistants.",
    "filename": "python/packages/autogen-core/src/autogen_core/tool_agent/_caller_loop.py"
  },
  {
    "code": false,
    "content": "# Tool Agent Documentation\n\nThis document provides a comprehensive overview of the `ToolAgent` class and its related components defined in the supplied source code. The code is structured to manage tool execution within a messaging context, optimizing for modularity and error handling.\n\n## Overview\n\nThe main functionality of the code revolves around a `ToolAgent` class that processes messages of type `FunctionCall`. It leverages a collection of tools to perform specific tasks, executes them with the provided arguments, and returns the results encapsulated in a `FunctionExecutionResult`. The agent also incorporates error handling mechanisms through various exception classes.\n\n## Exception Classes\n\n### ToolException\n\nThe base class for all exceptions related to tool execution. It provides common attributes to capture the context of the error:\n\n- **call_id**: An identifier for the function call that led to the exception.\n- **content**: A string that describes the error.\n- **name**: The name of the tool that was being executed.\n\n### ToolNotFoundException\n\nInherits from `ToolException`. This exception is raised when a specified tool cannot be found in the agent's list of available tools.\n\n### InvalidToolArgumentsException\n\nAlso inherits from `ToolException`. Raised when the arguments provided for a tool are deemed invalid, often due to parsing issues.\n\n### ToolExecutionException\n\nAnother subclass of `ToolException`. It is raised when an error occurs during the execution of a tool, which is not covered by the other exceptions.\n\n## ToolAgent Class\n\n### Definition and Initialization\n\nThe `ToolAgent` class inherits from `RoutedAgent` and is primarily responsible for handling function calls related to tools:\n\n- **description**: A string that describes the agent's purpose.\n- **tools**: A list of `Tool` objects that the agent can use to perform tasks.\n\nUpon initialization, the class stores the list of tools for subsequent operations.\n\n### Properties\n\n#### tools\n\nA property that returns the list of tools available to the agent. This encapsulation allows for controlled access to the internal tools list.\n\n## Message Handling\n\n### handle_function_call Method\n\nThis method is marked with the `@message_handler` decorator, indicating that it listens for incoming `FunctionCall` messages. Here\u2019s how it operates:\n\n1. **Finding the Tool**: The method attempts to locate a tool in the agent's tool list that matches the name specified in the incoming message. If no tool is found, a `ToolNotFoundException` is raised.\n\n2. **Argument Parsing**: If the tool exists, it attempts to parse the message's arguments from a JSON string. If parsing fails, an `InvalidToolArgumentsException` is raised, detailing the parsing error.\n\n3. **Tool Execution**: If the arguments are valid, it proceeds to execute the tool using the parsed arguments. The execution can potentially raise a `ToolExecutionException` if any unforeseen errors occur.\n\n4. **Result Handling**: Upon successful completion, the result of the tool execution is converted to a string using the tool's defined method. A `FunctionExecutionResult` message is then returned, encapsulating the output.\n\n## Summary of Code Flow\n\n1. The `ToolAgent` instance is initialized with a description and a list of tools.\n2. The agent listens for `FunctionCall` messages.\n3. Upon receiving a message, it attempts to find the corresponding tool.\n4. If found, it parses the provided arguments.\n5. If arguments are valid, it executes the tool and formats the result.\n6. Any errors encountered during this process are encapsulated through specific exception classes, ensuring robust error handling and clear messaging.\n\nThis structure allows for the addition of new tools with minimal changes to the agent logic while providing clear pathways for error management, thereby enabling robust tool execution in a structured messaging context.",
    "filename": "python/packages/autogen-core/src/autogen_core/tool_agent/_tool_agent.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface for a toolkit that likely facilitates the handling of various tools, including stream processing and function execution within a broader workflow or workbench context. It imports and organizes several classes and schemas from different submodules, making them accessible for external use. The use of `__all__` allows for the specification of a public API for the module.\n\n## Imports\n\nThe module starts by importing a set of classes and schemas from different parts of the package. Here's a breakdown of the imports:\n\n- **Base Classes**  \n  The `BaseStreamTool`, `BaseTool`, and `BaseToolWithState` classes are foundational tools meant to be extended for specific purposes within the toolkit. They likely provide common functionality that can be shared among various tool implementations.\n\n- **Schemas**  \n  `ParametersSchema` and `ToolSchema` probably define the structure or validation requirements for parameters and tools, respectively.\n\n- **Specific Tools**  \n  `StreamTool` could represent a specialized tool designed for processing streaming data, while `FunctionTool` likely wraps around functions to provide them as tools within the workbench environment.\n\n- **Workbench Components**  \n  The `Workbench`, `StaticWorkbench`, `StaticStreamWorkbench`, `ToolResult`, `TextResultContent`, and `ImageResultContent` classes are likely designed to manage and present results from the tools executed within a workbench setup. They may handle different data types, encapsulating the results of tool executions along with relevant metadata.\n\n## Public API\n\nThe `__all__` list explicitly declares the public API of the module, outlining which classes and functions can be accessed when using `from module import *`. This serves several purposes:\n\n- **Encapsulation**  \n  By controlling what is exposed, the module can prevent direct access to internal components that are not intended for use outside of the module, encouraging better practices and reducing the chance of incorrect usage.\n\n- **Ease of Access**  \n  Users of the module can easily see which tools and classes are available for them to use, streamlining their interaction with the toolkit.\n\n## Key Classes and Their Roles\n\n### Base Classes\n\n- **BaseTool**: This class probably provides a framework for creating tools within the module, encapsulating common functionalities needed by all tools. \n- **BaseToolWithState**: This likely extends `BaseTool` to support maintaining internal state information across invocations, useful for tools that need to remember context or progress.\n- **BaseStreamTool**: This may provide specialized functionalities needed for streaming tools, such as buffering or continuous processing capabilities.\n\n### Tool Classes\n\n- **Tool**: This is likely a more generalized utility for representing a tool within the workbench.\n- **StreamTool**: Designed for tools that work with data streams, it may encapsulate the logic and necessary configurations to handle streaming data effectively.\n- **FunctionTool**: This acts as a wrapper for standard functions, turning them into tools that can be used within the workbench framework.\n\n### Workbench Components\n\n- **Workbench**: This may be the main hub for executing and managing tools, providing infrastructure for input/output as well as result handling.\n- **StaticWorkbench**: A variant of `Workbench`, potentially designed for static data rather than streaming, giving users a different set of capabilities for non-streaming tasks.\n- **StaticStreamWorkbench**: Presumably combines aspects of both static and streaming workbenches, perhaps designed for scenarios that require both paradigms.\n\n## Result Handling Classes\n\n- **ToolResult**: Likely encapsulates the results returned from tool executions, including success/failure statuses and relevant output data.\n- **TextResultContent**: This class is expected to manage the textual output produced by tools, providing a way to access and format the results.\n- **ImageResultContent**: Similar to `TextResultContent`, but specifically designed for visual data outputs, allowing users to handle and display images generated as a result of tool execution.\n\n## Conclusion\n\nThis module provides a structured foundation for tool integration and management within a workflow, facilitating both streaming and static data processing. It abstracts common functionalities through base classes and organizes tool results in a way that's accessible and easy to interact with. Overall, the design suggests a robust toolkit aimed at enhancing productivity in contexts where various tools need to be systematically utilized and managed. This modular approach allows for flexibility in extending and customizing functionality according to user requirements.",
    "filename": "python/packages/autogen-core/src/autogen_core/tools/__init__.py"
  },
  {
    "code": false,
    "content": "# Tool Framework Documentation\n\nThis documentation describes a Python-based tool framework designed to define and implement various tools with structured input and output properties. Built on top of the `Pydantic` library, it emphasizes type safety, schema validation, and the ability to handle tool state.\n\n## 1. Overview\n\nThe framework leverages Python's typing system, abstract base classes, and protocols to facilitate tool definition and functionality. It encapsulates tool properties, input arguments, and results, enabling both synchronous and asynchronous operations. Logging and structured schemas allow for deeper tracing of tool executions.\n\n## 2. Definitions of Key Classes and Protocols\n\n### 2.1. Tool Protocol\n\nThe `Tool` Protocol defines the essential properties and methods that any tool must implement. This includes attributes for tool name and description, schema generation, argument and return type management, and methods for running the tool and managing its state.\n\n```python\n@runtime_checkable\nclass Tool(Protocol):\n    ...\n```\n\n**Key Methods**:\n- `run_json`: Executes the tool given a dictionary of arguments and handles cancellation.\n- `save_state_json`: Prepares the current state for saving in JSON format.\n- `load_state_json`: Loads state from a JSON representation.\n\n### 2.2. StreamTool Protocol\n\nThe `StreamTool` extends the `Tool` protocol, adding the capability for tools that yield a stream of results. This is particularly useful in scenarios where operations can produce intermediate results over time.\n\n```python\n@runtime_checkable\nclass StreamTool(Tool, Protocol):\n    ...\n```\n\n**Key Method**:\n- `run_json_stream`: Runs the tool with arguments and returns streamed outputs.\n\n## 3. Base Classes for Tool Implementation\n\n### 3.1. BaseTool\n\nThe `BaseTool` class provides a foundational implementation for creating tools while enforcing specific behavior and schema validation.\n\n```python\nclass BaseTool(ABC, Tool, Generic[ArgsT, ReturnT], ComponentBase[BaseModel]):\n    ...\n```\n\n**Key Features**:\n- Normalizes annotated types for input and output.\n- Generates a JSON schema for the tool's arguments.\n- Handles logging of tool execution events.\n\n**Abstract Method**:\n- `run`: Must be implemented by subclasses; contains the core logic for the tool.\n\n### 3.2. BaseStreamTool\n\nThe `BaseStreamTool` class is designed for tools that need to provide streaming responses. It builds upon the `BaseTool` and implements the streaming method outlined in the `StreamTool` protocol.\n\n```python\nclass BaseStreamTool(BaseTool[ArgsT, ReturnT], StreamTool, ABC, Generic[ArgsT, StreamT, ReturnT]):\n    ...\n```\n\n### 3.3. BaseToolWithState\n\nThis class adds capabilities for managing tool state. It extends `BaseTool` and introduces methods for saving and loading state, facilitating continuity across tool executions.\n\n```python\nclass BaseToolWithState(BaseTool[ArgsT, ReturnT], ABC, Generic[ArgsT, ReturnT, StateT], ComponentBase[BaseModel]):\n    ...\n```\n\n## 4. Tool Schema Definition\n\nThe `ToolSchema` and `ParametersSchema` are typed dictionaries that define the structure of a tool's schema:\n\n```python\nclass ParametersSchema(TypedDict):\n    ...\n```\n\nThese schemas specify:\n- The type of the tool's parameters.\n- Required properties and constraints like whether additional properties are allowed.\n\nTools must comply with these schemas to ensure proper validation.\n\n## 5. Argument and Return Type Management\n\nThe framework utilizes Pydantic's features to manage argument types and ensures that input values conform to specified schemas.\n\n### 5.1. Argument Validation\n\nThe `run_json` method validates input arguments against the defined Pydantic model using `model_validate`, ensuring both runtime safety and type correctness.\n\n### 5.2. Return Value Formatting\n\nThe method `return_value_as_string` allows tools to serialize their return values (if they are Pydantic models) into JSON format or strings. This enhances inter-tool communication.\n\n## 6. Event Logging\n\nThe framework incorporates logging facilities to capture tool execution details:\n\n```python\nlogger.info(event)\n```\n\nEach execution of a tool logs relevant details such as the tool name, arguments used, and the result produced. This is crucial for monitoring and debugging.\n\n## 7. State Management\n\nTools derived from `BaseToolWithState` can manage internal state, allowing them to pause and resume operations or to save the context across sessions.\n\n### 7.1. State Serialization\n\nState data is serialized to JSON for easy storage, allowing the tool states to be recreated or inspected later.\n\n### 7.2. State Restoration\n\nTools can restore their internal state from a JSON representation, facilitating a seamless user experience.\n\n## 8. Conclusion\n\nThis tool framework provides a robust and extensible platform for defining, executing, and managing tools in a structured manner. It emphasizes type safety, logging, and state management while leveraging modern Python features and libraries. By adhering to these protocols and base classes, developers can create powerful tools integrated into larger systems or applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/tools/_base.py"
  },
  {
    "code": false,
    "content": "# FunctionTool Documentation\n\n## Overview\nThe `FunctionTool` class is designed to create custom tools by encapsulating standard Python functions, allowing them to be executed as either asynchronous or synchronous commands. This class accepts functions that have a defined signature with type annotations, making it suitable for integration with a machine learning model, as it can generate the necessary schema for input validation.\n\n## Class Definition\n\n### FunctionToolConfig\n```python\nclass FunctionToolConfig(BaseModel):\n    \"\"\"Configuration for a function tool.\"\"\"\n    source_code: str\n    name: str\n    description: str\n    global_imports: Sequence[Import]\n    has_cancellation_support: bool\n```\nThe `FunctionToolConfig` class extends `BaseModel` and encapsulates the configuration for a tool. It contains:\n- `source_code`: The actual code of the function to be executed.\n- `name`: A human-readable identifier for the function tool.\n- `description`: A string explaining the purpose of the function tool.\n- `global_imports`: A sequence of import statements necessary for the function's execution.\n- `has_cancellation_support`: A boolean indicating if the function can handle cancellation tokens, which allow for safe interruption of long-running operations.\n\n### FunctionTool\n```python\nclass FunctionTool(BaseTool[BaseModel, BaseModel], Component[FunctionToolConfig]):\n    \"\"\"\n    Create custom tools by wrapping standard Python functions.\n    ...\n    \"\"\"\n```\n`FunctionTool` is a specialized tool class that modifies a standard function with additional context, allowing it to be used with a machine learning model. It inherits from `BaseTool` and `Component`, which provides foundational capabilities and ensures compatibility within its ecosystem.\n\n### Constructor\n```python\ndef __init__(self, func: Callable[..., Any], description: str, name: str | None = None, global_imports: Sequence[Import] = [], strict: bool = False) -> None:\n```\nThe constructor initializes a `FunctionTool` instance. Key parameters include:\n- `func`: The function to wrap.\n- `description`: A description of what the function does.\n- `name`: An optional custom name for the tool (defaults to the function's name).\n- `global_imports`: A sequence of additional imports required for the function.\n- `strict`: A boolean that enforces strict argument handling in schemas.\n\nDuring initialization, the function's signature is analyzed to create a model for input verification and argument mapping.\n\n## Method Overview\n\n### run\n```python\nasync def run(self, args: BaseModel, cancellation_token: CancellationToken) -> Any:\n```\nThe `run` method is designed to execute the encapsulated function. It takes two parameters:\n- `args`: An instance of `BaseModel` containing the arguments for the function.\n- `cancellation_token`: An instance of `CancellationToken` that allows invocation of a cancellation signal.\n\nDepending on whether the wrapped function is a coroutine or standard function, it executes the function appropriately, maintaining cancellation support when indicated by the function's signature.\n\n### _to_config\n```python\ndef _to_config(self) -> FunctionToolConfig:\n```\nThis method converts the current `FunctionTool` instance into a `FunctionToolConfig` instance. It extracts relevant attributes such as the function\u2019s source code, imports, name, and description, which can later be used for serialization or introspection.\n\n### _from_config\n```python\n@classmethod\ndef _from_config(cls, config: FunctionToolConfig) -> Self:\n```\nThe class method `_from_config` allows reconstruction of a `FunctionTool` instance from a configuration object. It executes the imports defined in the configuration and compiles the function's code, which it then makes callable. A crucial security warning is included to alert users that loading configurations from untrusted sources could lead to arbitrary code execution.\n\n## Example Usage\n```python\nasync def example():\n    stock_price_tool = FunctionTool(get_stock_price, description=\"Fetch the stock price for a given ticker.\")\n    cancellation_token = CancellationToken()\n    result = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n    print(stock_price_tool.return_value_as_string(result))\n\nasyncio.run(example())\n```\nIn the example, `FunctionTool` is used to wrap an asynchronous function that retrieves a stock price. The tool is then executed with specific arguments to fetch and print the stock price, demonstrating its operational capabilities.\n\n## Conclusion\n`FunctionTool` provides a robust and flexible way to expose Python functions as interactive tools, integrating them with async frameworks and offering validation mechanisms through type annotations. Its design encourages safe code execution and structured interactions with models, making it a valuable component in asynchronous programming paradigms.",
    "filename": "python/packages/autogen-core/src/autogen_core/tools/_function_tool.py"
  },
  {
    "code": false,
    "content": "# Documentation for StaticWorkbench and StaticStreamWorkbench\n\nThis module provides implementations for a `StaticWorkbench` and `StaticStreamWorkbench`, which allow users to manage tools without changing their toolsets after execution. The workbenches utilize tools that are subclasses of `BaseTool`. Let's break down the components and functionality.\n\n## Overview of Classes\n\n### StaticWorkbenchConfig\n\nThe `StaticWorkbenchConfig` class inherits from `BaseModel` and serves as a configuration model for the `StaticWorkbench`. It establishes the schema for configuration related to tools and tool overrides.\n\n#### Attributes:\n- `tools`: A list of `ComponentModel` instances representing the tools included in the workbench.\n- `tool_overrides`: A dictionary that maps original tool names to their override configurations, allowing customization of tool appearances.\n\n### StaticWorkbenchState\n\nThe `StaticWorkbenchState` class also inherits from `BaseModel`. It represents the internal state of the `StaticWorkbench`.\n\n#### Attributes:\n- `type`: A literal string to identify the state type for validation purposes.\n- `tools`: A dictionary mapping tool names to their state representations.\n\n### StaticWorkbench\n\nThe `StaticWorkbench` class combines the tools and configuration defined by the earlier classes. It is designed for static tool management.\n\n#### Initialization:\n- Takes a list of tools and optional tool overrides. It performs checks to ensure no name conflicts arise between tool overrides and existing tool names.\n\n#### Key Methods:\n- **list_tools**: Asynchronously retrieves a list of tool schemas. It applies overrides if any exist for the tools.\n- **call_tool**: Asynchronously calls a specified tool by its name, mapping any overrides back to the original name. It handles arguments, return values, and exceptions gracefully.\n- **start, stop, reset**: Placeholder methods that do nothing, providing a structure for potential future functionality.\n- **save_state**: Asynchronously captures the states of the included tools and returns them.\n- **load_state**: Loads the state back into the workbench from a given state mapping.\n- **_to_config and _from_config**: Internal methods for serialization and deserialization of configurations.\n- **_format_errors**: Formats exceptions into a user-friendly string.\n\n### StaticStreamWorkbench\n\nThe `StaticStreamWorkbench` class extends `StaticWorkbench` and is tailored for tools that provide streaming results.\n\n#### Initialization:\n- Inherits behavior from `StaticWorkbench`.\n\n#### Key Methods:\n- **call_tool_stream**: Similar to `call_tool`, but designed for streaming tool outputs if the tool supports it. It manages async generators to yield results as they come.\n\n## Functionality Breakdown\n\n### Tool Management\n\nThe workbenches primarily focus on managing a static set of tools:\n- Tools can be listed and overridden, allowing for customization in how they appear to consumers without altering their fundamental behavior.\n- The workbench ensures that no name conflicts occur during the initialization phase of tool overrides.\n\n### Asynchronous Execution\n\nBoth `StaticWorkbench` and `StaticStreamWorkbench` handle tools asynchronously:\n- They leverage `asyncio` to ensure non-blocking calls to tool methods, which allows for better performance in a concurrent environment.\n- Cancellation tokens can be employed to cancel operations if necessary, adding responsiveness.\n\n### Error Handling\n\nA critical feature is the error handling mechanism:\n- Exceptions raised during tool execution are captured and formatted, ensuring that users receive informative error messages instead of raw exception traces. This acts to enhance user experience and debugging.\n\n### State Management\n\nState capture and restoration capabilities:\n- The workbench allows for saving the state of the tools it encompasses, facilitating persistence across sessions.\n- It can load previously saved states to restore the workbench to a prior condition, which is particularly useful in scenarios requiring the reexecution of dynamic workflows.\n\n## Summary\n\nThe `StaticWorkbench` and `StaticStreamWorkbench` classes represent a structured approach to managing a fixed set of tools in a user-friendly and asynchronous manner. They provide features for customization, error management, and state persistence, making them ideal for applications that require reliable and robust tool execution environments.",
    "filename": "python/packages/autogen-core/src/autogen_core/tools/_static_workbench.py"
  },
  {
    "code": false,
    "content": "# Documentation for Tool Execution Framework\n\nThis documentation provides an overview of a tool execution framework built using Pydantic for data handling and abstract base classes for structured implementations. The framework defines model structures for handling results from tools and a workbench class as a foundation for managing tool execution.\n\n## Result Content Models\n\n### TextResultContent\n\nThe `TextResultContent` class is a Pydantic model that represents textual results produced by tool executions. It contains:\n\n- `type`: A literal string indicating this is a `TextResultContent`.\n- `content`: A string field that holds the actual text result.\n\n```python\nclass TextResultContent(BaseModel):\n    type: Literal[\"TextResultContent\"] = \"TextResultContent\"\n    content: str\n```\n\n### ImageResultContent\n\nThe `ImageResultContent` class is similar to the `TextResultContent`, but it is designed for image results. It includes:\n\n- `type`: A literal string to confirm this is an `ImageResultContent`.\n- `content`: This field holds an instance of the `Image` class, which is presumably defined elsewhere in the project, representing the image data.\n\n```python\nclass ImageResultContent(BaseModel):\n    type: Literal[\"ImageResultContent\"] = \"ImageResultContent\"\n    content: Image\n```\n\n### ResultContent\n\n`ResultContent` is an annotated type that combines both `TextResultContent` and `ImageResultContent`. It uses a discriminator to identify the result type and facilitate proper deserialization.\n\n```python\nResultContent = Annotated[TextResultContent | ImageResultContent, Field(discriminator=\"type\")]\n```\n\n### ToolResult\n\nThe `ToolResult` class aggregates results from tool executions and serves as the central structure for output. Key attributes include:\n\n- `type`: A literal string representing this as a `ToolResult`.\n- `name`: The name of the executed tool.\n- `result`: A list that may contain a mixture of `TextResultContent` and `ImageResultContent` entries.\n- `is_error`: A boolean that indicates if the execution resulted in an error.\n\nAdditionally, this class includes the method `to_text`, which converts the result to a string representation, allowing images to be replaced by a placeholder or included as base64 encoded strings.\n\n```python\nclass ToolResult(BaseModel):\n    # class attributes and methods\n```\n\n## Workbench Class\n\n`Workbench` serves as an abstract base class, adhering to the principles of component-based architecture. It defines essential behaviors and lifecycle management for tools:\n\n- **Lifecycle Methods**:\n  - `start()`: Initializes resources, preparing the workbench for use.\n  - `stop()`: Cleans up resources when the workbench is no longer required.\n  - `reset()`: Resets the workbench to its initial state.\n  \n- **State Management**:\n  - `save_state()`: Persists the current state of the workbench.\n  - `load_state(state: Mapping[str, Any])`: Loads pre-saved state into the workbench.\n  \n- **Tool Management**:\n  - `list_tools()`: Returns a dynamic list of available tools.\n  - `call_tool(...)`: Executes a specified tool and produces a `ToolResult`.\n\nThe `Workbench` class can be utilized in an asynchronous context manager, automatically managing the starting and stopping of the workbench.\n\n```python\nclass Workbench(ABC, ComponentBase[BaseModel]):\n    # class methods and abstract methods\n```\n\n## StreamWorkbench Class\n\nThe `StreamWorkbench` class extends from `Workbench`, indicating its specialized functionality for streaming results from tools. It includes an additional abstract method, `call_tool_stream(...)`, which enables the execution of tools and the streaming of their results. This allows for asynchronous processing and real-time feedback.\n\n```python\nclass StreamWorkbench(Workbench, ABC):\n    # class methods\n```\n\n## Summary of Functionality\n\nThis framework is structured to facilitate tool execution within a workbench environment:\n\n1. **Result Handling**: The framework defines structured models for handling both textual and image results of tool executions, allowing for flexible output formats.\n   \n2. **Abstract Management**: The `Workbench` class encapsulates the responsibilities of lifecycle management and state handling for the tools it orchestrates, promoting clean and manageable code.\n\n3. **Dynamic and Asynchronous**: The design supports dynamic lists of tools and asynchronous execution, making it suitable for real-time applications and environments where tools may change or require concurrent processing.\n\n4. **Streaming**: By extending `Workbench` with `StreamWorkbench`, it provides further capabilities to handle streaming results, offering additional flexibility in how tool results are processed.\n\nThis documentation should provide a clear understanding of the intended use, structure, and capabilities of the tool execution framework.",
    "filename": "python/packages/autogen-core/src/autogen_core/tools/_workbench.py"
  },
  {
    "code": false,
    "content": "# Module Documentation\n\nThis module serves as an interface for functionalities related to JSON schema and Pydantic models. It imports specific functions from separate modules to facilitate these capabilities and provides them for external access.\n\n## Imports\n\n### `schema_to_pydantic_model`\n\n- **Source**: Imported from the module `._json_to_pydantic`.\n- **Purpose**: This function is intended to convert a JSON schema into a Pydantic model. Pydantic is a data validation and settings management library for Python, leveraging Python type annotations.\n- **Usage**: When invoked, it takes a JSON schema as input and outputs a corresponding Pydantic model, which can be used for data validation and serialization.\n\n### `extract_json_from_str`\n\n- **Source**: Imported from the module `._load_json`.\n- **Purpose**: This function is designed to extract JSON data from a string format.\n- **Usage**: It accepts a string that represents JSON data and returns a Python object. This is useful for converting JSON strings into usable data structures, such as dictionaries or lists.\n\n## Exposed Interfaces\n\n### `__all__`\n\n- **Definition**: The `__all__` variable is defined as a list containing the names of the functions that are intended to be publicly accessible from this module.\n- **Contents**: \n  - `\"schema_to_pydantic_model\"`\n  - `\"extract_json_from_str\"`\n- **Functionality**: By defining `__all__`, the module specifies which elements can be imported when a user employs the `from module import *` syntax. This helps in maintaining a clean namespace and avoiding unintended access to internal functions.\n\n## Summary\n\nThis module integrates JSON handling and Pydantic model generation capabilities by importing two specific functions from their respective modules. It sets up a clean interface for users, allowing them to:\n- Convert JSON schemas into Pydantic models for application development.\n- Extract JSON data from string representations for manipulation and processing in Python.\n\nThe overall architecture promotes modularity and reusability of code, providing key functionalities without exposing inner workings, thus aligning with good software design principles.",
    "filename": "python/packages/autogen-core/src/autogen_core/utils/__init__.py"
  },
  {
    "code": false,
    "content": "# Schema to Pydantic Model Conversion\n\n## Overview\n\nThis code module provides functionality to convert JSON Schema definitions into fully-typed Pydantic models. It utilizes the Pydantic library to enforce data validation and serialization rules based on the provided JSON schema. The key features supported by this module include handling primitive types, constraints on those types, support for nested objects, and reference resolution.\n\n## Exception Classes\n\n### `SchemaConversionError`\n\nThis is the base exception for any issue related to schema conversion. It allows for organized error handling and provides a clear indication of conversion-related issues.\n\n### `ReferenceNotFoundError`\n\nThis exception is raised when a JSON Schema reference (`$ref`) cannot be resolved. It indicates that the specified reference is missing from the `$defs` section of the schema.\n\n### `FormatNotSupportedError`\n\nThis exception is thrown when a format specified in the schema is not recognized or supported. The Pydantic types related to format conventions like emails, UUIDs, and IP addresses are predefined; if a schema requests an unsupported format, this error will emerge.\n\n### `UnsupportedKeywordError`\n\nThis exception occurs when the schema contains a keyword that is not supported within the conversion logic. It helps to highlight unsupported features in the JSON schema.\n\n## Type Mappings\n\nTwo dictionaries, `TYPE_MAPPING` and `FORMAT_MAPPING`, facilitate the connection between JSON Schema types/keywords and their corresponding Python/Pydantic types:\n\n- **Type Mapping:**\n  Maps basic JSON types (e.g., `string`, `integer`) to their Python representations. \n\n- **Format Mapping:**\n  Maps specific formats (e.g., `uuid`, `email`, `ipv4`) to defined Pydantic types that validate and parse data accordingly.\n\n## Helper Function: `_make_field`\n\n```python\ndef _make_field(default: Any, *, title: Optional[str] = None, description: Optional[str] = None) -> Any:\n    ...\n```\n\nThis helper function constructs a Pydantic `Field` with the necessary attributes. It allows for specifying defaults, titles, and descriptions, making it easier to handle Pydantic field creation throughout the conversion process.\n\n## Class: `_JSONSchemaToPydantic`\n\n### Overview\n\nThis internal class encapsulates the core conversion logic. It manages model caching, reference resolution, and translates JSON schemas into Pydantic models via a series of private methods.\n\n### Methods\n\n1. **`__init__`**  \n   Initializes an empty model cache to store dynamically generated models for reuse.\n\n2. **`_resolve_ref`**  \n   Resolves a `$ref` to its corresponding definition in the schema. It raises a `ReferenceNotFoundError` if the reference does not exist.\n\n3. **`get_ref`**  \n   Retrieves a model from the cache. If the reference is not found, it raises a `ReferenceNotFoundError`.\n\n4. **`_get_item_model_name`**  \n   Generates unique names for item models in arrays by creating a hash based on the parent model and field name.\n\n5. **`_process_definitions`**  \n   Processes `$defs` in the schema, caching model names for quick access later.\n\n6. **`json_schema_to_pydantic`**  \n   The primary method that translates a JSON schema into a Pydantic model. It handles `$ref`, merges `allOf` properties, and invokes subsequent conversion steps.\n\n7. **`_resolve_union_types`**  \n   Manages the resolution of union types through JSON Schemas, extracting allowable types and their constraints.\n\n8. **`_extract_field_type`**  \n   Determines the Pydantic type for a schema field, incorporating constraints and formats accordingly.\n\n9. **`_json_schema_to_model`**  \n   Converts the JSON schema into a Pydantic model, handling various schema structure elements including properties and required fields.\n\n## Public Function: `schema_to_pydantic_model`\n\n```python\ndef schema_to_pydantic_model(schema: Dict[str, Any], model_name: str = \"GeneratedModel\") -> Type[BaseModel]:\n    ...\n```\n\nThis function serves as the public API for converting a JSON schema to a Pydantic model. It safely encapsulates the processes handled by the `_JSONSchemaToPydantic` class and handles any exceptions that arise during conversion. The function exposes the following functionality:\n\n- It supports various JSON schema features, including:\n  - Primitive types and their constraints.\n  - Nested object schemas.\n  - Enum support converted to Python `Literal` type.\n  - Union types management, allowing for `anyOf` and `oneOf` constructs.\n  - Merging of schemas through `allOf`.\n  \n### Examples\n\nThe documentation details multiple use-case examples demonstrating how to define various JSON schemas and convert them into Pydantic models. This includes simple models, nested structures, and examples of merging models and handling self-referential structures.\n\n### Return and Exception Handling\n\nOn successful execution, the function returns a dynamically generated Pydantic model class. Various exceptions are defined and raised based on the encountered errors during the conversion process, ensuring users are informed of the nature of the issue. These include reference resolution failures and unsupported formats.\n\n## Conclusion\n\nThis module provides a robust foundation for converting JSON Schema definitions into Pydantic models, facilitating data validation and manipulation in Python applications. With extensive handling for various schema types, this utility can be invaluable for developers needing strict data contracts in their applications.",
    "filename": "python/packages/autogen-core/src/autogen_core/utils/_json_to_pydantic.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code is a Python script that defines a function for extracting JSON objects from a given string. It specifically looks for JSON-formatted text wrapped in code blocks marked by triple backticks (` ``` `), which may optionally specify a language. The function ensures that the content is valid JSON and raises an error for unsupported languages.\n\n## Import Statements\n\nThe script begins with several import statements that bring in necessary modules:\n\n```python\nimport json\nimport re\nfrom typing import Any, Dict, List\n```\n\n- `json`: This module provides functionality for encoding and decoding JSON data.\n- `re`: This module is used for working with regular expressions, allowing for pattern matching within strings.\n- `typing`: This is used for type hints in function signatures, enhancing code readability and helping with static type checking.\n\n## Function: `extract_json_from_str`\n\n### Purpose\n\nThis function is designed to extract JSON objects from a string containing possibly multiple code blocks. It handles both cases where the relevant JSON data is wrapped in backticks as well as scenarios where the entire string itself is a valid JSON object.\n\n### Function Signature\n\n```python\ndef extract_json_from_str(content: str) -> List[Dict[str, Any]]:\n```\n\n- **Parameters**:\n  - `content` (str): A string possibly containing one or more JSON objects.\n  \n- **Returns**:\n  - A list of dictionaries, where each dictionary represents a JSON object extracted from the input string.\n\n### Implementation Steps\n\n1. **Regular Expression Setup**:\n   A pattern is compiled to find code blocks in the input string. It looks for content wrapped in triple backticks and optionally followed by a programming language identifier.\n\n   ```python\n   pattern = re.compile(r\"```(?:\\s*([\\w\\+\\-]+))?\\n([\\s\\S]*?)```\")\n   ```\n\n2. **Finding Matches**:\n   The `findall` method is used with the compiled pattern to extract all matches from the input string.\n\n   ```python\n   matches = pattern.findall(content)\n   ```\n\n3. **Result Initialization**:\n   An empty list is initialized to store the JSON objects that are to be extracted.\n\n   ```python\n   ret: List[Dict[str, Any]] = []\n   ```\n\n4. **Handling Absence of Matches**:\n   If no matches are found, the function assumes that the entire provided `content` is a JSON object and attempts to parse it.\n\n   ```python\n   if not matches:\n       ret.append(json.loads(content))\n   ```\n\n5. **Processing Matches**:\n   For each match found, the function checks if a language is specified. If it is and it is not \"json\", a `ValueError` is raised. Only the content inside the backticks is processed further.\n\n   ```python\n   for match in matches:\n       language = match[0].strip() if match[0] else None\n       if language and language.lower() != \"json\":\n           raise ValueError(f\"Expected JSON object, but found language: {language}\")\n       content = match[1]\n       ret.append(json.loads(content))\n   ```\n\n6. **Final Return**:\n   The function returns the list of parsed JSON objects.\n\n   ```python\n   return ret\n   ```\n\n### Error Handling\n\nThe function includes error handling to ensure that the language of the code block specified by the user is \"json\". If not, it raises a `ValueError`, signaling that only JSON is valid.\n\n## Summary\n\nThe script provides a utility function, `extract_json_from_str`, to facilitate the extraction of JSON data from strings that may contain formatted text in code blocks. It is particularly useful for parsing text that includes snippets of JSON not readily isolated or for validating and extracting standalone JSON content. The use of regular expressions allows for flexible capturing of multiple JSON objects from various string formats.",
    "filename": "python/packages/autogen-core/src/autogen_core/utils/_load_json.py"
  },
  {
    "code": false,
    "content": "# Protocol Buffer Generated Code Documentation\n\nThis document provides a high-level description of the generated Python protocol buffer code from the file `serialization_test.proto`. The code mainly focuses on defining message types that can be serialized and deserialized using Google's Protocol Buffers (protobuf).\n\n## Purpose\n\nThe purpose of this code is to create Python classes that correspond to the message definitions found in the provided `.proto` file. This generated code allows for the effective encoding, decoding, and handling of structured data, which can be used in network communications or data storage.\n\n## Imports and Dependencies\n\nThe code imports several modules from the Google Protocol Buffers library:\n\n- `descriptor`\n- `descriptor_pool`\n- `runtime_version`\n- `symbol_database`\n- `builder`\n\nThese modules provide functionality for defining protocol buffer descriptors, managing message types, and performing various operations related to serialization and deserialization of protocol messages.\n\n## Runtime Version Check\n\nAt the beginning of the code, there is a runtime version validation step. This checks that the current version of the protobuf runtime (5.29.0) matches the expected version for the generated code. If there is a version mismatch, it would signal that the code may not function correctly.\n\n## Message Definitions\n\n### ProtoMessage\n\nThe first message defined in this protobuf file is `ProtoMessage`:\n\n```proto\nmessage ProtoMessage {\n  string message = 1;\n}\n```\n- **Purpose**: This message contains a single field called `message`, which is of type string. \n- **Use Case**: It can be used to encapsulate text-based messages for transmission or storage.\n\n### NestingProtoMessage\n\nThe second message defined is `NestingProtoMessage`:\n\n```proto\nmessage NestingProtoMessage {\n  string message = 1;\n  ProtoMessage nested = 2;\n}\n```\n- **Purpose**: This message also contains a string field called `message`, similar to `ProtoMessage`, but adds another field called `nested`.\n- **Nested Message**: The `nested` field is of type `ProtoMessage`, indicating that `NestingProtoMessage` can contain another instance of `ProtoMessage`.\n- **Use Case**: This structure allows for the representation of more complex data, where a message can contain other messages, enabling hierarchical or nested data representation.\n\n## Descriptor and Globals\n\nThe code defines a `DESCRIPTOR` pool where the serialized file containing the message definitions is added. The descriptors created are used for the classes generated for the defined messages.\n\nA global variable dictionary (`_globals`) is populated to hold the generated message descriptors and enumeration types. The builder functions utilize this dictionary to create the message classes.\n\n## Building Message Descriptors\n\nUsing the `builder` module, the code constructs message and enum descriptors based on the previously defined descriptors. This step is crucial as it prepares the message types for instantiation and use within the application.\n\n## Descriptor Options\n\nAfter building the descriptors, the code checks for a specific condition related to the use of C descriptors. If the condition is not met, it clears the loaded options for the descriptor, ensuring that the Python runtime uses the correct descriptors for serialization and deserialization tasks.\n\n### Finalization\n\nThis part of the code with `@@protoc_insertion_point` comments is used as placeholders by the protocol buffer compiler, indicating where additional generated code may be inserted or other customizations if necessary.\n\n## Conclusion\n\nThis generated code provides foundational structures for working with protocol buffers in Python. It defines two message types, `ProtoMessage` and `NestingProtoMessage`, which can be utilized in various applications requiring structured data serialization. These generated classes enable developers to easily encode and decode data as needed, ensuring efficient data handling across different platforms or services.",
    "filename": "python/packages/autogen-core/tests/protos/serialization_test_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Python Generated Code Documentation\n\n## Overview\n\nThis code is part of a gRPC client-server implementation generated by the protocol buffer (protobuf) compiler for Python. It serves as the backbone for communication between gRPC clients and servers using defined services and messages. The module ensures that the version of the gRPC library being used is compatible with the generated code. \n\n## Dependencies and Imports\n\nThe script begins by importing necessary modules:\n\n```python\nimport grpc\nimport warnings\n```\n\n- **grpc**: This is the core library for gRPC in Python, enabling the creation and management of gRPC clients and servers.\n- **warnings**: This module allows the generation of warning messages, although it is not actively used in the provided code.\n\n## Version Management\n\nTwo constants are defined to manage versioning:\n\n```python\nGRPC_GENERATED_VERSION = '1.70.0'\nGRPC_VERSION = grpc.__version__\n```\n\n- **GRPC_GENERATED_VERSION**: This specifies the version of gRPC that the generated code is compatible with.\n- **GRPC_VERSION**: This dynamically retrieves the current version of the installed `grpc` library.\n\n## Version Compatibility Check\n\nA boolean flag, `_version_not_supported`, is initialized to track if the installed version of gRPC is suitable:\n\n```python\n_version_not_supported = False\n```\n\n### Import and Comparison\n\nThe script attempts to import a utility function for version comparison:\n\n```python\nfrom grpc._utilities import first_version_is_lower\n```\n\nThis utility is used to check if the installed version of gRPC is lower than the required version defined by `GRPC_GENERATED_VERSION`.\n\n### Exception Handling\n\nIf the import is successful, the flag is set accordingly:\n\n```python\n_version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\n```\n\nIf the import fails (indicating the utility is unavailable), the script assumes the version is unsupported:\n\n```python\n_version_not_supported = True\n```\n\n## Runtime Error Management\n\nThe code checks the status of the `_version_not_supported` flag:\n\n```python\nif _version_not_supported:\n```\n\nIf it evaluates to `True`, a `RuntimeError` is raised, providing a detailed message to the user:\n\n```python\nraise RuntimeError(\n    f'The grpc package installed is at version {GRPC_VERSION},'\n    + f' but the generated code in serialization_test_pb2_grpc.py depends on'\n    + f' grpcio>={GRPC_GENERATED_VERSION}.'\n    + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n    + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n)\n```\n\nThis message informs users of the mismatched versions and suggests appropriate actions to resolve the issue.\n\n## Conclusion\n\nIn summary, this script sets up an environment for gRPC-based communications, ensuring that the version of the gRPC library is compatible with the generated code. It manages dependencies effectively and provides useful runtime checks that enforce correct library usage, preventing potential errors in the execution of gRPC services.",
    "filename": "python/packages/autogen-core/tests/protos/serialization_test_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Cleanup Component Code\n\nThis code provides a framework for registering cleanup functions that should be executed upon program exit. It utilizes `asyncio`, a library for writing concurrent code using the async/await syntax, and tests the functionality using `pytest`. The primary focus is on components that require careful shutdown procedures, highlighting different implementations for synchronous and asynchronous cleanup.\n\n## Overview\n\nThe core of the implementation revolves around classes that manage registration and execution of cleanup functions. These cleanup functions are invoked at program exit, allowing for resource deallocation or other finalization tasks. The implementation provides both synchronous and asynchronous options for cleanup based on user needs.\n\n### Protocol Definition\n\n```python\nclass AtExitImpl(Protocol):\n    ...\n```\n\nThe `AtExitImpl` class defines a protocol (similar to an interface) that enforces methods for registering and unregistering cleanup callbacks. It comprises:\n- `register`: Registers a function to be called at exit.\n- `unregister`: Removes a previously registered function.\n\nClasses implementing this protocol must provide their versions of these methods.\n\n## AtExitSimulator Class\n\n```python\nclass AtExitSimulator(AtExitImpl):\n    ...\n```\n\n`AtExitSimulator` is a concrete class that simulates the behavior of the `atexit` module from the standard library but operates in an isolated environment. It stores registered functions in a list and provides the following methods:\n- `complete`: Calls all registered functions in the order they were added and then clears the list.\n- `register`: Adds a function to the list of cleanup functions.\n- `unregister`: Removes a specific function from the list.\n\nThis simulation allows for testing without relying on actual program termination behavior.\n\n## AsyncioAtExitWrapper Class\n\n```python\nclass AsyncioAtExitWrapper(AtExitImpl):\n    ...\n```\n\n`AsyncioAtExitWrapper` is designed to register cleanup functions for asynchronous contexts. It adapts functions for use with `asyncio_atexit`, allowing asynchronous cleanup operations to be registered and executed. Key features include:\n- Proper handling of a custom event loop passed in the kwargs.\n- Creation of a partial function to preserve the original function\u2019s signature while adding arguments.\n\nThis wrapper ensures that the cleanup functions are compatible with the asynchronous event loop, making it suitable for async applications.\n\n## CleanupComponent Class\n\n```python\nclass CleanupComponent:\n    ...\n```\n\n`CleanupComponent` integrates the cleanup functionality. It is initialized with an `AtExitImpl` instance and a flag indicating if the cleanup should be asynchronous. It has the following properties and methods:\n- `cleanup_has_run`: A flag indicating whether the cleanup has been executed.\n- `stop_has_run`: A flag that verifies if a subsequent stop method has run.\n- `cleanup`: A reference to either a synchronous or asynchronous cleanup method based on the initialization flag.\n\nThe class\u2019s methods include:\n- `_acleanup`: Asynchronous cleanup that invokes the `stop` method.\n- `_cleanup`: Synchronous cleanup that uses the asyncio event loop to run the stop method.\n\nThis structure allows for flexible cleanup strategies depending on application needs.\n\n## create_component Function\n\n```python\nasync def create_component(atexit_impl: AtExitImpl, /, use_async_cleanup: bool) -> CleanupComponent:\n    ...\n```\n\nThis asynchronous factory function simulates the creation of a `CleanupComponent` by introducing a slight delay with `await asyncio.sleep(0.001)`. It takes an implementation of `AtExitImpl` and a Boolean flag indicating the cleanup method\u2019s type (synchronous vs. asynchronous). It returns an instantiated `CleanupComponent`.\n\n## run_test_impl Function\n\n```python\ndef run_test_impl(debug_printer: t.Callable[[str], t.Any] | None = None) -> None:\n    ...\n```\n\nThis function runs a series of tests to validate the behavior of the component registration and cleanup process. It conducts the following:\n1. Validates if the cleanup and stop procedure were executed.\n2. Initially tests with the `AtExitSimulator` to ensure cleanup fails as expected when it's supposed to.\n3. Tests the `AsyncioAtExitWrapper` to confirm that cleanup executes correctly under asynchronous conditions.\n\nThe use of `pytest` allows for exception handling checks, ensuring that the design functions as required. \n\n## test_asyncio_atexit_assumptions Function\n\n```python\ndef test_asyncio_atexit_assumptions() -> None:\n    run_test_impl()\n```\n\nThis function simply acts as a test stub that calls `run_test_impl`. It serves as an entry point for automated testing of the cleanup system's functionality.\n\n## Main Entry Point\n\n```python\nif __name__ == \"__main__\":\n    debug_printer = print\n    run_test_impl(debug_printer=debug_printer)\n```\n\nAt runtime, this section checks if the script is executed as the main program. It initializes a `debug_printer` to output results and calls the test implementation to validate the entire cleanup process, allowing for easy observation of output during execution.\n\nIn summary, this code provides a structured approach to handling cleanup tasks with flexible registration options for both synchronous and asynchronous functions while facilitating thorough testing of exit conditions.",
    "filename": "python/packages/autogen-core/tests/regressions/test_clean_terminate.py"
  },
  {
    "code": false,
    "content": "# Code Documentation for `test_base_agent_create`\n\nThis script contains a unit test for the functionality of agent creation within a specific framework, utilizing the `pytest` library and some custom modules. The script focuses on ensuring that an agent is instantiated with the correct runtime context and identifier.\n\n## Imports\n\n```python\nimport pytest\nfrom autogen_core import AgentId, AgentInstantiationContext, AgentRuntime\nfrom autogen_test_utils import NoopAgent\nfrom pytest_mock import MockerFixture\n```\n\n1. **`pytest`**: A testing framework that provides tools for writing and running tests.\n2. **`AgentId`, `AgentInstantiationContext`, `AgentRuntime`**: Components from the `autogen_core` module that are likely responsible for managing agent identifiers, contexts, and runtime environments.\n3. **`NoopAgent`**: A test utility agent class imported from `autogen_test_utils`, presumably intended for testing purposes.\n4. **`MockerFixture`**: A fixture from the `pytest_mock` module that allows the creation of mock objects for testing.\n\n## Test Function Overview\n\n```python\n@pytest.mark.asyncio\nasync def test_base_agent_create(mocker: MockerFixture) -> None:\n```\n\n### Purpose\nThe function `test_base_agent_create` is defined as an asynchronous test function that specifically aims to validate the creation of an agent within a mocked runtime context. The use of `@pytest.mark.asyncio` indicates that the function will run in an asynchronous environment, which is beneficial for tests that involve async operations.\n\n## Mocking the Runtime\n\n```python\nruntime = mocker.Mock(spec=AgentRuntime)\n```\n\n### Mock Object Creation\nIn this line, a mock object `runtime` is created using `mocker.Mock()`, which mimics the behavior of the `AgentRuntime` class. The `spec` parameter restricts the methods and attributes of the mock to those defined in `AgentRuntime`, thereby ensuring that the mock only behaves like the real runtime environment it is simulating.\n\n## Context Management for Agent Instantiation\n\n```python\nwith AgentInstantiationContext.populate_context((runtime, AgentId(\"name2\", \"namespace2\"))):\n```\n\n### Context Setup\nThe `AgentInstantiationContext.populate_context` method is used to establish a context for agent instantiation. The method is provided a tuple containing the mocked `runtime` and a new `AgentId` instance with specified name and namespace values (\"name2\" and \"namespace2\"). This context management construct ensures that any agents created within this block will inherit these settings.\n\n## Agent Creation\n\n```python\nagent2 = NoopAgent()\n```\n\n### Instantiating the Agent\nHere, a new instance of `NoopAgent` is created within the established context. This essentially tests if the agent can be instantiated correctly using the mocked runtime and the defined agent ID.\n\n## Assertions\n\n```python\nassert agent2.runtime == runtime\nassert agent2.id == AgentId(\"name2\", \"namespace2\")\n```\n\n### Validation Checks\nTwo assertions are performed to confirm the correct properties of the instantiated agent:\n1. **`assert agent2.runtime == runtime`**: This checks if the `runtime` attribute of the agent matches the mocked runtime object, ensuring that the agent correctly received its runtime environment.\n2. **`assert agent2.id == AgentId(\"name2\", \"namespace2\")`**: This asserts that the agent\u2019s identifier is correctly set to the specified `AgentId`, confirming that the instantiation context was successfully applied.\n\n## Summary\nOverall, this test function effectively checks the agent creation mechanism within a mocked runtime context. Through the use of mocking and context management, it ensures that the agent is instantiated with the correct properties, validating both the runtime configuration and the agent identifier. This test contributes to enhancing the reliability and correctness of the agent framework being developed or tested.",
    "filename": "python/packages/autogen-core/tests/test_base_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level overview of a testing script that utilizes Python's `unittest.mock` library to conduct tests on caching functionalities provided by `CacheStore` and `InMemoryStore`. The script consists of several test functions that verify the behavior of setting and getting key-value pairs in a mock cache store and an in-memory store implementation.\n\n## Overview\n\nThe primary focus of this script is to validate the functionality of cache operations, such as storing and retrieving values using keys. The tests make use of the `Mock` class to simulate the behavior of the `CacheStore` and ensure that certain operations are performed correctly. The tests also include an examination of an `InMemoryStore`, which is expected to implement the caching logic correctly.\n\n## Test Functions Breakdown\n\n### `test_set_and_get_object_key_value`\n\nThis function tests the basic functionality of setting and getting an object in the mock cache store.\n\n- **Mock Store Creation**: A mock instance of `CacheStore` is created using `Mock(spec=CacheStore)`, ensuring that only the methods of `CacheStore` can be called on this mock instance.\n- **Setting Values**: The method `set` is invoked with a predefined `test_key` and `test_value`.\n- **Assert Set Call**: The `assert_called_with` method verifies that `set` was called with the correct parameters.\n- **Retrieving Value**: Finally, the `get` method is checked to confirm it returns the expected `test_value` for the `test_key`.\n\n### `test_get_non_existent_key`\n\nThis function checks the behavior of the mock store when attempting to retrieve a value using a key that does not exist.\n\n- **Mock Store Creation**: Again, a mock instance of `CacheStore` is created.\n- **Get Non-Existent Key**: The method `get` is set to return `None` for the `non_existent_key`.\n- **Assert Result**: The test asserts that calling `get` with the non-existent key results in `None`, validating the system handles absent keys correctly.\n\n### `test_set_overwrite_existing_key`\n\nThis function verifies that existing keys can be overwritten in the mock cache store.\n\n- **Mock Store Creation**: A new instance of `CacheStore` is mocked.\n- **Initial Value Setting**: The store is first set with an `initial_value` associated with `test_key`.\n- **Overwrite Operation**: The `set` method is called again with a `new_value` for the same key.\n- **Assert Overwrite**: The test checks to ensure that the `get` method now returns the updated `new_value` after the overwrite, and it confirms that the `set` operation was invoked with the new data.\n\n### `test_inmemory_store`\n\nThis function tests the actual implementation of the `InMemoryStore`, a subclass of `CacheStore` intended for in-memory caching.\n\n- **Store Creation**: An instance of `InMemoryStore` is created to hold integer values.\n- **Setting and Getting Values**: A key-value pair is set and then retrieved for verification.\n- **Overwrite Verification**: The key is updated with a new value, and the new value is confirmed through another retrieval.\n- **Default Value Check**: The test attempts to get a non-existent key while specifying a default return value. This ensures that if a key does not exist, the default value is provided correctly.\n\n## Conclusion\n\nThe script serves to rigorously test both the mock implementation of the `CacheStore` and the concrete implementation of the `InMemoryStore`. Each test case is designed to cover critical operations such as setting, retrieving, and overwriting values, along with handling scenarios involving non-existent keys. By using mocking, the tests avoid dependencies on actual storage or side effects, thus isolating the functionality under scrutiny. The tests are streamlined, focusing on specific behaviors to ensure reliable cache management in future developments.",
    "filename": "python/packages/autogen-core/tests/test_cache_store.py"
  },
  {
    "code": false,
    "content": "# Async Agent Cancellation Testing\n\nThis document provides a high-level overview of a Python script that tests the cancellation behavior of asynchronous agents using the `asyncio` library. The implementation focuses on two types of agents, `LongRunningAgent` and `NestingLongRunningAgent`, which handle messages and respond to cancellation requests.\n\n## Import Statements\n\nThe script begins by importing necessary packages and modules:\n- `asyncio`: For asynchronous programming in Python.\n- `dataclass` from the `dataclasses` module: For creating simple classes with minimal boilerplate.\n- `pytest`: For testing functionalities in Python.\n- Various components from `autogen_core`: These include classes and functions that relate to agent management, context, and messaging.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\nimport pytest\nfrom autogen_core import (\n    AgentId,\n    AgentInstantiationContext,\n    CancellationToken,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\n```\n\n## MessageType Declaration\n\nA minimalist `MessageType` class is declared using the `dataclass` decorator. This serves as a placeholder for message objects that will be passed to the agents.\n\n```python\n@dataclass\nclass MessageType: ...\n```\n\n### Note on Cancellation\nA comment indicates that when implementing cancellation, the proper way to manage it is through the use of a `CancellationToken`, rather than canceling a future directly. This hint is crucial for understanding the correct approach to cancellation in this framework.\n\n## LongRunningAgent Class\n\nThe `LongRunningAgent` class extends the `RoutedAgent`. It represents a basic asynchronous agent that simulates a long-running operation (e.g., sleeping).\n\n### Initialization\n\nUpon instantiation, it initializes flags (`called` and `cancelled`) to track the state of the agent.\n\n```python\nclass LongRunningAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__(\"A long running agent\")\n        self.called = False\n        self.cancelled = False\n```\n\n### Message Handling\n\nThe `on_new_message` method, decorated with `@message_handler`, is defined to handle incoming messages. This method implements an asynchronous task to \"sleep\" for a specified duration (100 seconds). The method also links the cancellation token to the sleeping task so that if the cancellation is requested, the sleep is interrupted.\n\n```python\n@message_handler\nasync def on_new_message(self, message: MessageType, ctx: MessageContext) -> MessageType:\n    self.called = True\n    sleep = asyncio.ensure_future(asyncio.sleep(100))\n    ctx.cancellation_token.link_future(sleep)\n    try:\n        await sleep\n        return MessageType()\n    except asyncio.CancelledError:\n        self.cancelled = True\n        raise\n```\n\n## NestingLongRunningAgent Class\n\nThe `NestingLongRunningAgent` class also extends `RoutedAgent`, but incorporates functionality to communicate with a nested `LongRunningAgent`.\n\n### Initialization\n\nIt initializes similarly to its counterpart, but additionally takes a nested agent ID as an argument.\n\n```python\nclass NestingLongRunningAgent(RoutedAgent):\n    def __init__(self, nested_agent: AgentId) -> None:\n        super().__init__(\"A nesting long running agent\")\n        self.called = False\n        self.cancelled = False\n        self._nested_agent = nested_agent\n```\n\n### Message Handling\n\nThe `on_new_message` method handles incoming messages by sending them to a nested agent. It awaits the response while connecting the cancellation token similarly, ensuring coherent cancellation behavior.\n\n```python\n@message_handler\nasync def on_new_message(self, message: MessageType, ctx: MessageContext) -> MessageType:\n    self.called = True\n    response = self.send_message(message, self._nested_agent, cancellation_token=ctx.cancellation_token)\n    try:\n        val = await response\n        assert isinstance(val, MessageType)\n        return val\n    except asyncio.CancelledError:\n        self.cancelled = True\n        raise\n```\n\n## Test Function: Cancellation with Token\n\nThe `test_cancellation_with_token` function sets up a `SingleThreadedAgentRuntime` environment to test the cancellation behavior of the `LongRunningAgent`. \n\n### Setup and Execution\n\n1. The long-running agent is registered.\n2. An asynchronous task is created to send a message to the agent with a cancellation token.\n3. It checks the task's state and processes any unhandled messages. It then cancels the token and asserts that the task throws a `CancelledError`.\n\n```python\n@pytest.mark.asyncio\nasync def test_cancellation_with_token() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    \n    await LongRunningAgent.register(runtime, \"long_running\", LongRunningAgent)\n    agent_id = AgentId(\"long_running\", key=\"default\")\n    token = CancellationToken()\n    response = asyncio.create_task(runtime.send_message(MessageType(), recipient=agent_id, cancellation_token=token))\n    assert not response.done()\n```\n\n### Assertions\n\nThe test validates several conditions:\n- The response is canceled correctly.\n- The state of the agent reflects that the message was received and subsequently cancelled.\n\n```python\nassert response.done()\nlong_running_agent = await runtime.try_get_underlying_agent_instance(agent_id, type=LongRunningAgent)\nassert long_running_agent.called\nassert long_running_agent.cancelled\n```\n\n## Test Function: Nested Cancellation - Outer Called\n\nThe function `test_nested_cancellation_only_outer_called` tests cancellation where only the outer agent (`NestingLongRunningAgent`) is activated, and cancellation is triggered.\n\n### Execution Steps\n\n1. Register both long-running agents and send a message.\n2. Cancel the token after processing.\n3. Verify that the nested agent remains in its initial called state and is not cancelled.\n\n```python\n@pytest.mark.asyncio\nasync def test_nested_cancellation_only_outer_called() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    \n    await LongRunningAgent.register(runtime, \"long_running\", LongRunningAgent)\n    await NestingLongRunningAgent.register(runtime, \"nested\", ...)\n    \n    # Assertions here...\n    long_running_agent = await runtime.try_get_underlying_agent_instance(long_running_id, type=LongRunningAgent)\n    assert long_running_agent.called is False\n    assert long_running_agent.cancelled is False\n```\n\n## Test Function: Nested Cancellation - Inner Called\n\nFinally, the `test_nested_cancellation_inner_called` function checks the situation where both agents receive the cancellation.\n\n### Execution Approach\n\nThe approach is similar to previously described tests, but it ensures both agents react to the cancellation token. This verifies the cascading effect of cancellation through agent layers.\n\n```python\n@pytest.mark.asyncio\nasync def test_nested_cancellation_inner_called() -> None:\n    # Setup as done previously...\n    \n    # Validate inner and outer agent states post-cancellation\n    assert long_running_agent.called\n    assert long_running_agent.cancelled\n```\n\n## Summary\n\nThis script exemplifies the use of cancellation tokens in an asyncio-based agent framework. It defines two key agent classes to handle long-running operations, demonstrating cancellation handling effectively through unit tests. Each test simulates different scenarios of message processing and cancellation, showing the flexibility and robustness of the agent architecture.",
    "filename": "python/packages/autogen-core/tests/test_cancellation.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Async Message Processing Test\n\n## Overview\n\nThis code defines a test using the `pytest` framework, designed to verify the functionality of an asynchronous message processing system. It specifically tests whether a registered closure can successfully receive published messages and log these messages into a queue.\n\n## Import Statements\n\nThe code begins by importing the necessary libraries and modules:\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\n\nimport pytest\nfrom autogen_core import (\n    ClosureAgent,\n    ClosureContext,\n    DefaultSubscription,\n    DefaultTopicId,\n    MessageContext,\n    SingleThreadedAgentRuntime,\n)\n```\n\n- **`asyncio`**: A library to write concurrent code using the async/await syntax.\n- **`dataclasses`**: Provides a decorator and functions for automatically adding special methods to classes, like `__init__`, `__repr__`, etc.\n- **`pytest`**: A framework for writing and running tests in Python.\n- **`autogen_core`**: A module that includes classes and functions for managing message publishing and closure management related to message handling.\n\n## Data Class Definition\n\nThe code defines a simple data class to represent a message:\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\n\n- **`Message`**: This class has a single field, `content`, which is expected to store the string content of messages.\n\n## Test Function Definition\n\nThe core of the code is the `test_register_receives_publish` asynchronous test function:\n\n```python\n@pytest.mark.asyncio\nasync def test_register_receives_publish() -> None:\n```\n\n- The `@pytest.mark.asyncio` decorator allows the function to be executed as an asynchronous test with pytest. The function has no parameters and is defined to return `None`.\n\n## Runtime Initialization\n\nWithin the test function, an instance of `SingleThreadedAgentRuntime` is created:\n\n```python\nruntime = SingleThreadedAgentRuntime()\n```\n\n- **`SingleThreadedAgentRuntime`**: This is likely a runtime management feature allowing closure registration and message publishing in a single-threaded context.\n\n## Queue Setup\n\nAn `asyncio.Queue` is initialized to store messages that are processed:\n\n```python\nqueue = asyncio.Queue[tuple[str, str]]()\n```\n\n- This queue will hold tuples of `(key, message content)` that represent processed messages.\n\n## Closure Callback Definition\n\nAn asynchronous function named `log_message` is defined:\n\n```python\nasync def log_message(closure_ctx: ClosureContext, message: Message, ctx: MessageContext) -> None:\n    key = closure_ctx.id.key\n    await queue.put((key, message.content))\n```\n\n- **`log_message`**: This closure will be invoked upon receiving a message. It extracts the key from the closure context and logs the message content into the queue.\n\n## Closure Registration\n\nThe `log_message` closure is registered with the `ClosureAgent`:\n\n```python\nawait ClosureAgent.register_closure(runtime, \"name\", log_message, subscriptions=lambda: [DefaultSubscription()])\n```\n\n- This effectively links the closure to the agent runtime named \"name\". It subscribes the closure to event broadcasts (in this case, messages published on a default topic).\n\n## Message Publishing\n\nThree messages are published to the agent runtime:\n\n```python\nawait runtime.publish_message(Message(\"first message\"), topic_id=DefaultTopicId())\nawait runtime.publish_message(Message(\"second message\"), topic_id=DefaultTopicId())\nawait runtime.publish_message(Message(\"third message\"), topic_id=DefaultTopicId())\n```\n\n- Each `publish_message` call creates an instance of the `Message` class, and messages with different content are sent to the `DefaultTopicId()`.\n\n## Runtime Idle Check and Assertions\n\nAfter publishing messages, the runtime is instructed to stop when idle:\n\n```python\nawait runtime.stop_when_idle()\n```\n\n- Following this, the function checks the size and contents of the queue with several assertions:\n\n```python\nassert queue.qsize() == 3\nassert queue.get_nowait() == (\"default\", \"first message\")\nassert queue.get_nowait() == (\"default\", \"second message\")\nassert queue.get_nowait() == (\"default\", \"third message\")\nassert queue.empty()\n```\n\n- These assertions ensure that all published messages were received and correctly registered in the queue, confirming that the message logging functionality works as intended.\n\n## Summary\n\nThis pytest-based asynchronous test effectively validates the mechanism of registering closures and handling published messages. By setting up a single-threaded runtime environment and using a queue to log the messages, the test ensures that the system can correctly receive and track multiple messages in sequence. The test framework verifies that the expected behavior occurs and allows for streamlined adjustments or enhancements in the message handling system.",
    "filename": "python/packages/autogen-core/tests/test_closure_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code is a Python script that utilizes the `pytest` testing framework to validate functionality related to code execution and module imports. It defines a data processing function that creates a pandas DataFrame from two dictionary sources, and it includes tests to verify the correct behavior of function imports and aliasing under specific requirements.\n\n## Imports\n\nThe script begins by importing several modules:\n\n- `textwrap`: Used for formatting multi-line strings.\n- `pytest`: A framework for testing Python code, allowing for simple test case creation.\n- `autogen_core.code_executor`: Contains classes related to function and module handling.\n- `pandas`: A powerful data manipulation and analysis library for Python, particularly useful for handling structured data like DataFrames.\n\n### Import Breakdown\n\n- **Alias**: A class likely used to create aliases for modules/functions.\n- **FunctionWithRequirements**: A class likely designed to represent functions with specific import requirements.\n- **FunctionWithRequirementsStr**: Similar to `FunctionWithRequirements`, it may allow defining functions from string representations.\n- **ImportFromModule**: A utility to handle imports explicitly.\n- **build_python_functions_file**: A function that likely generates Python files based on the defined functions and their import requirements.\n\n## Template Function\n\n### Definition\n\nThe `template_function()` creates and returns a pandas DataFrame:\n\n```python\ndef template_function() -> DataFrame:\n    ...\n```\n\n### Functionality\n\n- Two dictionaries (`data1` and `data2`) are created, each containing personal data (name, location, age) for individuals.\n- These dictionaries are converted into DataFrames (`df1` and `df2`) using `DataFrame.from_dict()`.\n- Finally, the DataFrames are concatenated into a single DataFrame using `concat([])` and returned.\n\nThis function serves as an example of basic data manipulation with pandas, encapsulated within a single callable function.\n\n## Test for Hashability Import\n\n### Definition\n\n```python\n@pytest.mark.asyncio\nasync def test_hashability_Import() -> None:\n    ...\n```\n\n### Purpose\n\nThe `test_hashability_Import` function is an asynchronous test case designed to:\n\n1. **Validate Import Requirements**: It checks whether the expected import statements are generated correctly for the specified function.\n2. **Test the Aliasing Mechanism**: It confirms that aliasing works as intended when a function defined in a string is processed.\n\n### Steps in the Test\n\n1. **Creating FunctionWithRequirements**:\n   - The `template_function` is wrapped in the `FunctionWithRequirements` class, indicating that it depends on the `\"pandas\"` module and specifically requires the `DataFrame` and `concat` functions from it. The result is stored in the `function` variable.\n\n2. **Building Python Functions File**:\n   - The `build_python_functions_file` function is called with the list containing `function`, generating a string/module that contains the import statement.\n   - An assertion checks if `\"from pandas import DataFrame, concat\"` is present, verifying that the necessary import statement was constructed correctly.\n\n3. **Defining Function as String**:\n   - A second function, `template_function2`, is defined as a string. It returns a pandas Series containing numbers.\n   - A `FunctionWithRequirementsStr` object is created for this function, indicating it requires the `\"pandas\"` module and uses an alias for pandas (`pd`).\n\n4. **Building the Second Functions Module**:\n   - The functions module is built again, this time including the string-based function.\n   - An assertion checks if the import statement `\"import pandas as pd\"` exists, confirming that the aliasing feature works as expected.\n\n## Summary\n\nThe code snippet is a practical example of using `pytest` to enforce correctness in the handling of function imports, specifically in relation to the pandas library. By constructing functions that require specific modules and validating the generated import statements, the code ensures that all dependencies are managed correctly. This is particularly useful in environments where dynamic code generation and execution are involved, ensuring reliability and maintaining clarity in data handling operations.",
    "filename": "python/packages/autogen-core/tests/test_code_executor.py"
  },
  {
    "code": false,
    "content": "# Documentation for Test Suite of Custom Components\n\nThis document provides a detailed overview of a test suite designed to validate the functionality of custom components within the `autogen_core` library. This suite includes various test cases, each testing the behavior of component creation, serialization, and type validation.\n\n## Overview\n\nThe code demonstrates the creation and testing of custom components using `pytest`. The custom components are based on the `ComponentBase` and `Component` classes from the `autogen_core` library. The tests validate a variety of functionalities, including serialization to/from JSON, loading components of potentially different classes, and error handling for type mismatches or misconfigurations.\n\n## Custom Component Definitions\n\n### MyComponent\n\n- **Class Definition**: Inherits from `ComponentBase` and implements the component interface, utilizing a configuration schema, defined by `MyConfig`, to store component information.\n- **Purpose**: It serves as a base class for custom components, defining methods to serialize (`_to_config`) and deserialize (`_from_config`) component data.\n\n### ComponentWithDescription and ComponentWithDocstring\n\n- **Functionality**: \n  - `ComponentWithDescription` explicitly sets a description and label.\n  - `ComponentWithDocstring` derives its description from its docstring.\n- **Usage**: These classes showcase mechanisms to provide user-readable information regarding what the component does or its purpose.\n\n## Test Cases\n\n### Basic Functionality Tests\n\n#### test_custom_component\n\nThis function tests the initialization and serialization of `MyComponent`.\n\n- **Steps**:\n  - Create an instance of `MyComponent`.\n  - Serialize the component and load a new instance to verify that the serialized properties match the original.\n\n#### test_custom_component_json\n\nSimilar to the previous test but uses JSON for serialization instead. It confirms that the JSON dumps and loads yield the same properties.\n\n### Generic Loader Tests\n\n#### test_custom_component_generic_loader\n\nThis test checks the functionality of `ComponentLoader` to load the component generically with the same assertions as above.\n\n#### test_custom_component_generic_loader_json\n\nIt follows the same approach but tests JSON serialization.\n\n### Error Handling Tests\n\n#### test_custom_component_incorrect_class\n\nThis test verifies that loading a component with an incorrect target class raises a `TypeError`. It tests resilience against type mismatches.\n\n### Nested Components\n\n#### test_nested_component_diff_module\n\nTests the ability to serialize and deserialize a component that contains a nested component (`MyOuterComponent` and `MyInnerComponent`). It verifies that the original and deserialized components maintain the same state.\n\n### Invalid Model Client Tests\n\n#### test_cannot_import_locals\n\nTests that attempting to load a non-importable component correctly raises a `TypeError`. This emphasizes that models must be globally importable to function correctly.\n\n#### test_type_error_on_creation\n\nVerifies that `ChatCompletionClient` fails as expected when trying to load an invalid client model.\n\n### Missing Attributes Handling\n\n#### test_fails_to_save_on_missing_attributes\n\nThis case checks that attempting to serialize a component that lacks required attributes raises an `AttributeError`.\n\n### Schema Validation\n\n#### test_schema_validation_fails_on_bad_config\n\nTests whether invalid configurations raise a `ValidationError`, ensuring robust input validation for component loading.\n\n#### test_config_optional_values\n\nValidates that optional values in configurations are correctly handled during the loading process.\n\n### Component Version Management\n\n#### test_component_version\n\nThis test checks whether components retain version information during serialization and loading, ensuring consistency across versions.\n\n#### test_component_version_from_dict and test_component_version_from_dict_non_existing_impl\n\nThese tests ascertain that versions can be managed and correctly loaded or raise appropriate errors when attempting to load unsupported versions.\n\n### FunctionTool Testing\n\n#### test_function_tool\n\nThis extensive test evaluates the use of `FunctionTool`, encompassing:\n\n- Verifying synchronous and asynchronous function execution.\n- Checking that function tools handle imports correctly.\n- Ensuring both serialization and deserialization of function configuration work as intended.\n- Confirming error handling with invalid inputs and cancellation support.\n\n### Component Description Tests\n\n#### test_component_descriptions\n\nThis final test assesses the various approaches to set component descriptions and verifies the functionality of user-accessibility through docstrings or explicit string definitions.\n\n## Conclusion\n\nThe suite offers comprehensive testing solutions for custom components within the `autogen_core` framework, guaranteeing that functionalities are robust, errors are appropriately handled, and components manage their state effectively. Each test case is designed to reinforce the confidence in the behavior and reliability of the components being engineered.",
    "filename": "python/packages/autogen-core/tests/test_component_config.py"
  },
  {
    "code": false,
    "content": "# Overview of the Test Suite for Intervention Handlers\n\nThis document provides a high-level overview of a set of asynchronous unit tests that validate the behavior of intervention handlers in an agent-based messaging system. The tests utilize the `pytest` framework with asyncio support to simulate various scenarios involving message sending, publishing, and responses. \n\n## Rationale\n\nIntervention handlers are designed to allow developers to intercept and manipulate messages within the system. This test suite specifically focuses on testing various types of interventions, including counting messages, dropping messages, and raising exceptions during message processing. \n\nThe code performs these tests by subclassing a default intervention handler and overriding its methods to implement specific behaviors, thus allowing for robust testing of the agent messaging system's features.\n\n## Test Setup\n\nEach test begins by defining a custom intervention handler that inherits from `DefaultInterventionHandler`. This custom handler is then used to instantiate the `SingleThreadedAgentRuntime`, which manages agents and their interactions.\n\n### Example of Debug Intervention Handler\n\nHere is an excerpt from the tests to illustrate the structure of a custom intervention handler:\n\n```python\nclass DebugInterventionHandler(DefaultInterventionHandler):\n    def __init__(self) -> None:\n        self.num_send_messages = 0\n        self.num_publish_messages = 0\n        self.num_response_messages = 0\n```\n\nThis handler keeps track of the number of messages sent, published, and responded to, providing an interface for monitoring inter-agent communication.\n\n## Test Cases\n\n### 1. Testing Message Counting\n\nThe first test, `test_intervention_count_messages`, verifies whether the custom `DebugInterventionHandler` correctly counts the number of sent and published messages. \n\n- Inside the handler, overridden methods increment respective counters when messages are sent, published, or responded to.\n- After registering a Loopback agent and starting the runtime, the test sends and publishes messages, then checks the counters to ensure they match expected values.\n\n### 2. Testing Message Dropping on Send\n\nThe second test, `test_intervention_drop_send`, demonstrates the capability of an intervention handler to drop a message during the send operation.\n\n- A `DropSendInterventionHandler` is implemented to return `DropMessage` in the `on_send` method.\n- When a message is sent to the Loopback agent, the test expects a `MessageDroppedException` to be raised, confirming the message did not get sent and that the Loopback agent\u2019s call count remains zero.\n\n### 3. Testing Message Dropping on Response\n\nIn `test_intervention_drop_response`, another aspect of message handling is tested\u2014responses.\n\n- Similar to the previous test, a `DropResponseInterventionHandler` is created to drop responses. \n- When attempting to send a message and receive a response, the test expects the message to be dropped with the same exception raised.\n\n### 4. Raising Exceptions on Send\n\nThe fourth test, `test_intervention_raise_exception_on_send`, checks the behavior when an exception is raised during the send operation.\n\n- An `ExceptionInterventionHandler` is implemented which raises a custom `InterventionException`.\n- When sending a message, the test expects this exception to occur, showing that the message handling process is correctly interrupted.\n\n### 5. Raising Exceptions on Response\n\nThe final test, `test_intervention_raise_exception_on_respond`, evaluates the same exception-raising logic but during message responses.\n\n- Just like the send exception test, the response handler raises an exception. This test ensures the messaging system handles interrupts effectively when an exception occurs during a response.\n\n## Conclusion\n\nThis suite of tests effectively validates the functionality of intervention handlers in an asynchronous messaging system. Each test is well defined, targeting specific features and ensuring that the agents behave correctly under different scenarios. Through the use of intervention handlers, the tests demonstrate the flexibility and extensibility of the messaging infrastructure, allowing developers to customize behaviors in a controlled manner. \n\nBy capturing dropped messages and exceptions, these tests play a critical role in ensuring system robustness and reliability in message processing between agents.",
    "filename": "python/packages/autogen-core/tests/test_intervention.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Test Suite of `extract_json_from_str`\n\nThis document provides a high-level overview of a test suite designed for testing the `extract_json_from_str` function from the `autogen_core.utils` module. The tests validate the function's ability to correctly parse JSON strings, including those formatted as code blocks, and to handle invalid input gracefully.\n\n## Overview\n\nThe test suite is structured using the `pytest` framework, which simplifies the process of writing and running tests. The tests focus on evaluating how well the `extract_json_from_str` function performs under various scenarios, including successfully extracting JSON data from both structured strings and various code blocks, as well as handling erroneous inputs that should raise exceptions.\n\n## Test Functions\n\nThe test suite consists of two primary functions, both working to ensure the `extract_json_from_str` function behaves as expected in different circumstances.\n\n### `test_extract_json_from_str`\n\nThis function tests the basic functionality of `extract_json_from_str` by providing valid and invalid JSON strings.\n\n#### Steps:\n\n1. **Valid JSON Test**:\n    - A valid JSON string representing a person (`json_str`) is defined.\n    - The expected response (`json_resp`) is created as a list with a single dictionary containing the person's details.\n    - The function `extract_json_from_str` is called with `json_str`, and its output is compared against `json_resp` using an assertion to verify correctness.\n\n2. **Invalid JSON Test**:\n    - An invalid JSON string (`invalid_json_str`) is defined that omits a closing brace.\n    - This test expects the function to raise a `ValueError` when called with the invalid string, which is enforced through the `pytest.raises` context manager.\n\n### `test_extract_json_from_str_codeblock`\n\nThis function examines the ability of `extract_json_from_str` to handle JSON strings presented in Markdown code block format.\n\n#### Steps:\n\n1. **Code Block with Language**:\n    - A JSON code block string (`code_block_lang_str`) is defined.\n    - An expected response (`code_block_resp`) is specified, and the function output is asserted against it.\n\n2. **Code Block without Language**:\n    - Another code block string without a language identifier (`code_block_no_lang_str`) is created.\n    - The function is again tested with the expectation that it outputs the same response as the previous code block.\n\n3. **Multiple JSON Entries**:\n    - A multi-string containing JSON code blocks (`multi_json_str`) is defined, expecting a list of dictionaries (`multi_json_resp`) as the output.\n    - The function is tested with this multi-string, asserting that it equates to the expected response.\n\n4. **Invalid Language Code Block Test**:\n    - An invalid code block string that specifies an incorrect language (`invalid_lang_code_block_str`) is defined.\n    - Similar to the first invalid test, this test anticipates that the function raises a `ValueError` when invoked with the invalid string.\n\n## Exception Handling\n\nThe suite effectively checks how the `extract_json_from_str` function manages improper inputs. Both functions contain validation checks that ensure a `ValueError` is raised under specified invalid conditions. This signifies that the function has robust error handling, which is critical for applications relying on JSON parsing.\n\n## Conclusion\n\nThe test suite for `extract_json_from_str` demonstrates thorough coverage of both valid and invalid inputs for JSON string extraction, including various forms such as standard strings and code block representations. These tests help ensure that the `extract_json_from_str` function behaves reliably and helps identify possible issues before deployment, thus maintaining the robustness of the application leveraging this function.",
    "filename": "python/packages/autogen-core/tests/test_json_extraction.py"
  },
  {
    "code": false,
    "content": "# Documentation for Pydantic Model and JSON Schema Tests\n\n## Overview\nThis module tests the functionality of converting JSON schemas into Pydantic models. It utilizes pytest for testing various schema forms, ensuring both valid and invalid data are handled appropriately. This documentation breaks down the defined classes, fixtures, and tests into comprehensible sections.\n\n## Pydantic Models\n\n### Address Model\n```python\nclass Address(BaseModel):\n    street: str\n    city: str\n    zipcode: str\n```\nThe `Address` class is a Pydantic model containing three string fields: `street`, `city`, and `zipcode`. It handles validation for these fields when instances are created.\n\n### User Model\n```python\nclass User(BaseModel):\n    id: UUID\n    name: str\n    email: EmailStr\n    age: int = Field(..., ge=18)  # Minimum age = 18\n    address: Address\n```\nThe `User` model represents a user entity with several details, including their ID (UUID), name (string), email (validated as email format), age (with a minimum constraint of 18), and an address (of type `Address`).\n\n### Employee Model\n```python\nclass Employee(BaseModel):\n    id: UUID\n    name: str\n    manager: Optional[\"Employee\"] = None  # Recursive self-reference\n```\nThe `Employee` class contains an ID (UUID), name (string), and an optional reference to another `Employee` object for hierarchical relationships (i.e., manager).\n\n### Department Model\n```python\nclass Department(BaseModel):\n    name: str\n    employees: List[Employee]  # Array of objects\n```\nThe `Department` class consists of a name (string) and a list of `Employee` objects that belong to that department.\n\n### ComplexModel\n```python\nclass ComplexModel(BaseModel):\n    user: User\n    extra_info: Optional[Dict[str, Any]] = None  # Optional dictionary\n    sub_items: List[Employee]  # List of Employees\n    json_string: Optional[Json[Any]] = None  # Optional JSON string\n```\n`ComplexModel` incorporates a `User`, an optional dictionary (`extra_info`), a list of `Employee` objects (`sub_items`), and an optional JSON string (`json_string`).\n\n## Fixtures\nFixtures are defined for testing purposes, providing dynamic schemas for the main models.\n\n### Converter Fixture\n```python\n@pytest.fixture\ndef converter() -> _JSONSchemaToPydantic:\n    ...\n```\nThis fixture creates a fresh instance of `_JSONSchemaToPydantic` for each test run.\n\n### Sample JSON Schemas Fixtures\nMultiple fixtures generate JSON schemas for models:\n- `sample_json_schema`: Returns the schema for `User`.\n- `sample_json_schema_recursive`: Returns the schema for `Employee`.\n- `sample_json_schema_nested`: Returns the schema for `Department`.\n- `sample_json_schema_complex`: Returns the schema for `ComplexModel`.\n\n## Parameterized Tests\n\n### Testing JSON Schema to Pydantic Model Conversion\nMultiple tests ensure that fields expected in generated models match those in the input schemas. Each model is tested against its fields:\n\n```python\n@pytest.mark.parametrize(\n    \"schema_fixture, model_name, expected_fields\",\n    [ ... ]\n)\ndef test_json_schema_to_pydantic(...):\n    ...\n```\nThe test verifies that each expected field exists in the generated model.\n\n### Valid Data Tests\nThese tests verify that correctly structured data is accepted by the generated models:\n\n```python\n@pytest.mark.parametrize(\n    \"schema_fixture, model_name, valid_data\",\n    [ ... ]\n)\ndef test_valid_data_model(...):\n    ...\n```\nEach model is instantiated with representative valid data, and the resulting model is checked against the expected data structure.\n\n### Invalid Data Tests\nThis section of tests ensures that invalid data configurations raise appropriate exceptions:\n\n```python\n@pytest.mark.parametrize(\n    \"schema_fixture, model_name, invalid_data\",\n    [ ... ]\n)\ndef test_invalid_data_model(...):\n    ...\n```\nIf any of the invalid configurations are input, a `ValidationError` is expected.\n\n## Advanced Model Testing\n\n### Nested Structures\nTests are carried out for models with nested structures, like `ListDictModel`, `DictListModel`, and `NestedListModel`, confirming that schemas for nested lists and dictionaries produce the expected Pydantic models and validation behavior.\n\n### Special Cases\nTests for references, unsupported formats, and unknown types are included to ensure robustness:\n- **Reference Handling**: Ensures that missing references raise a `ReferenceNotFoundError`.\n- **Unsupported Formats**: Validates that unsupported formats raise a `FormatNotSupportedError`.\n- **Exception Handling for Unknown Types**: Confirms correct exception raises for unknown types or formats in schemas.\n\n### Enum and Metadata Support\nTests check for handling of literals, titles, descriptions, and enum fields to ensure generated models incorporate these metadata features as expected.\n\n## Summary\nThis module is structured to comprehensively verify the conversion from JSON schemas to Pydantic models, ensuring robust validation of both standard and nested data structures. The thorough test coverage also ensures that exceptional cases are handled accurately, leading to reliable schema handling and data integrity within generated models.",
    "filename": "python/packages/autogen-core/tests/test_json_to_pydantic.py"
  },
  {
    "code": false,
    "content": "# Documentation: Memory Component Testing\n\nThis document outlines the testing suite for the memory components in the `autogen_core` library. The testing is primarily focused on the `Memory` protocol and its implementation, usage of the `ListMemory` class, and validates that memory manipulations operate correctly under various conditions.\n\n## Overview\n\nThe testing suite employs the `pytest` framework and utilizes asynchronous testing capabilities through the `pytest.mark.asyncio` decorator. The primary purpose of these tests is to confirm the fulfillment of the expected behavior of memory-related functionalities, ensuring that all aspects of memory management and content handling comply with defined specifications.\n\n## Test Functions Description\n\n### 1. `test_memory_protocol_attributes`\n\nThis test validates that the `Memory` protocol exhibits all required attributes. It confirms the existence of methods such as `update_context`, `query`, `add`, `clear`, and `close`. The checks ensure that the protocol structure is compliant and that implementations of this protocol will have these foundational methods.\n\n```python\ndef test_memory_protocol_attributes() -> None:\n    ...\n```\n\n### 2. `test_memory_component_load_config_from_base_model`\n\nThis test evaluates the ability of the `Memory` component to be loaded from the `ComponentModel` class. It constructs a configuration for a `ListMemory` instance and verifies that the instance is created successfully and that its name and contents match the expected values.\n\n```python\ndef test_memory_component_load_config_from_base_model() -> None:\n    ...\n```\n\n### 3. `test_memory_component_dump_config_to_base_model`\n\nHere, the functionality of dumping the configuration of a `ListMemory` instance back to a `ComponentModel` is tested. The test ensures that the dumped configuration retains essential attributes and that the integrity of the data is maintained, confirming that the component can be serialized and deserialized correctly.\n\n```python\ndef test_memory_component_dump_config_to_base_model() -> None:\n    ...\n```\n\n### 4. `test_memory_abc_implementation`\n\nThis test ensures that the abstract base class (ABC) `Memory` is correctly implemented by subclasses. It verifies that a valid implementation of `Memory` satisfies the requirements and can be instantiated, while an invalid class fails the instantiation check. \n\n```python\ndef test_memory_abc_implementation() -> None:\n    ...\n```\n\n## Testing ListMemory Functionality\n\n### 5. `test_list_memory_empty`\n\nThis test examines the behavior of the `ListMemory` class when no content is present. It asserts the results from the `update_context()` and `query()` methods, expecting both to yield empty results when invoked on an empty memory instance.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_empty() -> None:\n    ...\n```\n\n### 6. `test_list_memory_add_and_query`\n\nIn this test, content is added to the `ListMemory` instance, followed by a query. The test ensures that both pieces of content are retrievable and that the results match the expected content types, demonstrating the add-and-retrieve functionality of memory.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_add_and_query() -> None:\n    ...\n```\n\n### 7. `test_list_memory_max_memories`\n\nThis test verifies that the `ListMemory` class respects any predefined limit on the number of memories stored. Here, five items are added, and a subsequent query confirms that all five items are retrievable, thus validating the storage limit handling.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_max_memories() -> None:\n    ...\n```\n\n### 8. `test_list_memory_update_context`\n\nThe purpose of this test is to ensure that the context within `BufferedChatCompletionContext` is updated correctly with memory contents. After adding two items to the memory, the test checks both the results from `update_context()` and the messages present in the context, confirming that the memory contents have been appropriately integrated.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_update_context() -> None:\n    ...\n```\n\n### 9. `test_list_memory_clear`\n\nThis test assesses the `clear()` method of the `ListMemory` class. After adding content, calling the `clear()` method should empty the memory, and subsequent queries should return no results, validating the efficacy of the clear operation.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_clear() -> None:\n    ...\n```\n\n### 10. `test_list_memory_content_types`\n\nIn this test, the `ListMemory` class is scrutinized for its ability to handle varying content types, including text, JSON, and binary formats. The results of a query after adding different types demonstrate that the memory can successfully store and retrieve diverse content formats.\n\n```python\n@pytest.mark.asyncio\nasync def test_list_memory_content_types() -> None:\n    ...\n```\n\n## Conclusion\n\nThe testing suite provides comprehensive coverage of all essential functionalities associated with the memory components in the `autogen_core` library. Each test is designed to confirm that the memory can handle various operations reliably, ensuring that the design principles and protocols are adhered to throughout. This ensures robustness and reliability in the memory management aspects of the system.",
    "filename": "python/packages/autogen-core/tests/test_memory.py"
  },
  {
    "code": false,
    "content": "# Documentation for Chat Completion Context Test Module\n\nThis document provides a detailed overview of a test module designed for various chat completion context classes in an asynchronous environment using pytest. The tests in this module examine the behavior of different message context handling mechanisms, ensuring they work correctly under specified conditions.\n\n## Overview of Chat Completion Contexts\n\nThe module tests four types of chat completion contexts:\n- `BufferedChatCompletionContext`: Maintains a limited number of messages based on a defined buffer size.\n- `HeadAndTailChatCompletionContext`: Retains a specified number of messages from the beginning (head) and the end (tail) of the message list.\n- `UnboundedChatCompletionContext`: Allows an unlimited number of messages to be stored.\n- `TokenLimitedChatCompletionContext`: Controls the maximum number of tokens used in messages, managing the retention based on token limits.\n\nEach test case utilizes asynchronous functionality through pytest's `asyncio` marker to simulate real-world scenarios without blocking the execution thread.\n\n## Detailed Descriptions of Test Cases\n\n### Buffered Model Context Test\n\n```python\n@pytest.mark.asyncio\nasync def test_buffered_model_context() -> None:\n```\n\nThis test checks the functionality of the `BufferedChatCompletionContext`. It initializes an instance with a buffer size of 2. \n\n1. Three messages are added sequentially to the context.\n2. It asserts that the context retains only the last two messages.\n3. After clearing the context, it checks that no messages remain.\n4. It tests the saving and loading state functionality, ensuring that previously stored messages can be retrieved after clearing and restoring the context.\n\n### Head and Tail Model Context Test\n\n```python\n@pytest.mark.asyncio\nasync def test_head_and_tail_model_context() -> None:\n```\n\nThis test focuses on the `HeadAndTailChatCompletionContext` which keeps the first and last message. \n\n1. The context is initialized to hold 1 message from the head and 1 from the tail.\n2. Multiple messages are added, and the test checks that exactly three messages are retained (1 head + 1 tail + 1 placeholder).\n3. A state-saving functionality test is included to ensure the messages can be stored and accurately reloaded.\n\n### Unbounded Model Context Test\n\n```python\n@pytest.mark.asyncio\nasync def test_unbounded_model_context() -> None:\n```\n\nThis test verifies the `UnboundedChatCompletionContext`, which should not restrict the number of stored messages.\n\n1. It adds several messages and asserts that they are all retrievable.\n2. Upon clearing the context, it ensures that no messages exist.\n3. It also incorporates state-saving tests to confirm that the messages can be properly saved and restored.\n\n### Token Limited Model Context with Token Limit\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(... \"model_client,token_limit\")\nasync def test_token_limited_model_context_with_token_limit(...) -> None:\n```\n\nThis test examines the behavior of the `TokenLimitedChatCompletionContext` with a specified token limit.\n\n1. It is parameterized for multiple model clients (OpenAI and Ollama) with varying token limits.\n2. Three messages are added, and due to the token limit, fewer messages should be retrievable.\n3. The test asserts that the retrieved messages do not match the initially added messages in count, confirming that some were pruned due to token limitations.\n\n### Token Limited Model Context without Token Limit\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(... \"model_client\")\nasync def test_token_limited_model_context_without_token_limit(...) -> None:\n```\n\nThis variant checks the `TokenLimitedChatCompletionContext` without an imposed token limit.\n\n1. Messages are added, and because there is no token limit, all messages are expected to be retrievable.\n2. The test ensures that the count of retrieved messages matches the count of added messages.\n\n### Token Limited Model Context OpenAI with Function Result\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(... \"model_client,token_limit\")\nasync def test_token_limited_model_context_openai_with_function_result(...) -> None:\n```\n\nThis test explores how the `TokenLimitedChatCompletionContext` behaves when a function result message is included.\n\n1. Like previous tests, it is parameterized but includes a function execution result message.\n2. After adding messages, it verifies that the resulting list retains only relevant user and assistant messages, excluding the function execution result due to token restrictions.\n\n## Conclusions\n\nEach test validates crucial functionalities of the message context management classes within the asynchronous chat model architecture. The tests ensure that the message retention logic aligns with user expectations and that state management performs accurately, which are vital for any real-time chat application. This modular approach also ensures scalability and adaptability when introducing new message handling or completion strategies.",
    "filename": "python/packages/autogen-core/tests/test_model_context.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe provided code is a unit test designed to validate the functionality of the `validate_model_info` function from the `autogen_core.models` module. It uses the `pytest` framework for testing and aims to ascertain that the function correctly identifies valid and invalid representations of model information.\n\n## Imports\nThe code imports the following components:\n- `pytest`: A testing framework that allows for writing simple as well as scalable test cases.\n- `ModelInfo`: A type or data structure presumably defined in the `autogen_core.models` module, representing the information regarding a model.\n- `validate_model_info`: A function that likely checks the correctness and completeness of the `ModelInfo` structured data.\n\n## Test Function: `test_model_info`\nThe core of the code is a single test function named `test_model_info`, which contains two primary test cases aimed at validating model information.\n\n### 1. Valid Model Information\nIn the first part of the test, a valid model information dictionary is created:\n\n```python\ninfo: ModelInfo = {\n    \"family\": \"gpt-4o\",\n    \"vision\": True,\n    \"function_calling\": True,\n    \"json_output\": True,\n    \"structured_output\": True,\n}\n```\n- **Structure**: The dictionary includes several key attributes:\n  - `family`: A string representing the model's name or type.\n  - `vision`: A boolean indicating whether the model has vision capabilities.\n  - `function_calling`: A boolean indicating support for calling functions.\n  - `json_output`: A boolean signaling the capability of producing JSON outputs.\n  - `structured_output`: A boolean for indicating support for structured output formats.\n\nAfter defining the valid data, the `validate_model_info(info)` function is called to verify that this structure meets the expected criteria. There is no assertion in this context, but the absence of exceptions signifies success.\n\n### 2. Invalid Model Information\nThe second part of the test checks if the `validate_model_info` function correctly handles invalid input:\n\n```python\ninfo = {\n    \"family\": \"gpt-4o\",\n    \"vision\": True,\n    \"function_calling\": True,\n}  # type: ignore\n```\n- **Structure**: This dictionary omits the `json_output` and `structured_output` keys. Thus, it represents an incomplete model information structure.\n\nThe code then initiates a context manager using `with pytest.raises(ValueError):`, expecting the `validate_model_info(info)` call within this context to raise a `ValueError`. If the error is raised, the test passes, confirming that the validation function is capable of recognizing and rejecting invalid model information. If no error is raised, the test will fail.\n\n## Conclusion\nThis script serves as a test suite for the `validate_model_info` function, ensuring it correctly distinguishes between valid and invalid model information structures. It utilizes the `pytest` framework to structure the tests and assert expected behaviors. By testing both valid and invalid cases, it helps maintain the integrity of the model validation process within the application.",
    "filename": "python/packages/autogen-core/tests/test_models.py"
  },
  {
    "code": false,
    "content": "# Agent Messaging System\n\nThis document describes the implementation of a messaging system utilizing agents that respond to messages through RPC (Remote Procedure Call) and broadcast messages. The code employs an async architecture with message handlers and event-driven design patterns to ensure efficient message processing.\n\n## Overview of Key Classes\n\n### 1. `CounterAgent`\n\nThe `CounterAgent` class extends `RoutedAgent`, serving as an example of an agent that tracks messages it receives. It has two primary handlers:\n- **RPC Message Handler:** Responds to incoming RPC messages. Each time the agent processes an RPC message, it increments a counter (`num_calls_rpc`).\n- **Broadcast Message Handler:** Handles broadcast messages that are not requests for RPC. This increments a different counter (`num_calls_broadcast`).\n\nThe class is equipped with methods decorated with `@message_handler` to distinguish the types of incoming messages based on the context.\n\n### 2. `RoutedAgentMessageCustomMatch`\n\nThis class is another agent derived from `RoutedAgent`. It demonstrates custom matching for message handlers using static methods:\n- **Handler One:** Invoked when the message value equals \"one\".\n- **Handler Two:** Invoked when the message value equals \"two\".\n\nBoth handlers update internal flags indicating which handlers have been called, allowing the testing framework to validate behavior based on messages sent to the agent.\n\n### 3. `EventAgent`\n\nThe `EventAgent` class captures messages as events based on their values. It features two event handlers:\n- **on_event_one:** Activated when the message value is \"one\".\n- **on_event_two:** Activated when the message value is \"two\".\n\nEach handler tracks the number of times they are invoked, allowing the testing framework to check that events are processed correctly.\n\n### 4. `RPCAgent`\n\nSimilar to `EventAgent`, `RPCAgent` specifically processes RPC messages:\n- **on_rpc_one:** Increments a counter when receiving a message with the value \"one\".\n- **on_rpc_two:** Increments another counter for messages with the value \"two\".\n\nOnce again, it tracks the number of times each handler is invoked, providing a mechanism for assertions in tests.\n\n## Message Handling and Testing\n\n### Testing with `pytest`\n\nThe testing framework `pytest` is used extensively throughout the script. Each test function marked with `@pytest.mark.asyncio` runs within an asynchronous event loop:\n- **Test for Unhandled Messages:** The `test_routed_agent` function checks that unhandled messages are logged appropriately when sent to agents that do not handle them.\n- **Message Handler Router Test:** The `test_message_handler_router` function tests the interactions of the `CounterAgent` when processing both RPC and broadcast messages to ensure that the respective counters are incremented correctly.\n\nEach test creates a runtime environment via `SingleThreadedAgentRuntime`, registers the agents, adds subscriptions, and publishes messages to validate the expected behavior.\n\n### Custom Message Type Test\n\nIn `test_routed_agent_message_matching`, custom message types are tested using the `RoutedAgentMessageCustomMatch` agent. The test sends messages with specific values to leverage the custom matching logic and verifies that the correct handlers are triggered.\n\n### Event Handling Test\n\nThe `test_event` function focuses on the `EventAgent`, publishing messages with various values to ensure the expected event handlers are invoked, reflecting the reactive nature of the agent system.\n\n### RPC Handling Test\n\nLastly, the `test_rpc` function evaluates the functionality of the `RPCAgent`, sending messages with RPC patterns and verifying that the counters are updated accordingly, confirming that broadcast messages do not affect the counts.\n\n## Logical Structure of the Code\n\nThe code is organized into logical sections corresponding to the various functionalities of the agents, message handling, and the associated test suites. Each class and test function is clearly defined and has a specific role, ensuring maintainability and clarity. All agents derive from `RoutedAgent`, which abstracts common behavior, allowing for diverse implementations to handle messages dynamically based on context.\n\n## Conclusion\n\nThis messaging system demonstrates a flexible framework for building agents that can communicate and respond to messages in an asynchronous manner. With clearly defined behaviors and rigorous testing using pytest, it establishes a robust architecture suitable for expanding features and integrating new message types or agent behaviors. The system can be adapted for various use cases involving event-driven programming in Python.",
    "filename": "python/packages/autogen-core/tests/test_routed_agent.py"
  },
  {
    "code": false,
    "content": "# Agent Management and Messaging Tests\n\nThis document provides an overview of a series of tests designed for an agent management system using asynchronous programming in Python. The tests are built using the `pytest` framework and primarily verify the behavior of agents in a runtime system when it comes to registration, message handling, and exceptional scenarios. \n\n## Setup and Imports\n\nThe code begins with importing necessary libraries, including `logging` and `pytest`. It also imports several components from the `autogen_core` and `autogen_test_utils` modules, which define agents and support structures necessary for testing. Among the components, we find essential classes like `SingleThreadedAgentRuntime`, `AgentId`, and `MessageContext`, along with various types of agents used in tests.\n\n```python\nimport logging\nimport pytest\nfrom autogen_core import (\n    AgentId,\n    AgentInstantiationContext,\n    AgentType,\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    TopicId,\n    TypeSubscription,\n    event,\n    try_get_known_serializers_for_type,\n    type_subscription,\n)\nfrom autogen_core._default_subscription import default_subscription\nfrom autogen_test_utils import (\n    CascadingAgent,\n    CascadingMessageType,\n    LoopbackAgent,\n    LoopbackAgentWithDefaultSubscription,\n    MessageType,\n    NoopAgent,\n)\nfrom autogen_test_utils.telemetry_test_utils import MyTestExporter, get_test_tracer_provider\nfrom opentelemetry.sdk.trace import TracerProvider\n```\n\n## Tracing Setup\n\nA testing telemetry exporter is instantiated, followed by the definition of a `tracer_provider` fixture that clears old records and returns an instance of a test tracer provider.\n\n```python\ntest_exporter = MyTestExporter()\n\n@pytest.fixture\ndef tracer_provider() -> TracerProvider:\n    test_exporter.clear()\n    return get_test_tracer_provider(test_exporter)\n```\n\nThis setup is essential for monitoring and logging events during the test execution, aiding in debugging and performance analysis.\n\n## Agent Registration Tests\n\n### `test_agent_type_register_factory`\n\nThis test verifies that factories can be registered for different agent types. It checks that an error is raised if the expected class does not match the real class, while no errors should arise if `expected_class` is omitted.\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_type_register_factory() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    ...\n```\n\n### `test_agent_type_must_be_unique`\n\nThe goal of this test is to ensure that each agent type can only be registered once. It tests for both successful registration and error handling when a duplicate registration is attempted.\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_type_must_be_unique() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    ...\n```\n\n### `test_agent_type_register_instance`\n\nIn this test, the focus is on ensuring the correct registration and retrieval of agent instances. It ensures that attempts to register duplicates or instances of different types are properly handled with exceptions.\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_type_register_instance() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    ...\n```\n\n## Message Handling Tests\n\n### `test_agent_type_register_instance_publish_new_source`\n\nThis test validates the behavior when a message is published to an unregistered source. The behavior of the runtime and agents is assessed, particularly focusing on raising runtime errors when attempting to publish from a source that does not match registered subscriptions.\n\n```python\n@pytest.mark.asyncio\nasync def test_agent_type_register_instance_publish_new_source() -> None:\n    runtime = SingleThreadedAgentRuntime()\n    ...\n```\n\n### `test_register_receives_publish`\n\nWhen a message is published, this test checks to ensure the intended agent receives it and that other agents do not. It also validates telemetry data by checking the exported spans for correctness.\n\n```python\n@pytest.mark.asyncio\nasync def test_register_receives_publish(tracer_provider: TracerProvider) -> None:\n    runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n    ...\n```\n\n## Exceptional Cases\n\n### `test_register_receives_publish_with_construction`\n\nIn scenarios where agent construction fails (e.g., due to throwing an exception in the factory), this test ensures that such failures are logged appropriately.\n\n```python\n@pytest.mark.asyncio\nasync def test_register_receives_publish_with_construction(caplog: pytest.LogCaptureFixture) -> None:\n    runtime = SingleThreadedAgentRuntime()\n    ...\n```\n\n### `test_event_handler_exception_propogates`\n\nThis test checks that exceptions raised in message handlers propagate correctly through the system, emphasizing the need for proper error reporting in a live system.\n\n```python\n@default_subscription\nclass FailingAgent(RoutedAgent):\n    ...\n    \n@pytest.mark.asyncio\nasync def test_event_handler_exception_propogates() -> None:\n    runtime = SingleThreadedAgentRuntime(ignore_unhandled_exceptions=False)\n    ...\n```\n\n## Conclusion of Tests\n\nThe tests demonstrate a comprehensive suite focusing on various aspects of agent registration, message publishing, and exception handling. Each test builds upon the core functionalities provided by the runtime environment to ensure that the agent system behaves as expected under normal and erroneous conditions. The overall structure adheres to the needs of asynchronous programming practices and the `pytest` testing framework. With clear assertions and coverage of edge cases, these tests serve to maintain reliability and performance in agent management systems.",
    "filename": "python/packages/autogen-core/tests/test_runtime.py"
  },
  {
    "code": false,
    "content": "# Documentation for Serialization Tests Module\n\nThis module contains a series of tests based on the serialization and deserialization of various message formats using a serialization registry. It includes tests for Pydantic models, dataclass implementations, Protocol Buffers (protobuf), and custom types, showcasing the flexibility of the serialization framework.\n\n## Overview\n\nThe primary goal of this module is to validate and demonstrate the serialization capabilities provided by different data models (Pydantic, dataclasses, protobuf). Tests ensure that serialized data maintains its integrity during deserialization and that various message formats can be handled appropriately.\n\n## Imports\n\nThe script makes use of several external libraries:\n\n- **dataclasses**: For creating simple classes for data holding.\n- **typing**: For type hinting capabilities (e.g., Union).\n- **pytest**: For writing and running tests.\n- **autogen_core**: Provides serialization utilities and a registry for message serializers.\n- **PIL**: Python Imaging Library, used for handling image data.\n- **protos.serialization_test_pb2**: Contains Protocol Buffers generated classes for testing.\n- **pydantic**: A data validation and settings management library.\n\n## Data Classes and Pydantic Models\n\nThe script defines several data structures, including:\n\n1. **PydanticMessage**: A basic model containing a string message.\n2. **NestingPydanticMessage**: A nested Pydantic model that includes another `PydanticMessage`.\n3. **DataclassMessage**: A dataclass that mirrors the `PydanticMessage` structure.\n4. **NestingDataclassMessage**: A similar structure using dataclasses containing a nested `DataclassMessage`.\n5. **NestingPydanticDataclassMessage**: A dataclass version that includes a nested Pydantic model.\n\n## Serialization Tests for Pydantic\n\n### `test_pydantic`\n\nThis test validates serialization and deserialization of a simple `PydanticMessage`. It checks the generated type name, serialization to JSON, and successful deserialization back to the original message.\n\n### `test_nested_pydantic`\n\nThis test checks nested serialization for `NestingPydanticMessage`. It asserts that both parent and nested messages are serialized and deserialized accurately.\n\n## Serialization Tests for Dataclasses\n\n### `test_dataclass`\n\nTests the serialization processes for the basic `DataclassMessage`. It verifies the integrity of the serialized JSON and ensures that deserialization restores the message correctly.\n\n### `test_nesting_dataclass_dataclass`\n\nThis test is designed to validate the behavior of nested dataclass structures. It expects a `ValueError` when trying to register a serializer for the `NestingDataclassMessage`, which suggests that nested dataclass structures may not be supported.\n\n## Protocol Buffers Serialization Tests\n\n### `test_proto`\n\nThis test focuses on Protocol Buffers using the `ProtoMessage`. It checks whether the type name, serialization, and deserialization work correctly for protobuf messages.\n\n### `test_nested_proto`\n\nHere, the test checks nested structures in Protocol Buffers with `NestingProtoMessage`. It validates that both parent and nested serialization and deserialization operate as expected.\n\n## Union Syntax in Dataclasses\n\n### `test_nesting_union_old_syntax_dataclass`\n\nThis test evaluates the use of Union types in dataclass definitions. It ensures that an attempt to register serializers for a dataclass using a Union type, whether old or new syntax, throws a `ValueError`.\n\n### `test_nesting_dataclass_pydantic`\n\nThis test checks for exceptions when attempting to register a serializer for a dataclass that contains a nested Pydantic model. It expects a `ValueError`.\n\n## Custom Type Serialization Test\n\n### `test_custom_type`\n\nThis test demonstrates the capability to define and use custom serializers through the `CustomStringTypeSerializer` class. It validates that custom serialization and deserialization works as expected, maintaining the integrity of string messages.\n\n## Image Serialization Test\n\n### `test_image_type`\n\nThis test showcases the serialization of image data using the `Image` wrapper class. It tests the serialization and deserialization of a `PydanticImageMessage` containing an image, checking both dimensions and color mode to ensure proper handling of image data.\n\n## Type Name Retrieval Tests\n\n### `test_type_name_for_protos`\n\nThis section focuses on validating type name retrieval for different protobuf message types. It tests both instantiated and non-instantiated types, ensuring consistency in type naming through the serialization registry.\n\n## Conclusion\n\nThis module highlights the versatility and effectiveness of the serialization framework by testing various data models and structures. Each test case serves to guarantee that the system can effectively handle serialization, maintain data integrity during the process, and provide helpful error messages when required. This lays a solid foundation for future extension and robust error handling within the serialization layer.",
    "filename": "python/packages/autogen-core/tests/test_serialization.py"
  },
  {
    "code": false,
    "content": "# Documentation of StatefulAgent Code\n\n## Overview\n\nThe provided code defines a simple agent framework using asynchronous programming techniques. The primary focus is on creating a `StatefulAgent`, which can persist its state and interact with a single-threaded agent runtime. The code includes two pytest test functions to validate the agent's state-saving mechanisms.\n\n## Class: `StatefulAgent`\n\nThis class extends `BaseAgent` and represents a stateful agent that can remember its state between interactions. The agent is initialized with a default state of 0.\n\n### Constructor: `__init__`\n\n```python\ndef __init__(self) -> None:\n    super().__init__(\"A stateful agent\")\n    self.state = 0\n```\n\n- **Purpose**: Initializes the `StatefulAgent` by calling the parent class's constructor with a description and setting the initial state to 0.\n\n### Method: `on_message_impl`\n\n```python\nasync def on_message_impl(self, message: Any, ctx: MessageContext) -> None:\n    raise NotImplementedError\n```\n\n- **Purpose**: This method is intended for handling incoming messages but is not implemented in the current version (it raises `NotImplementedError`). This suggests that subclasses should provide specific implementations for message processing.\n\n### Method: `save_state`\n\n```python\nasync def save_state(self) -> Mapping[str, Any]:\n    return {\"state\": self.state}\n```\n\n- **Purpose**: Asynchronously retrieves the current state of the agent and returns it as a dictionary. This allows the state to be serialized and stored externally.\n\n### Method: `load_state`\n\n```python\nasync def load_state(self, state: Mapping[str, Any]) -> None:\n    self.state = state[\"state\"]\n```\n\n- **Purpose**: Asynchronously loads the state from a given dictionary. It sets the `state` of the agent to the value provided in the input mapping, allowing the agent to restore its previous state.\n\n## Test Function: `test_agent_can_save_state`\n\nThis function tests the state saving and loading functionality of the `StatefulAgent`.\n\n### Steps\n\n1. **Initialize Runtime**: A `SingleThreadedAgentRuntime` instance is created to manage the agent.\n  \n2. **Register Agent**: The `StatefulAgent` is registered with the runtime using a specific name.\n\n3. **Get Agent Instance**: An instance of `StatefulAgent` is retrieved using its `AgentId`.\n\n4. **State Validation**: Initially, the state of the agent is validated to be 0.\n\n5. **Modify State**: The agent's state is modified to 1, and this change is validated.\n\n6. **Save State**: The current state of the agent is saved.\n\n7. **Further State Change**: The agent's state is changed to 2.\n\n8. **Load State**: The previously saved state is loaded back into the agent.\n\n9. **Final State Validation**: The state is checked to confirm it has reverted to 1, demonstrating that the save/load functionality works as intended.\n\n## Test Function: `test_runtime_can_save_state`\n\nThis function tests the state saving capabilities of the overall `SingleThreadedAgentRuntime`.\n\n### Steps\n\n1. **Initialize Runtime**: A new `SingleThreadedAgentRuntime` instance is initialized.\n\n2. **Register Agent**: The `StatefulAgent` is registered with this runtime.\n\n3. **Get Agent Instance**: An instance of `StatefulAgent` is retrieved, similarly to the previous test.\n\n4. **Initial State Check**: The agent's state is checked to confirm it starts at 0.\n\n5. **Modify State**: The state is updated to 1, and this is validated.\n\n6. **Save Runtime State**: The entire runtime's state is saved, which includes the states of registered agents.\n\n7. **Initialize New Runtime**: A second `SingleThreadedAgentRuntime` instance is created.\n\n8. **Register Agent Again**: The `StatefulAgent` is registered with the new runtime.\n\n9. **Get Second Agent Instance**: The agent instance is retrieved from the new runtime.\n\n10. **Load Runtime State**: The runtime state saved from the first runtime is loaded into the second runtime.\n\n11. **Final State Validation**: The state of the agent in the new runtime is checked to confirm it reflects the loaded state of 1, ensuring state persistence across different runtime instances.\n\n## Conclusion\n\nOverall, this code provides a foundational setup for creating stateful agents within a single-threaded environment. The functionality for saving and loading state is demonstrated through thorough testing, ensuring that agents can maintain their state even after being reloaded or transferred between runtime instances. The design can be further extended to implement specific message handling and additional functionalities as required.",
    "filename": "python/packages/autogen-core/tests/test_state.py"
  },
  {
    "code": false,
    "content": "# StaticWorkbench and ToolOverride Testing Documentation\n\nThis document provides an overview of the tests for the `StaticWorkbench` and `ToolOverride` classes from the `autogen_core` library. The tests are structured to validate various functionalities, including tool overrides, serialization, and the handling of conflicts.\n\n## Overview\n\n`StaticWorkbench` is a component designed to host tools that can be called with specified parameters. Tools can be overridden for name and description, allowing for flexible usage and clear API design. The testing framework utilizes `pytest` to verify the expected behavior across different scenarios.\n\n## Tool Definitions and Setup\n\nTools are defined using `FunctionTool`, which associates a callable function with metadata such as name and description. The functions themselves are simple mathematical operations, such as doubling a number and adding two integers. The tools are created as follows:\n\n```python\ndef test_tool_func_1(x: Annotated[int, \"The number to double.\"]) -> int:\n    return x * 2\n\ndef test_tool_func_2(a: Annotated[int, \"First number\"], b: Annotated[int, \"Second number\"]) -> int:\n    return a + b\n```\n\nEach tool is instantiated with a name and description, and additional configuration can be passed via `global_imports`.\n\n## Tool Overrides\n\nThe primary focus of the tests is on how tool overrides impact the behavior of `StaticWorkbench`. Overrides are defined using a dictionary that maps original tool names to `ToolOverride` instances, which can change either the name, description, or both.\n\n```python\noverrides: Dict[str, ToolOverride] = {\n    \"double\": ToolOverride(name=\"multiply_by_two\", description=\"Multiplies a number by 2\"),\n    \"add\": ToolOverride(description=\"Performs addition of two integers\"),\n}\n```\n\n## Test: Static Workbench with Tool Overrides\n\nIn the first test (`test_static_workbench_with_tool_overrides`), multiple tools are registered with overrides applied. The test follows these steps:\n\n1. Tools are created as `FunctionTool` instances.\n2. The `StaticWorkbench` context manager is instantiated with the tools and overrides.\n3. The test checks if the tools list successfully reflects the overrides:\n   - For the `double` tool, both name and description are overridden.\n   - For the `add` tool, only the description is changed.\n4. Each tool is called with their respective override names to verify the expected results.\n5. An attempt to call a non-existent tool checks error handling.\n\nThis comprehensive approach ensures that the override system works as intended.\n\n## Test: Static Workbench Without Overrides\n\nThe second test (`test_static_workbench_without_overrides`) evaluates the original behavior of `StaticWorkbench` without any overrides. This verifies that the tools are available under their default names and descriptions.\n\n```python\nasync with StaticWorkbench(tools=[test_tool]) as workbench:\n    tools = await workbench.list_tools()\n    assert len(tools) == 1\n    assert tools[0].get(\"name\") == \"double\"\n    assert tools[0].get(\"description\") == \"A test tool that doubles a number.\"\n```\n\nThis confirms that basic functionality remains intact when no additional configuration is applied.\n\n## Test: Serialization With Overrides\n\nIn `test_static_workbench_serialization_with_overrides`, the test ensures that the `StaticWorkbench` maintains serialization and deserialization capabilities with overrides. Steps include:\n\n1. Creating a workbench with tool overrides.\n2. Dumping the configuration of the workbench to verify it includes the overrides.\n3. Recreating the workbench from the configuration to check if the overrides persist.\n\nThis test demonstrates the integrity of the configuration system.\n\n## Test: Partial Overrides\n\nThe `test_static_workbench_partial_overrides` test examines how `StaticWorkbench` reacts to different types of overrides, such as name-only and description-only changes. The approach ensures that:\n\n- Name changes do not alter the descriptive context.\n- Descriptions can be updated without affecting the original name.\n\nThe assertions verify that calling the tools behaves consistently with the modified configurations.\n\n## Test: Tool Override Model Functionality\n\nThe `test_tool_override_model` function tests the `ToolOverride` model effectively, ensuring proper instantiation with various combinations of name and description. Checks include verifying the default values when parameters are omitted.\n\n```python\noverride1 = ToolOverride(name=\"new_name\", description=\"new_desc\")\nassert override1.name == \"new_name\"\n```\n\nThis test ensures that the override mechanism can flexibly adapt to user requirements without unnecessary complexity.\n\n## Test: Conflict Detection in Tool Overrides\n\n`test_static_workbench_conflict_detection` assesses the robustness of the `StaticWorkbench` system against conflicts in tool name overrides. The primary scenarios tested include:\n\n1. Valid overrides that do not conflict with existing tool names.\n2. Attempts to override a tool with a name that already exists.\n3. Duplicate override names generating errors.\n\nThese situations are crucial for ensuring that an erroneous configuration does not compromise the system's operational integrity.\n\n## Conclusion\n\nThe tests for `StaticWorkbench` and `ToolOverride` cover critical scenarios encompassing tool definition, overriding behavior, serialization, and conflict management. Each test is designed to ensure that the system remains reliable and flexible, accommodating a variety of operational needs while maintaining clarity in the API.",
    "filename": "python/packages/autogen-core/tests/test_static_workbench_overrides.py"
  },
  {
    "code": false,
    "content": "# Documentation for Agent Subscription Testing Code\n\n## Overview\n\nThis code is a test suite written using the `pytest` framework, specifically designed to validate the functionality of subscription handling in an agent-based messaging system. The tests cover various aspects of subscriptions, including type matching, mapping agents to topics, default subscriptions, and deduplication of subscriptions. The code uses asynchronous programming paradigms, leveraging the `asyncio` capabilities of Python.\n\n### Imports\n\nThe code begins by importing necessary modules and classes:\n\n- **pytest**: A testing framework for Python.\n- Components from `autogen_core`: These include various classes related to agent and topic management such as `AgentId`, `DefaultSubscription`, `TypeSubscription`, and `SingleThreadedAgentRuntime`.\n- **CantHandleException**: An exception raised in specific handling scenarios.\n- Components from `autogen_test_utils`: This includes `LoopbackAgent` and `MessageType`, which facilitate agent interactions and message formatting, respectively.\n\n## Test Function Descriptions\n\n### Test 1: `test_type_subscription_match`\n\nThis function tests the functionality of the `TypeSubscription` class in terms of matching subscription criteria.\n\n- **Matching Logic**: It asserts specific conditions for the `is_match` method of the `TypeSubscription` object:\n  - TopicId with type \"t0\" should not match.\n  - TopicId with type \"t1\" should match regardless of the source identifier.\n  \nThe tests confirm that the subscription is correctly identifying matching and non-matching topics, thereby validating the basic functionality of subscription matching.\n\n### Test 2: `test_type_subscription_map`\n\nThis function tests the `map_to_agent` functionality of the `TypeSubscription`.\n\n- **Mapping Logic**: It asserts that a correctly typed `TopicId` maps to a corresponding `AgentId`. If the topic type does not match the subscription, a `CantHandleException` is raised.\n\nThis ensures that the subscription correctly translates the topic type into an agent identifier, revealing correct functionality in scenarios where subscriptions apply or do not apply.\n\n### Test 3: `test_non_default_default_subscription`\n\nIn this test case, the behavior of default subscriptions is examined using an agent runtime.\n\n- **Agent Registration**: The `LoopbackAgent` is registered with a runtime without default subscriptions, which means it does not react to messages initially.\n- **Message Publishing**: Messages are published to a default topic both with and without a subscription to observe agent response.\n- **Subscription Handling**: The test adds a subscription to the default topic, verifies that the agent's call count increments, and tests behavior when adding other topic subscriptions. \n- **Subscription Removal**: Finally, the subscription is removed, and the test checks the agent's call count again.\n\nThis provides a comprehensive look at how default subscriptions function within the agent runtime and ensures expected behavior.\n\n### Test 4: `test_skipped_class_subscriptions`\n\nIn this test, the behavior of agents registered with the option to skip class subscriptions is evaluated.\n\n- Similar to the previous test, the `LoopbackAgent` is registered and messages are published.\n- The test specifically validates that no calls are made to the agent when class subscriptions are skipped.\n\nThis ensures the design choice of skipping subscriptions operates correctly without triggering unexpected behavior.\n\n### Test 5: `test_subscription_deduplication`\n\nThis function verifies that attempts to add duplicate subscriptions are handled appropriately.\n\n- **Type Subscription**: Two instances of `TypeSubscription` created with identical parameters are added sequentially. The second addition raises a `ValueError`, confirming deduplication functionality.\n- **Default Subscription**: Similarly, a default subscription is tested for duplicate handling.\n\nThe test confirms that the system correctly prevents duplicate subscriptions, maintaining the integrity of the subscription model.\n\n## Summary \n\nThe provided code demonstrates robust testing of a subscription-based messaging framework involving agents. These tests comprehensively verify the key functionalities associated with subscriptions, including matching logic, agent mapping, default behavior, and deduplication efforts. By ensuring these functionalities work as intended, the tests contribute to the reliability and correctness of the overall messaging system.",
    "filename": "python/packages/autogen-core/tests/test_subscription.py"
  },
  {
    "code": false,
    "content": "# Code Documentation: Tool Agent and Async Testing\n\nThis document explains the functionality and purpose of the provided code, which involves the implementation of a tool agent system, mock client, and asynchronous testing using the `pytest` framework.\n\n## Overview\n\nThe code is structured around testing a `ToolAgent`, which integrates various tools that can either return a value or raise an exception based on a given input. It creates a mock environment to facilitate testing of these tools asynchronously. The primary components of this script include function definitions, test cases, and logging configuration, managing tool executions, and simulating user interaction.\n\n## Logging Configuration\n\nThe code starts by setting up logging to capture events within the `ToolAgent`. It uses the `logging` library to create a log entry log specifically for the tool events:\n\n```python\nimport logging\nlogging.getLogger(EVENT_LOGGER_NAME).setLevel(logging.INFO)\n```\n\nThis allows tracking of various actions taken by the tool agent, aiding in debugging and testing.\n\n## Tool Functions\n\nThree functions are defined to serve as tools for the `ToolAgent`. Each tool handles input in a specific way:\n\n1. **_pass_function**: Accepts a string input and returns \"pass\".\n   \n   ```python\n   def _pass_function(input: str) -> str:\n       return \"pass\"\n   ```\n\n2. **_raise_function**: Raises an exception with the message \"raise\" when called.\n   \n   ```python\n   def _raise_function(input: str) -> str:\n       raise Exception(\"raise\")\n   ```\n\n3. **_async_sleep_function**: Simulates an asynchronous operation by sleeping for 10 seconds before returning \"pass\".\n   \n   ```python\n   async def _async_sleep_function(input: str) -> str:\n       await asyncio.sleep(10)\n       return \"pass\"\n   ```\n\nThese functions represent a stable foundation to test the behavior of the `ToolAgent`.\n\n## Test Cases\n\nThe code includes multiple test cases implemented using `pytest`. Each test function is designated to validate specific behaviors and exceptions related to the `ToolAgent`.\n\n### `test_tool_agent`\n\nThis asynchronous test case verifies the behavior of the `ToolAgent` under various scenarios:\n\n1. **Function Registration**: The `ToolAgent` is registered with the defined tools within a `SingleThreadedAgentRuntime`.\n\n2. **Execution of Tools**: It sends a message to the `ToolAgent` for each tool, validating the response matches expectations. Specifically, it tests:\n   - Successful execution of the \"pass\" function.\n   - Exception handling for the \"raise\" function.\n   - Validating behavior for an invalid tool name and incorrect arguments.\n   - Testing for cancellation handling by initiating a sleep function and then cancelling the operation.\n\n```python\n@pytest.mark.asyncio\nasync def test_tool_agent(caplog: pytest.LogCaptureFixture) -> None:\n    # Test logic here...\n```\n\n### `test_caller_loop`\n\nThis test focuses on the tool agent's interaction with a mock version of a chat completion client. The key features being tested include:\n\n1. **Mocking Client Behavior**: A mock class, `MockChatCompletionClient`, simulates the expected responses from a client that processes messages and makes tool calls.\n\n2. **Tool Agent Interaction**: Similar to the previous test, it validates that the agent communicates correctly and that the expected message structure is returned.\n\n3. **Message Assertions**: The test checks that the response contains the appropriate types of messages, including assistant and function execution results.\n\n```python\n@pytest.mark.asyncio\nasync def test_caller_loop() -> None:\n    # Test logic here...\n```\n\n## Classes and Methods in Use\n\n### `SingleThreadedAgentRuntime`\n\nThis class is central to managing the lifecycle of the tool agent and facilitating communication between the tools and messages from the user.\n\n### `ToolAgent`\n\nThe core agent class that orchestrates tool execution. It handles registrations and the calling of functions based on specified input.\n\n### `CancellationToken`\n\nThis object allows for the cancellation of long-running tasks or operations, showcasing the ability to manage async lifecycle events effectively.\n\n## Error Handling\n\nThroughout the tests, exceptions such as `ToolExecutionException`, `ToolNotFoundException`, and `InvalidToolArgumentsException` are anticipated and handled gracefully, ensuring robust testing against poor inputs and system failures.\n\n## Conclusion\n\nThe provided code snippet offers a comprehensive framework for testing a tool agent system in an asynchronous environment. It validates tool execution, exception management, and inter-component communication via a mocked chat client. With these features, the script contributes to testing both individual function behavior and their integration within a broader application context.",
    "filename": "python/packages/autogen-core/tests/test_tool_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Tool Framework Code\n\nThis code is part of a framework that allows users to define tools, run them asynchronously, and validate their input and output types using Python's type annotations and data models. The framework employs Pydantic for data validation and relies on pytest for unit testing of the tool functionalities.\n\n## Data Model Definitions\n\n### `MyArgs`\n\n```python\nclass MyArgs(BaseModel):\n    query: str = Field(description=\"The description.\")\n```\n\n`MyArgs` is a Pydantic model that represents the input arguments expected by a tool. It contains a single field:\n\n- **`query`**: A string that represents a query with a description.\n\n### `MyNestedArgs`\n\n```python\nclass MyNestedArgs(BaseModel):\n    arg: MyArgs = Field(description=\"The nested description.\")\n```\n\n`MyNestedArgs` is another Pydantic model that encapsulates `MyArgs`, providing a nested structure for the input. This can be useful for organizing complex input data.\n\n### `MyResult`\n\n```python\nclass MyResult(BaseModel):\n    result: str = Field(description=\"The other description.\")\n```\n\n`MyResult` is a Pydantic model used to define the output structure of the tools. It contains a single field:\n\n- **`result`**: A string that holds the result of the tool's execution.\n\n## Tool Implementations\n\n### `MyTool`\n\n```python\nclass MyTool(BaseTool[MyArgs, MyResult]):\n    ...\n```\n\n`MyTool` is a specialized tool that extends `BaseTool`. It uses the `MyArgs` model for input and produces a `MyResult` model as output. This tool maintains a count of how many times it has been called, facilitating tracking and testing.\n\n- **`run` method**: Asynchronously executes the tool's functionality, increments the call count, and returns a fixed result (\"value\").\n\n### `MyNestedTool`\n\n```python\nclass MyNestedTool(BaseTool[MyNestedArgs, MyResult]):\n    ...\n```\n\n`MyNestedTool` is similar to `MyTool`, but uses `MyNestedArgs` to allow more complex input structures. \n\nThe `run` function is also implemented similarly to `MyTool`, returning a preset result and tracking call counts.\n\n## Tool Schema Generation Tests\n\n### `test_tool_schema_generation`\n\nThis test checks if the schema generated for `MyTool` meets expectations, ensuring correct naming, descriptions, and parameter requirements.\n\n### `test_func_tool_schema_generation`\n\nThis test confirms that the schema can be generated for a function tool by examining a defined function's properties like parameters, types, and descriptions.\n\n## Strict Mode Tests\n\n### `test_func_tool_schema_generation_strict`\n\nThis set of tests evaluates the behavior of tool schema generation in strict mode, which enforces constraints on parameters and their defaults. It ensures exceptions are raised when strict enforcement attempts to validate insufficient function signatures.\n\n## Asynchronous Execution Tests\n\n### `test_run_func_call_tool_with_kwargs_and_args`\n\nThis test checks the functionality of invoking a tool with both keyword and positional arguments, ensuring that the results are returned correctly as per the tool's design.\n\n### `test_tool_run`\n\nThis test runs `MyTool` and validates the output while also checking that the call count increments appropriately on repeated calls.\n\n## Tool Properties Tests\n\n### `test_tool_properties`\n\nVerifies that attributes such as `name`, `description`, `args_type`, and `return_type` are correctly assigned and retrievable for `MyTool`. Similar tests apply for `MyNestedTool`.\n\n## Type Signature Tests\n\n### `test_get_typed_signature`\n\nTests are written to check the accuracy of the function signature extraction using the `get_typed_signature` utility. It ensures parameters and return annotations are collected correctly for functions of various complexities.\n\n## Function Tool Tests\n\n### `test_func_tool_no_args`\n\nFocuses on function tools that do not take any arguments, ensuring the tool is configurated correctly and returns the expected types.\n\n### `test_func_tool_with_pydantic_model_conversion_success` and `test_func_tool_with_pydantic_model_conversion_failure`\n\nThese tests check conversion success and failure for functions utilizing Pydantic models as input types, validating required fields and responses.\n\n## General Observations \n\n1. **Asynchronous Execution**: The tools defined in the framework are designed to run asynchronously, which adds an additional layer of concurrency and performance for handling multiple requests.\n\n2. **Validation with Pydantic**: By using Pydantic, the framework ensures that input is validated cleanly and that errors are raised when input does not conform to the specified models.\n\n3. **Dynamic Tool Creation**: The `FunctionTool` implementation allows users to define tools dynamically based on normal Python functions, leading to increased flexibility.\n\n4. **Comprehensive Testing**: The framework includes robust tests that cover various scenarios, ensuring reliability and correctness in behavior for both expected and edge cases.\n\n5. **Support for Nested Structures**: The ability to handle nested argument models provides users with a powerful way to structure complex data inputs, making the tool framework scalable and versatile for different applications. \n\nBy following this structured approach, users can implement custom tools with predefined input and output specifications, test them thoroughly, and ensure they comply with expected behaviors.",
    "filename": "python/packages/autogen-core/tests/test_tools.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides an overview of the provided Python code, which involves testing functions for handling types and data models. The code primarily utilizes the `dataclasses` and `pydantic` libraries, as well as functionality from the `autogen_core` module.\n\n## Overview\n\nThe code tests several functionalities associated with type handling within the context of asynchronous message processing. It defines a series of test functions that assert specific behaviors of type-related utilities and data models, specifically focusing on checking type definitions, handling messages, and confirming nested data model capabilities.\n\n## Type Handling Tests\n\n### `test_get_types()`\n\nThis function is responsible for validating the behavior of the `get_types` function from the `autogen_core._type_helpers` module.\n\n- **Type Assertions**: The function asserts that `get_types` can correctly interpret different types, such as:\n  - Union types (e.g., `Union[int, str]`) and their modern equivalent using the pipe operator (`int | str`).\n  - Single types (like `int` and `str`).\n  - Optional types (e.g., `Optional[int]`).\n  - Direct checks for types like `NoneType`.\n\nThe tests ensure that `get_types` behaves correctly under various circumstances, returning expected type tuples for valid types and handling edge cases like `None` properly.\n\n## Message Handler Tests\n\n### `test_handler()`\n\nThis function checks the functionality of message handlers defined within a `RoutedAgent` subclass named `HandlerClass`.\n\n- **Handler Declaration**: Two handlers are defined:\n  - `handler` which accepts an integer message.\n  - `handler2` which accepts a message that can be a string or a boolean.\n\n- **Handler Assertions**: The function asserts the following for each handler:\n  - `target_types`: The expected types of messages that the handler can receive.\n  - `produces_types`: The types of outputs that the handlers can produce, which in this case shows what can be returned via the handlers.\n\nThe assertions ensure that the handlers are set up correctly and their configurations match the intended usage.\n\n## Main Handler Class\n\n### `HandlerClass`\n\nThis is a subclass of `RoutedAgent` that implements asynchronous message handling through decorated methods:\n\n- **Asynchronous Handlers**: It defines an asynchronous method `handler` that takes an integer input message and uses the `@message_handler()` decorator to register it. However, the function currently returns `None`.\n\nThe class design allows for easy expansion of message handling capabilities in a structured manner.\n\n## Nested Data Model Tests\n\n### `test_nested_data_model()`\n\nThis function aims to test the presence of nested base models, utilizing the `has_nested_base_model` utility.\n\n- **Base Model Definition**: A base model named `MyBaseModel` is created using Pydantic, which requires a single string field, `message`.\n\n- **Nested Data Structures**: Several nested data models are introduced using the `dataclass` decorator, marking various combinations of lists and unions with the `MyBaseModel` type:\n  - Single nested instance.\n  - Various levels of nesting through lists.\n  - Unions that allow for either a `MyBaseModel` instance or a string.\n\n- **Assertions**: The function verifies that each of these defined models is recognized as containing a nested base model, using the `has_nested_base_model` function which checks the nested structures.\n\nThis ensures that the application can correctly identify models with nested complexities, crucial for systems that rely on deep data structures.\n\n## Summary\n\nThe code demonstrates a modular approach to testing type handling and nested model capabilities within an asynchronous messaging framework. By utilizing tests for type definitions and message handlers, it establishes reliable behaviors expected in the system. Through rigorous assertions, it validates that the components function as intended, paving the way for stable development in message-based architectures. \n\nFuture enhancements could extend functionalities or focus on integrating additional types and handlers to broaden the system\u2019s effectiveness in real-world applications.",
    "filename": "python/packages/autogen-core/tests/test_types.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Code\n\nThis code defines tools and tests using Python's asyncio, Pydantic for data validation, and pytest for testing. It primarily focuses on creating tools that can both perform a standard execution and a streaming execution. Below is a structured description of its components and functionality.\n\n## Class Definitions\n\n### StreamArgs\n```python\nclass StreamArgs(BaseModel):\n    count: int\n```\n`StreamArgs` is a data model defined with Pydantic that encapsulates the arguments for the stream tool. It contains:\n- `count`: An integer indicating how many items to count.\n\n### StreamResult\n```python\nclass StreamResult(BaseModel):\n    final_count: int\n```\n`StreamResult` is another Pydantic model representing the result of a stream operation. It includes:\n- `final_count`: An integer detailing the final count after the processing.\n\n### StreamItem\n```python\nclass StreamItem(BaseModel):\n    current: int\n```\n`StreamItem` model represents each item yielded by the streaming process. It contains:\n- `current`: The current count during the streaming operation.\n\n### StreamTool\n```python\nclass StreamTool(BaseStreamTool[StreamArgs, StreamItem, StreamResult]):\n```\n`StreamTool` extends `BaseStreamTool` and serves as a counting tool that can execute both in a standard way and as a stream:\n- **Constructor**: Initializes the tool, defines its type, name, and description.\n- **run method**: Returns a final count when called.\n- **run_stream method**: Yields intermediate counts using an async generator until the specified count is reached or the process is canceled.\n\n### StreamToolWithError\n```python\nclass StreamToolWithError(BaseStreamTool[StreamArgs, StreamItem, StreamResult]):\n```\n`StreamToolWithError` is similar to `StreamTool` but designed to raise an error:\n- **Constructor**: Initializes the tool similarly to `StreamTool`.\n- **run method**: Raises a `ValueError` to simulate an error.\n- **run_stream method**: Yields the first count and then raises a `ValueError`.\n\n## Function Definitions\n\n### test_static_workbench\n```python\n@pytest.mark.asyncio\nasync def test_static_workbench() -> None:\n```\nThis is a test function that verifies the functionality of the static workbench. Key actions include:\n- **Defining Tools**: Creates two tools using `FunctionTool`, one that doubles a number and one that simulates an error.\n- **Validating Tool List**: Confirms that both tools are registered with the correct parameters.\n- **Calling Tools**: Executes both tools and asserts that the output matches expectations, including handling errors correctly.\n- **State Saving**: Tests saving the state of the workbench and verifies that the tools can still be accessed upon loading it again.\n\n### test_static_stream_workbench_call_tool_stream\n```python\n@pytest.mark.asyncio\nasync def test_static_stream_workbench_call_tool_stream() -> None:\n```\nIn this function, the static stream workbench's streaming capabilities are tested:\n- **Creating Tools**: Similar to previous functions but includes a streaming tool.\n- **Stream Testing**: Asserts that the streaming tool provides intermediate results and final outcomes.\n- **Error Handling**: Tests the error scenario with the stream tool that raises an error.\n- **Tool Not Found and State without Arguments**: Validates the responses when a non-existent tool is called and processes when no or null arguments are provided.\n\n### test_static_stream_workbench_call_tool_stream_cancellation\n```python\n@pytest.mark.asyncio\nasync def test_static_stream_workbench_call_tool_stream_cancellation() -> None:\n```\nThis function tests if cancellation functionality operates as intended within the stream:\n- **Cancellation Token**: Introduces a cancellation token that terminates the stream after receiving a defined number of results.\n- **Assert Results**: Ensures that results are collected until cancellation occurs.\n\n### test_static_stream_workbench_inheritance\n```python\n@pytest.mark.asyncio\nasync def test_static_stream_workbench_inheritance() -> None:\n```\nThis test confirms that `StaticStreamWorkbench` inherits capabilities from both `StaticWorkbench` and `StreamWorkbench`:\n- **Tool Listing**: Checks whether the workbench retains the tools after instantiation.\n- **Functionality Tests**: Verifies both standard and streaming functionalities operate correctly.\n\n## Utility of the Code Structure\n\nThe code is structured to facilitate testing of asynchronous tool functionalities within a framework that supports both immediate and streamed responses. By using Pydantic, it ensures data validation and structure for inputs and outputs, streamlining the handling of issues like errors or unexpected input types. The tests serve not only to verify functionality but also to ensure that tools can handle various edge cases, including errors and cancelled operations. Overall, the combination of async programming, Pydantic, and pytest provides a solid foundation for developing and testing asynchronous tools in a robust manner.",
    "filename": "python/packages/autogen-core/tests/test_workbench.py"
  },
  {
    "content": "# AutoGen Extensions\n\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html)\n\nAutoGen is designed to be extensible. The `autogen-ext` package contains many different component implementations maintained by the AutoGen project. However, we strongly encourage others to build their own components and publish them as part of the ecosytem.",
    "filename": "python/packages/autogen-ext/README.md"
  },
  {
    "code": false,
    "content": "# Pytest Plugin for Selectively Running gRPC Tests\n\nThis script is a Pytest plugin that enhances the test selection process based on command-line options. It specifically allows for the selective execution of tests that are marked with the `grpc` keyword, thereby enabling or disabling tests based on the presence of the `--grpc` option.\n\n## Overview of Key Functions\n\n### `pytest_addoption`\n\nThis function is responsible for adding custom command-line options to the Pytest interface. It enhances the ability to tailor test runs based on specific criteria defined by the user.\n\n- **Parameters**:\n  - `parser`: A `pytest` built-in object that facilitates the addition of command-line options.\n\n- **Functionality**:\n  - The function implements the `--grpc` option, which, when provided on the command line, signals that the test runner should include gRPC-related tests in the execution. The option has:\n    - **Action**: `store_true`, meaning it doesn't require an additional value; its presence alone implies a boolean `True`.\n    - **Default**: `False`, indicating that tests will not include gRPC tests unless specified.\n    - **Help**: A brief description provided in the help output when the command line is invoked with the `-h` or `--help` options.\n\n### `pytest_collection_modifyitems`\n\nThis function is invoked after the test items (i.e., test functions or classes) have been collected but before they are executed. It modifies the test items based on the aforementioned command-line options.\n\n- **Parameters**:\n  - `config`: Contains the configuration information, including any command-line arguments provided.\n  - `items`: A list of collected test items.\n\n- **Functionality**:\n  - **Retrieving the option**: It checks if the `--grpc` option has been passed.\n  - **Defining markers**:\n    - `skip_grpc`: A marker that will be used to skip tests that require the `--grpc` option.\n    - `skip_non_grpc`: A marker for tests that should be skipped if the `--grpc` option is passed.\n  - **Iterating through test items**:\n    - If a test item has the \"grpc\" keyword and the `--grpc` option was not passed, it marks the item with `skip_grpc`.\n    - Conversely, if the `--grpc` option is passed and the test does not have the \"grpc\" keyword, it marks the item with `skip_non_grpc`.\n\n## Summary of Behavior\n\n### Test Execution Modality\n\nThis Pytest plugin introduces a more flexible testing mechanism based on command-line options. By using the `--grpc` flag, the user can control whether gRPC-related tests are included in the execution. \n\n### Test Item Marking Logic\n\nThe logic that marks test items ensures that:\n- When running tests without the `--grpc` flag, any tests marked with the `grpc` keyword will be skipped.\n- Conversely, when executing with the `--grpc` flag, all non-gRPC tests will be excluded from the test run.\n\n### Use Cases\n\nThis plugin is particularly useful in environments where tests can be categorized into distinct groups, such as:\n- Different service types (e.g., REST vs. gRPC).\n- Various features that may be optionally tested.\n\n### Conclusion\n\nOverall, the implementation of these two functions provides a straightforward method to control the execution of tests based on the desired testing strategy. By leveraging the built-in flexibility of Pytest, developers can efficiently manage their testing suite and focus on specific areas of functionality as needed. This is especially impactful in large codebases where categorizing tests can streamline the testing process.",
    "filename": "python/packages/autogen-ext/conftest.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level overview of a Python script used for importing and managing package metadata. It consists of the use of built-in libraries and a version retrieval method for a specific package.\n\n## Overview\n\nThe script's primary function is to import the `importlib.metadata` library and retrieve the version of a package named `autogen_ext`. This can be useful for applications or libraries that need to check the version of their dependencies.\n\n## Libraries and Modules\n\n### `importlib.metadata`\n\n`importlib.metadata` is a standard library module that provides tools for accessing package metadata, including version numbers, entry points, and other package-related information. It is especially useful in dynamic import scenarios or for embedding metadata into applications.\n\n## Code Breakdown\n\n### Version Retrieval\n\n```python\n__version__ = importlib.metadata.version(\"autogen_ext\")\n```\n\n- This line of code calls the `version` function from the `importlib.metadata` module.\n- The function is passed the string `\"autogen_ext\"`, which refers to the package whose version is being queried.\n- The retrieved version information is then assigned to the variable `__version__`. This is a common practice in Python packages and modules to define the current version of the software.\n\n### Use of `__version__`\n\nThe variable `__version__` can be utilized elsewhere in the code to check the version of the `autogen_ext` package. It adheres to the convention where single underscore variables are often used to represent metadata about the module itself. It makes it easier to manage dependencies and ensures compatibility when the code is integrated into larger systems.\n\n## Practical Applications\n\nThis method of retrieving package versions is particularly beneficial for developers who need to:\n\n1. **Ensure Compatibility**: By checking the version of `autogen_ext`, other parts of the application can determine if they are using compatible versions of this dependency.\n   \n2. **Dynamic Behavior**: Depending on the version retrieved, the application could adjust its behavior to accommodate different features or requirements present in the specific version of the package.\n\n3. **Documentation and Reporting**: The versioning information can be included in logs, error reports, or documentation, enabling better support and debugging practices.\n\n## Conclusion\n\nThe presented code snippet effectively demonstrates how to leverage Python's standard libraries to manage and retrieve package metadata, specifically focusing on versioning. By utilizing `importlib.metadata`, developers can streamline dependency management in their applications, ensuring that the necessary package versions are being utilized correctly.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for AzureAIAgent Module\n\n## Overview\nThe provided code is a Python module that handles the importation of a specific class, `AzureAIAgent`, from a sibling module called `_azure_ai_agent`. Its main purpose is to ensure that the required dependencies for the `AzureAIAgent` class are available and to provide a clear error message if they are not.\n\n## Importing AzureAIAgent\nThe module begins with a `try` block that attempts to import the `AzureAIAgent` class:\n\n```python\nfrom ._azure_ai_agent import AzureAIAgent\n```\n\n- This statement utilizes a relative import to access the `AzureAIAgent` class from the `_azure_ai_agent` module located in the same package.\n- If the import is successful, the code proceeds without any issues.\n\n## Handling Import Errors\nThe `try` block is paired with an `except` clause that specifically catches `ImportError` exceptions:\n\n```python\nexcept ImportError as e:\n    raise ImportError(\n        \"Dependencies for AzureAIAgent not found. \"\n        'Please install autogen-ext with the \"azure\" extra: '\n        'pip install \"autogen-ext[azure]\"'\n    ) from e\n```\n\n- If the import fails, the code raises a new `ImportError`.\n- The new error message provides clear guidance to the user, informing them that dependencies for `AzureAIAgent` are missing and suggesting the command needed to install them.\n- The `from e` clause preserves the original exception context, enabling better debugging by showing both the new and the original exception trace.\n\n## Defining the Public API\nFinally, the module defines the public interface using the `__all__` variable:\n\n```python\n__all__ = [\"AzureAIAgent\"]\n```\n\n- This indicates which names should be considered public when the module is imported using a wildcard import (e.g., `from module import *`).\n- In this case, `AzureAIAgent` is the only class that is made publicly accessible from this module, reinforcing the intention that it is the primary element users will want to work with.\n\n## Summary\nIn summary, this module serves as a protective layer for the `AzureAIAgent` class implementation. Its primary functions are:\n\n1. Attempt to import `AzureAIAgent` from `_azure_ai_agent`.\n2. Handle any import errors by issuing a clear message that directs users to install the necessary dependencies.\n3. Control which components of the module are publicly accessible, ensuring that only `AzureAIAgent` can be imported from this context.\n\nBy enforcing these checks and guiding users in case of issues, the module enhances user experience and eases the setup process for those looking to utilize `AzureAIAgent`.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/azure/__init__.py"
  },
  {
    "code": false,
    "content": "# AzureAIAgent Documentation\n\n## Overview\n\nThe `AzureAIAgent` class is part of the AutoGen framework, designed to leverage the Azure AI Assistant API for creating intelligent assistants. This class provides capabilities such as code interpretation, Bing search grounding, file management, and custom tool integrations, all while supporting multi-turn conversations. It integrates seamlessly with AutoGen\u2019s messaging system, allowing developers to create advanced AI agents with minimal effort.\n\n### Installation\n\nTo install the Azure AI Foundry Agent Service, use the following command:\n\n```bash\npip install \"autogen-ext[azure]\"\n```\n\n### Usage Examples\n\nThe documentation includes comprehensive examples demonstrating how to create agents for specific tasks, such as using Bing for information retrieval, implementing file search capabilities, and employing a code interpreter.\n\n## Class Definition\n\n### `AzureAIAgent`\n\nThe `AzureAIAgent` class inherits from the `BaseChatAgent`. It is initialized with various parameters needed to define the agent, including its name, description, project client, deployment name, and the tools it will utilize.\n\n### Initialization\n\n#### Constructor\n\n```python\ndef __init__(self, name: str, description: str, project_client: AIProjectClient, deployment_name: str, instructions: str, tools: Optional[ListToolType] = None, ...)\n```\n\n- **`name`**: A valid Python identifier for the agent.\n- **`description`**: A brief description of the agent's purpose.\n- **`project_client`**: An instance of `AIProjectClient` for communicating with the Azure AI API.\n- **`deployment_name`**: The specific model to be used (e.g., \"gpt-4\").\n- **`instructions`**: Detailed instructions that guide the agent's behavior.\n- **`tools`**: Optional tools the agent can utilize, including file search, code interpreter, and more.\n\nThe constructor also sets up default values for various attributes and ensures proper initialization of agent and thread states.\n\n## Properties\n\nSeveral properties provide access to the internal state of the agent:\n\n- **`produced_message_types`**: Indicates the types of messages the agent will produce, defaulting to `TextMessage`.\n- **`thread_id`**: Accessing this property requires the thread to be initialized; throws a ValueError if not.\n- **`agent_id`**, **`deployment_name`**, **`instructions`**, **`description`**: Various properties that fetch metadata associated with the agent.\n\n## Tools Handling\n\n### `_add_tools`\n\n```python\ndef _add_tools(self, tools: Optional[ListToolType], converted_tools: List[ToolDefinition]) -> None\n```\n\nThis private method manages the addition and conversion of various tool formats (string identifiers, `ToolDefinition` objects, `Tool` objects, or callables) to ensure they are compatible with the Azure AI Agent API.\n\n## Message Processing\n\n### `on_messages`\n\n```python\nasync def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: Optional[CancellationToken] = None, message_limit: int = 1) -> Response\n```\n\nThis method serves as the primary entry point for processing incoming messages. It internally calls `on_messages_stream` and retrieves the final response from the agent.\n\n### `on_messages_stream`\n\n```python\nasync def on_messages_stream(self, messages: Sequence[BaseChatMessage], cancellation_token: Optional[CancellationToken] = None, message_limit: int = 1, polling_interval: float = 0.5) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]\n```\n\nThis method handles the full interaction flow with the Azure AI agent, processing the input messages, creating a run, executing tool calls, and yielding final responses. It also handles the various states of the run and updates based on required actions.\n\n### `handle_text_message`\n\n```python\nasync def handle_text_message(self, content: str, cancellation_token: Optional[CancellationToken] = None) -> None\n```\n\nThis method adds a user message to the ongoing conversation thread, enabling the agent to respond appropriately.\n\n## File Upload Capabilities\n\n### `_upload_files`\n\n```python\nasync def _upload_files(self, file_paths: str | Iterable[str], purpose: str = \"assistant\", polling_interval: float = 0.5, cancellation_token: Optional[CancellationToken] = None) -> List[str]\n```\n\nThis method allows uploading files that the agent can utilize during its operations, making it crucial for functionalities like code interpretation or file searching.\n\n### `on_upload_for_file_search`\n\n```python\nasync def on_upload_for_file_search(self, file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: Optional[str] = None, ...) -> None\n```\n\nThis method assists in uploading files specifically for file search capabilities and handles the creation and configuration of vector stores.\n\n## Tool Execution\n\n### `_execute_tool_call`\n\n```python\nasync def _execute_tool_call(self, tool_call: FunctionCall, cancellation_token: CancellationToken) -> str\n```\n\nThis method executes a designated tool call, handling the invocation of external functions and returning the result back to the caller. It ensures that the correct tool is being used and manages processing the arguments accordingly.\n\n## State Management\n\n### `save_state`\n\n```python\nasync def save_state(self) -> Mapping[str, Any]\n```\n\nThis method serializes the agent's current state, preparing it for future restoration.\n\n### `load_state`\n\n```python\nasync def load_state(self, state: Mapping[str, Any]) -> None\n```\n\nThis method deserializes a previously saved state, allowing the agent to continue from where it left off.\n\n## Conclusion\n\nThe `AzureAIAgent` class is a comprehensive solution for creating intelligent chat agents that interact with Azure AI capabilities. It supports file management, tool integration, and advanced conversational dynamics, making it ideal for developers looking to implement robust AI assistants in their applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/azure/_azure_ai_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Azure AI Agent State and Utility Functions\n\n## Overview\n\nThe provided code defines a data model and a utility function for managing the state of an \"Azure AI Agent.\" The aim is to facilitate the agent\u2019s ability to maintain persistent session information, which includes tracking conversation history and resources. Additionally, the code outlines a type alias for various tool definitions that the agent may utilize.\n\n## Data Model: AzureAIAgentState\n\n### Class Definition\n\nThe main component of the code is the `AzureAIAgentState` class, which extends `BaseModel` from the `pydantic` library. This built-in functionality of `pydantic` allows for automatic data validation and serialization.\n\n### Attributes\n\n- **type (`str`)**: This attribute is set to a default value of \"AzureAIAgentState.\" It serves as an identifier for this specific state model.\n  \n- **agent_id (`Optional[str]`)**: This optional field holds the ID for the Azure AI agent. It can be used to uniquely identify the agent within any given session.\n\n- **thread_id (`Optional[str]`)**: Similar to `agent_id`, this optional field stores the ID of the conversation thread. This can be valuable for organizing interactions within multi-threaded communication scenarios.\n\n- **initial_message_ids (`List[str]`)**: This list keeps track of message IDs from the initial interaction. Storing these IDs can be useful for reconstructing the conversation history or references.\n\n- **vector_store_id (`Optional[str]`)**: This optional field presumably contains the ID of a vector store, which could be utilized for file search capabilities related to the agent\u2019s functionalities.\n\n- **uploaded_file_ids (`List[str]`)**: This list maintains the IDs of files uploaded during the agent's session, allowing for easy retrieval and management of those resources.\n\n### Purpose and Role\n\nThe `AzureAIAgentState` class is structured to provide a clear and organized way to encapsulate the essential information that an AI agent needs throughout its operational lifecycle. By using this model, developers can easily save and load an agent's session state, which is essential for enabling a persistent and context-aware interaction experience.\n\n## Type Alias: ListToolType\n\nThe code also defines a type alias named `ListToolType`. This alias is a complex collection that can include various types associated with tools that the Azure AI agent can use:\n\n- **Literal Members**: Strings representing specific tool types such as `\"file_search\"`, `\"code_interpreter\"`, etc. These literals help in type validation whenever tools are referenced.\n\n- **Tool Definitions**: Includes several tool definitions provided by different modules such as `AzureAISearchToolDefinition`, `BingGroundingToolDefinition`, among others. This extension allows the agent to leverage external resources effectively.\n\n- **Callable Types**: The `ListToolType` includes callable types for both synchronous and asynchronous functions. By allowing functions to be used as tools, the design grants the agent flexibility in how tools can be invoked.\n\nThis type alias enhances type safety and code clarity, ensuring that any tools used within the agent are properly categorized and validated.\n\n## Utility Function: has_annotations\n\n### Function Definition\n\nThe `has_annotations` function is defined as follows:\n\n```python\ndef has_annotations(obj: Any) -> TypeGuard[list[MessageTextUrlCitationAnnotation]]:\n    return obj is not None and isinstance(obj, list)\n```\n\n### Purpose\n\nThis function serves as a type guard to check whether a given object (`obj`) contains annotations related to message text and URL citations. \n\n### Parameters\n\n- **obj (`Any`)**: The input parameter that can be of any type.\n\n### Return Type\n\nThe return type is a `TypeGuard` that asserts `obj` is a list of `MessageTextUrlCitationAnnotation` objects if the function returns `True`. \n\n### Function Logic\n\nThe function checks two conditions:\n1. The object should not be `None`.\n2. The object should be an instance of a list.\n\nIf both conditions are satisfied, it returns `True`, indicating that the given object can be treated as a list of annotations.\n\n### Role\n\nThe `has_annotations` function aids in safely handling data structures that are expected to contain annotations. By present a TypeGuard, it helps developers write more robust code that can make informed decisions based on the presence or absence of annotations.\n\n## Conclusion\n\nIn summary, the code effectively sets up a model for managing the state of an Azure AI Agent along with defining utility functions and type aliases that contribute to the functionality and robustness of the agent's implementation. The `AzureAIAgentState` class encapsulates essential state information, while the `has_annotations` function assists in safely checking for specific data types. Together, these elements support the creation of flexible and context-aware AI interactions.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/azure/_types.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided source code snippet is a simple module designed to expose functionality from another module named `_file_surfer`. It specifically imports a class or object called `FileSurfer` and allows it to be accessible when this module is imported elsewhere. This is a common practice in Python for encapsulating functionality and controlling what is exposed in an API.\n\n## Imports\n\n```python\nfrom ._file_surfer import FileSurfer\n```\n\n- This line indicates that `FileSurfer` is being imported from a sibling module (denoted by the dot `.`) named `_file_surfer`.\n- This import statement ensures that the `FileSurfer` class or function is available in the current namespace of this module. \n\n## Expected Role of `FileSurfer`\n\nThe module `_file_surfer` is not provided, so we cannot analyze its contents directly. However, naming conventions suggest that `FileSurfer` likely relates to file operations such as reading, writing, or managing file contents. The exact functionality would depend on how `FileSurfer` is defined within `_file_surfer`.\n\n## Module Exposure\n\n```python\n__all__ = [\"FileSurfer\"]\n```\n\n- The `__all__` variable defines the public API of the module. \n- By declaring `[\"FileSurfer\"]`, the module is instructing any users of its API which symbols should be considered public when using `from module import *`. This is a way to control what gets exported, improving encapsulation and usability of the module.\n\n## Implications for Users\n\n- When other scripts or modules import this module, they will be able to use `FileSurfer` without directly interacting with `_file_surfer`. This abstraction helps in maintaining a clean namespace and reduces dependency on the inner workings of `_file_surfer`.\n- It limits users to the functionality provided by `FileSurfer`, allowing for a more manageable and modular codebase.\n\n## Conclusion\n\nIn summary, this module is a lightweight facade for the `FileSurfer` functionality defined in `_file_surfer`. It promotes better encapsulation practices in Python programming by using the `__all__` declaration and a targeted import statement. \n\nThis kind of modular design is beneficial in larger codebases where clarity and maintainability are paramount. While the actual functionality of `FileSurfer` is unknown without access to `_file_surfer`, the structure of this module serves as a clean gateway to its functionality.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/file_surfer/__init__.py"
  },
  {
    "code": false,
    "content": "# FileSurfer Agent Documentation\n\n## Overview\n\nThe `FileSurfer` class is a specialized chat agent designed for local file browsing and preview services. It can navigate through various file types, read contents, and present a structured view to users. Built on the foundation of a chat-based interaction, it utilizes a model client for generating responses and interacting with user commands related to file management.\n\n## Key Components\n\n1. **Dependencies**: The script imports various modules, including:\n   - `json`, `os`, `traceback` for standard operations.\n   - `List`, `Sequence`, `Tuple` from `typing` for type definitions.\n   - `BaseChatAgent` and other message-related classes from `autogen_agentchat`, which provide the chat functionality.\n   - `Component`, `ComponentModel`, and `FunctionCall` from `autogen_core`, which represent underlying agency components.\n   - `pydantic.BaseModel` for structured configuration and validation.\n   - Additional tools for file operations are imported from a tool definitions module.\n\n2. **Configuration Class**: \n   - `FileSurferConfig`: This Pydantic model defines the configuration attributes for the `FileSurfer` agent, such as the agent's `name`, `model_client`, and an optional `description`.\n\n3. **FileSurfer Class**: \n   - Inherits from `BaseChatAgent` and `Component[FileSurferConfig]`. This class encapsulates the functionalities of the FileSurfer agent and provides methods for responding to user queries about files.\n\n## Constructor and Initialization\n\n### Constructor `__init__`\nThe `__init__` method initializes a new instance of `FileSurfer`. Key parameters include:\n- `name`: The agent's identifying name.\n- `model_client`: An instance of `ChatCompletionClient` that the agent will use for generating replies.\n- `description`: A short description of the agent; it defaults to \"An agent that can handle local files.\"\n- `base_path`: The root directory for the file browsing, set to the current working directory by default.\n\nDuring initialization, it also sets up:\n- A chat history list to keep track of the conversation.\n- A file browser instance (`MarkdownFileBrowser`) responsible for navigating and rendering files.\n\n## Message Handling\n\n### `on_messages` Method\nThis asynchronous method processes incoming messages:\n- It appends each message to the chat history.\n- It attempts to generate a reply using the `_generate_reply` method. If successful, it appends the assistant's message to history and returns the response.\n- If any error occurs, it catches the exception, constructs an error message, and sends that as a response.\n\n### `on_reset` Method\nThis asynchronous method is designed to reset the chat history, clearing any stored messages when called.\n\n## File Browser Interaction\n\n### `_get_browser_state` Method\nThis private method retrieves the current state of the file browser, including:\n- The current path and page title.\n- A brief summary of the viewport's position, such as the current page and total number of pages. It returns this information as a tuple.\n\n### `_generate_reply` Method\nAn integral part of the functionality, this asynchronous method generates a response to user tasks. Key functionality includes:\n- Asserting the last message is from the user and extracting its content.\n- Preparing context messages that specify the current file or directory being viewed.\n- It interacts with the model client to create a response based on the accumulated chat history and the current browser state.\n  \nThe method processes any response that may involve tool function calls (opening a path, navigating pages, or searching) or returns a direct response based on the model\u2019s output.\n\n## Context Compatibility\n\n### `_get_compatible_context` Method\nThis method ensures message compatibility with the underlying model client by checking if the model has visual capabilities. If not, it removes any image-related messages from the context to maintain coherence.\n\n### Configuration Methods\n\n#### `_to_config`\nTransforms the `FileSurfer` instance back into its configuration model, capturing its configuration details for persistence or reinitialization.\n\n#### `_from_config`\nA class method that creates a new instance of `FileSurfer` from a provided configuration object. It sets up the agent with the necessary properties by loading the model client and setting default descriptions if not provided.\n\n## Concluding Remarks\n\nThe `FileSurfer` agent is designed to enhance the user experience when working with local files through chat-based interaction. Its structured architecture allows for easy extensibility while facilitating effectively tailored communication with users concerning their file navigation and previewing needs. The handling of responses, error management, and context maintenance emphasizes robustness and reliability, making it an effective tool within the `autogen` framework.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/file_surfer/_file_surfer.py"
  },
  {
    "code": false,
    "content": "# Markdown File Browser Documentation\n\n## Overview\nThe `MarkdownFileBrowser` class provides a simple interface for browsing files and directories, rendering their contents in Markdown format. This class leverages the `MarkItDown` package for converting text files to Markdown while allowing navigation through file structures. It supports features such as viewing file contents, searching within the content, and paging through large files.\n\n## Class Definition\n### `MarkdownFileBrowser`\nThis is the main class that offers various functionalities to interact with files and directories.\n\n#### Constructor (`__init__`)\nThe constructor initializes a `MarkdownFileBrowser` instance. It takes the following parameters:\n- `viewport_size`: Specifies the number of characters that fit into the viewing area (default: 8192).\n- `base_path`: Sets the root directory for the file browsing operation. Users cannot navigate outside this path (default: current working directory).\n- `cwd`: Defines the initial current working directory (defaults to the base path if not provided).\n\n### Properties\n- **`path`**: Returns the current path, reflecting the last entry in the browsing history.\n- **`viewport`**: Provides the content visible in the current viewport based on pagination.\n- **`page_content`**: Returns the complete content of the currently opened page.\n\n## Methods and Functionality\n### Path Management\n- **`_validate_path(path: str) -> bool`**: Checks if a given path is within the defined base path. Returns `True` if valid, otherwise `False`.\n- **`set_path(path: str) -> None`**: Sets the current working path and attempts to open the specified file or directory. It handles relative paths and updates the browsing history accordingly.\n- **`open_path(path: str) -> str`**: Public method that calls `set_path` to open a specified file or directory, returning the content of the viewport.\n\n### Content and Viewport Management\n- **`_set_page_content(content: str, split_pages: bool = True) -> None`**: Assigns content to the current page and splits it into pages for viewing.\n- **`_split_pages() -> None`**: Divides the page content into smaller segments based on the viewport size to facilitate navigation without cutting words.\n\n### Navigation\n- **`page_down() -> None`**: Scrolls the viewport down to the next page, if available.\n- **`page_up() -> None`**: Scrolls the viewport up to the previous page, if available.\n\n### Searching\n- **`find_on_page(query: str) -> Union[str, None]`**: Searches for the specified `query` within the current viewport, allowing navigation to subsequent matches.\n- **`find_next() -> Union[str, None]`**: Continues searching for the next match of the previously provided `query` within the viewport.\n- **`_find_next_viewport(query: Optional[str], starting_viewport: int) -> Union[int, None]`**: Helper method that searches the current content for additional matches in a loop starting from a specified viewport.\n\n## Error Handling\nThe class includes error handling for various scenarios while opening files:\n- **Unsupported Format**: If a file format cannot be converted to Markdown.\n- **File Not Found**: If the specified file or directory does not exist.\nThese exceptions are handled within the `_open_path` method, updating the page content with appropriate error messages.\n\n## Directory Handling\n- **`_fetch_local_dir(local_path: str) -> str`**: Renders the contents of a directory as an HTML structure, which is later converted to Markdown text. It includes listing of names, sizes, and last modified dates for the files and directories present.\n\n## Conclusion\nThe `MarkdownFileBrowser` class encapsulates a user-friendly way to browse and visualize file contents in Markdown format. It streamlines file management and provides a rich set of features including pagination, searching, and error management, making it a valuable utility for developers and users dealing with documentation and file content in Markdown or text form.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/file_surfer/_markdown_file_browser.py"
  },
  {
    "code": false,
    "content": "# High-Level Documentation of Code\n\nThis code defines a set of tool schemas for interacting with a text-based file browser. Each tool schema specifies a command that can be executed to manipulate the viewport, allowing users to open files, scroll through content, and search for text. The code employs two primary classes, `ToolSchema` and `ParametersSchema`, to structure the properties and parameters associated with each tool.\n\n## ToolSchema Overview\n\nThe `ToolSchema` acts as a blueprint for defining the tools available in the text-based file browser application. Each instance of `ToolSchema` is created with a unique name and description, along with optional parameters that can be supplied when calling the tool. The schema format provides the necessary structure to ensure that tools are defined consistently.\n\n## Defining Tools\n\n### 1. Opening a File or Directory\n\n```python\nTOOL_OPEN_PATH = ToolSchema(\n    name=\"open_path\",\n    description=\"Open a local file or directory at a path in the text-based file browser and return current viewport content.\",\n    parameters=ParametersSchema(\n        type=\"object\",\n        properties={\n            \"path\": {\n                \"type\": \"string\",\n                \"description\": \"The relative or absolute path of a local file to visit.\",\n            },\n        },\n        required=[\"path\"],\n    ),\n)\n```\n\nThe `TOOL_OPEN_PATH` schema allows the user to open a specified local file or directory. It requires one parameter, `path`, which can be a relative or absolute string indicating the location of the file. Upon completion, it returns the current content of the viewport, providing immediate feedback on what has been opened.\n\n### 2. Scrolling Up the Viewport\n\n```python\nTOOL_PAGE_UP = ToolSchema(\n    name=\"page_up\",\n    description=\"Scroll the viewport UP one page-length in the current file and return the new viewport content.\",\n)\n```\n\n`TOOL_PAGE_UP` enables the user to scroll upward by one page-length in the currently open file. The operation updates the viewport content to reflect the new position after the scrolling action, making it easier to navigate large files.\n\n### 3. Scrolling Down the Viewport\n\n```python\nTOOL_PAGE_DOWN = ToolSchema(\n    name=\"page_down\",\n    description=\"Scroll the viewport DOWN one page-length in the current file and return the new viewport content.\",\n)\n```\n\nSimilar to `TOOL_PAGE_UP`, the `TOOL_PAGE_DOWN` schema allows scrolling downward by one page-length. This functionality supports seamless navigation through text files, ensuring that users can easily find and read material that may not be immediately visible in the viewport.\n\n### 4. Finding Text on the Page\n\n```python\nTOOL_FIND_ON_PAGE_CTRL_F = ToolSchema(\n    name=\"find_on_page_ctrl_f\",\n    description=\"Scroll the viewport to the first occurrence of the search string. This is equivalent to Ctrl+F.\",\n    parameters=ParametersSchema(\n        type=\"object\",\n        properties={\n            \"search_string\": {\n                \"type\": \"string\",\n                \"description\": \"The string to search for on the page. This search string supports wildcards like '*'\",\n            },\n        },\n        required=[\"search_string\"],\n    ),\n)\n```\n\n`TOOL_FIND_ON_PAGE_CTRL_F` allows users to initiate a search within the currently displayed content. It requires one parameter, `search_string`, which represents the term to be found, allowing the use of wildcards for more flexible searching. This capability mirrors the common Ctrl+F functionality, providing an efficient user experience.\n\n### 5. Finding Next Occurrence\n\n```python\nTOOL_FIND_NEXT = ToolSchema(\n    name=\"find_next\",\n    description=\"Scroll the viewport to next occurrence of the search string.\",\n)\n```\n\nThe `TOOL_FIND_NEXT` schema facilitates searching for the next occurrence of a previously specified search string. This makes it easier to continue navigating through the text after an initial search has been executed, enhancing the user\u2019s ability to locate specific content within larger documents.\n\n## Conclusion\n\nIn summary, the code provides a structured way to create various tools for managing file navigation and search operations within a text-based file browser. Each tool schema contains specific functionality, ranging from opening files to scrolling and searching, thereby creating an efficient and user-friendly interface for handling text documents. The design promotes modularity and clarity, which are essential for maintainable and extensible software development.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/file_surfer/_tool_definitions.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module is primarily concerned with the import of the `MagenticOneCoderAgent` class from a specified submodule. It also handles import errors gracefully by providing a user-friendly message about dependency installation.\n\n## Importing Dependencies\n\nThe code attempts to import the `MagenticOneCoderAgent` class from the internal module `._magentic_one_coder_agent`. This signifies that `MagenticOneCoderAgent` is likely part of a larger package structure, which organizes related functionality in separate files/modules.\n\n```python\ntry:\n    from ._magentic_one_coder_agent import MagenticOneCoderAgent\n```\n\nIf the import is unsuccessful, it raises an `ImportError`. This specific error handling is beneficial in simplifying debugging for the user, allowing them to understand that an essential dependency is missing rather than encountering a less descriptive error.\n\n```python\nexcept ImportError as e:\n    raise ImportError(\n        \"Dependencies for MagenticOneCoderAgent not found. \"\n        'Please install autogen-ext with the \"magentic-one\" extra: '\n        'pip install \"autogen-ext[magentic-one]\"'\n    ) from e\n```\n\nIn this block, the `ImportError` message suggests a solution, specifying the command needed to install the required package with the appropriate extras. This is a common practice in Python to enhance user experience and support.\n\n## Public API Definition\n\nThe line `__all__ = [\"MagenticOneCoderAgent\"]` declares that `MagenticOneCoderAgent` is part of the public API for this module. This means that when someone imports from this module using the `from module import *` syntax, only `MagenticOneCoderAgent` will be imported.\n\n```python\n__all__ = [\"MagenticOneCoderAgent\"]\n```\n\nThis is a useful feature in Python as it allows the module author to control what is accessible to users, thus hiding internal components that should remain private. \n\n## Summary\n\nIn summary, this Python module is focused on importing the `MagenticOneCoderAgent` from an inner module. It includes robust error handling to inform users about missing dependencies and provides an organized public API definition. The structure facilitates easy interaction with the components of the module while ensuring that users receive clear instructions on resolving import issues related to dependencies.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/magentic_one/__init__.py"
  },
  {
    "code": false,
    "content": "# MagenticOne Coder Agent Documentation\n\n## Overview\n\nThe MagenticOne Coder Agent is an AI-powered assistant designed to assist users with coding tasks. It leverages a language model client to generate Python and shell script code snippets in response to user requests. The agent is encapsulated in a class, inheriting from the `AssistantAgent` base class, and includes specific instructions on how to interact with users and execute tasks.\n\n## Purpose\n\nThe primary function of the MagenticOne Coder Agent is to provide coding support by suggesting executable code snippets based on user requests. The agent is programmed to communicate in an efficient and helpful manner, solving tasks step by step when needed. Its capabilities are highlighted in the system message, which sets the tone and responsibilities of the assistant.\n\n## Core Components\n\n### Constants\n\n- **MAGENTIC_ONE_CODER_DESCRIPTION**: A string that describes the functionality and skills of the assistant, emphasizing its language, Python, and Linux command line capabilities.\n  \n- **MAGENTIC_ONE_CODER_SYSTEM_MESSAGE**: A detailed instruction set that outlines how the agent should interact with users. This includes:\n  - Conditions under which the agent should generate code snippets.\n  - Guidelines for user feedback and error handling.\n  - Instructions on verifying and validating the output produced by the code.\n\n### Class Definition\n\n#### MagenticOneCoderAgent\n\nThe `MagenticOneCoderAgent` class is a subclass of `AssistantAgent`. It serves as the main structure for creating an instance of the coding assistant. \n\n**Attributes:**\n- **component_provider_override**: This attribute is set to a specific module path, indicating where to find the necessary implementations for the agent.\n\n**Methods:**\n- **__init__()**: The constructor initializes the agent with a name, a model client (an instance of `ChatCompletionClient`), and any additional keyword arguments. During initialization, it calls the parent class constructor with the necessary parameters:\n  - `name`: A string representing the agent's identifier.\n  - `model_client`: An instance of `ChatCompletionClient` which likely connects to an AI model for generating responses.\n  - `description` and `system_message`: These are set to the predefined constants, thereby sealing the configuration to match the original framework expectations.\n\n## Interaction Guidelines\n\nThe system message includes specific guidelines on how the agent should handle requests:\n1. **Code Generation**: The agent is to create code when it requires information or when it needs to perform a specific task. Code outputs are categorized into Python or shell scripts, designated with respective code blocks to ensure clarity and usability.\n2. **Execution Strategy**: The agent should clearly outline its plan before providing code and should include only complete, executable snippets without requiring user modifications.\n3. **Error Handling**: If an execution returns an error, the agent is tasked with diagnosing the issue and providing a corrected code snippet. Further analysis may be required to adjust its approach if the error persists after corrections.\n4. **Verification**: Once a potential solution is found, the agent must carefully verify the output and, whenever possible, provide verifiable evidence to support its claims to enhance user trust in the responses.\n\n## Conclusion\n\nThe MagenticOne Coder Agent is a specialized AI-driven tool crafted for aiding coding tasks across Python and shell scripting. Through its well-defined interaction model and procedural guidelines, it aims to efficiently assist users in executing their coding requirements with minimal friction and maximum clarity. This agent exemplifies the integration of AI capabilities within coding environments, focusing on supportive and interactive user experiences.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/magentic_one/_magentic_one_coder_agent.py"
  },
  {
    "code": false,
    "content": "# Code Overview\n\nThis code snippet is a module designed to expose specific classes for use in other parts of a Python application. It imports classes from specific internal modules and indicates which of these classes are part of the public API for the module.\n\n## Imports\n\n```python\nfrom ._openai_agent import OpenAIAgent\nfrom ._openai_assistant_agent import OpenAIAssistantAgent\n```\n\nIn this section, two classes are imported from their respective internal modules:\n\n- `OpenAIAgent` is imported from the `._openai_agent` module, which likely contains the core functionalities or abstractions for interacting with the OpenAI API or related functionality.\n- `OpenAIAssistantAgent` is imported from the `._openai_assistant_agent` module, which may extend or provide additional utilities and functionalities for assisting functionalities involving OpenAI technology.\n\n## Public API Declaration\n\n```python\n__all__ = [\n    \"OpenAIAgent\",\n    \"OpenAIAssistantAgent\",\n]\n```\n\nThis section defines the `__all__` list. The purpose of this list is to explicitly declare which names should be considered public when the module is imported using the `from module import *` syntax. Here, only `OpenAIAgent` and `OpenAIAssistantAgent` are provided in the `__all__`, which means these are the classes designated for external use.\n\n### Role of Imported Classes\n\n1. **OpenAIAgent**: \n   - This class likely serves as a foundational agent for interfacing with OpenAI's services, managing requests, responses, and possibly configurations related to the API.\n   - It could encapsulate various operations such as sending queries to the OpenAI system, handling responses, and maintaining session states.\n\n2. **OpenAIAssistantAgent**: \n   - This class is likely a specialized version or extension of `OpenAIAgent`.\n   - Its role may include additional functionalities tailored for conversational or assistive tasks, such as understanding context, managing dialogues, and generating appropriate responses based on specific user interactions.\n\n## Conclusion\n\nThe module essentially serves as a streamlined interface for working with OpenAI agents, making it easier for developers to import and utilize these classes in a consistent manner. By controlling the public API with the `__all__` declaration, it helps maintain a clean and manageable interface while allowing for future expansions and internal changes without affecting external code.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/openai/__init__.py"
  },
  {
    "code": false,
    "content": "# OpenAIAgent Documentation\n\n## Overview\nThe provided code defines an `OpenAIAgent` class, part of a modular framework designed to utilize OpenAI's API for building chat agents. This agent is capable of performing multi-turn conversations and incorporates various built-in tools to enhance its functionalities, such as search capabilities and image generation.\n\nThe implementation leverages asynchronous programming through the `asyncio` module, which allows it to handle multiple tasks concurrently, particularly useful when interacting with web APIs. \n\n## Tool Configuration Models\nSeveral `TypedDict` classes define the configuration structures for built-in tools that the agent can use:\n\n- **FileSearchToolConfig**: Configurations for the `file_search` tool, which searches across specified vector stores for relevant files.\n  \n- **WebSearchToolConfig**: Configurations for the `web_search_preview` tool that defines options like location and context size for web searches.\n\n- **ComputerUseToolConfig**: It handles display dimensions and environment specifications for simulated computer interfaces.\n\n- **MCPToolConfig**, **CodeInterpreterToolConfig**, **ImageGenerationToolConfig**, and **LocalShellToolConfig**: Each define parameters for their respective tools utilized by the agent.\n\nThese configurations ensure that tools are used with the correct parameters, helping prevent errors and inconsistencies during usage.\n\n## Message Classes\nThe code also introduces various message types employed in interactions with the agent:\n\n- **ImageMessage**: Inherits from `BaseChatMessage` and specifies how image content is structured and converted into different formats, such as strings for display or models for API consumption.\n\n- **OpenAIMessage and related types**: Define how messages will be formatted when sent to the OpenAI API, encapsulating different content types, including text and images.\n\n## The OpenAIAgent Class\nThe core functionality resides in the `OpenAIAgent` class, which inherits from `BaseChatAgent` and implements the `Component` interface. Below are key features:\n\n- **Initialization**: The constructor accepts parameters like `name`, `description`, `client`, `model`, `instructions`, and `tools`, while also allowing configurations for temperature, output token limits, and message storage.\n\n- **Tool Management**: The agent can manage built-in tools, enabling the addition or configuration of both parameterless and parameter-required tools. Validation ensures tools are configured correctly based on whether they need specific parameters.\n\n- **Message Handling and API Interaction**: \n    - The agent formats messages to meet the API's requirements, maintaining a history of interactions and correctly handling responses.\n    - Asynchronous functions like `on_messages` and `on_messages_stream` enable the agent to process incoming messages, retrieve responses from the API, and yield results back to the client.\n\n## State Management\nState persistence is handled through methods like `save_state` and `load_state`, which enable the agent to save its current interaction history and restoration of that state later. An internal model tracks the response ID and message history, facilitating continuity in conversations.\n\n## API Configuration Methods\nThe agent provides methods for converting its configuration to and from a declarative format. The `_to_config` method serializes the internal state, while `_from_config` reconstructs the agent from a previously saved configuration. These methods ease the process of agent state management, facilitating creation and restoration from configurations.\n\n## Error Handling\nWhen interacting with OpenAI\u2019s API, error handling mechanisms are integrated to log and return useful messages if API calls fail. The method `on_messages_stream` yields responses based on the successes or failures of the API calls, ensuring the agent can gracefully handle failures and communicate issues back to users.\n\n## Examples of Usage\nThe documentation includes examples that illustrate how to instantiate the `OpenAIAgent`, both with basic tool configuration and more complex scenarios. This helps users understand how to effectively utilize the agent\u2019s functionalities and how to set up different environments for their needs.\n\n### Basic Usage\n```python\nasync def example():\n    client = AsyncOpenAI()\n    agent = OpenAIAgent(\n        name=\"SimpleAgent\",\n        description=\"A simple OpenAI agent using the Responses API\",\n        client=client,\n        model=\"gpt-4.1\",\n        instructions=\"You are a helpful assistant.\",\n        tools=[\"web_search_preview\"],\n    )\n    await Console(agent.run_stream(task=\"Search for recent AI developments\"))\n```\n\n### Advanced Configuration\n```python\nasync def example_with_configs():\n    client = AsyncOpenAI()\n    tools = [\n        {\n            \"type\": \"code_interpreter\",\n            \"container\": {\"type\": \"auto\"},\n        },\n        {\n            \"type\": \"web_search_preview\",\n            \"user_location\": {\n                \"type\": \"approximate\",\n                \"country\": \"US\",\n                \"region\": \"CA\",\n                \"city\": \"San Francisco\",\n            },\n            \"search_context_size\": \"low\",\n        },\n    ]\n\n    agent = OpenAIAgent(\n        name=\"ConfiguredAgent\",\n        description=\"An agent with configured tools\",\n        client=client,\n        model=\"gpt-4.1\",\n        instructions=\"You are a helpful assistant with specialized tools.\",\n        tools=tools,\n    )\n    await Console(agent.run_stream(task=\"Search for recent AI developments\"))\n```\n\n## Conclusion\nThe `OpenAIAgent` class is a well-structured implementation that allows for flexible configuration and efficient interaction with OpenAI's capabilities. With built-in handling for various tools and comprehensive message management, it serves as a solid foundation for building sophisticated chat applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/openai/_openai_agent.py"
  },
  {
    "code": false,
    "content": "# OpenAIAssistantAgent Documentation\n\n## Overview\n\nThe `OpenAIAssistantAgent` is designed to interface with OpenAI's Assistant API, providing functionalities to generate responses by managing multi-turn conversations, executing code, handling files, and using custom functions. The class inherits from `BaseChatAgent` and maximizes asynchronous programming using the `asyncio` library for efficient handling of multiple tasks.\n\n### Key Functionalities:\n\n- Supports code interpretation and execution.\n- Handles file uploads and searches.\n- Allows integration of custom functions.\n- Maintains the context of conversations.\n- Offers validation for user inputs and processing methodologies.\n  \n## Dependencies\n\nThe agent uses various external libraries:\n\n- `asyncio`: For asynchronous operations.\n- `aiofiles`: For performing file operations asynchronously.\n- `pydantic`: For data validation and settings management.\n- `autogen_core` and `autogen_agentchat`: Custom libraries that contain classes and functions required for the agent's functionality.\n- `openai`: The OpenAI library for API interactions.\n\n## Core Components\n\n### Conversion Functions\n\n- **_convert_tool_to_function_param**: This utility function transforms `Tool` objects from the `autogen` library into a form compatible with OpenAI's Assistant API, specifically into `FunctionToolParam`.\n\n### State Management\n\n- **OpenAIAssistantAgentState**: A data model that stores the required state of an assistant, which includes:\n  - `assistant_id`: ID of the assistant.\n  - `thread_id`: ID for tracking conversation threads.\n  - `initial_message_ids`: IDs of initial messages in the context.\n  - `vector_store_id`: ID for file searches and vectors.\n  - `uploaded_file_ids`: IDs of files uploaded.\n\n## Class Initialization\n\n### OpenAIAssistantAgent\n\nThis class initializes the agent, requiring several parameters:\n\n- `name`: The name of the assistant.\n- `description`: A brief explanation of its capabilities.\n- `client`: An instance of either `AsyncOpenAI` or `AsyncAzureOpenAI`.\n- `model`: The AI model being used (e.g. \"gpt-4\").\n- `instructions`: Parameters for guiding the assistant's behavior.\n- `tools`: Optional tools that the assistant can utilize for functionality extensions.\n\n### Initialization Logic\n\nWithin the constructor:\n- Tools are either converted into compatible parameters or validated.\n- The Assistant and Thread are initialized only when required.\n- State handling ensures that initial data is maintained for future interactions.\n\n## Asynchronous Operations\n\n### Ensuring Initialization\n\n- **_ensure_initialized**: This asynchronous method checks whether the assistant and thread are created. If not, it retrieves or creates them as necessary.\n\n### Messaging Operations\n\n- **on_messages** and **on_messages_stream**:\n  - These methods handle incoming messages from users, processing each in sequence.\n  - They support tool calls for execution by invoking helper methods and handling responses accordingly.\n\n### Tool Executions\n\n- **_execute_tool_call**: Executes a specified tool call, allowing the assistant to perform actions as defined by the associated functions. It takes care of argument unpacking and error handling.\n\n## File Operations\n\n- **_upload_files**: Manages file uploads to the assistant\u2019s context. It ensures files are stored with their IDs for future reference.\n  \n- **on_upload_for_code_interpreter** and **on_upload_for_file_search**: Handle different types of file uploads, one specifically geared towards code interpretation and the other for file search functionalities.\n\n### Memory Management\n\n- **delete_uploaded_files**, **delete_assistant**, and **delete_vector_store** methods clean up resources that were created during the assistant's lifecycle, ensuring that unnecessary storage isn't retained after operations are completed.\n\n## State Management\n\n- **save_state and load_state**: These methods are utilized for saving the current state of the assistant and restoring it later. They allow the assistant to retrieve its context, making it resilient across sessions.\n\n### Error Handling\n\nThroughout the class, various error checks and exceptions are properly managed to ensure that informative messages are returned when something goes wrong, enhancing user experience.\n\n## Examples\n\n### Basic Usage\n\n```python\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom autogen_ext.agents.openai import OpenAIAssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\nasync def example_usage():\n    client = AsyncOpenAI(api_key=\"your-api-key\")\n    assistant = OpenAIAssistantAgent(name=\"PythonHelper\",\n                                      description=\"Helps with Python programming\",\n                                      client=client,\n                                      model=\"gpt-4\",\n                                      instructions=\"You are a helpful Python programming assistant.\",\n                                      tools=[\"code_interpreter\"])\n\n    response = await assistant.on_messages([TextMessage(source=\"user\", content=\"Hello!\")])\n    print(response)\n\nasyncio.run(example_usage())\n```\n\n### Azure OpenAI Integration\n\n```python\nfrom openai import AsyncAzureOpenAI\nimport asyncio\nfrom azure.identity import DefaultAzureCredential\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.openai import OpenAIAssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\nasync def azure_example():\n    credential = DefaultAzureCredential()\n    client = AsyncAzureOpenAI(api_version=\"YOUR_API_VERSION\", azure_endpoint=\"YOUR_AZURE_ENDPOINT\", azure_ad_token_provider=credential)\n    assistant = OpenAIAssistantAgent(name=\"AzurePythonHelper\",\n                                      description=\"Assists with Azure Python programming\",\n                                      client=client,\n                                      model=\"gpt-4\",\n                                      instructions=\"You are a helpful Azure Python programming assistant.\",\n                                      tools=[\"code_interpreter\"])\n\n    response = await assistant.on_messages([TextMessage(source=\"user\", content=\"Hello Azure!\")])\n    print(response)\n\nasyncio.run(azure_example())\n```\n\n## Conclusion\n\nThe `OpenAIAssistantAgent` class serves as a comprehensive interface for leveraging the capabilities of the OpenAI Assistant API. It abstracts the complexities involved in managing messages, tool executions, and state management while allowing developers to build rich, interactive assistants that can handle a wide range of tasks and user requests effectively.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/openai/_openai_assistant_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Video Surfer Module\n\n## Overview\n\nThe given code is a simple module that imports a class or function named `VideoSurfer` from a sibling module called `._video_surfer`. The purpose of this module is to facilitate access to the `VideoSurfer` component for other parts of the application, especially for users who interface with this module.\n\n## Import Statement\n\n```python\nfrom ._video_surfer import VideoSurfer\n```\n\nThis line performs a relative import of the `VideoSurfer` class or function from the `._video_surfer` module. The dot (`.`) indicates that the import is from the current package's namespace, meaning that `_video_surfer` resides in the same directory as the current module.\n\n## Namespace Declaration\n\n```python\n__all__ = [\"VideoSurfer\"]\n```\n\nThe `__all__` list is a convention in Python that defines the public API of the module. By specifying `[\"VideoSurfer\"]`, this explicitly allows importers to access the `VideoSurfer` class or function when they use the `from module import *` syntax. This serves as a way to limit what is exposed to users of the module, encapsulating the internal components and avoiding namespace pollution.\n\n## Purpose of the VideoSurfer Class/Function\n\nAlthough the specifics of the `VideoSurfer` class or function are not included in the provided code, its name suggests that it may be designed for some operations related to video manipulation, processing, or analysis. Users can expect it to provide specific functionality that can involve aspects like video playback, editing, or streaming.\n\n## Usage Context\n\nIn practice, this module can be utilized in other scripts or modules within the same package where video-related functionality is required. It establishes a clear and organized structure for handling video functionalities by allowing users to import and use `VideoSurfer` without needing to know the internal workings of the `_video_surfer` module.\n\n## Encapsulation and Maintainability\n\nMaintaining a separate module for `VideoSurfer` helps in adhering to good design principles. It encourages encapsulation, allowing the internals of video processing to remain modular. Development teams can work on the `_video_surfer` component with minimal impact on other modules, making it easier to implement changes, bug fixes, or optimizations.\n\n## Conclusion\n\nIn summary, this module serves as a basic interface to expose the `VideoSurfer` functionality while providing a layer of abstraction over its implementation. By using the `__all__` list, it adequately scopes the module's public API, ensuring cleaner import practices and improving maintainability of the code.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/video_surfer/__init__.py"
  },
  {
    "code": false,
    "content": "# VideoSurfer Documentation\n\n## Overview\n\n`VideoSurfer` is a specialized agent designed to provide detailed answers about a local video file. It leverages various tools to extract significant information from the video, such as its length, screenshots at specific timestamps, and transcriptions of the audio. This agent is particularly useful for answering queries related to the content within the video, making it a valuable tool for developers and users needing to interact with video data.\n\n## Installation\n\nTo install the `VideoSurfer` agent, you can use the following pip command:\n\n```bash\npip install \"autogen-ext[video-surfer]\"\n```\n\nThis command will install the necessary packages and dependencies required for the agent to function.\n\n## Tools\n\nThe `VideoSurfer` agent utilizes a variety of tools that facilitate its operations. The available tools include:\n\n- `extract_audio`: Extracts audio from the video.\n- `get_video_length`: Determines the total duration of the video.\n- `transcribe_audio_with_timestamps`: Transcribes the audio track of the video while generating timestamps.\n- `get_screenshot_at`: Captures screenshots from designated timestamps within the video.\n- `save_screenshot`: Saves the captured screenshots to disk.\n- `transcribe_video_screenshot`: Transcribes video screenshots for text extraction.\n\nThese tools enable `VideoSurfer` to effectively process the video and answer user queries.\n\n## Class Definition\n\n### `VideoSurfer`\n\nThe `VideoSurfer` class inherits from `AssistantAgent`. Below are the main attributes and methods present in the class:\n\n### Attributes:\n- **name (str)**: The name assigned to the agent instance.\n- **model_client (ChatCompletionClient)**: The model client used to generate responses from the agent.\n- **tools (List[BaseTool | Callable])**: A list of callable tools that the agent can utilize. If not specified during instantiation, it defaults to built-in video processing tools.\n- **description (str)**: A brief description of the agent, defaulting to a predefined message.\n- **system_message (str)**: A message to guide the agent's behavior, set to provide instructions for handling video-related queries.\n\n### Methods:\n#### `__init__`\nThe constructor initializes the `VideoSurfer` agent with the provided parameters, setting up its name, model client, tools, description, and system message. It calls the parent class's constructor to ensure proper initialization.\n\n#### `vs_transribe_video_screenshot`\nThis asynchronous method transcribes a screenshot taken from the video at a specified timestamp.\n\n**Arguments**:\n- `video_path (str)`: The path to the video file.\n- `timestamp (float)`: The time within the video for which to capture a screenshot and transcribe.\n\n**Returns**:\n- **str**: The transcribed text from the video screenshot.\n\nThis method facilitates acquiring textual data from specific moments in the video, aiding in answering related queries.\n\n## Usage Examples\n\n### Basic Usage\n\nAn example of how to instantiate the `VideoSurfer` agent and generate a response to a query about a local video (e.g., `video.mp4`) is shown below:\n\n```python\nimport asyncio\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.video_surfer import VideoSurfer\n\nasync def main() -> None:\n    video_agent = VideoSurfer(\n        name=\"VideoSurfer\",\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)\n\n    stream = agent_team.run_stream(task=\"How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?\")\n    await Console(stream)\n\nasyncio.run(main())\n```\n\nThe above script initializes the agent and sets up a system to handle user tasks.\n\n### Advanced Usage with `UserProxyAgent`\n\nIn a more complex scenario, the `VideoSurfer` can be combined with a `UserProxyAgent` in a `MagenticOneGroupChat` setup:\n\n```python\nimport asyncio\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.agents.video_surfer import VideoSurfer\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n\n    video_agent = VideoSurfer(\n        name=\"VideoSurfer\",\n        model_client=model_client\n    )\n    web_surfer_agent = UserProxyAgent(name=\"User\")\n\n    agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client)\n\n    stream = agent_team.run_stream(task=\"Find a latest video about magentic one on youtube and extract quotes from it that make sense.\")\n    await Console(stream)\n\nasyncio.run(main())\n```\n\nThis implementation showcases how to combine multiple agents to perform tasks that may involve web searching and video content analysis.\n\n## Conclusion\n\nThe `VideoSurfer` agent stands out as a powerful tool for extracting and processing information from local video files. It combines various multimedia handling tools with AI capabilities, enabling detailed and context-aware responses to user queries about video content. This documentation serves as a concise guide for developers looking to integrate and utilize `VideoSurfer` in their applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/video_surfer/_video_surfer.py"
  },
  {
    "code": false,
    "content": "# Video Processing and Transcription Module Documentation\n\nThis module provides various utility functions for handling video files, including operations for audio extraction, transcription, screenshot capturing, and more. It leverages libraries such as OpenCV, FFMPEG, and Whisper for multimedia processing.\n\n## Overview of Functions\n\n### `extract_audio(video_path: str, audio_output_path: str) -> str`\nThis function extracts audio from a specified video file and saves it as an MP3 file.\n\n- **Parameters**:\n  - `video_path`: The path to the source video file.\n  - `audio_output_path`: The path where the extracted audio will be saved.\n\n- **Returns**: A confirmation message indicating the path to the saved audio file.\n\n### `transcribe_audio_with_timestamps(audio_path: str) -> str`\nThis function transcribes a given audio file, returning the transcription along with timestamps for each segment.\n\n- **Parameters**:\n  - `audio_path`: The path to the audio file for transcription.\n  \n- **Returns**: A formatted string containing the words spoken in the audio along with their respective start and end times.\n\n### `get_video_length(video_path: str) -> str`\nThis function calculates the length of a video in seconds by utilizing its frame rate and total frame count.\n\n- **Parameters**:\n  - `video_path`: The path to the video file being analyzed.\n  \n- **Returns**: A string indicating the duration of the video in seconds.\n\n### `save_screenshot(video_path: str, timestamp: float, output_path: str) -> None`\nThis function captures a frame (screenshot) from a video at a specific timestamp and saves it to the desired location.\n\n- **Parameters**:\n  - `video_path`: The path to the video file.\n  - `timestamp`: The time in seconds at which to capture the screenshot.\n  - `output_path`: The file path where the screenshot will be saved.\n\n### `transcribe_video_screenshot(video_path: str, timestamp: float, model_client: ChatCompletionClient) -> str`\nThis asynchronous function captures a screenshot from a video at the specified timestamp and sends it to an AI model for description.\n\n- **Parameters**:\n  - `video_path`: The path to the video file.\n  - `timestamp`: The particular time in seconds to capture the screenshot.\n  - `model_client`: An instance of `ChatCompletionClient` utilized for interacting with AI models.\n\n- **Returns**: A textual description of the content in the screenshot generated by the AI model.\n\n### `get_screenshot_at(video_path: str, timestamps: List[float]) -> List[Tuple[float, np.ndarray[Any, Any]]}`\nThis function captures screenshots at multiple specified timestamps and returns them as a list.\n\n- **Parameters**:\n  - `video_path`: The path to the video file.\n  - `timestamps`: A list of timestamps (in seconds) at which to capture screenshots.\n\n- **Returns**: A list of tuples where each tuple consists of a timestamp and the corresponding frame as a NumPy array.\n\n## Detailed Function Descriptions\n\n### `extract_audio()`\nUtilizes the `ffmpeg` library to extract audio. It processes the input video file and saves the audio track in MP3 format. This is especially useful for applications that require audio content separately from the video.\n\n### `transcribe_audio_with_timestamps()`\nEmploys the Whisper model for audio transcription. The function loads the model, transcribes the entire audio file, and formats the result to include timestamps for each segment of text. This output format is beneficial for aligning spoken content with a visual or audio timeline.\n\n### `get_video_length()`\nDetermines the duration of a video file using OpenCV. It calculates the video\u2019s length based on frames per second (FPS) and the total number of frames. This information can be important for editing or displaying video information accurately.\n\n### `save_screenshot()`\nEnables users to obtain visual content from videos by capturing a frame at a specified timestamp. This functionality is crucial for generating still images from moments of interest within the video.\n\n### `transcribe_video_screenshot()`\nThis asynchronous function integrates video processing and AI capabilities. It captures a frame, encodes it in base64 format, and sends the data to the AI model for interpretation. The resultant description can be utilized in applications requiring visual analytics or commentary.\n\n### `get_screenshot_at()`\nSimilar to `save_screenshot()`, but designed for multiple timestamps, this function captures and returns a list of frames. It ensures that users can easily gather visual data from specific times in the video, which can be used for review, analysis, or presentation.\n\n## Summary\nThe module effectively streamlines several critical processes related to video file handling. It encompasses functionalities for audio extraction, transcription, timestamp management, and frame capturing, making it particularly useful for developers working in areas like video content analysis, processing, or multimedia applications. Each function is designed with ease of use in mind while integrating sophisticated libraries to handle the complexities of video and audio data.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/video_surfer/tools.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis Python script serves as an interface for importing specific classes from other modules. It appears to be part of a package designed for web interaction, likely in a multimodal context (i.e., dealing with multiple forms of media or data).\n\n## Imports\n\nThe code begins with two import statements:\n\n```python\nfrom ._multimodal_web_surfer import MultimodalWebSurfer\nfrom .playwright_controller import PlaywrightController\n```\n\n- **MultimodalWebSurfer**: This class is imported from a private module named `_multimodal_web_surfer`. Based on its name, this class likely handles various data formats and media types when interfacing with web content.\n  \n- **PlaywrightController**: This class is imported from the `playwright_controller` module. Given the reference to \"Playwright,\" it may relate to automation of web browsers, suggesting that this class is responsible for controlling browser instances for tasks such as scraping or automated testing.\n\n## Module Export\n\nThe next section indicates which classes are intended to be publicly accessible by specifying the `__all__` variable:\n\n```python\n__all__ = [\"MultimodalWebSurfer\", \"PlaywrightController\"]\n```\n\n- **Purpose of `__all__`**: This variable defines a list of public objects of this module. By using `__all__`, the code restricts what is imported when a user imports the module using the wildcard (`*`). This is a common Python practice to signal which components are the main focus for users of the module.\n\n## Purpose and Role of Each Class\n\n### MultimodalWebSurfer\n\n- **Role**: The `MultimodalWebSurfer` class is likely tasked with browsing and interacting with web content that employs various modes of data representation, such as text, images, audio, and potentially more.\n  \n- **Potential Functionalities**: It might include methods for loading web pages, extracting multimedia content, and processing various types of user interactions (e.g., clicks, form submissions).\n\n### PlaywrightController\n\n- **Role**: The `PlaywrightController` class is presumably focused on leveraging the Playwright API to automate browser actions.\n\n- **Potential Functionalities**: This class would typically have methods for opening and closing browser windows, navigating to different URLs, executing scripts, and handling user inputs in a controlled and automated manner.\n\n## Conclusion\n\nIn summary, this script is part of a larger module intended for web automation and data manipulation. Through the two imported classes\u2014`MultimodalWebSurfer` and `PlaywrightController`\u2014the script integrates functional capabilities for interacting with and processing content from the web. By defining `__all__`, the module ensures that users only have access to these specific components when utilizing the package, promoting better encapsulation and clarity in the API.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for `WebSurferEvent` Class\n\n## Overview\nThe provided code defines a data class named `WebSurferEvent`. This class serves as a structured way to encapsulate information about web surfing events. Data classes in Python are convenient for storing data with minimal boilerplate code and automatically include features like initialization and representation methods.\n\n## Purpose\nThe primary purpose of the `WebSurferEvent` data class is to represent various attributes associated with a web event, which can include user actions, messages, and specific URLs. This organization helps in maintaining clarity and structure in data handling, especially when passing data between different parts of an application.\n\n## Attributes\nThe `WebSurferEvent` class has the following attributes:\n\n- **source** (`str`): A string denoting the source of the event, which could indicate the application or component that generated the event. This helps in tracing where the event originated.\n\n- **message** (`str`): A string that carries a descriptive message about the event. This may provide additional context or information related to the event.\n\n- **url** (`str`): A string that specifies the URL associated with the event. This could represent the web page the user is currently visiting or interacting with.\n\n- **action** (`str | None`): An optional string that can hold details about a specific action taken by the user, such as clicking a link or submitting a form. The use of `None` indicates that this attribute is not mandatory.\n\n- **arguments** (`Dict[str, Any] | None`): An optional dictionary that can store additional parameters or arguments related to the event. The `Any` type allows flexibility in the kind of data that can be included, meaning it could hold various types of information depending on the context.\n\n## Usage Scenario\nThis data class is likely intended for scenarios involving analytics, user tracking, or logging events in applications that require monitoring user interactions on the web. For example, in a web application, you could use this class to gather data when users navigate to different pages, click buttons, or perform other significant actions.\n\nWhen instantiating a `WebSurferEvent`, you can create an object that neatly packages all relevant information in a single entity. This makes it easier to manage the state and context of user actions throughout the application's lifecycle.\n\n## Advantages\nUsing the `WebSurferEvent` data class offers several advantages:\n\n1. **Clarity**: The structured format makes it clear what attributes are important for capturing the event data.\n\n2. **Reduced Boilerplate**: Data classes automatically implement methods such as `__init__`, `__repr__`, and `__eq__`, reducing the amount of code necessary for standard object-oriented programming tasks.\n\n3. **Type Safety**: By specifying types for attributes, the code becomes more robust and self-documenting, allowing developers to understand expected inputs at a glance.\n\n4. **Optional Attributes**: The design allows for flexibility by making certain attributes optional, thus accommodating a variety of event types without enforcing strict requirements.\n\n## Example Usage\nTo illustrate how the `WebSurferEvent` class might be used, here\u2019s a simple example:\n\n```python\nevent = WebSurferEvent(\n    source=\"homepage\",\n    message=\"User clicked on banner\",\n    url=\"https://example.com\",\n    action=\"click\",\n    arguments={\"banner_id\": 1234, \"utm_source\": \"newsletter\"}\n)\n```\n\nIn this example, a new event is created that captures a user click on a banner from the homepage of a given website. The arguments dictionary includes specific identifiers that could be useful for analytics tracking.\n\n## Conclusion\nThe `WebSurferEvent` data class encapsulates key information about web events, providing a structured and clear way to represent user interactions in web applications. Its design promotes clarity, reduces code complexity, and offers flexibility with optional attributes \u2014 making it a useful tool for developers working in areas related to web analytics and user interaction tracking.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_events.py"
  },
  {
    "code": false,
    "content": "# Multimodal Web Surfer Documentation\n\n## Overview\nThe `MultimodalWebSurfer` is a sophisticated agent designed to navigate the web by executing a variety of tasks such as searching, page interaction, and content manipulation. Based on the `BaseChatAgent` class, this agent operates asynchronously and employs a headless Chromium browser for its web surfing capabilities. It leverages the Playwright library for browser control and automation and is intended to work with multimodal models that can perform function/tool calling, thus enhancing its interactions.\n\n## Installation Requirements\nTo utilize the `MultimodalWebSurfer`, ensure to install the necessary packages with the following command:\n```bash\npip install \"autogen-ext[web-surfer]\"\n```\n\n## Configuration Class\n### `MultimodalWebSurferConfig`\nThis class manages the configuration options for the `MultimodalWebSurfer`. It encapsulates agent settings such as:\n- `name`: The name of the agent.\n- `model_client`: An instance of the multimodal model client.\n- `headless`: Specifies if the browser operates in headless mode.\n- Various optional parameters such as `start_page`, `downloads_folder`, and `debug_dir`.\n\nThese configurations enhance the agent's performance and adaptability to different operational scenarios.\n\n## Class: `MultimodalWebSurfer`\n### Purpose\nThis class defines the core functionality of the multimodal web surfer, allowing the agent to interact with web pages effectively. It provides essential methods for initializing, browsing, and performing actions on web content.\n\n### Key Attributes\n- **Attributes**: The agent has attributes for managing browser context, chat history, and logging, ensuring that it can track ongoing interactions and maintain a structured state throughout its operations.\n- **Default Tools**: A collection of default interactive tools is defined, which the agent can use, including actions like visiting URLs, searching the web, clicking elements, and scrolling.\n\n### Initialization\nUpon instantiation, the `__init__` method ensures required parameters are passed while initializing components such as the Playwright controller, setting up logging, and maintaining download management capabilities. \n\n### Browser Lifecycle\nThe `MultimodalWebSurfer` controls the lifecycle of the web browser:\n1. **Initialization**: The browser is initialized on the first call to the `on_messages` or `on_messages_stream` methods.\n2. **Navigation**: Methods for navigating back, visiting new URLs, and executing various browsing commands ensure that the agent can interact meaningfully with web content.\n3. **Closing**: When the agent's services are no longer needed, the `close` method is called to cleanly shut down the browser and context.\n\n## Message Handling\n### On Messages\n- **`on_messages` Method**: Processes messages received from the chat interface, updating chat history and generating replies. It internally calls the `_generate_reply` method.\n- **Streaming Responses**: Through `on_messages_stream`, the method can yield responses asynchronously, allowing for a more interactive experience.\n\n### Reply Generation\nThe `_generate_reply` method plays a crucial role in crafting responses based on user inputs. Key steps include:\n1. Capturing the current state of the web page and its interactive elements.\n2. Preparing prompts for the model client based on the user's request and current web state.\n3. Handling responses, whether they are direct answers or instructions for tool execution.\n\n## Tool Execution\n### `_execute_tool` Method\nThis method dictates how the agent will handle tool commands received from the model. It can initiate a variety of web interactions such as:\n- Navigating to URLs.\n- Scrolling pages.\n- Clicking on web elements.\n- Answering questions based on the webpage content.\n\nEach action is logged for accountability and troubleshooting purposes.\n\n## Page Interaction and Summarization\n### Page Summarization\nThe `_summarize_page` method enables the agent to summarize the contents of a webpage based on user queries. It extracts the textual content of the page, utilizes the model client for summarization, and appropriately formats the response, including any visible text content.\n\n### Doc Generation\nAdditional methods such as `_get_state_description` and `_format_target_list` assist in describing the current state of the web page, identifying interactive elements, and formatting information for the user.\n\n## Conclusion\nThe `MultimodalWebSurfer` agent combines web automation with natural language processing capabilities, allowing it to perform complex web interactions in an intelligent and efficient manner. This system is designed for versatile usage scenarios, whether for educational purposes, data scraping, or assisting users in navigating digital content. Care should be taken as it interacts with potentially untrusted web content, highlighting the importance of monitoring and oversight during operation.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py"
  },
  {
    "code": false,
    "content": "# Web Surfer Tool Documentation\n\nThe following documentation provides a high-level overview and description of a codebase designed for a web-surfing tool. This tool includes components for guiding interactions with web page elements and summarizing content.\n\n## High-Level Overview\n\nThe primary function of the code is to assist a user in navigating and interacting with web pages. It generates structured prompts for an interactive assistant that helps the user choose appropriate actions based on the current context of the webpage. It also provides the ability to summarize webpage content to answer user questions effectively. \n\n### Key Components\n\n1. **Prompt Templates**: \n    - *`WEB_SURFER_TOOL_PROMPT_MM`*: A multi-mode prompt that describes the state of a web page and provides a framework for the assistant to generate responses. It includes explicit details about visible interactive elements, their descriptions, and potential response methods.\n    - *`WEB_SURFER_TOOL_PROMPT_TEXT`*: A prompt designed for textual interactivity, similar to `WEB_SURFER_TOOL_PROMPT_MM`, but formatted for text-based interactions, emphasizing the assistant's approach to respond based on user requests.\n   \n   Both templates utilize placeholders (`{state_description}`, `{visible_targets}`, `{tool_names}`, etc.) to dynamically generate response content based on the specific context of the webpage being navigated.\n\n2. **System Message**:\n    - *`WEB_SURFER_QA_SYSTEM_MESSAGE`*: This message defines the role of the assistant as helpful and informative, with a specific functionality centered around summarizing lengthy documents to provide concise answers to user inquiries.\n\n3. **Function Definitions**:\n    - *`WEB_SURFER_QA_PROMPT(title: str, question: str | None = None) -> str`*: This is a function designed to create a prompt for summarizing webpage content. \n      \n      - **Parameters**:\n        - `title`: Represents the title of the webpage being visited.\n        - `question`: An optional query that, if provided, directs the summarization to focus more sharply on the user's inquiry.\n\n      - **Functionality**: \n        - The function constructs a base prompt that incorporates the title of the webpage. \n        - If a question is supplied, the prompt specifically asks for a summary in relation to that question; otherwise, it asks for a general summary of the webpage content.\n\n## Interaction Logic\n\n### Tool Selection\n\nThe interactive tool prompts encourage the user to select an appropriate action based on the options available. It presents a logic tree suggesting that actions are contingent upon:\n\n- **Current Viewport**: If the required action involves elements visible in the current view, such as clicking or inputting text.\n- **Current Webpage Context**: If information is needed from other parts of the webpage, like scrolling to find additional content or providing a summary.\n- **Another Website**: If the answer requires information from an entirely different site, prompting the assistant to perform a web search.\n\nThese conditions help streamline user interactions and ensure that actions taken are relevant and efficient.\n\n### Summarization Capabilities\n\nThe summarization tool emanates from the tasks set forth in the `WEB_SURFER_QA_SYSTEM_MESSAGE`, where the assistant is characterized as a capable summarizer. The design emphasizes the assistant's ability to distill large amounts of information into digestible content, which is critical for user queries that depend on extracting key insights from lengthy documents or web pages.\n\n## Conclusion\n\nThis codebase presents a structured approach to interactively assist users in navigating and utilizing web content effectively. From generating contextual prompts to guiding user actions based on visibility and relevance, the tool is well-equipped to handle various user inquiries and enhance the web surfing experience. The function for summarization adds a valuable layer, enabling users to gain quick insights from comprehensive data sources, thus providing a holistic tool for web engagement.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_prompts.py"
  },
  {
    "code": false,
    "content": "# Image Annotation Module Documentation\n\nThis module is designed to annotate images with markers for specified Regions of Interest (ROIs). It utilizes the Python Imaging Library (PIL) to manipulate images, overlay annotations, and manage visual representations of ROIs.\n\n## Constants\n\n### `TOP_NO_LABEL_ZONE`\n\n```python\nTOP_NO_LABEL_ZONE = 20  # Don't print any labels close to the top of the page\n```\n\n- This constant defines a zone (in pixels) near the top of the image where labels should not be placed to ensure clarity and avoid overlap with the image edge.\n\n## Main Function: `add_set_of_mark`\n\n```python\ndef add_set_of_mark(\n    screenshot: bytes | Image.Image | io.BufferedIOBase, ROIs: Dict[str, InteractiveRegion]\n) -> Tuple[Image.Image, List[str], List[str], List[str]]:\n```\n\n### Overview\n\nThe `add_set_of_mark` function serves as the entry point for annotating an image with markers. It accepts an image in various formats (bytes, `Image.Image`, or aIO stream) and a dictionary of ROIs.\n\n### Parameters\n\n- **`screenshot`**: This can be a byte array, a PIL Image object, or a binary I/O stream representing the image.\n- **`ROIs`**: A dictionary mapping strings to `InteractiveRegion` objects that detail where to annotate the image.\n\n### Returns\n\nThe function returns a tuple containing:\n\n- An annotated image (PIL Image).\n- A list of visible rectangles (ROIs within the image bounds).\n- A list of rectangles above the viewable area.\n- A list of rectangles below the viewable area.\n\n### Function Flow\n\n1. The function first checks the type of `screenshot`. If it's already an `Image.Image`, it directly calls the internal `_add_set_of_mark` method.\n2. If it's in bytes format, it wraps it in a `BytesIO` stream, allowing compatibility with the `Image.open` method.\n3. It then processes the image for annotations and cleans up by closing the image object before returning the results.\n\n## Internal Function: `_add_set_of_mark`\n\n```python\ndef _add_set_of_mark(\n    screenshot: Image.Image, ROIs: Dict[str, InteractiveRegion]\n) -> Tuple[Image.Image, List[str], List[str], List[str]]:\n```\n\n### Overview\n\nThis function is responsible for performing the actual drawing and annotation of the ROIs on the provided screenshot.\n\n### Parameters\n\n- **`screenshot`**: A PIL image that is to be annotated.\n- **`ROIs`**: A dictionary mapping names to corresponding `InteractiveRegion` objects.\n\n### Returns\n\nSimilar to the main function, it returns a tuple containing:\n\n- The final annotated image.\n- Lists of ROIs categorized as visible, above, or below the image.\n\n### Core Logic\n\n1. The method initializes lists to categorize the ROIs and a default font for annotations.\n2. It converts the image to an RGBA format for compatibility with transparency when drawing over it.\n3. A drawing context (`ImageDraw`) is created for the overlaid annotations.\n4. Each ROI is processed:\n   - It verifies each rectangle for valid dimensions.\n   - Based on the rectangle's midpoints, it categorizes into visible, above, or below.\n   - If visible, it calls `_draw_roi` to perform the actual drawing.\n5. Finally, an alpha-composite image is created, merging the original with the overlay, and cleaned up before returning.\n\n## Internal Function: `_draw_roi`\n\n```python\ndef _draw_roi(\n    draw: ImageDraw.ImageDraw, idx: int, font: ImageFont.FreeTypeFont | ImageFont.ImageFont, rect: DOMRectangle\n) -> None:\n```\n\n### Overview\n\nThis function handles the drawing of individual ROIs on the annotated image. It manages both the rectangle and its label.\n\n### Parameters\n\n- **`draw`**: The drawing context from `ImageDraw`.\n- **`idx`**: The index representing the ROI being drawn, which is used to generate a unique color and label.\n- **`font`**: The font used for the annotations.\n- **`rect`**: A dictionary that defines the rectangle\u2019s dimensions and positioning.\n\n### Core Logic\n\n1. It calculates a unique color for the ROI based on its index using the `_color` function.\n2. It computes the luminance to determine whether to use a dark or light text color for proper contrast.\n3. It draws a rectangle for the ROI and computes the bounding box for the label.\n4. If the label's vertical positioning is too close to the top, it adjusts the label's location to prevent overlap.\n5. Finally, it draws the label with the chosen text color.\n\n## Internal Function: `_color`\n\n```python\ndef _color(identifier: int) -> Tuple[int, int, int, int]:\n```\n\n### Overview\n\nThis utility function generates a color based on a numeric identifier. The generated color is intended for use as a label color for different ROIs.\n\n### Parameters\n\n- **`identifier`**: An integer used to seed the random color generation.\n\n### Returns\n\nThe function returns a tuple of four integers, representing the RGBA color values.\n\n### Logic\n\n1. It creates a randomized color based on the identifier.\n2. The hue values are generated randomly with constraints on certain channels (e.g., green being higher).\n3. A full RGBA color is formed with the alpha channel set to 255 (opaque).\n\n## Conclusion\n\nThis module provides a robust solution for annotating images with specified regions, allowing for categorization and systematic labeling. The use of PIL allows for efficient image manipulation, while the outlined functions work cohesively to achieve the annotation goals.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_set_of_mark.py"
  },
  {
    "code": false,
    "content": "# Overview of Web Interaction Tools\n\nThis code defines a set of web interaction tools intended for managing browser actions programmatically. Each tool is encapsulated within the `ToolSchema` structure and specifies certain actions that can be performed, along with the parameters needed for each action. The tools can be leveraged in scenarios where a system must interact with web pages or browser functionalities in an automateable format.\n\n## Core Functionality\n\n### Loading Tools\nThe core functionality begins with the `_load_tool` function. This function takes a dictionary defining a specific tool configuration and extracts necessary data to create an instance of `ToolSchema`. It is responsible for extracting:\n\n- **Name** of the tool\n- **Description** of the tool\n- **Parameters** required for the tool execution\n\nThe `ParametersSchema` within the `ToolSchema` contains information about the parameter types, along with any necessary properties and requirements. This ensures that each tool has a clear definition regarding how it should interact with the tool parameters.\n\n```python\ndef _load_tool(tooldef: Dict[str, Any]) -> ToolSchema:\n    return ToolSchema(\n        name=tooldef[\"function\"][\"name\"],\n        description=tooldef[\"function\"][\"description\"],\n        parameters=ParametersSchema(\n            type=\"object\",\n            properties=tooldef[\"function\"][\"parameters\"][\"properties\"],\n            required=tooldef[\"function\"][\"parameters\"][\"required\"],\n        ),\n    )\n```\n\n## Tool Definitions\n\nThe script defines a variety of tools that can be used for different interactions with web pages. Each tool is instantiated using the `_load_tool` function with a corresponding configuration that describes its operation.\n\n### Tool: Visit URL\nThe `TOOL_VISIT_URL` allows navigation directly to a specified URL. The description provides clear guidance on when to use this tool effectively, emphasizing the use of fully-qualified URLs.\n\n```python\nTOOL_VISIT_URL: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Web Search\nThe `TOOL_WEB_SEARCH` performs a search on Bing.com based on a user-defined query. The description details its purpose clearly and indicates the reasoning requirement, which helps maintain the tool's designed intent.\n\n```python\nTOOL_WEB_SEARCH: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: History Back\nThe `TOOL_HISTORY_BACK` enables navigation to the previous page in browser history, emulating a typical back button action. The tool's straightforward function assists users in retracing their steps on a web platform.\n\n```python\nTOOL_HISTORY_BACK: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Scroll Up and Down\n`TOOL_SCROLL_UP` and `TOOL_SCROLL_DOWN` provide functionality for scrolling through a page. Each tool describes the action of scrolling either up or down one page and requires reasoning for its invocation. \n\n```python\nTOOL_SCROLL_UP: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n\nTOOL_SCROLL_DOWN: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Click Action\nThe `TOOL_CLICK` tool allows for simulating a mouse click on a target designated by an ID. This utility serves to automate interactions with clickable elements on web pages.\n\n```python\nTOOL_CLICK: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Input Text\n`TOOL_TYPE` permits inputting text into a specified field identified by an ID. This action is critical for automating form submissions or text entries in user interfaces.\n\n```python\nTOOL_TYPE: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Element Scrolling\nThe `TOOL_SCROLL_ELEMENT_DOWN` and `TOOL_SCROLL_ELEMENT_UP` tools focus on scrolling specific HTML elements (like menus or divs) instead of the entire window. These tools enhance the specificity of interactions possible within structured layouts.\n\n```python\nTOOL_SCROLL_ELEMENT_DOWN: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n\nTOOL_SCROLL_ELEMENT_UP: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Hover Action\nThe `TOOL_HOVER` functionality allows the automation of mouse hovering over elements, potentially triggering hover states or tooltips.\n\n```python\nTOOL_HOVER: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Page Interaction with AI\nThe `TOOL_READ_PAGE_AND_ANSWER` and `TOOL_SUMMARIZE_PAGE` tools leverage AI to interact with page content by answering questions or summarizing that content. These integrations expand the possibilities of automation beyond mere interactions to cognitive interpretations of web content.\n\n```python\nTOOL_READ_PAGE_AND_ANSWER: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n\nTOOL_SUMMARIZE_PAGE: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n### Tool: Delaying Actions\nLastly, the `TOOL_SLEEP` tool implements a delay, allowing time for pages to load or tasks to complete. This is vital in ensuring that operations happen in a predictable manner, avoiding issues from race conditions.\n\n```python\nTOOL_SLEEP: ToolSchema = _load_tool(\n    {\n        ...\n    }\n)\n```\n\n## Conclusion\n\nCollectively, these tools present a versatile toolkit for automated web interactions. They allow for seamless integration and control over web actions, reducing manual effort while improving efficiency in web navigation and operations. The structured definitions improved through the `ToolSchema` ensure consistency across all tools, facilitating easier maintenance and usage in broader automation workflows.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_tool_definitions.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level overview of the provided Python code, which defines data structures and functions for handling and transforming geometric and interactive region content typically used in web contexts.\n\n## Imports and Type Definitions\n\nThe code begins by importing necessary modules and types:\n\n```python\nfrom typing import Any, Dict, List, TypedDict, Union\nfrom autogen_core import FunctionCall, Image\nfrom autogen_core.models import FunctionExecutionResult\n```\n\nThese imports allow for strong typing through `TypedDict`, and include essential types such as `FunctionCall` and `Image` from `autogen_core`. The type aliases defined here serve as shorthand for complex types throughout the code:\n\n- `UserContent`: A type that can be either a string or a list of strings and images.\n- `AssistantContent`: A type that holds a string or a list of function calls.\n- `FunctionExecutionContent`: A list that contains the results of function executions.\n- `SystemContent`: A simple string type representing system content.\n\n## TypedDict Structures\n\nThe code defines several `TypedDict` classes, which are used for type-checking dictionaries that have a specific structure.\n\n### DOMRectangle\n\n```python\nclass DOMRectangle(TypedDict):\n    x: Union[int, float]\n    y: Union[int, float]\n    width: Union[int, float]\n    height: Union[int, float]\n    top: Union[int, float]\n    right: Union[int, float]\n    bottom: Union[int, float]\n    left: Union[int, float]\n```\n\nThe `DOMRectangle` class is designed to represent a rectangular area in a Document Object Model (DOM). It captures:\n\n- `x` and `y` for the coordinates of the top-left corner.\n- `width` and `height` for the dimensions of the rectangle.\n- `top`, `right`, `bottom`, and `left` for the rectangle's bounding box offsets.\n\n### VisualViewport\n\n```python\nclass VisualViewport(TypedDict):\n    height: Union[int, float]\n    width: Union[int, float]\n    offsetLeft: Union[int, float]\n    offsetTop: Union[int, float]\n    pageLeft: Union[int, float]\n    pageTop: Union[int, float]\n    scale: Union[int, float]\n    clientWidth: Union[int, float]\n    clientHeight: Union[int, float]\n    scrollWidth: Union[int, float]\n    scrollHeight: Union[int, float]\n```\n\nThe `VisualViewport` class holds information pertinent to a browser's visual viewport, including:\n\n- Dimensions like `height` and `width`.\n- Offsets from the viewport, such as `offsetLeft` and `offsetTop`.\n- The `scale` factor and various dimensions related to scrolling and client view.\n\n### InteractiveRegion\n\n```python\nclass InteractiveRegion(TypedDict):\n    tag_name: str\n    role: str\n    aria_name: str\n    v_scrollable: bool\n    rects: List[DOMRectangle]\n```\n\nThe `InteractiveRegion` class represents an interactive DOM element, encapsulating its:\n\n- `tag_name`, `role`, and `aria_name` for accessibility purposes.\n- A boolean distinguishing whether the region is vertically scrollable.\n- A list of `DOMRectangle` objects that define the shapes and sizes of the element\u2019s interactive areas.\n\n## Helper Functions\n\nThe code contains several helper functions intended for extracting specific types of values from dictionaries.\n\n### _get_str\n\n```python\ndef _get_str(d: Any, k: str) -> str:\n    val = d[k]\n    assert isinstance(val, str)\n    return val\n```\n\nThis function retrieves a string from a dictionary, asserting that the value is indeed a string.\n\n### _get_number\n\n```python\ndef _get_number(d: Any, k: str) -> Union[int, float]:\n    val = d[k]\n    assert isinstance(val, int) or isinstance(val, float)\n    return val\n```\n\n`_get_number` is used to extract numeric values (int or float) from dictionaries. It checks the type for safety.\n\n### _get_bool\n\n```python\ndef _get_bool(d: Any, k: str) -> bool:\n    val = d[k]\n    assert isinstance(val, bool)\n    return val\n```\n\nThis function extracts boolean values, enforcing type consistency.\n\n## Converters to TypedDict\n\nAll the subsequent functions convert raw dictionary forms of their respective types into structured `TypedDict` instances.\n\n### DOM Rectangle Conversion\n\n```python\ndef domrectangle_from_dict(rect: Dict[str, Any]) -> DOMRectangle:\n    return DOMRectangle(\n        x=_get_number(rect, \"x\"),\n        y=_get_number(rect, \"y\"),\n        width=_get_number(rect, \"width\"),\n        height=_get_number(rect, \"height\"),\n        top=_get_number(rect, \"top\"),\n        right=_get_number(rect, \"right\"),\n        bottom=_get_number(rect, \"bottom\"),\n        left=_get_number(rect, \"left\"),\n    )\n```\n\nThe `domrectangle_from_dict` function accepts a dictionary representing a rectangle and returns a structured `DOMRectangle`. It uses the `_get_number` function to ensure accurate type extraction for each key.\n\n### Interactive Region Conversion\n\n```python\ndef interactiveregion_from_dict(region: Dict[str, Any]) -> InteractiveRegion:\n    typed_rects: List[DOMRectangle] = []\n    for rect in region[\"rects\"]:\n        typed_rects.append(domrectangle_from_dict(rect))\n\n    return InteractiveRegion(\n        tag_name=_get_str(region, \"tag_name\"),\n        role=_get_str(region, \"role\"),\n        aria_name=_get_str(region, \"aria-name\"),\n        v_scrollable=_get_bool(region, \"v-scrollable\"),\n        rects=typed_rects,\n    )\n```\n\nThe `interactiveregion_from_dict` function processes a dictionary related to an interactive region, converting each rectangle to a `DOMRectangle` and returning a fully structured `InteractiveRegion`.\n\n### Visual Viewport Conversion\n\n```python\ndef visualviewport_from_dict(viewport: Dict[str, Any]) -> VisualViewport:\n    return VisualViewport(\n        height=_get_number(viewport, \"height\"),\n        width=_get_number(viewport, \"width\"),\n        offsetLeft=_get_number(viewport, \"offsetLeft\"),\n        offsetTop=_get_number(viewport, \"offsetTop\"),\n        pageLeft=_get_number(viewport, \"pageLeft\"),\n        pageTop=_get_number(viewport, \"pageTop\"),\n        scale=_get_number(viewport, \"scale\"),\n        clientWidth=_get_number(viewport, \"clientWidth\"),\n        clientHeight=_get_number(viewport, \"clientHeight\"),\n        scrollWidth=_get_number(viewport, \"scrollWidth\"),\n        scrollHeight=_get_number(viewport, \"scrollHeight\"),\n    )\n```\n\nThe `visualviewport_from_dict` function constructs a `VisualViewport` from a dictionary, ensuring that all necessary values are properly typed as numbers. \n\nThis code facilitates the structured handling of DOM-related geometric data, often essential in user interface programming, especially for web applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_types.py"
  },
  {
    "code": false,
    "content": "# PlaywrightController Documentation\n\n## Overview\nThe `PlaywrightController` class is a helper utility designed to facilitate interactions with web pages using the Playwright library. It enables tasks such as navigating to pages, retrieving page content, and executing user-like actions (e.g., clicking, scrolling, filling forms) through an asynchronous interface. The controller is equipped with functionalities to manage downloads and animate cursor movements if desired.\n\n## Initialization\n### `__init__` Method\nThe constructor of the `PlaywrightController` class initializes various parameters to manage the interactions with a web page.\n\n- **Parameters**:\n    - `downloads_folder`: Specify the folder for saving downloads. If set to `None`, downloads will not be saved.\n    - `animate_actions`: Boolean flag to enable or disable cursor action animations.\n    - `viewport_width` and `viewport_height`: Set dimensions for the browser viewport.\n    - `_download_handler`: A callable function that handles downloaded files.\n    - `to_resize_viewport`: A boolean flag indicating whether to resize the viewport upon page load.\n\nThe constructor also reads a JavaScript file (`page_script.js`) to facilitate further interactions.\n\n## Asynchronous Actions\nThe class provides several asynchronous methods for performing various page interactions. Each method is designed to operate on a `Page` object from the Playwright API.\n\n### **Pause Execution**\n#### `sleep`\nThis method pauses the execution for a specified duration, measured in milliseconds. It utilizes `page.wait_for_timeout()`.\n\n### **Page Metadata and State Retrieval**\n#### `get_interactive_rects`\nThis method retrieves information about interactive regions within the web page. It executes a script to gather this information and converts the results into a specific format.\n\n#### `get_visual_viewport`\nThis method obtains the visual viewport dimensions of the current web page.\n\n#### `get_focused_rect_id`\nIt retrieves the ID of the currently focused web element, returning `None` if no element has focus.\n\n#### `get_page_metadata`\nThis method fetches metadata associated with the page, providing insights into its structure and content.\n\n### **Page Navigation**\n#### `on_new_page`\nThis method registers callbacks for new pages, handling downloads and viewport resizing if necessary.\n\n#### `back`\nEnables navigation to the previously visited page using the `page.go_back()` method.\n\n#### `visit_page`\nThis method navigates to a specified URL. It handles cases where a URL might trigger a download instead of a regular page load.\n\n## User Interactions\nThese methods simulate user-like interactions on the web page.\n\n### **Scrolling and Cursor Animations**\n#### `page_down` and `page_up`\nThese methods scroll the web page down or up, respectively, by one viewport height minus a specified pixel offset.\n\n#### `gradual_cursor_animation`\nAnimates the cursor's movement from a start coordinate to an end coordinate, making it visually appear as if the cursor is moving fluidly across the screen.\n\n#### `add_cursor_box` and `remove_cursor_box`\nThese methods manage the addition and removal of a visual cursor indicator around specified elements to enhance user experience during interactions.\n\n### **Element Interaction**\n#### `click_id`\nSimulates a click on an element identified by a unique identifier. It handles potential pop-ups that might result from the click action.\n\n#### `hover_id`\nSimulates hovering over a specified element, highlighting it if `animate_actions` is enabled.\n\n#### `fill_id`\nFills an input element with a specified value, optionally sending an Enter key press after filling. The method can simulate typing speed based on the length of the input.\n\n#### `scroll_id`\nScrolls a specified element up or down based on the provided direction.\n\n## Text Retrieval\nThese methods pull text content from the web page.\n\n### **get_webpage_text**\nThis method retrieves a chunk of text from the web page, limited to a specified number of lines.\n\n### **get_visible_text**\nThis function retrieves the text visible in the browser viewport, returning an approximate representation.\n\n### **get_page_markdown**\nA method intended to retrieve the markdown content of the page (currently not implemented). It integrates the external library `markitdown` for potential conversion tasks.\n\n## Conclusion\nThe `PlaywrightController` class serves as a comprehensive controller for managing web page interactions using Playwright. With methods to navigate, interact with, and retrieve information from web pages, it can be integrated into broader automation or testing workflows while providing options for animated feedback during user simulations.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/playwright_controller.py"
  },
  {
    "code": false,
    "content": "# AzureTokenProvider Module Documentation\n\n## Overview\n\nThis module defines an `AzureTokenProvider`, which is designed to provide access tokens for Azure services using the `DefaultAzureCredential` class. It leverages Pydantic for data validation and Azure's identity library for authentication. The `AzureTokenProvider` integrates with a component-based architecture described by the `Component` and `ComponentBase` classes from `autogen_core`. \n\n## Imports\n\nThe following modules and classes are imported:\n\n- **List**: Type hinting for lists from the `typing` module.\n- **Component and ComponentBase**: Base classes for components from the `autogen_core` package.\n- **BaseModel**: Pydantic class for creating models with validation.\n- **TokenProvider**: Abstract representation of a token provider.\n- **DefaultAzureCredential and get_bearer_token_provider**: Azure libraries for authentication and obtaining bearer tokens.\n\n## Class: TokenProviderConfig\n\n### Description\n\n`TokenProviderConfig` is a data model that defines the configuration schema for the token provider. This class uses Pydantic's `BaseModel` to ensure that configuration properties are validated according to their types.\n\n### Attributes\n\n- **provider_kind**: A string indicating the type of token provider (e.g., \"DefaultAzureCredential\").\n- **scopes**: A list of strings representing the permissions required for the token.\n\n## Class: AzureTokenProvider\n\n### Description\n\n`AzureTokenProvider` extends both `ComponentBase` and `Component`, providing a standard way to generate Azure tokens based on the configured settings. \n\n### Attributes\n\n- **component_type**: A static string attribute representing the type of component this class defines (\"token_provider\").\n- **component_config_schema**: Attribute specifying that `TokenProviderConfig` is the schema for the component's configuration.\n- **component_provider_override**: String indicating where the provider can be overridden within the system.\n\n### Initialization\n\nThe `__init__` method initializes an instance of `AzureTokenProvider`. It accepts an instance of `TokenProvider` and one or more scopes as positional arguments.\n\n```python\ndef __init__(self, credential: TokenProvider, *scopes: str):\n```\n\n- **credential**: An instance of a token provider, expected to be of type `DefaultAzureCredential`.\n- **scopes**: A variable number of scope strings that the token will cover.\n\nThe method internally calls `get_bearer_token_provider` to create a token provider that can be used to retrieve tokens when called. \n\n### Call Method\n\n```python\ndef __call__(self) -> str:\n```\n\nThis special method allows instances of `AzureTokenProvider` to be called like regular functions. When executed, it returns an Azure access token by invoking the internal bearer token provider.\n\n### Configuration Methods\n\n#### _to_config\n\n```python\ndef _to_config(self) -> TokenProviderConfig:\n```\n\nThis method generates a `TokenProviderConfig` instance representing the current configuration of the token provider. It checks if the credential is an instance of `DefaultAzureCredential` and returns the corresponding configuration. If not, it raises a `ValueError`.\n\n#### _from_config\n\n```python\n@classmethod\ndef _from_config(cls, config: TokenProviderConfig) -> Self:\n```\n\nThis class method creates a new instance of `AzureTokenProvider` from a `TokenProviderConfig` object. It verifies that the `provider_kind` is \"DefaultAzureCredential\" before instantiating the token provider. Otherwise, it raises a `ValueError`.\n\n## Example Usage\n\nTo use the `AzureTokenProvider`, a developer would typically:\n\n1. Instantiate a `DefaultAzureCredential` to authenticate with Azure.\n2. Create an instance of `AzureTokenProvider`, passing the credential and required scopes.\n3. Invoke the instance to obtain a bearer token.\n\nThis provides a seamless way to manage authentication when accessing Azure resources.\n\n## Error Handling\n\nError handling in this module primarily involves checking the type of credential used for token generation. If an unsupported type is detected, a `ValueError` is raised, alerting the developer to the misconfiguration.\n\n## Conclusion\n\nThe `AzureTokenProvider` class encapsulates the functionality required to interact with Azure's identity management, offering a simple yet powerful interface for acquiring access tokens. It incorporates best practices for configuration management through Pydantic, ensuring that user inputs are validated and consistent.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/auth/azure/__init__.py"
  },
  {
    "code": false,
    "content": "It seems that you've not provided any source code for analysis. Please share the code you'd like me to review, and I'd be happy to help summarize and describe it as per your requirements!",
    "filename": "python/packages/autogen-ext/src/autogen_ext/cache_store/__init__.py"
  },
  {
    "code": false,
    "content": "# DiskCacheStore Documentation\n\nThis documentation provides a high-level overview and detailed breakdown of the `DiskCacheStore` class and its configuration within a caching framework using the `diskcache` library.\n\n## Overview\n\nThe `DiskCacheStore` class is a specialized implementation of the `CacheStore` that utilizes `diskcache` as its underlying storage system. It allows objects to be stored and retrieved from disk, thus providing persistent caching capabilities. The implementation is generic, supporting any type `T` for the cache values.\n\n## Dependencies\n\nThe code relies on several libraries and frameworks, including:\n- `diskcache`: A disk and file-based cache library that allows quick retrieval of data.\n- `pydantic`: A data validation and settings management library.\n- `autogen_core`: Presumably a framework providing the base classes `CacheStore` and `Component`.\n\nThese dependencies are instrumental in ensuring the functionality and reliability of the caching solution.\n\n## Class: DiskCacheStoreConfig\n\n### Purpose\n\nThe `DiskCacheStoreConfig` class serves as a configuration model for the `DiskCacheStore`. This class is defined using Pydantic, which offers data validation and makes it easy to manage configurations.\n\n### Attributes\n- `directory`: A string that specifies the path where the cache data will be stored. This is critical for initializing the `diskcache` instance.\n\nThis class can be extended in the future to include additional parameters for customizing the cache, such as size limits.\n\n## Class: DiskCacheStore\n\n### Purpose\n\nThe `DiskCacheStore` class is a concrete implementation of a cache that saves data on disk. It inherits from `CacheStore[T]` and `Component[DiskCacheStoreConfig]`, utilizing generics to allow for various data types.\n\n### Initialization\n\n- **Constructor**: The `__init__` method takes a `cache_instance` of type `diskcache.Cache`, which is the actual object used for caching. Users of the class must manage the lifecycle of this cache instance, ensuring it is properly created and destroyed.\n\n### Methods\n\n- **get(key: str, default: Optional[T] = None) -> Optional[T]**: \n    - This method retrieves a value from the cache using the provided `key`.\n    - If the key is not found, it returns a user-defined `default`, which defaults to `None`.\n    - The returned value is cast to an optional type to match the generic type signature.\n\n- **set(key: str, value: T) -> None**: \n    - This method sets a value in the cache with the associated `key`.\n    - The value is cast to `Any` prior to storing, allowing flexibility in data types.\n\n- **_to_config() -> DiskCacheStoreConfig**:\n    - This internal method creates a `DiskCacheStoreConfig` instance, extracting the directory from the current `diskcache` instance.\n    - It provides a convenient way to serialize or describe the current state of the cache store configuration.\n\n- **_from_config(cls, config: DiskCacheStoreConfig) -> Self**:\n    - This class method initializes a new `DiskCacheStore` instance using the provided `DiskCacheStoreConfig`.\n    - It constructs a new `diskcache.Cache` instance based on the configuration supplied, allowing for easy recalibration of cache settings.\n\n## Usage Example\n\nTo use the `DiskCacheStore`, a user would typically follow these steps:\n\n1. **Configuration**: Define a `DiskCacheStoreConfig` object with the desired directory for cache storage.\n   \n2. **Initialization**: Create a `diskcache.Cache` instance with the defined directory, then pass this instance to `DiskCacheStore`.\n\n3. **Storing and Retrieving Data**: Use the `set` method to store values in the cache and the `get` method to retrieve them.\n\nHere is a code snippet representing these steps:\n\n```python\nconfig = DiskCacheStoreConfig(directory='/path/to/cache')\ncache_instance = diskcache.Cache(config.directory)\ndisk_cache_store = DiskCacheStore(cache_instance)\n\ndisk_cache_store.set('my_key', 'my_value')\nretrieved_value = disk_cache_store.get('my_key')\n```\n\n## Conclusion\n\nThe `DiskCacheStore` class provides a robust mechanism for caching data on disk. By leveraging the `diskcache` library and Pydantic for configuration, it ensures both performance and reliability. The class's method design facilitates easy data management while allowing for future customizations through the configuration model.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/cache_store/diskcache.py"
  },
  {
    "code": false,
    "content": "# Redis Store Documentation\n\nThis document provides a high-level overview of the `RedisStore` class, which is designed to facilitate caching using Redis as a backend. It explains the configuration parameters, methods, and functionalities of the class in a modular fashion.\n\n## Overview\n\nThe `RedisStore` class is a generic cache store that employs Redis to store and retrieve data. It utilizes Pydantic models for automatic serialization and deserialization of complex objects, while also handling primitive data types natively. This class extends `CacheStore` and `Component`, making it part of a larger ecosystem that supports caching solutions.\n\n## Configuration\n\n### RedisStoreConfig\n\nThe class `RedisStoreConfig` is a Pydantic model that encapsulates the configuration required to connect to a Redis database. It includes attributes such as:\n\n- **host**: The hostname of the Redis server, defaulting to `\"localhost\"`.\n- **port**: The port number for the Redis server, defaulting to `6379`.\n- **db**: The database index to use, defaulting to `0`.\n- **username**: An optional username for authentication.\n- **password**: An optional password for authentication.\n- **ssl**: A boolean to specify whether to use SSL for the connection.\n- **socket_timeout**: An optional timeout parameter for socket operations.\n\n### Usage\n\nInstances of `RedisStore` are initialized with a Redis client object. The user is responsible for managing the lifespan of this Redis instance, ensuring that it remains active during its use in the application.\n\n## Methods\n\n### Initialization\n\n```python\ndef __init__(self, redis_instance: redis.Redis):\n```\n\nThe constructor takes a `redis.Redis` instance as an argument and assigns it to the `cache` attribute for later use. This sets up the connection to the Redis database.\n\n### `get` Method\n\n```python\ndef get(self, key: str, default: Optional[T] = None) -> Optional[T]:\n```\n\nThis method retrieves a value from the Redis cache based on the specified key. The method handles both primitive data types and more complex objects:\n\n- If the value is found and is a valid JSON, it is parsed and returned.\n- If the value is a string representation of data, it will be decoded.\n- If retrieval fails, the method returns a specified default value or `None`.\n\nPossible exceptions during retrieval (Redis errors or connection issues) are gracefully caught and handled.\n\n### `set` Method\n\n```python\ndef set(self, key: str, value: T) -> None:\n```\n\nThe `set` method stores a value in the Redis cache. It has the capability to handle various data types:\n\n- Pydantic models are serialized into JSON and stored.\n- Lists containing Pydantic models are also serialized.\n- Primitive types are stored directly without modification.\n\nErrors during this process, such as encoding issues or Redis errors, are logged but do not halt execution.\n\n## Configuration Methods\n\n### `_to_config`\n\n```python\ndef _to_config(self) -> RedisStoreConfig:\n```\n\nThis helper method extracts the connection information from the Redis instance and returns an instance of `RedisStoreConfig`. This provides a structured way to access current connection parameters, which can be useful for diagnostics or analytics.\n\n### `_from_config`\n\n```python\n@classmethod\ndef _from_config(cls, config: RedisStoreConfig) -> Self:\n```\n\nThis class method generates a new `RedisStore` instance from a given `RedisStoreConfig`. It uses the configuration values to create a new `redis.Redis` instance that will be used for caching. This approach supports dynamic initialization based on existing configurations, enhancing flexibility.\n\n## Error Handling\n\nThe `RedisStore` class implements robust error handling throughout its methods, focusing on catching specific exceptions such as `RedisError` and `ConnectionError`. This design ensures the application using `RedisStore` remains resilient, even in the face of issues such as network failures or bad input data.\n\n## Example Usage\n\nWhile the documentation doesn't provide explicit examples, users can instantiate `RedisStore` by creating a Redis client, followed by calls to `set` and `get` methods for data storage and retrieval, respectively. This class is designed for seamless integration into applications needing efficient caching solutions leveraging Redis's capabilities.\n\n## Conclusion\n\nThe `RedisStore` class is a robust implementation that abstracts many complexities involved in data caching with Redis. By leveraging Pydantic for data compatibility and including detailed configuration handling, it provides a versatile tool for developers looking to implement caching solutions in their applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/cache_store/redis.py"
  },
  {
    "code": false,
    "content": "# Code Executor Utilities for AutoGen-Ext\n\nThis documentation provides an overview of the code executor utilities for the AutoGen-Ext project, focusing on the creation of code execution environments using Docker and local command line executors. \n\n## Overview\n\nThe module begins by importing necessary libraries, specifically handling the case where Docker support may not be available. It checks if the required Docker libraries can be imported, and if not, sets the relevant variables to None. This allows the rest of the code to function without Docker if it's not present.\n\n## Imports and Dependency Management\n\n```python\nimport warnings\nfrom typing import Optional\n\nfrom autogen_core.code_executor import CodeExecutor\n```\n\nThe module imports the `warnings` module to manage user warnings, and `Optional` from `typing` to specify optional arguments. The `CodeExecutor` is imported from the `autogen_core.code_executor` namespace, serving as the base class for different code execution implementations.\n\n\n### Optional Docker Support\n\nThe module attempts to import Docker components and manage potential import errors:\n\n```python\ntry:\n    import docker as docker_client\n    from docker.errors import DockerException\n\n    from .docker import DockerCommandLineCodeExecutor\n\n    _docker_available = True\nexcept ImportError:\n    docker_client = None  # type: ignore\n    DockerException = Exception  # type: ignore\n    DockerCommandLineCodeExecutor = None  # type: ignore\n    _docker_available = False\n```\n\nWithin a try-except block, it attempts to import the Docker client and related items. If Docker cannot be imported, the module sets available resources to None and marks Docker as unavailable. This allows the rest of the code to continue running without Docker functionality.\n\n## Docker Availability Check\n\n### Function: _is_docker_available\n\n```python\ndef _is_docker_available() -> bool:\n    \"\"\"Check if Docker is available and running.\"\"\"\n    ...\n```\n\nThe `_is_docker_available` function checks the Docker service status. If Docker is available, it pings the Docker server:\n\n- Returns `True` if Docker is available and responding.\n- Catches `DockerException` exceptions, returning `False` if an error occurs.\n- Returns `False` if Docker is not installed or not running.\n\n## Creating Default Code Executors\n\n### Function: create_default_code_executor\n\n```python\ndef create_default_code_executor(work_dir: Optional[str] = None) -> CodeExecutor:\n    \"\"\"Create a default code executor, preferring Docker if available.\"\"\"\n    ...\n```\n\nThis function is responsible for creating a code executor based on the available environment, prioritizing Docker if it's usable. The function can take an optional working directory argument.\n\n### Logic Breakdown\n\n1. **Docker Check**: \n   - The function first checks if Docker is available via the `_is_docker_available` function.\n   - If Docker is available, it attempts to return an instance of `DockerCommandLineCodeExecutor`.\n\n2. **Docker Failure Handling**: \n   - If an exception occurs during Docker initialization, the function falls back to using a local executor.\n\n3. **Warning for Local Execution**:\n   - If Docker is not available, the function issues a warning to users indicating that they should use Docker for better security and isolation.\n   - It suggests visiting Docker's installation guide.\n\n4. **Local Executor Creation**:\n   - Finally, the function creates an instance of `LocalCommandLineCodeExecutor`, using the provided working directory if given.\n\n### Return Value\n\nThe function returns an instance of either `DockerCommandLineCodeExecutor` or `LocalCommandLineCodeExecutor`, depending on the availability of Docker.\n\n## Warning Management\n\nThe code uses the warnings library to inform the user when Docker is not being used:\n\n```python\nwarnings.warn(\n    \"Docker is not available or not running. Using LocalCommandLineCodeExecutor instead...\",\n    UserWarning,\n    stacklevel=2,\n)\n```\n\nThis is crucial for ensuring that users understand the potential security implications of not using Docker for code execution.\n\n## Exposed Interface\n\nThe module's public interface is defined by the `__all__` variable:\n\n```python\n__all__ = [\"create_default_code_executor\"]\n```\n\nThis signifies that only the `create_default_code_executor` function will be publicly accessible, encapsulating the module's functionality effectively.\n\n## Summary\n\nIn summary, this module provides a utility to create code execution environments favoring Docker for improved security and isolation. It handles the assessment of Docker's availability, issues warnings when necessary, and gracefully falls back to local execution without Docker. This design allows for flexibility in environments where Docker may or may not be suitable.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis module provides a variety of utilities for handling Python functions, generating stubs, processing code related to Python package management, inferring programming languages from code snippets, and managing file content. Below is a high-level description of its components and their respective functionality.\n\n## Imports and Dependencies\n\nThe code utilizes several standard Python libraries, including:\n\n- **inspect**: To retrieve source code from callable objects.\n- **re**: For regular expression operations, primarily used for parsing.\n- **shutil**: To check for system commands and manage file operations.\n- **dataclasses**: For creating data classes, which are used to hold structured data.\n- **pathlib**: For handling filesystem paths in a flexible way.\n- **textwrap**: For formatting text, especially docstrings.\n\nIt also imports various types from `typing` for type hinting and function signatures.\n\n## Data Class\n\n### `CommandLineCodeResult`\n```python\n@dataclass\nclass CommandLineCodeResult(CodeResult):\n    \"\"\"A code result class for command line code executor.\"\"\"\n    code_file: Optional[str]\n```\nThis class extends the `CodeResult` class to encapsulate the result of executing code in a command line environment. It holds an optional `code_file` attribute that can store the name of the code file being executed.\n\n## Utility Functions\n\n### `_to_code()`\n```python\ndef _to_code(func: Union[FunctionWithRequirements[T, P], Callable[P, T], FunctionWithRequirementsStr]) -> str:\n```\nThis function converts a callable or a function with requirements into its source code representation. It checks for the type of the input and retrieves the function\u2019s source code using the `inspect` module. If the function has decorators, it removes them before returning the code.\n\n### `_import_to_str()`\n```python\ndef _import_to_str(im: Import) -> str:\n```\nThis utility converts an `Import` object (which can include module imports or aliases) into a string representation suitable for Python import statements. It handles both standard imports and aliases, dynamically generating the appropriate import statement.\n\n### `build_python_functions_file()`\n```python\ndef build_python_functions_file(\n    funcs: Sequence[Union[FunctionWithRequirements[Any, P], Callable[..., Any], FunctionWithRequirementsStr]],\n) -> str:\n```\nThis function creates a Python file's content containing the provided functions. It begins by gathering all global imports from the given functions, converting them into strings via `_import_to_str()`, and then appends the source code of each function. The content is returned as a single string, formatted for use in a Python file.\n\n## Function Stub Generation\n\n### `to_stub()`\n```python\ndef to_stub(func: Union[Callable[..., Any], FunctionWithRequirementsStr]) -> str:\n```\nThe `to_stub` function generates a stub version of a function, represented as a string. It captures the function's signature and optionally includes its docstring. The stub replaces the body of the function with `...` to indicate that the implementation is not provided.\n\n## File Name Extraction and Management\n\n### `get_file_name_from_content()`\n```python\ndef get_file_name_from_content(code: str, workspace_path: Path) -> Optional[str]:\n```\nThis function extracts the intended filename from the content of a code string. It checks if the first line specifies a filename in the format `# filename: <name>`. If the filename is relative, it resolves it against the provided `workspace_path`. If the file is not contained within the workspace, it raises a `ValueError`.\n\n## Code Modification and Analysis\n\n### `silence_pip()`\n```python\ndef silence_pip(code: str, lang: str) -> str:\n```\nThis function modifies `pip install` commands within the provided code by appending the `-qqq` flag to suppress installation output. It uses regular expressions to find relevant lines and adjust them appropriately, enhancing user experience by reducing verbosity.\n\n### `get_required_packages()`\n```python\ndef get_required_packages(code: str, lang: str) -> set[str]:\n```\nThe purpose of `get_required_packages` is to parse and extract package requirements from pip commands in the input code. A set of required packages is collected, and the function returns this set, enabling further processing or installation.\n\n## Language Handling\n\n### `lang_to_cmd()`\n```python\ndef lang_to_cmd(lang: str) -> str:\n```\nThis function maps programming language names to their corresponding command-line counterparts. It handles various names for Python, shell languages, and PowerShell, ensuring correct command execution according to the specified language. If an unsupported language is provided, it raises a `ValueError`.\n\n### `infer_lang()`\n```python\ndef infer_lang(code: str) -> str:\n```\nThe `infer_lang` function attempts to determine the language of a code snippet by examining its contents. It specifically checks for Python syntax. If the code can compile without errors, it is classified as Python; otherwise, the function returns \"unknown\". This could be enhanced for more robust language detection.\n\n## Conclusion\n\nIn summary, this module is a comprehensive toolkit for managing and manipulating Python code, particularly in the context of dynamic generation and execution in command-line environments. It offers features like importing handling, stub generation, file management, code modification for package management, and language inference, making it useful for developers working with automated code generation and execution.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/_common.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for Azure Container Code Executor\n\nThis module involves importing specific classes from a related module designed to facilitate interactions with Azure container services. Below is a high-level description of its purpose and functionality, including the defined components.\n\n## Overview\n\nThe module is composed of a straightforward import statement that brings in two essential components\u2014`ACADynamicSessionsCodeExecutor` and `TokenProvider`\u2014from another module named `._azure_container_code_executor`. Its main purpose is to expose these classes for use in other parts of an application or system that requires functionality related to Azure container management and session executions.\n\n## Classes and Their Purposes\n\n### `TokenProvider`\n\nThe `TokenProvider` class is typically responsible for managing authentication tokens necessary for secure interactions with Azure services. It likely provides mechanisms for acquiring, refreshing, and validating tokens, which are crucial for accessing various Azure resources in a secure manner. This class may also facilitate integration with identity services and authorization processes, ensuring that access to resources adheres to security protocols.\n\n### `ACADynamicSessionsCodeExecutor`\n\nThe `ACADynamicSessionsCodeExecutor` class is designed to handle the execution of code within dynamic Azure container sessions. This enables developers to run code snippets in a cloud-based environment dynamically and manage those sessions effectively. Its core functionalities may include starting, stopping, and maintaining sessions, executing commands, and managing the lifecycle of these executions in response to various triggers or user inputs.\n\n## Module Exports\n\nThe statement `__all__ = [\"TokenProvider\", \"ACADynamicSessionsCodeExecutor\"]` explicitly defines the public API of this module. By listing the two classes in the `__all__` variable, it indicates which components should be accessible when this module is imported elsewhere. This helps to encapsulate the module's namespace and provide a cleaner interface, preventing unintended interactions with internal components.\n\n## Intended Use Cases\n\nThis module is primarily intended for developers who work with Azure Container instances and require utilities to manage code execution and token management seamlessly. Common use cases might include:\n\n- Automating container deployments with dynamic session management.\n- Creating applications that require on-demand computational resources in Azure.\n- Implementing secure authentication flows for accessing Azure services programmatically.\n\n## Integration and Dependencies\n\nTo utilize this module effectively, developers need to ensure they have access to the underlying `_azure_container_code_executor` module, which likely holds the implementations for `TokenProvider` and `ACADynamicSessionsCodeExecutor`. Dependencies may include Azure SDKs and libraries compatible with the specific functionality being leveraged.\n\n## Conclusion\n\nIn summary, this module serves as a conduit for accessing powerful tools that facilitate the management of Azure containers and sessions. The `TokenProvider` and `ACADynamicSessionsCodeExecutor` classes are central to offering robust functionalities that enable secure, dynamic interactions with the Azure cloud infrastructure. By explicitly defining the public interface, the module is prepared for efficient use in larger applications or systems that capitalize on Azure's computational capabilities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/azure/__init__.py"
  },
  {
    "code": false,
    "content": "# ACADynamicSessionsCodeExecutor Documentation\n\n## Overview\n\nThe `ACADynamicSessionsCodeExecutor` class is designed for executing Python code in an Azure Container Apps Dynamic Sessions environment. This experimental executor provides functionality akin to a Jupyter notebook, allowing for incremental execution of code. Each instance operates within a specific session, making it suitable for running user-defined functions and utilizing a defined set of libraries.\n\n### Key Features\n- Executes user-defined Python code blocks.\n- Handles file uploads and downloads.\n- Checks for available Python packages in the execution environment.\n- Maintains a unique session for code execution.\n- Assesses code requirements before execution.\n\n## Class Definition\n\n### TokenProvider Protocol\n\nThe `TokenProvider` is a Protocol that defines the structure for obtaining an access token. This token is necessary for authentication when making requests to the Azure Container Apps services.\n\n#### Methods\n- `get_token`: Retrieves an access token for specified scopes and optional claims and tenant ID.\n\n## ACADynamicSessionsCodeExecutor Class\n\n### Purpose\n\nThe `ACADynamicSessionsCodeExecutor` extends the `CodeExecutor` class and provides a robust implementation for executing Python code in an Azure-based environment.\n\n### Attributes\n\n- `SUPPORTED_LANGUAGES`: Contains the list of languages supported by this executor, currently only \"python\".\n- `FUNCTION_PROMPT_TEMPLATE`: A string template for formatting function prompts to be used in interactions.\n- `pool_management_endpoint`: Endpoint for Azure Container Apps Dynamic Sessions.\n- `credential`: An instance of `TokenProvider` used for token retrieval.\n- `timeout`: Timeout duration for code execution (default is 60 seconds).\n\n### Initialization\nThe constructor takes multiple parameters which define the environment and behavior of the code executor:\n\n```python\ndef __init__(\n    self,\n    pool_management_endpoint: str,\n    credential: TokenProvider,\n    timeout: int = 60,\n    work_dir: Union[Path, str, None] = None,\n    functions: Sequence[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]] = [],\n    functions_module: str = \"functions\",\n    suppress_result_output: bool = False,\n    session_id: Optional[str] = None,\n):\n```\n\n#### Key Parameters\n- `pool_management_endpoint`: Required URL for Azure dynamic sessions.\n- `credential`: A TokenProvider implementing function for token access.\n- `work_dir`: Directory for code execution, defaulting to a temporary directory if not provided.\n- `functions`: List of callable functions available for use within code execution.\n- `session_id`: A unique identifier for the session, generated if not provided.\n\n## Methods\n\n### Functionality Overview\n\n#### Token Management\n- `_ensure_access_token`: Ensures that a valid access token is available for making API calls.\n\n#### Code Formatting\n- `format_functions_for_prompt`: Formats user-defined functions into a string suitable for providing context during execution.\n\n#### Package Management\n- `get_available_packages`: Retrieves installed packages in the execution environment.\n- `_populate_available_packages`: Populates available packages into internal state.\n\n#### Directory Management\n- `_setup_cwd`: Configures the current working directory for file read/write operations.\n\n### Code Execution\n- `execute_code_blocks`: Executes a list of `CodeBlock` instances, validating setup and package availability before execution.\n- `_execute_code_dont_check_setup`: Initiates code execution against the Azure endpoint without validating initial conditions.\n\n### File Management\n- `upload_files`: Uploads specified files to the Azure environment by creating a multipart POST request.\n- `download_files`: Downloads files from the Azure environment back to the local storage.\n\n### Restart and Lifecycle Management\n- `start`: Marks the executor as active.\n- `stop`: Cleans up temporary resources and stops the executor from running.\n- `restart`: Resets internal states, generating a new session ID and clearing cached data.\n\n### Error Handling\nThe executor is designed to raise meaningful exceptions in various scenarios, such as:\n- **Timeouts**: When file operations or code executions exceed the specified duration.\n- **File Not Found**: When attempting to upload or download files that do not exist.\n- **Connection Errors**: For issues networking with the Azure service.\n\n### Example Usage\n\nTo utilize the `ACADynamicSessionsCodeExecutor`, you would instantiate it with the necessary parameters and then call its methods to manage code execution and file handling:\n\n```python\nexecutor = ACADynamicSessionsCodeExecutor(\n    pool_management_endpoint=\"https://your-endpoint.azure.com\",\n    credential=<YourTokenProviderInstance>,\n    functions=[<YourFunctions>],\n)\n\nawait executor.start()\nresult = await executor.execute_code_blocks([CodeBlock(code=\"print('Hello, world!')\")], cancellation_token=<YourCancellationToken>)\nprint(result.output)\nawait executor.stop()\n```\n\n## Conclusion\n\nThe `ACADynamicSessionsCodeExecutor` provides a sophisticated structure for executing Python code in a manageable and dynamic environment. It takes care of token management, package checking, file input/output, and robust error handling, making it a valuable tool for developers leveraging Azure services for code execution tasks.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/azure/_azure_container_code_executor.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis documentation provides an overview of the given source code, detailing its purpose and functionality.\n\n## Overview\n\nThe provided code is part of a module that likely focuses on executing code within a Docker container environment. Docker is widely used for creating, deploying, and running applications in containers, allowing developers to package applications with all their dependencies, ensuring they run seamlessly in any environment.\n\n## Imports and Dependencies\n\n```python\nfrom ._docker_code_executor import DockerCommandLineCodeExecutor\n```\n\n- The code imports a class named `DockerCommandLineCodeExecutor` from a local module (indicated by the leading dot, which signifies a relative import). \n- The underscore in `_docker_code_executor` suggests that this module might be intended for internal use within the package and is not usually exposed to the end-user.\n\n## Exposed Classes and Functions\n\n```python\n__all__ = [\"DockerCommandLineCodeExecutor\"]\n```\n\n- The `__all__` list explicitly defines what is public and accessible when the module is imported using the `from module import *` syntax.\n- In this case, it exposes only the `DockerCommandLineCodeExecutor` class, which means that any functions or classes not listed in `__all__` will not be imported with this syntax, helping to control the public API of the module.\n\n## Class: DockerCommandLineCodeExecutor\n\n### Purpose\n\n- While the definition of the `DockerCommandLineCodeExecutor` class is not included in the provided snippet, we can infer its role based on the name.\n- This class likely provides functionalities to execute commands or scripts from the command line within a Docker container environment, giving users the ability to run code with the isolation and encapsulation that Docker provides.\n\n### Potential Roles and Functions\n\nAlthough specific methods and attributes of the `DockerCommandLineCodeExecutor` are not detailed, we can hypothesize its possible functionalities:\n\n- **Command Execution**: The class might include methods to execute shell commands inside a specified Docker container.\n- **Response Handling**: It could handle the responses from the executed commands, capturing standard output and error, and possibly formatting these for easier consumption.\n- **Container Management**: The class may have functionalities to start and stop Docker containers or establish connections to them before executing commands.\n- **Error Management**: It could implement error-handling mechanisms to catch issues during execution, returning appropriate messages or exceptions based on different failure conditions.\n\n## Conclusion\n\nOverall, this code snippet serves as an entry point for utilizing Docker to execute command-line code in a controlled manner. By exposing the `DockerCommandLineCodeExecutor`, it allows integration of Docker capabilities directly from the hosting application, making it a useful component for developers looking to manage code execution environments efficiently. Future users of this module can leverage the exposed class to harness Docker's power within their applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/docker/__init__.py"
  },
  {
    "code": false,
    "content": "# Docker Command Line Code Executor Documentation\n\n## Overview\nThe `DockerCommandLineCodeExecutor` class provides an asynchronous Python interface for executing code blocks within a Docker container. This class is specifically designed to handle both Python scripts and shell commands, using Docker as the execution environment. It is part of the `autogen` framework, aiming to facilitate automated code execution within isolated environments.\n\n## Dependencies\nBefore using the `DockerCommandLineCodeExecutor`, ensure that the `docker` package is installed with the `autogen-ext` extension. Missing dependencies can lead to runtime errors, and the script raises a `RuntimeError` if the required packages are not available.\n\n## Configuration Class\n### `DockerCommandLineCodeExecutorConfig`\nThis class is a Pydantic model that encapsulates the various configuration parameters for the `DockerCommandLineCodeExecutor`.\n\n- **Parameters**:\n  - `image`: Default is `\"python:3-slim\"`, specifies the Docker image to use.\n  - `container_name`: An optional string for naming the Docker container.\n  - `timeout`: Sets the timeout for code execution. Default is 60 seconds.\n  - `work_dir`: Specifies the working directory within the container.\n  - `bind_dir`: Directory to bind for the executor container.\n  - `auto_remove`: If true, automatically removes the container when stopped.\n  - `stop_container`: If true, the executor stops the container when the context manager exits.\n  - `functions_module`: Module name for user-defined functions.\n  - `extra_volumes` and `extra_hosts`: For additional mounts and host mappings.\n  - `init_command`: Command to run before executing other commands.\n  - `delete_tmp_files`: If true, temporary files are deleted after execution.\n\n## Core Class\n### `DockerCommandLineCodeExecutor`\nInherits from `CodeExecutor` and the `Component` class, and implements the main logic for executing code within a Docker container.\n\n#### Constructor\nThe constructor initializes various parameters from the config, such as image, container name, timeout, and working directories. It also manages deprecation warnings regarding the current directory as the working directory.\n\n#### Properties\n- **`timeout`**: Returns the execution timeout.\n- **`work_dir`**: Resolves to either the user-defined working directory or a temporary directory.\n- **`bind_dir`**: Returns the bind directory, defaulting to the work directory.\n\n## Methods\n### `start()`\nStarts the Docker container according to the defined configuration. It checks for existing containers, pulls the required image, and prepares the environment for running code.\n\n### `stop()`\nStops the running container and cleans up. It manages cancellation futures and ensures resources are freed if temporary directories are used.\n\n### `restart()`\nRestarts the running container. It requires the container to be in a running state and handles user feedback in case of failure.\n\n### `execute_code_blocks()`\nAsynchronously executes a series of code blocks. This method ensures that any user-defined functions are properly set up before code execution.\n\n### `_execute_code_dont_check_setup()`\nHandles the actual code execution by writing code to temporary files and executing it within the Docker container. It returns a `CommandLineCodeResult` object containing the output and exit code.\n\n### `_setup_functions()`\nSets up any user-defined functions by writing them to a module file inside the container and ensuring that the required packages are installed.\n\n### `_wait_for_ready()`\nA helper function that waits for the Docker container to reach a \"running\" status within a specified timeout.\n\n## Usage Notes\n- **Supported Languages**: The executor currently supports Python and shell script languages (like bash, powershell, etc.). Users should specify the language in the `CodeBlock`.\n- **Error Handling**: The executor captures and logs errors both from Docker operations and code execution, facilitating debugging.\n- **Cleanup**: Users should be aware of the temporary files and resources the executor may create. The configuration allows deletion of these files based on user preference.\n\n## Future Enhancements\nConsider updating the class to support more programming languages or improved error management, ensuring a more versatile code execution platform. Furthermore, enhancements could target security compliance and resource management when handling multiple concurrent executions.\n\n## Conclusion\nThe `DockerCommandLineCodeExecutor` provides a powerful tool for executing code within a controlled Docker environment, making it suitable for various automated coding tasks. This design emphasizes safety, modularity, and the ability to configure the execution environment as needed.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/docker/_docker_code_executor.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis code snippet is part of a Python module that appears to facilitate interaction with Jupyter notebooks running in Docker containers. It primarily focuses on importing certain classes and defining an `__all__` list for public exports of the module.\n\n## Imports\n\nThe code begins with imports from two modules:\n\n1. **Docker Execution Components**: \n   - `DockerJupyterCodeExecutor`: This class likely handles the execution of code within a Dockerized Jupyter environment.\n   - `DockerJupyterCodeResult`: This class probably encapsulates the results of code executed in the Docker environment.\n\n2. **Jupyter Server Components**: \n   - `DockerJupyterServer`: This class is likely responsible for creating and managing a Jupyter server instance within a Docker container.\n   - `JupyterClient`: This class probably serves as a client interface to interact with Jupyter servers.\n   - `JupyterKernelClient`: This class likely manages kernel interactions in the Jupyter ecosystem.\n\n## Public API Definitions\n\nThe `__all__` variable is defined next, which is a convention in Python that denotes the public API of the module. It specifies a list of names that will be accessible when the module is imported using the `from module import *` syntax. \n\n### Public Classes\n\nThe classes included in the `__all__` list are:\n\n- **DockerJupyterCodeExecutor**: Facilitates the execution of code snippets in a Dockerized Jupyter context.\n- **DockerJupyterServer**: Creates and manages the lifecycle of a Jupyter server hosted in Docker.\n- **JupyterClient**: Acts as a client for various operations interacting with the Jupyter server.\n- **JupyterKernelClient**: Handles interactions with a specific kernel within the Jupyter environment.\n- **DockerJupyterCodeResult**: Stores and provides access to the output or result of executed code.\n\nThis structure suggests that the module serves as a middleware to integrate Docker functionality with the Jupyter notebook experience, enhancing capabilities and flexibility for users who wish to run Jupyter notebooks in isolated and reproducible Docker environments.\n\n## Expected Use Cases\n\nWhile the provided code snippet does not contain functional implementations or usage examples, it can be inferred that these classes will be employed in situations that require:\n\n1. **Code Execution**: Running Python (or other supported language) code in a Jupyter notebook while ensuring an isolated environment through Docker.\n2. **Server Management**: Setting up and running Jupyter servers within Docker containers, possibly for testing, development, or educational purposes.\n3. **Client Interaction**: Communicating with Jupyter servers and services to send execution requests and retrieve results dynamically.\n\n## Conclusion\n\nOverall, this code lays a foundational structure for a module that enables the execution and management of Jupyter notebooks in Docker containers. It provides the necessary components to handle code execution and server-client interactions within this specialized context, supporting users needing an isolated, easily manageable Jupyter environment.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/docker_jupyter/__init__.py"
  },
  {
    "code": false,
    "content": "# DockerJupyterCodeExecutor Documentation\n\n## Overview\n\nThe `DockerJupyterCodeExecutor` class enables the execution of Python code blocks in a stateful manner using a Jupyter server. Designed as a component of the `autogen` system, it allows for chaining of code executions, retaining variable states across different executions, and provides comprehensive error handling.\n\nThis executor is particularly useful in environments where Jupyter notebooks are preferred for code execution, allowing for interactive programming workflows.\n\n## Installation Requirements\n\nTo use the `DockerJupyterCodeExecutor`, you must install its dependencies. The recommended command to install the required packages is:\n\n```bash\npip install \"autogen-ext[docker-jupyter-executor]\"\n```\n\n## Class Definitions\n\n### 1. DockerJupyterCodeResult\n\n```python\n@dataclass\nclass DockerJupyterCodeResult(CodeResult):\n    output_files: list[Path]\n```\n\n- **Purpose**: This class acts as a result container for code execution. In addition to storing the exit code and output, it maintains a list of output files generated during execution.\n\n### 2. DockerJupyterCodeExecutorConfig\n\n```python\nclass DockerJupyterCodeExecutorConfig(BaseModel):\n    jupyter_server: Union[JupyterConnectable, JupyterConnectionInfo]\n    kernel_name: str = \"python3\"\n    timeout: int = 60\n    output_dir: Optional[Union[Path, str]] = None\n```\n\n- **Purpose**: This configuration class defines the parameters for the `DockerJupyterCodeExecutor`, including the type of Jupyter server, kernel name, execution timeout, and output directory.\n\n### 3. DockerJupyterCodeExecutor\n\n```python\nclass DockerJupyterCodeExecutor(CodeExecutor, Component[DockerJupyterCodeExecutorConfig]):\n```\n\n- **Purpose**: This is the primary class for executing code blocks. It inherits from `CodeExecutor` and is a component of the `autogen` system. It manages the connection to the Jupyter server and facilitates the execution of Python code within the server's kernel.\n\n## Constructor and Initialization\n\n### Constructor\n\n```python\ndef __init__(self, jupyter_server: Union[JupyterConnectable, JupyterConnectionInfo], kernel_name: str = \"python3\", timeout: int = 60, output_dir: Path | None = None):\n```\n\n- **Parameters**:\n  - `jupyter_server`: Specifies the Jupyter server connection.\n  - `kernel_name`: The name of the kernel to execute codes, defaulting to \"python3\".\n  - `timeout`: Duration in seconds before a code execution is considered failed, default is 60.\n  - `output_dir`: Directory to save output files, defaults to a temporary directory if not provided.\n\n- **Functionality**: The constructor initializes the executor, checks the validity of inputs (like timeout), and sets up the Jupyter client needed for kernel communication.\n\n## Methods Overview\n\n### 1. Asynchronous Initialization\n\n- **`async def start(self) -> None`**: Starts a new kernel session by checking available kernels and initializing a specified kernel.\n- **`async def restart(self) -> None`**: Restarts the current kernel session and resets necessary client attributes.\n\n### 2. Code Execution\n\n- **`async def execute_code_blocks(self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken) -> DockerJupyterCodeResult`**:\n  - **Purpose**: Executes a list of code blocks in the Jupyter kernel. It handles both standard execution and returns any generated output files.\n  - **Workflow**:\n    - Ensures the kernel is ready.\n    - Iterates through code blocks, executing each one and capturing outputs, including images and HTML content.\n    - Returns an instance of `DockerJupyterCodeResult`.\n\n### 3. Resource Management\n\n- **`async def stop(self) -> None`**: Stops the current Jupyter kernel and closes the client connection, ensuring the environment is clean after execution.\n- **`async def __aenter__(self) -> Self`**: Allows the use of the async context manager to start the kernel upon entering the context.\n- **`async def __aexit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None) -> None`**: Ensures that the kernel is stopped when exiting the context.\n\n### 4. Utility Methods\n\n- **`def _save_image(self, image_data_base64: str) -> str`**: Saves base64 encoded image data to a PNG file and returns the file path.\n- **`def _save_html(self, html_data: str) -> str`**: Saves HTML data to an HTML file and returns the file path.\n\n## Example Usage\n\nBelow are simplified examples of how to use `DockerJupyterCodeExecutor` in an async context:\n\n### Basic Execution\n\n```python\nasync with DockerJupyterServer() as jupyter_server:\n    async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:\n        code_blocks = [CodeBlock(code=\"print('hello world!')\", language=\"python\")]\n        code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())\n        print(code_result)\n```\n\n### Custom Jupyter Server Usage\n\n```python\nasync with DockerJupyterServer(custom_image_name=\"your_custom_images_name\", expose_port=8888) as jupyter_server:\n    async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:\n        code_blocks = [CodeBlock(code=\"print('hello world!')\", language=\"python\")]\n        code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())\n        print(code_result)\n```\n\n## Conclusion\n\nThe `DockerJupyterCodeExecutor` is a powerful tool for executing Python code blocks in a controlled and stateful environment provided by a Jupyter server. By enabling execution context management, output handling, and providing utility functions, it facilitates the seamless integration of code execution into larger applications or workflows.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/docker_jupyter/_docker_jupyter.py"
  },
  {
    "code": false,
    "content": "# Documentation\n\n## Overview\n\nThis Python module provides an asynchronous API for interacting with Jupyter kernel gateways through HTTP and WebSocket protocols. It supports operations such as managing kernels, executing code, and running a Jupyter server within a Docker container. The module encapsulates various components that facilitate communication between users and Jupyter notebooks, including connection information structures, client classes, and Docker management for the Jupyter server.\n\n## Key Components\n\n### JupyterConnectionInfo Class\n\nThe `JupyterConnectionInfo` class stores essential connection parameters required for communicating with a Jupyter gateway server. It includes:\n\n- **host**: The address of the Jupyter server.\n- **use_https**: A boolean indicating whether to use HTTPS.\n- **port**: An optional integer for the server port.\n- **token**: An optional authentication token for secure access.\n\nThis dataclass provides a structured approach to manage connection credentials, thus facilitating the setup of Jupyter connections.\n\n### JupyterConnectable Protocol\n\nThe `JupyterConnectable` is a Protocol that ensures any defined connectable object will provide a `connection_info` property. This promotes a standard interface for classes that can connect to a Jupyter server, like the `DockerJupyterServer` class.\n\n### JupyterClient Class\n\nThe `JupyterClient` class is the core component for client-side interactions with the Jupyter server. The class provides various methods, including:\n\n- **list_kernel_specs**: Fetches available kernel specifications.\n- **list_kernels**: Retrieves a list of currently running kernels.\n- **start_kernel**: Asynchronously starts a kernel based on a specified kernel specification.\n- **delete_kernel**: Deletes a specified kernel.\n- **restart_kernel**: Restarts a specified kernel.\n- **get_kernel_client**: Creates a communication channel with a specified kernel via WebSocket.\n\n#### Initialization\n\nUpon initialization, the class constructs a synchronous `requests.Session` and an optional `aiohttp.ClientSession` for making HTTP requests. The session is set to retry failed connections, improving reliability.\n\n### JupyterKernelClient Class\n\nThe `JupyterKernelClient` class interacts with an individual Jupyter kernel over WebSocket. It provides methods for:\n\n- **execute**: Sending code to the kernel for execution and receiving results.\n- **wait_for_ready**: Confirming when the kernel is ready to accept commands.\n\nThe class handles message formats through the Jupyter messaging protocol, enabling the user to receive execution output, error messages, and data items.\n\n### DataItem and ExecutionResult Classes\n\nThe `DataItem` and `ExecutionResult` classes serve as data structures for representing output from kernel executions:\n\n- **DataItem**: Contains the MIME type and data associated with output elements, which can include images or plain text.\n- **ExecutionResult**: Represents the overall result of code execution, including a success flag, output string, and a list of associated data items.\n\n### DockerJupyterServer Class\n\nThe `DockerJupyterServer` class manages the lifecycle of a Jupyter kernel gateway server running inside a Docker container. Key features include:\n\n- **Initialization**: Defines parameters for the Docker container, such as image configuration, token generation, and directory bindings.\n  \n- **Default Dockerfile**: It uses a default Dockerfile to set up the Jupyter kernel gateway if a custom image is not provided. This includes installing necessary packages.\n\n- **Connection Handling**: Implements an `async` context manager for resource management. Upon entering the context, users can obtain a `JupyterClient` instance to interact with the running Jupyter server.\n\n#### Methods\n\n- **get_client**: Returns an instance of the `JupyterClient` using the connection information for the running Docker container.\n- **stop**: Stops and cleans up the running Docker container, ensuring proper shutdown.\n- **_wait_for_ready**: Monitors the Docker container until it's in a 'running' state, raising an error if it fails to start.\n\n### Docker Management\n\nThe Docker handling within `DockerJupyterServer` includes robust error checks for image existence and dependencies. It generates a random token for authentication unless specified otherwise. The server can expose a designated port for managing Jupyter communications, enhancing the usability of the setup.\n\n### Context Management\n\nBoth the `JupyterClient` and `DockerJupyterServer` leverage Python's `async` context management features, which ensure that resources are properly managed and released on completion of the asynchronous operations. This architecture helps in maintaining clean operations, especially when working with external services like Docker and Jupyter servers.\n\n## Conclusion\n\nThis module effectively combines elements of asynchronous programming, Docker handling, and Jupyter communication protocols to provide a comprehensive solution. It serves as a flexible foundation for building interactive applications based on Jupyter notebooks and can be extended for more specialized functionalities as needed.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/docker_jupyter/_jupyter_server.py"
  },
  {
    "code": false,
    "content": "# Documentation for Jupyter Code Executor Module\n\nThis module serves as an interface to facilitate the execution of code in Jupyter environments. It imports two primary components: `JupyterCodeExecutor` and `JupyterCodeResult`, which are presumably defined in a sub-module named `_jupyter_code_executor`.\n\n## Imports\n\n### JupyterCodeExecutor\n\n`JupyterCodeExecutor` is likely a class designed to handle the execution of code within Jupyter Notebooks. Its functionality may include compiling and running code snippets provided by the user, managing the execution context, and potentially handling input and output in a Jupyter-compatible manner.\n\n### JupyterCodeResult\n\n`JupyterCodeResult` appears to be a class or data structure used to encapsulate the results of the code execution. It may include attributes that represent the output, execution time, error messages, or other relevant information about the execution outcome.\n\n## Exported Components\n\nThe `__all__` list indicates which components of this module are intended to be public and accessible when the module is imported. By listing `\"JupyterCodeExecutor\"` and `\"JupyterCodeResult\"`, this module explicitly defines its API, allowing users to import these classes without directly accessing the underlying sub-module.\n\n## Key Functionalities\n\nThough the specifics of the methods and properties within `JupyterCodeExecutor` and `JupyterCodeResult` are not provided in the code snippet, we can infer several potential functionalities based on common practices in similar modules:\n\n- **Code Execution**: The `JupyterCodeExecutor` may provide methods to execute Python (or possibly other languages) code strings. This includes lower-level operations like invoking a Jupyter kernel to execute the code.\n\n- **Result Handling**: After execution, the resulting output is likely encapsulated in an instance of `JupyterCodeResult`. This facilitates structured access to information about the execution, such as standard output and error streams.\n\n- **Error Management**: If execution errors occur, `JupyterCodeExecutor` might be equipped to catch these errors and wrap them appropriately within `JupyterCodeResult`, providing users with meaningful feedback.\n\n## Intended Use Cases\n\nThe module is likely designed for use in situations where interactive code execution is required, such as:\n\n- **Development Tools**: It can be used in IDEs or extensions that wish to enable running and testing code snippets interactively.\n\n- **Educational Platforms**: Online learning environments may leverage this functionality to allow students to write and execute code as part of their lessons.\n\n- **Data Processing Workflows**: Users analyzing data can integrate the module to run code segments dynamically, facilitating exploratory data analysis.\n\n## Conclusion\n\nIn summary, this module provides an interface for executing code within Jupyter Notebooks, encapsulating both execution logic and result management. While the exact details of the classes and their methods remain unspecified in the provided code snippet, their expected roles are clear based on conventional practices in Jupyter-related development environments. \n\nOverall, the design is aimed at enhancing the user experience by promoting an efficient and organized approach to code execution and result handling within Jupyter.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/jupyter/__init__.py"
  },
  {
    "code": false,
    "content": "# JupyterCodeExecutor Documentation\n\n## Overview\n\nThe `JupyterCodeExecutor` class is part of the `autogen_ext.code_executors` module and is designed to execute Python code blocks using a Jupyter Notebook kernel. It allows asynchronous code execution while managing output files and addressing errors gracefully. This executor is particularly useful in environments where code generated by Large Language Models (LLMs) needs to be run safely and effectively.\n\n## Components and Configuration\n\n### Data Classes\n\n#### JupyterCodeResult\n- **Purpose**: Represents the outcome of executing code within the Jupyter Notebook context, capturing output and related files.\n- **Attributes**:\n  - `output_files` (list[Path]): A list of file paths that were generated as output from code execution.\n\n#### JupyterCodeExecutorConfig\n- **Purpose**: Holds configuration settings for the `JupyterCodeExecutor`.\n- **Attributes**:\n  - `kernel_name` (str): Specifies the Jupyter kernel to use, defaulting to \"python3\".\n  - `timeout` (int): Sets the maximum execution time in seconds for code blocks (default is 60 seconds).\n  - `output_dir` (Optional[str]): Directory path where output files will be saved.\n\n### Class: JupyterCodeExecutor\n- **Inherits**: `CodeExecutor` and is a `Component` of the specified configuration type.\n- **Functionality**: This class encapsulates all functionalities required to set up a Jupyter Notebook environment to execute code blocks asynchronously.\n\n## Initialization and Resource Management\n\n### Constructor (`__init__`)\n- **Parameters**:\n  - `kernel_name`: Specifies the kernel for execution.\n  - `timeout`: The maximum time allowed for executing a single code block.\n  - `output_dir`: (Optional) A path where output files will be saved; defaults to a temporary directory.\n- **Behavior**: \n  - Initializes instance variables and creates a temporary output directory if none is provided.\n  - Ensures that timeout is a valid positive integer.\n\n### Resource Management Methods\n#### start()\n- **Purpose**: Initializes the Jupyter execution environment.\n- **Behavior**: Creates a new notebook and sets up a kernel context. Must be called before executing any code blocks.\n\n#### stop()\n- **Purpose**: Cleans up resources and terminates the execution environment.\n- **Behavior**: Exits the kernel context and releases resources, marking the executor as stopped.\n\n#### restart()\n- **Purpose**: Restarts the executor by stopping and then starting it again.\n\n## Code Execution\n\n### Method: execute_code_blocks\n- **Parameters**:\n  - `code_blocks`: A list of `CodeBlock` instances to be executed.\n  - `cancellation_token`: Controls cancellation of execution if needed.\n- **Returns**: An instance of `JupyterCodeResult`.\n- **Behavior**: Iterates through each code block, executing them synchronously. If a code block fails (non-zero exit code), execution stops.\n\n### Method: _execute_code_block\n- **Parameters**:\n  - `code_block`: A single `CodeBlock` to execute.\n  - `cancellation_token`: Controls cancellation of execution if needed.\n- **Returns**: An instance of `JupyterCodeResult`.\n- **Behavior**: Executes a code block and gathers outputs, handling different types of notebook cell outputs (streams, errors, etc.).\n\n#### Auxiliary Methods\n- **_execute_cell**: Responsible for executing a single cell in the notebook and appending it to the notebook\u2019s cell list.\n- **_save_image**: Saves base64-encoded image outputs to the filesystem and returns their paths.\n- **_save_html**: Saves HTML outputs to a file and returns the file path.\n\n## Properties\n\n### output_dir\n- A property that provides the output directory path, including deprecation warnings if the current directory is used.\n\n## Example Usages\n\nThe `JupyterCodeExecutor` can be utilized in various contexts, including:\n- Direct execution of simple code blocks (e.g., printing output).\n- Integration with conversational agents to handle queries programmatically.\n- Use in reactive systems to execute code snippets based on user input dynamically.\n\n### Example 1: Direct use\n```python\nasync with JupyterCodeExecutor() as executor:\n    code_blocks = [CodeBlock(code=\"print('Hello, World!')\", language=\"python\")]\n    result = await executor.execute_code_blocks(code_blocks, CancellationToken())\n    print(result)  # Outputs the execution result\n```\n\n### Example 2: With an Assistant Agent\n```python\ntool = PythonCodeExecutionTool(executor)\nagent = AssistantAgent(\"assistant\", model_client=model_client, tools=[tool])\nresult = await agent.run(task=\"Calculate the 10th Fibonacci number using Python.\")\nprint(result)  # Outputs the calculation result\n```\n\n## Summary\n\nThe `JupyterCodeExecutor` class provides a robust way to execute code blocks against a Jupyter Notebook kernel, ensuring flexibility and error handling in asynchronous environments. It abstracts away the complexity of managing the Jupyter execution lifecycle, making it suitable for integration into larger applications that require dynamic code execution capabilities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/jupyter/_jupyter_code_executor.py"
  },
  {
    "code": false,
    "content": "# LocalCommandLineCodeExecutor Documentation\n\n## Overview\n\nThe `LocalCommandLineCodeExecutor` is a class designed to execute code blocks in a local command-line environment. It is particularly useful for executing Python scripts and shell commands securely. Each code block is run in isolation within a unique temporary file and process, thus ensuring that code execution does not interfere with the user's local environment. Given the potential risks associated with executing code directly on a local machine, the implementation includes several safety checks to sanitize the command line input and prevent harmful commands.\n\n## Class and Configuration\n\n### LocalCommandLineCodeExecutorConfig\n\nThis configuration model extends `BaseModel` from the Pydantic library and specifies the parameters used to initialize the command-line executor. The properties include:\n\n- **timeout (int)**: Defines the maximum execution time for each code block (default is 60 seconds).\n- **work_dir (Optional[str])**: Specifies the working directory for executing code blocks. If set to `None`, a temporary directory will be used.\n- **functions_module (str)**: Specifies the module name under which user-defined functions will be stored (default is `\"functions\"`).\n- **cleanup_temp_files (bool)**: Indicates whether temporary files should be deleted after execution (default is `True`).\n\n### LocalCommandLineCodeExecutor\n\nThis class inherits from `CodeExecutor` and `Component`, managing code execution in a local environment. Key highlights of this class include:\n\n- **Initialization**: The constructor accepts parameters including timeout, working directory, functions to execute, and virtual environment context. Various checks are performed to ensure values conform to expected types and conditions.\n  \n- **Supported Languages**: The class supports executing Python and shell scripts, recognized by their associated language identifiers.\n\n- **Warnings**: It warns users about the potential risks of executing commands on the local machine, and encourages the use of safer alternatives like Docker.\n\n## Method Definitions\n\n### `format_functions_for_prompt`\n\nThis experimental method formats available functions into a string that can be used in prompts to the user. The function uses Python's `Template` to substitute in the module name and formatted function stubs, making it easier for users to reference available functions when writing code blocks.\n\n### `execute_code_blocks`\n\nThis is the primary method used to execute a list of code blocks. Upon execution:\n\n1. It first checks if the functions are set up; if not, it sets them up using `_setup_functions`.\n2. It then calls `_execute_code_dont_check_setup`, which processes each code block without re-checking the setup.\n\n### `_setup_functions`\n\nThis asynchronous method prepares the user-defined functions:\n\n- It creates a Python script containing the functions using `build_python_functions_file`.\n- If the functions have dependencies, it installs them using `pip`.\n- It verifies that the functions can be executed without syntax errors by attempting to load them.\n\n### `_execute_code_dont_check_setup`\n\nThis asynchronous method runs the provided code blocks. Steps include:\n\n1. For each code block:\n   - It normalizes the language identifier.\n   - It generates a unique filename for the code block if one isn't specified.\n   - It creates a subprocess to execute the code in the appropriate environment.\n   - The method logs the output, ensuring that standard output and error messages are captured.\n\n2. The method handles cleanup of temporary files based on the `cleanup_temp_files` flag after execution.\n\n## Workflow and Behavior\n\nUpon starting, the class can be used as follows:\n\n1. **Initialization**: Create an instance of `LocalCommandLineCodeExecutor` with the required parameters.\n2. **Starting Execution**: Call the `start` method to set up the environment.\n3. **Executing Code**: Invoke `execute_code_blocks` with one or more code blocks for execution. The results are returned for review.\n4. **Stopping Execution**: Use the `stop` method to cleanly shut down the executor and remove temporary resources if created.\n\n## Safety and Notes\n\nThe executor employs various safety measures, including a built-in command sanitizer that filters out potentially dangerous commands. When operating in a Windows environment, users are prompted to set the appropriate event loop policy for subprocess handling, and warnings are issued to ensure users understand the implications of executing local code.\n\n## Final Remarks\n\nThis class serves as a powerful tool for executing code securely in a controlled environment. It abstracts many details of subprocess management and dependency resolution, making it accessible for quick development and testing tasks. However, users are cautioned about the inherent risks involved in executing arbitrary code on their machines and are advised to consider safer alternatives like containerized environments when possible.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/code_executors/local/__init__.py"
  },
  {
    "code": false,
    "content": "It appears there was no source code provided for analysis. Please share the code you would like me to examine, and I'll be happy to produce a clear, high-level description and documentation for it.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/__init__.py"
  },
  {
    "content": "# Task-Centric Memory\n_(EXPERIMENTAL, RESEARCH IN PROGRESS)_\n\n**Task-Centric Memory** is an active research project aimed at giving AI agents the ability to:\n\n* Accomplish general tasks more effectively by learning quickly and continually beyond context-window limitations.\n* Remember guidance, corrections, plans, and demonstrations provided by users.\n* Learn through the agent's own experience and adapt quickly to changing circumstances.\n* Avoid repeating mistakes on tasks that are similar to those previously encountered.\n\n## Installation\n\nInstall AutoGen and its extension package as follows:\n\n```bash\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" \"autogen-ext[task-centric-memory]\"\n```\n\n## Quickstart\n\n<p align=\"right\">\n  <img src=\"../../../../imgs/task_centric_memory_2.png\" alt=\"Description\" width=\"150\" align=\"right\" style=\"margin-left: 10px;\">\n</p>\n\nThis first code snippet runs a basic test to verify that the installation was successful,\nas illustrated by the diagram to the right.\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.experimental.task_centric_memory import MemoryController\nfrom autogen_ext.experimental.task_centric_memory.utils import PageLogger\n\n\nasync def main() -> None:\n   client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n   logger = PageLogger(config={\"level\": \"DEBUG\", \"path\": \"./pagelogs/quickstart\"})  # Optional, but very useful.\n   memory_controller = MemoryController(reset=True, client=client, logger=logger)\n\n   # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.\n   await memory_controller.add_memo(task=\"What color do I like?\", insight=\"Deep blue is my favorite color\")\n   await memory_controller.add_memo(task=\"What's another color I like?\", insight=\"I really like cyan\")\n   await memory_controller.add_memo(task=\"What's my favorite food?\", insight=\"Halibut is my favorite\")\n\n   # Retrieve memories for a new task that's related to only two of the stored memories.\n   memos = await memory_controller.retrieve_relevant_memos(task=\"What colors do I like most?\")\n   print(\"{} memories retrieved\".format(len(memos)))\n   for memo in memos:\n      print(\"- \" + memo.insight)\n\n\nasyncio.run(main())\n```\n\n<p align=\"right\">\n  <img src=\"../../../../imgs/task_centric_memory_3.png\" alt=\"Description\" width=\"150\" align=\"right\" style=\"margin-left: 10px;\">\n</p>\n\nThis second code example shows one way to incorporate task-centric memory directly into an AutoGen agent,\nin this case a subclass of RoutedAgent.\nTo keep the code short, only the simplest form of memory retrieval is exercised by this agent.\n\n```python\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler\nfrom autogen_core.models import ChatCompletionClient, LLMMessage, SystemMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.experimental.task_centric_memory import MemoryController\nfrom autogen_ext.experimental.task_centric_memory.utils import PageLogger\n\n\n@dataclass\nclass Message:\n   content: str\n\n\nclass MemoryEnabledAgent(RoutedAgent):\n   def __init__(\n           self, description: str, model_client: ChatCompletionClient, memory_controller: MemoryController\n   ) -> None:\n      super().__init__(description)\n      self._model_client = model_client\n      self._memory_controller = memory_controller\n\n   @message_handler\n   async def handle_message(self, message: Message, context: MessageContext) -> Message:\n      # Retrieve relevant memories for the task.\n      memos = await self._memory_controller.retrieve_relevant_memos(task=message.content)\n\n      # Format the memories for the model.\n      formatted_memos = \"Info that may be useful:\\n\" + \"\\n\".join([\"- \" + memo.insight for memo in memos])\n      print(f\"{'-' * 23}Text appended to the user message{'-' * 24}\\n{formatted_memos}\\n{'-' * 80}\")\n\n      # Create the messages for the model with the retrieved memories.\n      messages: List[LLMMessage] = [\n         SystemMessage(content=\"You are a helpful assistant.\"),\n         UserMessage(content=message.content, source=\"user\"),\n         UserMessage(content=formatted_memos, source=\"user\"),\n      ]\n\n      # Call the model with the messages.\n      model_result = await self._model_client.create(messages=messages)\n      assert isinstance(model_result.content, str)\n\n      # Send the model's response to the user.\n      return Message(content=model_result.content)\n\n\nasync def main() -> None:\n   client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n   logger = PageLogger(config={\"level\": \"DEBUG\", \"path\": \"./pagelogs/quickstart2\"})  # Optional, but very useful.\n   memory_controller = MemoryController(reset=True, client=client, logger=logger)\n\n   # Prepopulate memory to mimic learning from a prior session.\n   await memory_controller.add_memo(task=\"What color do I like?\", insight=\"Deep blue is my favorite color\")\n   await memory_controller.add_memo(task=\"What's another color I like?\", insight=\"I really like cyan\")\n   await memory_controller.add_memo(task=\"What's my favorite food?\", insight=\"Halibut is my favorite\")\n\n   # Create and start an agent runtime.\n   runtime = SingleThreadedAgentRuntime()\n   runtime.start()\n\n   # Register the agent type.\n   await MemoryEnabledAgent.register(\n      runtime,\n      \"memory_enabled_agent\",\n      lambda: MemoryEnabledAgent(\n         \"A agent with memory\", model_client=client, memory_controller=memory_controller\n      ),\n   )\n\n   # Send a direct message to the agent.\n   request = \"What colors do I like most?\"\n   print(\"User request: \" + request)\n   response = await runtime.send_message(\n      Message(content=request), AgentId(\"memory_enabled_agent\", \"default\")\n   )\n   print(\"Agent response: \" + response.content)\n\n   # Stop the agent runtime.\n   await runtime.stop()\n\n\nasyncio.run(main())\n```\n\n## Sample Code\n\nThe example above modifies the agent's code.\nBut it's also possible to add task-centric memory to an agent or multi-agent team _without_ modifying any agent code.\nSee the [sample code](../../../../../../samples/task_centric_memory) for that and other forms of fast, memory-based learning.\n\n\n## Architecture\n\n<p align=\"right\">\n  <img src=\"../../../../imgs/task_centric_memory.png\" alt=\"Description\" width=\"300\" align=\"right\" style=\"margin-left: 10px;\">\n</p>\n\nThe block diagram to the right outlines the key components of the architecture in the most general form.\nThe memory components are shown in blue, and the green blocks represent external components.\n\nThe **Memory Controller** implements the fast-learning methods described below,\nand manages communication with a **Memory Bank** containing a vector DB and associated structures.\n\nThe **Agent or Team** is the AI agent or team of agents to which memory is being added.\nThe sample code shows how to add task-centric memory to a simple AssistantAgent or a MagenticOneGroupChat team.\n\nThe **Apprentice, app, or service** represents the code that instantiates the agent and memory controller,\nand routes information between them, effectively wrapping agent and memory into a combined component.\nThe term _Apprentice_ connotes that this combination uses memory to learn quickly on the job.\nThe Apprentice class is a minimal reference implementation provided as utility code for illustration and testing,\nbut most applications will use their own code instead of the Apprentice.\n\n## Memory Creation and Storage\n\nEach stored memory (called a _memo_) contains a text insight and (optionally) a task description.\nThe insight is intended to help the agent accomplish future tasks that are similar to a prior task.\nThe memory controller provides methods for different types of learning.\nIf the user provides advice for solving a given task, the advice is extracted by the model client and stored as an insight.\nIf the user demonstrates how to perform a task,\nthe task and demonstration are stored together as an insight used to solve similar but different tasks.\nIf the agent is given a task (free of side-effects) and some means of determining success or failure,\nthe memory controller repeats the following learning loop in the background some number of times:\n\n1. Test the agent on the task a few times to check for a failure.\n2. If a failure is found, analyze the agent's response in order to:\n   1. Diagnose the failure of reasoning or missing information,\n   2. Phrase a general piece of advice, such as what a teacher might give to a student,\n   3. Temporarily append this advice to the task description,\n   4. Return to step 1.\n   5. If some piece of advice succeeds in helping the agent solve the task a number of times, add the advice as an insight to memory.\n3. For each insight to be stored in memory, an LLM is prompted to generate a set of free-form, multi-word topics related to the insight. Each topic is embedded to a fixed-length vector and stored in a vector DB mapping it to the topic\u2019s related insight.\n\n## Memory Retrieval and Usage\n\nThe memory controller provides methods for different types of memory retrieval.\nWhen the agent is given a task, the following steps are performed by the controller:\n1. The task is rephrased into a generalized form.\n2. A set of free-form, multi-word query topics are generated from the generalized task.\n3. A potentially large number of previously stored topics, those most similar to each query topic, are retrieved from the vector DB along with the insights they map to.\n4. These candidate memos are filtered by the aggregate similarity of their stored topics to the query topics.\n5. In the final filtering stage, an LLM is prompted to validate only those insights that seem potentially useful in solving the task at hand.\n\nRetrieved insights that pass the filtering steps are listed under a heading like\n\"Important insights that may help solve tasks like this\", then appended to the task description before it is passed to the agent as usual.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe provided code is a Python module that focuses on memory management functionalities. It organizes and exposes certain classes and configurations from its submodules to facilitate the implementation of memory control features in a larger application. \n\n## Module Imports\nThis script imports the following components from other modules:\n- `MemoryBankConfig` from the `_memory_bank` module\n- `MemoryController` and `MemoryControllerConfig` from the `memory_controller` module\n\nThese imports suggest that the module is involved in managing how memory is configured and controlled in the context of the application.\n\n## Components Defined\n### `MemoryBankConfig`\n- **Purpose**: This class likely contains configuration settings and parameters related to a memory bank. This could include settings such as size, access methods, or structure of the memory bank.\n- **Role**: It serves as a customizable component which allows users of the module to define how memory banks should behave, tailored to the needs of the application.\n\n### `MemoryController`\n- **Purpose**: This class acts as the primary handler for controlling memory operations. It is probably responsible for managing access to the memory resources defined by the `MemoryBankConfig`.\n- **Role**: It interfaces with various memory banks and orchestrates actions such as reading from and writing to memory, ensuring that these operations are performed efficiently and reliably.\n\n### `MemoryControllerConfig`\n- **Purpose**: Similar to `MemoryBankConfig`, this class holds configuration settings specifically for the `MemoryController`. It might encompass parameters such as controller behavior, interaction rules, and scheduling of memory access.\n- **Role**: By allowing customized configurations, it facilitates the fine-tuning of memory control operations, enabling optimal memory management for different scenarios.\n\n## Namespace Management\nThe `__all__` directive is defined at the end of the script:\n```python\n__all__ = [\"MemoryController\", \"MemoryControllerConfig\", \"MemoryBankConfig\"]\n```\n- **Purpose**: This line specifies the public interface of the module. It indicates which classes will be available for import when the module is imported via `from module_name import *`.\n- **Significance**: This helps in managing the namespace, making sure that only the intended components are accessible, thus avoiding unintended interference or misuse of internal classes or functions.\n\n## Summary\nIn summary, this module serves as an organized interface for memory management functionalities, encapsulating essential classes and their configurations. By using a structured approach with defined configurations for both memory banks and controllers, it enables developers to efficiently manage memory resources in their applications. The clear segregation of configuration and operation responsibilities enhances maintainability and usability of the memory management system.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/__init__.py"
  },
  {
    "code": false,
    "content": "# MemoryBank Module Documentation\n\n## Overview\nThe `MemoryBank` module is designed to store and manage insights or memories related to tasks. Using a vector database, it facilitates the retrieval of relevant memories based on input topics. The `MemoryBank` class allows for the addition, retrieval, and management of these memories, making it suitable for applications that require task-based knowledge retrieval, such as AI-driven assistance or learning platforms.\n\n## Key Components\n\n### Data Structures\n#### 1. **Memo**\n```python\n@dataclass\nclass Memo:\n    task: str | None\n    insight: str\n```\nThe `Memo` data class serves as an atomic unit of memory. Each `Memo` instance contains:\n- `task`: A description of the task (optional).\n- `insight`: Key information or solutions related to the task.\n\nThis encapsulation allows for easy handling of task-related knowledge units.\n\n#### 2. **MemoryBankConfig**\n```python\nclass MemoryBankConfig(TypedDict, total=False):\n    path: str\n    relevance_conversion_threshold: float\n    n_results: int\n    distance_threshold: int\n```\n`MemoryBankConfig` is a type definition for configuration settings that can be passed to the `MemoryBank`. It includes parameters for:\n- Path to the memory bank directory.\n- Thresholds for relevance and distance, and the maximum number of results to retrieve.\n\nThis structure allows for flexibility in configuring the memory bank's behavior without extensive code changes.\n\n## MemoryBank Class\n### Initialization\n```python\nclass MemoryBank:\n    def __init__(self, reset: bool, config: MemoryBankConfig | None = None, logger: PageLogger | None = None) -> None:\n```\nThe `MemoryBank` constructor initializes the memory storage system. Key functionalities include:\n- **Resetting Data**: Optionally clears existing data when a new instance is created.\n- **Configuration Loading**: Loads from a provided configuration or sets default values for memory management.\n- **Initialization of the Vector Database**: Sets up a `StringSimilarityMap` for efficient memory retrieval.\n- **Disk Loading**: Loads existing memories from the disk unless a reset is requested.\n\n### Memory Management Functions\n#### Reset Functions\n```python\ndef reset(self) -> None\ndef _reset_memos(self) -> None\n```\nThe `reset` method clears all in-memory and on-disk contents, while `_reset_memos` specifically clears just the memo dictionary used for tracking insights and tasks.\n\n#### Saving Memos\n```python\ndef save_memos(self) -> None\n```\nThis method persists the current state of the memo dictionary and any associated string pairs to disk, ensuring data integrity and retrieval capabilities across sessions.\n\n#### Memo Existence Check\n```python\ndef contains_memos(self) -> bool\n```\nA simple utility to check if any memos exist, returning a boolean indicating the presence of stored insights.\n\n### Adding Memos\n#### General Memo Addition\n```python\ndef add_memo(self, insight_str: str, topics: List[str], task_str: Optional[str] = None) -> None\n```\nThe `add_memo` method allows storing a new insight, associating it with relevant topics and an optional task description. It increments the last memo ID and maps topics to the newly created memo.\n\n#### Task-Solution Pair Addition\n```python\ndef add_task_with_solution(self, task: str, solution: str, topics: List[str]) -> None\n```\nThis function adds a task along with its solution to the memory bank, creating a cohesive context that can be retrieved later. It enhances the understanding of problem-solving related to the task.\n\n### Retrieving Memos\n```python\ndef get_relevant_memos(self, topics: List[str]) -> List[Memo]\n```\nThe `get_relevant_memos` function fetches memos that are sufficiently relevant to the provided topics. It uses the vector database to find related topics and ranks the memos by relevance, returning only those that meet a specified threshold. This is crucial for the application of task assistance where relevant context needs to be retrieved quickly.\n\n## Conclusion\nThe `MemoryBank` class encapsulates functionality for adding, managing, and retrieving task-related insights stored as `Memo` instances. Through effective use of configuration, data persistence, and relevance-based retrieval, it forms a robust foundation for any application requiring intelligent memory management. This module can be extended or integrated into systems needing dynamic, context-aware assistance or learning capabilities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/_memory_bank.py"
  },
  {
    "code": false,
    "content": "# Prompter Class Documentation\n\n## Overview\n\nThe `Prompter` class centralizes the process of interacting with a chat model, providing various methods to engage with the model in meaningful ways. These methods allow for tasks such as extracting topics, validating insights, and generally communicating with the model to generate responses based on user-defined inputs. The class utilizes an asynchronous approach, allowing for non-blocking calls to the model.\n\n## Initialization\n\n### `__init__`\n\n```python\ndef __init__(self, client: ChatCompletionClient, logger: PageLogger | None = None) -> None:\n```\n\n- **Parameters**:\n  - `client`: An instance of `ChatCompletionClient`, used to communicate with the chat model.\n  - `logger`: An optional `PageLogger` instance for logging model calls. If not provided, a new `PageLogger` instance will be created which does not log any data.\n\n- **Functionality**: \n  - Initializes the `Prompter` instance and sets a default system message indicating that the model acts as a helpful assistant. It also tracks the number of calls made to the model and the time spent on these calls.\n\n## Model Call\n\n### `call_model`\n\n```python\nasync def call_model(self, summary: str, user_content: UserContent, system_message_content: str | None = None, keep_these_messages: bool = True) -> str:\n```\n\n- **Parameters**:\n  - `summary`: A brief description of the purpose of the model call, used for logging.\n  - `user_content`: The content provided by the user, wrapped in the `UserContent` type.\n  - `system_message_content`: Optional custom system message content. If not provided, a default message is used.\n  - `keep_these_messages`: A boolean determining whether to retain the user and model messages in the chat history.\n\n- **Functionality**:\n  - Prepares the input messages, validates them, and interacts with the model. \n  - Logs the interaction details and manages the chat history. \n  - Returns the response generated by the model as a string.\n\n### History Management\n\n#### `_clear_history`\n\n```python\ndef _clear_history(self) -> None:\n```\n\n- **Functionality**: \n  - Clears the chat history, resetting the internal message list.\n\n## Learning and Analysis Methods\n\n### `learn_from_failure`\n\n```python\nasync def learn_from_failure(self, task_description: str, memory_section: str, final_response: str, expected_answer: str, work_history: str) -> str:\n```\n\n- **Parameters**:\n  - `task_description`: A brief description of the task that led to a failure.\n  - `memory_section`: Additional context related to the task.\n  - `final_response`: The response generated by the students that was incorrect.\n  - `expected_answer`: The correct answer or response for the task.\n  - `work_history`: The complete work history leading to the attempted solution.\n\n- **Functionality**: \n  - This method formulates a set of messages to instruct the model to analyze a failure in student work. It prompts the model to review the work, identify misconceptions, and provide general advice for improvement.\n\n### `find_index_topics`\n\n```python\nasync def find_index_topics(self, input_string: str) -> List[str]:\n```\n\n- **Parameters**:\n  - `input_string`: The text from which topics need to be extracted.\n\n- **Functionality**: \n  - Requests the model to identify and extract meaningful topics related to the provided string. It returns a list of topics, formatted as specified.\n\n### `generalize_task`\n\n```python\nasync def generalize_task(self, task_description: str, revise: bool | None = True) -> str:\n```\n\n- **Parameters**:\n  - `task_description`: The specific task description to be generalized.\n  - `revise`: Indicates whether the generated list should be revised to include only critical terms.\n\n- **Functionality**: \n  - Seeks to rewrite the task description in simpler, more general language. It may additionally prompt the model to revise this list to filter out irrelevant items.\n\n### `validate_insight`\n\n```python\nasync def validate_insight(self, insight: str, task_description: str) -> bool:\n```\n\n- **Parameters**:\n  - `insight`: A potential key insight that may help in solving a task.\n  - `task_description`: The task that the insight may pertain to.\n\n- **Functionality**: \n  - Evaluates whether the provided insight is potentially useful for solving the given task and returns a boolean indicating its validity.\n\n## Task and Advice Extraction\n\n### `extract_task`\n\n```python\nasync def extract_task(self, text: str) -> str | None:\n```\n\n- **Parameters**:\n  - `text`: The text to analyze for any individual tasks.\n\n- **Functionality**: \n  - Determines if there is a valid task or question within the text. Returns either the identified task description or `None`.\n\n### `extract_advice`\n\n```python\nasync def extract_advice(self, text: str) -> str | None:\n```\n\n- **Parameters**:\n  - `text`: The text from which advice may be extracted.\n\n- **Functionality**: \n  - Extracts any potentially useful advice from the text and outputs it, or returns `None` if no advice is found.\n\n## Conclusion\n\nThe `Prompter` class serves as the main interface for engaging with a chat model, facilitating various educational-related tasks such as feedback, analysis, and topic extraction. Its design emphasizes maintainability and extensibility, making it suitable for use in applications that require interactive learning or problem-solving assistance. Through its well-defined methods and comprehensive logging capabilities, the class enables efficient communication with the underlying chat model while ensuring that both user and model messages are tracked appropriately.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/_prompter.py"
  },
  {
    "code": false,
    "content": "# StringSimilarityMap Documentation\n\n## Overview\n\nThe `StringSimilarityMap` class offers a method for storing string pairs and retrieving them based on similarity metrics using a vector database. It utilizes embeddings to represent the string inputs, allowing for efficient querying of related string pairs. This implementation is integrated with Chroma's vector embedding capabilities and supports functionalities such as saving, loading, and clearing string pairs.\n\n## Class Initialization\n\n### `__init__(self, reset: bool, path_to_db_dir: str, logger: PageLogger | None = None) -> None`\n\nThe constructor initializes the `StringSimilarityMap` class instance with the following parameters:\n\n- **reset**: A boolean flag that determines whether to clear the database upon initialization.\n- **path_to_db_dir**: The path to the directory where the database and string pair dictionary will be stored.\n- **logger**: An optional logging object for debugging purposes.\n\n#### Responsibilities:\n- Initializes the logger or creates a default one.\n- Sets up the database client and creates a vector database collection named \"string-pairs\".\n- Loads existing string pairs from a pickle file if it exists and reset is not set.\n- If reset is true, the database is cleared.\n\n## Internal Method: Logging\n\n### `_log_string_pairs(self) -> None`\n\nThis private method logs all string pairs currently stored in the map. It iterates through the internal `uid_text_dict` dictionary and logs each entry with its corresponding input and output strings.\n\n### Responsibilities:\n- Provides visibility into the current contents of the string similarity map for debugging purposes.\n\n## Saving and Resetting\n\n### `save_string_pairs(self) -> None`\n\nThis method serializes the internal dictionary of string pairs (`uid_text_dict`) and saves it to a specified path using the pickle module.\n\n### Responsibilities:\n- Ensures that the current state of string pairs can be preserved on disk.\n\n### `reset_db(self) -> None`\n\nThis method deletes all contents of the database both in memory and on disk. It creates a new collection \"string-pairs\" and resets the internal dictionary.\n\n### Responsibilities:\n- Resets the entire string pair map, ensuring it can start fresh without any prior data.\n\n## String Pair Management\n\n### `add_input_output_pair(self, input_text: str, output_text: str) -> None`\n\nThis method adds a new input-output string pair to the database. It increments the last assigned ID and stores the pair both in the vector database and in the internal dictionary.\n\n### Responsibilities:\n- Handles the creation of new string pairs, logging relevant information for debugging.\n\n## Querying for Similarity\n\n### `get_related_string_pairs(self, query_text: str, n_results: int, threshold: Union[int, float]) -> List[Tuple[str, str, float]]`\n\nThis method retrieves a specified number of related string pairs based on the input `query_text`, within a certain distance threshold. It queries the vector database for similar entries and filters the results based on the threshold.\n\n### Responsibilities:\n- Searches for string pairs that are similar to a given query and meets the distance criteria.\n- Returns a list of tuples where each tuple contains an input string, an output string, and their similarity distance.\n\n## Usage and Functionality\n\nThe `StringSimilarityMap` class serves as a robust utility for applications needing to manage and query string pairs based on similarity. It is designed to be extendable and integrated into larger systems requiring semantic string comparison or association tasks.\n\nOverall, this class abstracts the complexities of handling vector embeddings and string associations, providing a clear interface for adding, saving, and retrieving string pairs based on similarity criteria. This is particularly useful in scenarios such as natural language processing or information retrieval systems.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/_string_similarity_map.py"
  },
  {
    "code": false,
    "content": "# MemoryController Documentation\n\nThe `MemoryController` class is designed for managing a memory bank that facilitates fast, memory-based learning. It plays a crucial role in handling tasks and insights related to a given subject or queries, allowing efficient storage, retrieval, and training mechanisms based on previously acquired knowledge.\n\n## Overview of `MemoryController`\n\nThe `MemoryController` takes in multiple parameters during initialization, including a reset option to clear the memory bank, a client model to execute commands, and a configuration dictionary to customize its behavior. A logger can also be provided for detailed output during its operation. The overall structure aims to facilitate a more adaptive learning environment by enabling a dynamic flow of insights stored in the memory bank.\n\n### Key Configuration Parameters\n\nThe main configurable aspects of the `MemoryController` include:\n- **generalize_task**: Controls whether the tasks are generalized for better insight extraction.\n- **revise_generalized_task**: Intends to critique and improve on generalized task representations.\n- **generate_topics**: Affects how the system retrieves relevant insights driven either by tasks or derived topics.\n- **validate_memos**: Ensures a validation stage for memos before they're considered relevant.\n- **max_memos_to_retrieve**: Limits the maximum number of memos fetched during retrievals.\n- **max_train_trials & max_test_trials**: Configure the number of learning and testing attempts when dealing with tasks.\n\n## Class Methods\n\n### `__init__`\nThe constructor method initializes the `MemoryController` instance, setting up necessary properties such as logging, configuration defaults, and creating instances of supporting classes like `Prompter`, `MemoryBank`, and `Grader`.\n\n### `reset_memory`\nThis method clears the current memory both in RAM and on disk. It's important for scenarios where starting fresh is needed.\n\n### `train_on_task`\nThis asynchronous method is responsible for training on a specific task. It continually assigns the task to the agent, learns from failures by creating insights, and stores these insights in the memory bank if useful.\n\n### `test_on_task`\nAnother asynchronous method that assigns a task multiple times while retrieving relevant memos from memory. The success of each trial is tracked, and a final success rate is reported.\n\n### `add_memo`\nThis method adds an insight to the memory bank, optionally tying it to a specific task. It also extracts relevant topics from the insight to facilitate future retrieval.\n\n### `add_task_solution_pair_to_memory`\nDesigned for storing a task-solution pair in memory. This is especially useful for entries that exemplify a method for solving similar tasks later.\n\n## Retrieval and Validation\n\n### `retrieve_relevant_memos`\nThis method retrieves memos that seem relevant to the current task. It conducts a quick validation for each memo before considering it relevant based on operational settings, particularly the `validate_memos` flag.\n\n### Connection to the Grader\n\nThe `Grader` class plays a supportive role in verifying the accuracy of responses generated by the task assignment. The methodologies within `MemoryController` engage this class to assess correctness by matching produced answers against expected outcomes.\n\n## Insights Management\n\n### `_iterate_on_task`\nThis internal method drives learning by repeatedly assigning tasks and analyzing failures. It aims to derive useful insights, which are stored if valuable.\n\n### `_test_for_failure`\nResponsible for determining if any failure exists in solving the assigned task. Executes multiple trials and provides detailed feedback whenever a response is deemed incorrect.\n\n## User Message Handling\n\n### `handle_user_message`\nIntroduces the final layer of interaction by processing user messages. It involves possible extraction of advice from the user's text and assigning necessary tasks, while also ensuring prior advice doesn't override existing memory.\n\n## Conclusion\n\n`MemoryController` provides a robust framework for managing task-related insights within an interactive learning environment. By smoothly integrating various components such as a memory bank and a grading system, it offers a comprehensive structure for adaptive learning and insight retrieval. Its focus on configurability and memory management makes it suitable for applications needing dynamic context handling and memory utilization.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/memory_controller.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as a central point for importing key classes and configurations from various components of a larger application. It provides a clean interface for external users or other modules to access these components seamlessly.\n\n## Imported Components\n\nThe module imports several classes and configurations, which can be summarized as follows:\n\n1. **Apprentice**: This class likely encapsulates functionalities related to an apprentice feature within the application, possibly managing learning processes or interactions.\n   \n2. **ApprenticeConfig**: A configuration class that likely provides settings or parameters specific to the `Apprentice`. This is used to customize its behavior or define initial states.\n\n3. **ChatCompletionClientRecorder**: A class that likely records interactions with a chat completion client. This can be useful for logging, debugging, or analytics purposes, ensuring that interactions with AI or automated responses are properly stored.\n\n4. **Grader**: This class likely plays a role in evaluating or grading submissions, possibly within a learning or assessment context. The specifics depend on how grading parameters are set up and function within the system.\n\n5. **PageLogger**: A logging class that probably handles the recording of page interactions or events, which can be especially important for tracking user activity within web or application pages.\n\n6. **PageLoggerConfig**: Similar to `ApprenticeConfig`, this class is likely used to define settings for the `PageLogger`, such as log levels or output formats.\n\n7. **Teachability**: This component likely assesses or manages a subject's readiness or capability to learn, possibly based on certain criteria or metrics.\n\n## Exported Components\n\nThe `__all__` variable is defined to specify the public interface of this module. By including the names of the classes and configurations in this list:\n\n- **Public Modules**: Only those components included in `__all__` will be available for import when the module is imported with a wildcard, e.g., `from module import *`. This encapsulation helps in maintaining a clean interface and prevents accidental usage of internal classes or functions that are not meant for public consumption.\n\n## Purpose and Use Cases\n\n### Learning and Evaluation\n\nThe primary intent of this module seems to be supporting functionalities related to learning and evaluation. The presence of classes such as `Apprentice` and `Grader` indicates that this module might be part of an educational technology platform, where the performance and learning rates of users are monitored and analyzed.\n\n### Interaction Logging\n\nThrough the inclusion of `ChatCompletionClientRecorder` and `PageLogger`, the module aims to support robust logging mechanisms, capturing interactions and events that might be vital for improving the user experience or debugging potential issues.\n\n### Configurability\n\nThe configurations (`ApprenticeConfig` and `PageLoggerConfig`) provide a layer of flexibility, allowing developers to tailor the behavior of the associated classes according to specific application needs or user requirements.\n\n### Integration-ready\n\nThis module is likely designed for easy integration into larger systems, where components can be imported individually as needed. The structured approach promotes reusability and modular programming practices.\n\n## Conclusion\n\nIn summary, this module acts as an aggregator for essential components related to an apprenticeship, evaluation, logging, and user interaction management. It supports various functionalities required for educational or assessment applications and serves as a clean interface for developers to extend its capabilities or integrate into other systems.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides an overview of the source code, detailing its structure, the purpose of its functions, and how it operates.\n\n## Overview\n\nThe provided code implements utility functions for handling message content and computing a hash representation of a directory's state. It leverages the `hashlib` and `os` libraries to perform hashing and directory traversal tasks respectively, while `autogen_core` offers functionality related to message handling in potentially a chatbot or automated system context. The code includes specific types for user and assistant content by utilizing type hinting.\n\n## Convenience Types\n\nThe code defines several convenience types using Python's `Union` and `List` constructs. These types help clarify the expected format of content:\n\n- `UserContent`: Represents either a string or a list of strings or images sent by the user.\n- `AssistantContent`: Consists of strings or a list of function calls made by the assistant.\n- `FunctionExecutionContent`: A list of results from function executions.\n- `SystemContent`: A simple string representing system messages.\n- `MessageContent`: A union of all the above types, encompassing the different types of messages that can be processed.\n\n## Function: `message_content_to_str`\n\n```python\ndef message_content_to_str(message_content: MessageContent | None) -> str:\n```\n\n### Purpose:\nThis function converts various types of message content into a single string format. \n\n### Operation:\n- If `message_content` is `None`, it returns an empty string.\n- If it is a basic string, it returns the string as is.\n- If it is a list, it iterates through the items:\n  - Strings are directly appended to a new list.\n  - Instances of the `Image` class are represented as `\"<Image>\"`.\n  - All other types are converted to strings and whitespace-trimmed.\n- Finally, it joins all items with a newline character and returns the resultant string.\n\n## Function: `text_from_user_content`\n\n```python\ndef text_from_user_content(user_content: UserContent) -> str:\n```\n\n### Purpose:\nThis function extracts and compiles text content from user-provided content, disregarding non-text items.\n\n### Operation:\n- If `user_content` is a string, it is returned directly.\n- If it is a list, the function constructs a new list of text strings by examining each item in the list:\n  - Text items are appended after stripping whitespace.\n- The function returns all text items joined by two newline characters, facilitating clear separation between blocks of text.\n\n## Function: `single_image_from_user_content`\n\n```python\ndef single_image_from_user_content(user_content: UserContent) -> Union[Image, None]:\n```\n\n### Purpose:\nThis function is designed to extract exactly one image from the user content, ensuring that only a single image is retrieved.\n\n### Operation:\n- If `user_content` is a string, the function returns `None`.\n- If it is a list, the function iterates through the items:\n  - An image, if found, is stored, and an assertion checks that no other image has been found yet.\n- If the function finds no images or more than one, it appropriately raises an error or returns the image found.\n\n## Function: `hash_directory`\n\n```python\ndef hash_directory(directory: str, hash_algo: str = \"sha256\") -> Tuple[str, int, int]:\n```\n\n### Purpose:\nThis function calculates a hash value representing the state of a specified directory, including both its internal structure and the content of its files.\n\n### Operation:\n- The function initializes a hashing object based on the specified algorithm (defaulting to SHA-256).\n- It declares counters for the number of files and subdirectories.\n- The function traverses the directory structure using `os.walk`, ensuring consistent ordering:\n  - It counts files and subdirectories as they are found.\n  - Names of directories and files are hashed for structure representation.\n  - It reads file contents in chunks and updates the hash with this data, while handling exceptions if any error occurs during file access.\n- Finally, it returns the computed hash value along with the counts of files and subdirectories.\n\n## Summary of Function Roles\n\n- **Message Conversion Functions**: `message_content_to_str`, `text_from_user_content`, and `single_image_from_user_content` are focused on processing and extracting content from user inputs that may include text and images.\n- **Directory Hashing Function**: `hash_directory` serves to generate a unique hash for a directory, thus facilitating efficient tracking of its state and integrity over time.\n\nThis code effectively combines content processing and directory hashing for applications likely in automated systems dealing with user interactions and file management.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/_functions.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Apprentice Class\n\n## Overview\n\nThe `Apprentice` class acts as a flexible interface for integrating an agent or team with a task-focused memory system. It leverages various components, such as a `ChatCompletionClient`, a memory controller, and an optional logging mechanism, to facilitate task assignments and responses based on user interactions.\n\n## Dependencies\n\nThe code begins by importing various libraries and modules necessary for its functionality, including `random`, `time`, and types. Critical components from the `autogen_agentchat` and `autogen_core` libraries are also imported, which handle agent operations and messaging.\n\n```python\nimport random\nimport time\nfrom typing import TYPE_CHECKING, Any, List, Sequence, Tuple, TypedDict\n# Modules from autogen agentchat\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage\nfrom autogen_core.models import ChatCompletionClient, LLMMessage, SystemMessage, UserMessage\nfrom .page_logger import PageLogger\n```\n\n## ApprenticeConfig TypedDict\n\nThe `ApprenticeConfig` class is defined as a `TypedDict`, designed to encapsulate configuration settings for the `Apprentice`. This structure helps in maintaining cleaner code changes when frequently altering settings, typically loaded from a YAML configuration file.\n\n### Configuration Options:\n- `name_of_agent_or_team`: A string indicating the target agent or team for handling tasks.\n- `disable_prefix_caching`: A boolean to determine whether to disable caching by prepending a random integer to the first message.\n- `MemoryController`: A nested configuration dictionary for the memory controller.\n\n## Constructor: `__init__`\n\nThe constructor initializes an `Apprentice` instance. It accepts the following parameters:\n\n- **client**: An instance of `ChatCompletionClient` to interact with the AI model.\n- **config**: An optional dictionary to override default settings.\n- **logger**: An optional logger that captures runtime information. If not provided, a default `PageLogger` is created.\n\n```python\ndef __init__(self, client: ChatCompletionClient, config: ApprenticeConfig | None = None, logger: PageLogger | None = None) -> None:\n```\n\n### Initialization Steps:\n- Creates an appropriate logger.\n- Applies configuration, particularly for the agent name, caching behavior, and memory controller settings.\n- Initializes the memory controller, which facilitates task assignments and memory management.\n\n## Memory Management Methods\n\n### 1. `reset_memory`\n\nThis method allows for the memory bank to be cleared, effectively resetting the state maintained by the `MemoryController`.\n\n```python\ndef reset_memory(self) -> None\n```\n\n### 2. `handle_user_message`\n\nHandles incoming user messages by funneling them to the memory controller. It returns the response generated for the user message.\n\n```python\nasync def handle_user_message(self, text: str, should_await: bool = True) -> str\n```\n\n### 3. `add_task_solution_pair_to_memory`\n\nThis method adds a task and its corresponding solution to memory for future retrieval, serving as a learning mechanism to associate tasks with effective solutions.\n\n```python\nasync def add_task_solution_pair_to_memory(self, task: str, solution: str) -> None\n```\n\n### 4. `assign_task`\n\nDirectly assigns a task to the designated agent or team while optionally using stored insights from memory.\n\n```python\nasync def assign_task(self, task: str, use_memory: bool = True, should_await: bool = True) -> str\n```\n\n### 5. `train_on_task`\n\nThis method is designed to facilitate learning by repeatedly assigning a task and using the expected answer to guide improvements in the agent's capabilities.\n\n```python\nasync def train_on_task(self, task: str, expected_answer: str) -> None\n```\n\n## Task Assignment Methods\n\n### 1. `assign_task_to_agent_or_team`\n\nThis method routes a task to the specific agent or team defined during initialization. It distinguishes the handling based on the name of the agent or team (either `AssistantAgent` or `MagenticOneGroupChat`) and ensures proper routing.\n\n```python\nasync def assign_task_to_agent_or_team(self, task: str) -> Tuple[str, str]\n```\n\n### 2. `_assign_task_to_assistant_agent`\n\nHandles the task assignment specifically to an `AssistantAgent`. This method generates a detailed system prompt, defines messages, and processes the task through the agent.\n\n```python\nasync def _assign_task_to_assistant_agent(self, task: str) -> Tuple[Any, Any]\n```\n\n### 3. `_assign_task_to_magentic_one`\n\nIn a similar fashion, this method deals with task assignments to the `MagenticOneGroupChat` team. It creates an instance of the necessary agents and orchestrates their response to the task, capturing and returning their collective output.\n\n```python\nasync def _assign_task_to_magentic_one(self, task: str) -> Tuple[str, str]\n```\n\n## Logging Mechanism\n\nThroughout the class methods, a `logger` is invoked to track function entry and exit points, along with important events, such as model calls and responses. This mechanism enhances the traceability and debugging capabilities of the class.\n\n### Logger Usage:\n- `self.logger.enter_function()`\n- `self.logger.leave_function()`\n- Logging detailed outputs: `self.logger.info()`, `self.logger.log_model_task()`\n\n## Conclusion\n\nThe `Apprentice` class serves as a powerful mediator between user messages, AI agents, and a task-driven memory framework. By encapsulating agent interactions within a controlled setting, the class allows for flexible task management and learning, making it a valuable component for applications leveraging conversational AI technologies.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/apprentice.py"
  },
  {
    "code": false,
    "content": "# ChatCompletionClientRecorder Documentation\n\n## Overview\n\nThe `ChatCompletionClientRecorder` class extends the functionality of the `ChatCompletionClient` to facilitate testing of language model interactions by allowing for both recording and replaying of messages and responses. This enables the streamlined creation of test cases where the exact input and expected output can be predefined and validated. The class supports two operational modes: **record** and **replay**.\n\n### Key Features\n\n- **Record Mode**: In this mode, the client records all messages and generated responses to a specified file for future use.\n- **Replay Mode**: This mode retrieves previously recorded messages and responses from disk and ensures that new messages match the recorded calls, returning the corresponding responses.\n- **Logging**: The client can log its operations to provide insights during recording and replaying sessions.\n\n## Class Definition\n\n### `ChatCompletionClientRecorder`\n\nThis class inherits from `ChatCompletionClient` and serves as a wrapper to implement recording and replaying. It provides detailed logging and error checking to ensure the integrity of the test sessions.\n\n### Constructor: `__init__`\n\nThe constructor initializes a new instance of the `ChatCompletionClientRecorder`. It takes several parameters:\n\n- **client**: An instance of `ChatCompletionClient`, which is the underlying client that will be used for recording or replaying.\n- **mode**: A string indicating the operational mode, either \u201crecord\u201d or \u201creplay\u201d.\n- **session_file_path**: The file path where the session data will be stored or retrieved.\n- **logger**: An optional logger instance to log activity (defaults to a new `PageLogger` if not provided).\n\nThe constructor also manages the initialization of recording or reading previous sessions based on the mode specified, with appropriate error handling during file operations.\n\n## Methods\n\n### `async create`\n\nThis method generates a response based on the provided messages, conditional on the mode:\n\n- **Record Mode**:\n  - Calls the base client's `create` method to get a response.\n  - Records the input messages and the response as a dictionary in the `records` list.\n  \n- **Replay Mode**:\n  - Checks that the current input messages match those that were previously recorded.\n  - Retrieves the pre-recorded response to return, and shifts the internal index to process the next record in the future.\n\n### `create_stream`\n\nThis method is a wrapper around the base client's `create_stream` functionality and does not participate in recording or replaying. It enables streaming responses when interacting with the underlying client.\n\n### `async close`\n\nThis method is intended to gracefully close the underlying client. It should be called upon the completion of using the client to ensure resources are released properly.\n\n### Usage Tracking Methods\n\n- **actual_usage**: Returns the actual usage statistics from the base client.\n- **total_usage**: Returns the cumulative usage statistics from the client.\n- **count_tokens**: Calculates the number of tokens present in the provided messages.\n- **remaining_tokens**: Computes the remaining tokens available for the given messages.\n\n### Properties\n\n- **capabilities**: This property returns the client's capabilities and issues a warning that it is deprecated in favor of the `model_info` property.\n- **model_info**: This property retrieves model-related information from the base client.\n\n### `finalize`\n\nThe `finalize` method is critical for managing the end state of the testing session:\n\n- **In Record Mode**:\n  - Saves the accumulated messages and responses to disk as a JSON file.\n  - Ensures the directory exists before attempting to write to disk.\n  \n- **In Replay Mode**:\n  - Checks that all recorded messages have been verified against the current input.\n  - Logs the completion status of the replay operation.\n\n## Error Handling\n\nThe implementation of `ChatCompletionClientRecorder` includes several points where exceptions may be raised due to file operations, mode mismatches, or input inconsistencies. Each error logs the pertinent details using the logger, ensuring that users are informed of what went wrong during the recording or replaying processes.\n\n## Conclusion\n\nThe `ChatCompletionClientRecorder` class is a powerful tool for developers aiming to test language model interactions in a controlled and reproducible manner. By extending the base client functionality with recording and replay capabilities, along with comprehensive logging, it supports robust testing strategies for applications that leverage language models. This capability is essential for debugging and validating code that interacts with language model APIs.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/chat_completion_client_recorder.py"
  },
  {
    "code": false,
    "content": "# Grader Class Documentation\n\n## Overview\nThe `Grader` class is designed to evaluate the performance of an AI \"apprentice\" on specific tasks through a series of tests. It provides functionality to log the testing process, assess the correctness of responses, and interact with a language model to generate responses and evaluate answers. The class operates asynchronously and relies on a dedicated model client for message processing.\n\n## Dependencies\nThe `Grader` class imports several modules and types:\n- **Image**: From `autogen_core`, likely used for handling image data.\n- **Message Types**: Includes `AssistantMessage`, `UserMessage`, `SystemMessage`, and others for structuring chat messages and responses.\n- **Logging**: `PageLogger` is used for logging activities within the class.\n- **Type Hinting**: Types like `List`, `Tuple`, and `Union` are used for parameter and return type annotations.\n\n## Initialization\n### `__init__`\n```python\ndef __init__(self, client: ChatCompletionClient, logger: PageLogger | None = None) -> None:\n```\n- The constructor takes two parameters:\n  - `client`: An instance of `ChatCompletionClient` used to interact with the language model.\n  - `logger`: An optional instance of `PageLogger` for logging operations. If not provided, a default logger is instantiated.\n- It initializes a chat history list to track previous messages exchanged with the model.\n\n## Testing Functionality\n### `test_apprentice`\n```python\nasync def test_apprentice(\n    self,\n    apprentice: Apprentice,\n    task_description: str,\n    expected_answer: str,\n    num_trials: int,\n    use_memory: bool,\n    client: ChatCompletionClient,\n) -> Tuple[int, int]:\n```\n- This asynchronous method evaluates the apprentice's performance over multiple trials by:\n  - Assigning a task described by `task_description`.\n  - Comparing each response to the `expected_answer`.\n  - Recording correct and incorrect responses.\n- Logging is performed at various stages to track the trial progress and success rate.\n- Returns the number of successful attempts and total trials as a tuple of integers.\n\n## Model Interaction\n### `call_model`\n```python\nasync def call_model(\n    self,\n    summary: str,\n    user_content: UserContent,\n    system_message_content: str | None = None,\n    keep_these_messages: bool = True,\n) -> str:\n```\n- This method interacts with the AI model to generate a response based on user input and chat history.\n- It prepares messages, including a system message (defaulting to a helpful assistant description if not provided).\n- The response received from the model is logged and the chat history is updated based on the `keep_these_messages` parameter.\n- Returns the model's response as a string.\n\n## Chat History Management\n### `_clear_history`\n```python\ndef _clear_history(self) -> None:\n```\n- A private method that clears the internal message history used for tracking interactions with the model, resetting it to an empty state.\n\n## Response Evaluation\n### `is_response_correct`\n```python\nasync def is_response_correct(\n    self, task_description: str, response_to_be_graded: str, correct_answer: str\n) -> Tuple[bool, str]:\n```\n- This method assesses whether a given response matches the correct answer for a specified task.\n- It prompts the model to extract an answer from the provided response, assesses this extraction, and then evaluates the correctness against the provided answer.\n- Detailed prompts guide the model in both extraction and correctness judgment, with results logged appropriately.\n- Returns a boolean indicating correctness and the extracted answer.\n\n## Conclusion\nThe `Grader` class effectively encapsulates the functionality required to test and evaluate the performance of an AI assistant. By systematically applying trial tests, logging the results, and interfacing with a language model, it serves as a robust mechanism for grading responses to various tasks. The architecture emphasizes maintainability and extensibility via asynchronous operation and structured logging.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/grader.py"
  },
  {
    "code": false,
    "content": "# Documentation for PageLogger Module\n\n## Overview\n\nThe `PageLogger` module is designed to facilitate logging of text and images into structured HTML pages. Each logging session generates a series of interlinked HTML pages that represent function/method calls, providing a visual call tree. This is highly useful for debugging and allows for better understanding of function flows and outputs.\n\n### Key Components\n\nThe central component is the `PageLogger` class, accompanied by the `Page`, and `PageStack` classes, which handle individual pages and the call stack, respectively. There are various utility functions and methods integrated to format the output effectively. \n\n## PageLogger Class\n\n### Initialization\n\nThe `PageLogger` class initializes logging with optional configuration parameters:\n\n- `level`: Specifies the logging level (DEBUG, INFO, WARNING, etc.).\n- `path`: The directory where log files will be saved.\n\nThe constructor applies these configurations, sets up necessary paths, and initializes internal structures needed for logging.\n\n### Logging Methods\n\nThe class contains several logging methods categorized by severity:\n- `debug(line: str)`: Logs a debug message.\n- `info(line: str)`: Logs an informational message.\n- `warning(line: str)`: Logs a warning message.\n- `error(line: str)`: Logs an error message.\n- `critical(line: str)`: Logs a critical message.\n\nEach method checks if the logging level is appropriate before proceeding to log the message.\n\n### Page Management\n\n#### Creating and Adding Pages\n\nThe `PageLogger` allows adding new pages using the `_add_page(...)` method. A summary is provided to describe the content, and the new page is added to the stack, creating a record of the function calls leading to it.\n\nTo enter and leave functions, the methods `enter_function()` and `leave_function()` are utilized. These automatically create or finalize pages when functions are called and exited, respectively.\n\n### Finalization and Flushing\n\nThe `finalize()` method is responsible for writing a hash of the log directory to a file for change detection. The `flush(finished: bool = False)` method writes the state of the log to disk, including generating an HTML call tree view.\n\n## Page Class\n\nThe `Page` class represents individual HTML pages in the logger's output. \n\n### Key Features\n\n- Initializes with attributes such as `index`, `summary`, and `indent_level`.\n- Manages its own content and handles flushing the page content to an HTML file through `flush()` method.\n\n### Adding Content\n\nThe `add_lines(lines: str, flush: bool = False)` method allows adding text to the page, with an optional immediate flush to save the content to disk.\n\n## PageStack Class\n\nThe `PageStack` class manages a stack of `Page` instances, tracking the call order of functions.\n\n### Stack Operations\n\n- `push(page: Page)`: Adds a page to the top of the stack.\n- `pop()`: Removes and returns the top page from the stack.\n- `top()`: Retrieves the top page without removing it.\n- `size()`: Returns the current number of pages in the stack.\n- `write_stack_to_page(page: Page)`: Logs an indented view of the current call stack into the specified page.\n\n## Utilities and Formatting\n\n### HTML Structure\n\nTwo utility functions, `_html_opening(file_title: str)` and `_html_closing()`, create basic HTML structure for the logs. The opening function includes optional meta tags for automatic refresh, while the closing function finalizes the HTML document.\n\n### Message Formatting\n\nSeveral static methods help format message content:\n- `_decorate_text(text: str, color: str, weight: str = \"bold\", demarcate: bool = False)`: Applies HTML styling for text appearance.\n- `_link_to_image(image_path: str, description: str)`: Generates an HTML link to render images in the logs.\n- `_format_message_content(message_content: MessageContent)`: Prepares message content for logging, converting various types into a string format.\n\n### Logging Message Content\n\nTwo main methods handle logging complex data structures:\n- `log_message_content(message_content: MessageContent, summary: str)`: Creates a new page and logs the message's content.\n- `log_dict_list(content: List[Mapping[str, Any]], summary: str)`: Logs a list of dictionaries into a new page.\n\n## Conclusion\n\nThe `PageLogger` module offers a comprehensive logging solution, facilitating the real-time logging of events alongside a visual representation of call flows through HTML pages. Its structured hierarchy simplifies the tracking of function calls and outputs, making it an exceptional tool for debugging and monitoring programs.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/page_logger.py"
  },
  {
    "code": false,
    "content": "# Documentation for `Teachability` Class\n\nThe `Teachability` class extends the `Memory` class to provide functionalities that allow an AssistantAgent to learn and adapt through user interactions. This class aims to make chatbot interactions more dynamic by enabling the assistant to utilize user-provided teachings effectively.\n\n## Overview\n\nThe `Teachability` class integrates tightly with a `MemoryController`, which manages memory operations such as storing, retrieving, and clearing memories. The main functionalities include extracting relevant information from user messages, updating the context of conversations, and querying past memories based on user inputs. \n\n### Usage Steps\n\nTo utilize the `Teachability` class, the following steps are typically followed:\n\n1. Create an instance of `MemoryController`.\n2. Instantiate the `Teachability` class while passing the `MemoryController` instance.\n3. Create an `AssistantAgent`, providing the `Teachability` instance wrapped in a list as its memory parameter.\n4. Engage the `AssistantAgent` in conversation as needed.\n\n## Class Initialization\n\n### `__init__`\n\n```python\ndef __init__(self, memory_controller: \"MemoryController\", name: str | None = None) -> None:\n```\n\n- **Parameters**: \n  - `memory_controller`: An instance of `MemoryController` that manages memory functions.\n  - `name`: Optional; a string to identify this instance of `Teachability`.\n  \n- **Purpose**: Initializes the `Teachability` instance, sets up a logger for logging purposes, and assigns a name to the memory instance.\n\n### Property\n\n#### `name`\n\n```python\n@property\ndef name(self) -> str:\n```\n\n- **Returns**: The identifier name of the memory instance.\n  \n- **Purpose**: Provides a way to access the name of the `Teachability` instance.\n\n## Core Functionalities\n\n### Text Extraction\n\n#### `_extract_text`\n\n```python\ndef _extract_text(self, content_item: str | MemoryContent) -> str:\n```\n\n- **Parameters**: \n  - `content_item`: Either a string or a `MemoryContent` object containing the content to be processed.\n\n- **Returns**: A string representation of the extractable text.\n\n- **Purpose**: This method extracts text from various content types while raising errors for unsupported types, ensuring that the AssistantAgent only works with valid and interpretable content.\n\n### Context Update\n\n#### `update_context`\n\n```python\nasync def update_context(self, model_context: ChatCompletionContext) -> UpdateContextResult:\n```\n\n- **Parameters**: \n  - `model_context`: An instance of `ChatCompletionContext`.\n\n- **Returns**: An `UpdateContextResult` object containing relevant memories.\n\n- **Purpose**: This method processes the last user message to find advice or relevant memories. It updates the model context with these details, enabling the AssistantAgent to provide context-aware responses.\n\n### Adding Memories\n\n#### `add`\n\n```python\nasync def add(self, content: MemoryContent, cancellation_token: CancellationToken | None = None) -> None:\n```\n\n- **Parameters**: \n  - `content`: A `MemoryContent` instance.\n  - `cancellation_token`: An optional cancellation token that can be used to cancel the operation.\n\n- **Purpose**: This method adds content to memory by extracting text and notifying the memory controller for potential storage of user advice or teachings.\n\n### Querying Memories\n\n#### `query`\n\n```python\nasync def query(self, query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) -> MemoryQueryResult:\n```\n\n- **Parameters**: \n  - `query`: A string or `MemoryContent` to search for relevant memories.\n  - `cancellation_token`: Optional token for canceling the operation.\n  - `**kwargs`: Additional optional parameters.\n\n- **Returns**: A `MemoryQueryResult` containing relevant memory contents.\n\n- **Purpose**: This method queries the memory for insights that are relevant to a user-provided query. It utilizes the `MemoryController` to filter and gather any relevant memories.\n\n### Memory Management\n\n#### `clear`\n\n```python\nasync def clear(self) -> None:\n```\n\n- **Purpose**: Clears all entries from the memory, effectively resetting the memory state.\n\n#### `close`\n\n```python\nasync def close(self) -> None:\n```\n\n- **Purpose**: Provides a cleanup function. Currently, it does not perform any actual cleanup operations for this specific implementation.\n\n## Conclusion\n\nThe `Teachability` class serves as a vital component in enhancing the interactive capabilities of an AssistantAgent by allowing it to learn and adapt through user interactions. By leveraging the functionalities of an underlying `MemoryController`, it efficiently manages memory, updates contextual awareness, and engages users with relevant advice. This design promotes a more personalized and responsive user experience in conversational AI applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/experimental/task_centric_memory/utils/teachability.py"
  },
  {
    "code": false,
    "content": "It seems there is no code provided for analysis. Please share the source code you'd like me to analyze, and I'll be happy to generate a high-level description along with any necessary documentation.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for Text Canvas\n\nThis module appears to be part of a package that implements functionality related to text manipulation or rendering through a canvas-like structure. It imports two components, `TextCanvas` and `TextCanvasMemory`, from other modules and exports them for use in other parts of the package.\n\n## Overview\n\nThe primary purpose of this module is to provide a clear interface for utilizing two classes, `TextCanvas` and `TextCanvasMemory`. By defining an `__all__` list, it specifies which components should be publicly accessible when the module is imported. This is part of a broader design pattern in Python that helps control the module's namespace and what is exposed to users.\n\n## Class Descriptions\n\n### TextCanvas\n\n- **Purpose**: \n  The `TextCanvas` class likely serves as a primary handler for creating, modifying, or displaying text on a canvas. A canvas can typically be thought of as a drawable area where text and other graphical elements can be rendered.\n  \n- **Role**: \n  This class may provide methods for rendering text, adjusting styles (such as font size, color, etc.), and positioning text within the canvas. It could also handle events related to user interaction if it includes graphical user interface elements.\n\n### TextCanvasMemory\n\n- **Purpose**: \n  The `TextCanvasMemory` class appears to manage the memory aspect of the `TextCanvas` functionality. This could include storing previous versions of text, caching rendered text to improve performance, or efficiently managing redraws for interactive applications.\n  \n- **Role**: \n  This class likely interacts with `TextCanvas` to ensure that any text being displayed can be quickly accessed or reverted to a previous state. It may implement efficient data structures to handle these tasks and ensure that memory usage is optimized during text operations.\n\n## Module Structure\n\nThe module relies on relative imports, indicating that `TextCanvas` and `TextCanvasMemory` are located within the same package as this module. This organization promotes modularity and makes it easier to maintain and extend each component independently without affecting other parts of the code base.\n\n### Exporting Components\n\nBy using the `__all__` list, the module defines a public API that states only `TextCanvas` and `TextCanvasMemory` are to be exposed when the module is imported. This is an important practice for encapsulation, allowing developers to maintain control over what parts of the codebase are accessible to others. This helps prevent misuse of internal components that are not designed for public consumption.\n\n### Usage Context\n\nThe module's classes appear to be designed for applications that require text rendering, such as games or graphical applications where text needs to be dynamically displayed. Potential use cases include displaying scores, messages, or overlaid information on visual backgrounds. The efficient handling and memory management signify that the module might be aimed at performance-critical applications, especially those that require real-time updates.\n\n## Conclusion\n\nThis module serves as a connector between the user and two integral classes that manage text rendering on a canvas. By encapsulating the functionality in `TextCanvas` and `TextCanvasMemory`, it allows for a clean and organized approach to handling text graphics. The modular and encapsulated design ensures maintainability and usability, making it a foundational part of the larger application that relies on text rendering capabilities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/canvas/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for the `BaseCanvas` Abstract Class\n\n## Overview\nThe `BaseCanvas` class serves as an abstract base class for a \"canvas\" that allows tracking of revisions for various file types, including text files, images, and structured data. As an abstract base class, it establishes a protocol that any concrete implementation must follow. This class is designed for use in systems where maintaining a revision history and file management is essential.\n\n**Warning**: This API is experimental and may undergo changes in future iterations.\n\n## Class Definition\n\n### `BaseCanvas`\n- **Type**: Abstract Base Class (ABC)\n- **Purpose**: To define a contract for subclasses that will implement functionality related to file storage and versioning.\n\n### Key Characteristics\n- Inherits from `ABC`, which allows the definition of abstract methods that must be implemented by any subclasses.\n- Contains several abstract methods that enforce the structure of derived classes. Each method is designed to perform specific tasks related to file revision management.\n\n## Abstract Methods\n\n### `list_files() -> Dict[str, int]`\n- **Purpose**: This method is designed to provide a list of files along with their latest revision numbers.\n- **Return Type**: A dictionary that maps filenames (as strings) to their latest revision number (as integers).\n- **Usage**: Subclasses must implement this method to return an overview of all files being tracked.\n\n### `get_latest_content(filename: str) -> Union[str, bytes, Any]`\n- **Purpose**: To fetch the latest content of a specified file based on its name.\n- **Parameters**: \n  - `filename`: A string representing the name of the file.\n- **Return Type**: The latest content of the file, which may be in different formats such as string, bytes, or any other type indicated by `Any`.\n- **Usage**: Implementing subclasses should provide the logic to retrieve the latest version of a file's content.\n\n### `add_or_update_file(filename: str, new_content: Union[str, bytes, Any]) -> None`\n- **Purpose**: This method is used to either create a new file or update the content of an existing file with a new revision.\n- **Parameters**:\n  - `filename`: A string representing the name of the file to be added or updated.\n  - `new_content`: The new content to be saved, which can be a string, bytes, or other formats.\n- **Return Type**: None (void function).\n- **Usage**: Subclasses are required to implement this method to ensure content can be saved or revised.\n\n### `get_diff(filename: str, from_revision: int, to_revision: int) -> str`\n- **Purpose**: This method is intended to compute and return the differences between two revisions of a file.\n- **Parameters**:\n  - `filename`: A string naming the file for which the difference is to be calculated.\n  - `from_revision`: An integer representing the starting revision number.\n  - `to_revision`: An integer representing the ending revision number.\n- **Return Type**: A string that describes the differences between the specified revisions.\n- **Usage**: Concrete implementations must provide the functionality to calculate and format differences appropriately.\n\n### `apply_patch(filename: str, patch_data: Union[str, bytes, Any]) -> None`\n- **Purpose**: To apply a provided patch to the latest revision of a file, modifying its content and updating the revision history.\n- **Parameters**:\n  - `filename`: A string for the name of the file to be modified.\n  - `patch_data`: The patch or difference data to be applied, which can be in various types (string, bytes, etc.).\n- **Return Type**: None (void function).\n- **Usage**: Implementing subclasses must execute the patch application logic and handle the revision increment.\n\n## Usage\nConcrete subclasses of `BaseCanvas` should implement all the abstract methods to provide specific behavior related to the management of file revisions. This class can be beneficial for any application involving version control, data management, or collaborative projects involving multiple types of file formats. It forms a foundational backbone for any system intending to provide robust file handling capabilities with revision tracking.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/canvas/_canvas.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document provides a high-level description of the Python code that defines tools to interact with a text canvas feature, allowing for file updates and patch applications. The code utilizes object-oriented programming principles, and `pydantic` for model validation, creating clear and structured argument and result types. \n\n## Imports\n\nThe code begins by importing necessary components:\n- **Cancellation Token**: Import from `autogen_core`, which likely manages task cancellation.\n- **BaseTool**: A base class from `autogen_core.tools` for creating tools.\n- **BaseModel**: The foundational model class from `pydantic`, providing data validation.\n- **TextCanvas**: A custom class imported from a local module, presumably managing a canvas of text files.\n\n## Data Models\n\n### UpdateFileArgs\n\n```python\nclass UpdateFileArgs(BaseModel):\n    filename: str\n    new_content: str\n```\n\nThis class represents the arguments necessary to update or create a file on the canvas. It contains:\n- `filename`: The name of the file to be created or updated.\n- `new_content`: The content that will overwrite the existing file or be used to create the new one.\n\n### UpdateFileResult\n\n```python\nclass UpdateFileResult(BaseModel):\n    status: str\n```\n\nThis model encapsulates the result of the update operation, providing:\n- `status`: A string indicating the success or failure of the update operation (e.g., \"OK\").\n\n## UpdateFileTool Class\n\n```python\nclass UpdateFileTool(BaseTool[UpdateFileArgs, UpdateFileResult]):\n    ...\n```\n\n### Purpose\n\n`UpdateFileTool` is a specialized tool that inherits from `BaseTool`, providing functionality to create or update a file in the `TextCanvas`.\n\n### Constructor\n\n- **Parameters**: Accepts a `canvas` object of type `TextCanvas`.\n- **Initialization**: Calls the parent constructor with:\n  - `args_type`: Set to `UpdateFileArgs` to specify input arguments.\n  - `return_type`: Set to `UpdateFileResult` to specify output results.\n  - `name`: Identifies the tool name as \"update_file\".\n  - `description`: Provides a description of the tool's functionality.\n\n### Async Run Method\n\n```python\nasync def run(self, args: UpdateFileArgs, cancellation_token: CancellationToken) -> UpdateFileResult:\n```\n\n- This asynchronous method performs the file update operation.\n- **Parameters**:\n  - `args`: An instance of `UpdateFileArgs`, containing filename and new content.\n  - `cancellation_token`: Used to handle task cancellation.\n- The method interacts with the `TextCanvas` to add or update the file with the provided content and returns an `UpdateFileResult` to indicate success.\n\n## Data Models for Patch Application\n\n### ApplyPatchArgs\n\n```python\nclass ApplyPatchArgs(BaseModel):\n    filename: str\n    patch_text: str\n```\n\nThis model defines the arguments needed to apply a patch to an existing file. It contains:\n- `filename`: The name of the file to which the patch will be applied.\n- `patch_text`: The text of the patch formatted as a unified diff.\n\n### ApplyPatchResult\n\n```python\nclass ApplyPatchResult(BaseModel):\n    status: str\n```\n\nSimilar to `UpdateFileResult`, this class contains:\n- `status`: A string that signifies the result of the patch application (e.g., \"PATCH APPLIED\").\n\n## ApplyPatchTool Class\n\n```python\nclass ApplyPatchTool(BaseTool[ApplyPatchArgs, ApplyPatchResult]):\n    ...\n```\n\n### Purpose\n\n`ApplyPatchTool` serves as a tool to apply unified diff patches to files on the `TextCanvas`.\n\n### Constructor\n\n- **Parameters**: Accepts a `canvas` parameter of type `TextCanvas`.\n- **Initialization**: Calls the parent constructor with:\n  - `args_type`: Set to `ApplyPatchArgs` for input arguments.\n  - `return_type`: Set to `ApplyPatchResult` for output results.\n  - `name`: Identifies the tool name as \"apply_patch\".\n  - `description`: Provides a description indicating that the diff/patch format is required.\n\n### Async Run Method\n\n```python\nasync def run(self, args: ApplyPatchArgs, cancellation_token: CancellationToken) -> ApplyPatchResult:\n```\n\n- Similar to the `UpdateFileTool`, this asynchronous method applies a patch to the specified file.\n- **Parameters**:\n  - `args`: An instance of `ApplyPatchArgs`, containing the filename and patch text.\n  - `cancellation_token`: For cancellation purposes during the operation.\n- The method invokes the `apply_patch` method on the `TextCanvas`, then returns an `ApplyPatchResult` indicating the status of the patch application.\n\n## Conclusion\n\nIn summary, the code defines two primary tools for a text canvas application \u2014 `UpdateFileTool` for updating or creating files and `ApplyPatchTool` for applying patches to existing files. The structured use of data models allows for clear input and output specifications, enhancing the usability and maintainability of the code.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/canvas/_canvas_writer.py"
  },
  {
    "code": false,
    "content": "# Documentation for the TextCanvas Module\n\nThis module provides an in-memory representation of files, enabling full version control features, including patch application, content retrieval for different revisions, and the generation of diffs between revisions. It leverages the `difflib` library for diff operations and offers an extensible structure for managing file versions.\n\n## Overview\n\n### Key Classes and Their Purpose\n\n- **FileRevision**: This class tracks individual file versions, storing the content of the file as well as a revision identifier (which could be an integer, timestamp, or git hash). It allows the `TextCanvas` to maintain multiple versions of a file.\n  \n- **TextCanvas**: This is the main class that extends `BaseCanvas`. It allows CRUD-like operations on text files with added functionality for revision control. `TextCanvas` maintains a dictionary that maps filenames to lists of `FileRevision` objects, preserving the order of revisions.\n\n### Key Features\n\nThe `TextCanvas` class introduces several pivotal features intended for managing file revisions:\n\n1. **apply_patch**: This method uses the `unidiff` library to apply patches accurately to file contents, ensuring that changes maintain context and integrity.\n2. **get_revision_content**: This feature allows access to the exact content of any given revision of a file.\n3. **get_revision_diffs**: Returns a chronological list of differences between all revisions of a file, useful for auditing changes.\n\n## Class Construction\n\n### FileRevision Class\n\n```python\nclass FileRevision:\n    ...\n    def __init__(self, content: str, revision: int) -> None:\n        ...\n```\n\n- **Attributes**:\n  - `content`: Stores the text content of the file.\n  - `revision`: Represents the version identifier.\n\n### TextCanvas Class\n\n```python\nclass TextCanvas(BaseCanvas):\n    ...\n    def __init__(self) -> None:\n        ...\n```\n\n#### Initialization\n\n- The `TextCanvas` is initialized with an empty dictionary named `_files`, which maps each filename to a list of its revisions. The last element in the list represents the most recent revision.\n\n## Revision Management and Utility Methods\n\n### Internal Utilities\n\n- **_latest_idx**: This private method retrieves the index of the latest revision for a file, helping in quick access to the most recent content.\n  \n- **_ensure_file**: This utility checks whether a specified file exists within the canvas, raising an error if it does not.\n\n### Revision Inspection Methods\n\n- **get_revision_content(filename: str, revision: int) -> str**: Fetches the content of a specific revision. If not found, returns an empty string.\n\n- **get_revision_diffs(filename: str) -> List[str]**: Generates a list of unified diffs, representing changes from one revision to the next. It employs `difflib` to obtain the diffs in a standard format.\n\n## File Operations\n\n### Listing and Fetching\n\n- **list_files() -> Dict[str, int]**: Returns a mapping of each file's name to its latest revision number.\n  \n- **get_latest_content(filename: str) -> str**: Retrieves the most recent content stored for the specified file.\n\n### File Modification\n\n- **add_or_update_file(filename: str, new_content: Union[str, bytes, Any]) -> None**: This method allows either the creation of a new file or updating an existing file with new content. It creates a new `FileRevision` entry in the list associated with the filename.\n\n- **get_diff(filename: str, from_revision: int, to_revision: int) -> str**: Computes the unified diff between two specified revisions, making use of the `difflib` library.\n\n- **apply_patch(filename: str, patch_data: Union[str, bytes, Any]) -> None**: Applies a specified patch to the latest revision of the given file. This method checks for the presence of the `unidiff` library and processes the patch, modifying the file's content and saving it as a new revision.\n\n## Additional Functionalities\n\n- **get_all_contents_for_context() -> str**: Returns a summary of all files in the canvas along with their latest revisions and content. This facilitates a quick overview of the current state of all files managed by the `TextCanvas`.\n\n## Conclusion\n\nThe `TextCanvas` class effectively abstracts and manages multiple revisions of text files. It maintains versioning history, enables straightforward modifications, and provides rich utilities for inspecting and applying changes, making it suited for applications requiring file version control. The use of `difflib` and optional `unidiff` enhances its capabilities significantly, offering robust tools for developers working with text file revisions.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/canvas/_text_canvas.py"
  },
  {
    "code": false,
    "content": "# TextCanvasMemory Documentation\n\n## Overview\n\n`TextCanvasMemory` is a specialized implementation of the `Memory` class designed to manage file-like content using a canvas interface. This tool is particularly useful in scenarios where agents need to read and write content over multiple interactions, such as collaborative document editing or maintaining persistent state across conversation turns. It assists agents by automatically incorporating the current canvas state into their context before each inference.\n\n### Key Features\n\n- **Persistent Storage**: Provides a mechanism to retain and modify documents through multiple interactions.\n- **Collaboration**: Supports collaborative editing, allowing multiple agents to work on the same document.\n- **Large Document Handling**: Facilitates working with content that exceeds the size limits of a single message.\n- **File Operations**: Includes functionalities to create and update files, as well as apply patches to existing content.\n\n## Class Definition\n\n### `TextCanvasMemory`\n\nThe `TextCanvasMemory` class inherits from `Memory`. It is the primary interface through which content is managed using a `TextCanvas`. Upon each turn of conversation, it injects the updated state of the files in the canvas into the model context.\n\n#### Constructor\n\n```python\ndef __init__(self, canvas: Optional[TextCanvas] = None):\n```\n\n- **Parameters**:\n  - `canvas`: An optional `TextCanvas` instance. If not provided, a new instance is created.\n  \n#### Example Usage\n\nThe following sections illustrate how to use `TextCanvasMemory` effectively, particularly by showing its integration with agent models.\n\n### Example: Single Agent Usage\n\nThis example demonstrates how to employ `TextCanvasMemory` with a single `AssistantAgent` tasked with writing a story.\n\n```python\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Create a canvas memory\n    text_canvas_memory = TextCanvasMemory()\n\n    # Get tools for working with the canvas\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create an agent with the canvas memory and tools\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates and updates stories.\",\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Send a message to the agent\n    await writer_agent.on_messages(\n        [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")],\n        CancellationToken(),\n    )\n\n    # Retrieve the content from the canvas\n    story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(\"Story content from canvas:\")\n    print(story_content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Example: Collaborative Agents\n\nThis example illustrates the use of `TextCanvasMemory` allowing multiple agents to contribute to a shared document collaboratively.\n\n```python\nasync def main():\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Create the shared canvas memory\n    text_canvas_memory = TextCanvasMemory()\n\n    # Create agents for writing and critique\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates stories.\",\n        memory=[text_canvas_memory],\n    )\n\n    critique_agent = AssistantAgent(\n        name=\"Critique\",\n        model_client=model_client,\n        description=\"A critique agent that provides feedback on stories.\",\n        memory=[text_canvas_memory],\n    )\n\n    team = RoundRobinGroupChat(\n        participants=[writer_agent, critique_agent],\n        max_turns=10,\n    )\n\n    await team.run(task=\"Create a children's book about a bunny and a sunflower\")\n    story = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(story)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Method Details\n\n### `update_context`\n\n```python\nasync def update_context(self, model_context: ChatCompletionContext) -> UpdateContextResult:\n```\n\nThis method is responsible for injecting the current state of the canvas into the model context, formatted as a system message. If there is content to update, it adds a message containing the canvas summary.\n\n- **Returns**: An `UpdateContextResult` containing either the updated memory content or an empty result if there was no content to update.\n\n### `query`\n\n```python\nasync def query(self, query: str | MemoryContent, cancellation_token: Optional[CancellationToken] = None, **kwargs: Any) -> MemoryQueryResult:\n```\n\nThis method is designed to search for content within the canvas, although in this implementation, it returns empty results, indicating that the querying functionality hasn't been fully implemented yet.\n\n### `add`\n\n```python\nasync def add(self, content: MemoryContent, cancellation_token: Optional[CancellationToken] = None) -> None:\n```\n\nThe `add` method is intended to manage updates to the canvas. However, it currently doesn't perform any operations as the actual changes are delegated to the associated tools.\n\n### `clear`\n\n```python\nasync def clear(self) -> None:\n```\n\nClears the canvas by replacing it with a new empty instance of `TextCanvas`.\n\n### `close`\n\n```python\nasync def close(self) -> None:\n```\n\nThis method is a placeholder for cleanup operations. Currently, it doesn't perform any actions.\n\n### Tool Retrieval Methods\n\n#### `get_update_file_tool`\n\n```python\ndef get_update_file_tool(self) -> UpdateFileTool:\n```\n\nReturns an `UpdateFileTool` instance, facilitating file updates within the memory's canvas.\n\n#### `get_apply_patch_tool`\n\n```python\ndef get_apply_patch_tool(self) -> ApplyPatchTool:\n```\n\nReturns an `ApplyPatchTool` instance, allowing for the application of patches to existing files in the canvas.\n\n## Conclusion\n\n`TextCanvasMemory` serves as a robust and flexible memory solution for agents in collaborative and document-focused contexts. Its design facilitates persistence, allows for extensive content manipulation, and supports real-time updates across multiple interactions. The provided examples showcase its potential in both individual and collaborative settings, emphasizing the features that make document creation and editing seamless in AI-driven applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/canvas/_text_canvas_memory.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for ChromaDB Integration\n\nThis module is a part of a larger package that focuses on providing configurations and functionalities related to a vector memory database, specifically for use with ChromaDB. The module seems to be tailored for applications in machine learning and natural language processing, where embeddings are used for data representation.\n\n## Imports and Configurations\n\nThe module imports a series of configuration classes from a module named `_chroma_configs`. Each imported configuration serves a particular purpose, allowing customization and flexibility when setting up vector memory instances.\n\n```python\nfrom ._chroma_configs import (\n    ChromaDBVectorMemoryConfig,\n    CustomEmbeddingFunctionConfig,\n    DefaultEmbeddingFunctionConfig,\n    HttpChromaDBVectorMemoryConfig,\n    OpenAIEmbeddingFunctionConfig,\n    PersistentChromaDBVectorMemoryConfig,\n    SentenceTransformerEmbeddingFunctionConfig,\n)\n```\n\n### Description of Imported Classes\n\n- **ChromaDBVectorMemoryConfig**: A configuration class likely used to manage parameters for connecting to or using the ChromaDB vector memory database.\n- **CustomEmbeddingFunctionConfig**: Permits the user to incorporate a user-defined embedding function, allowing for flexibility beyond predefined options.\n- **DefaultEmbeddingFunctionConfig**: Provides the most basic embedding function which can be used if no custom function is specified.\n- **HttpChromaDBVectorMemoryConfig**: Configures the client to interact with a ChromaDB vector memory instance over HTTP, indicating that the database may be remote or cloud-based.\n- **OpenAIEmbeddingFunctionConfig**: Specifically designed to use OpenAI's embedding functions, which can facilitate a range of applications, including semantic search and text analysis.\n- **PersistentChromaDBVectorMemoryConfig**: This configuration might be used for setting up a vector memory that maintains data across sessions, emphasizing persistence in storage.\n- **SentenceTransformerEmbeddingFunctionConfig**: Allows use of SentenceTransformer models for embedding sentences into the vector space, which is useful for tasks such as semantic similarity and clustering.\n\n## Core Class\n\nThe module also imports a core class that is likely to handle the main functionality of the vector memory operations.\n\n```python\nfrom ._chromadb import ChromaDBVectorMemory\n```\n\n- **ChromaDBVectorMemory**: This class is presumably the primary interface for interacting with the ChromaDB vector memory. It likely contains methods to perform actions such as adding, retrieving, or deleting vector entries and managing interaction with the configured embedding functions.\n\n## Public API Exports\n\nAt the end of the module, a list named `__all__` is defined. This list specifies which classes and configurations can be imported when the module is called with a wildcard import (`from module import *`).\n\n```python\n__all__ = [\n    \"ChromaDBVectorMemory\",\n    \"ChromaDBVectorMemoryConfig\",\n    \"PersistentChromaDBVectorMemoryConfig\",\n    \"HttpChromaDBVectorMemoryConfig\",\n    \"DefaultEmbeddingFunctionConfig\",\n    \"SentenceTransformerEmbeddingFunctionConfig\",\n    \"OpenAIEmbeddingFunctionConfig\",\n    \"CustomEmbeddingFunctionConfig\",\n]\n```\n\n### Purpose of the `__all__` List\n\nThe inclusion of these classes in the `__all__` list suggests that they are the primary focus of this module. Users of this module are encouraged to use these classes for their implementations, providing a clear contract of the module's capabilities and intended usage.\n\n## Conclusion\n\nThis module encapsulates a variety of configurations and functionalities necessary for effectively working with ChromaDB's vector memory. With specific classes designed for different embedding methods and connection specifications, coupled with a main class for database interaction, it is positioned to facilitate complex machine learning tasks requiring efficient vector representation and retrieval. The structure allows users to easily customize their implementations based on specific requirements, making it a versatile tool in the realm of vector databases.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/chromadb/__init__.py"
  },
  {
    "code": false,
    "content": "# ChromaDB Vector Memory Configuration Documentation\n\nThis module provides various configuration classes for integrating and utilizing ChromaDB's vector memory. The configurations allow users to specify different embedding functions and connection types, facilitating customized memory management for vector-based data.\n\n## Overview\n\nChromaDB is designed to handle vector embeddings efficiently, and this module defines configurations that allow users to customize how embeddings are generated and how the ChromaDB client interacts with the database. The choices include default embeddings, SentenceTransformer models, OpenAI's embedding API, and even user-defined custom functions. \n\n### Key Classes and Structure\n\nThe classes in this module are structured around Pydantic\u2019s `BaseModel`, which grants data validation, parsing, and serialization capabilities. This ensures that the configuration settings are both type-checked and easy to manage.\n\n1. **Embedding Function Configurations**\n   - Various classes define how embeddings should be configured based on the source of generation.\n   \n   - **DefaultEmbeddingFunctionConfig**: Represents the default embedding function provided by ChromaDB using Sentence Transformers' model. It is a simple configuration with a fixed function type.\n\n   - **SentenceTransformerEmbeddingFunctionConfig**: Allows users to specify a custom SentenceTransformer model for generating embeddings. Users can pass the specific model name they wish to utilize, defaulting to \"all-MiniLM-L6-v2\".\n\n   - **OpenAIEmbeddingFunctionConfig**: This class is for integrating OpenAI's embedding capabilities. It requires an API key for authentication and provides a model name that defaults to \"text-embedding-ada-002\".\n\n   - **CustomEmbeddingFunctionConfig**: Enables users to provide their own embedding function, which needs to return a compatible embedding function. This offers significant flexibility, albeit with the caveat that configurations including custom functions cannot be easily serialized.\n\n2. **Union Type for Embedding Functions**\n   - **EmbeddingFunctionConfig**: This is a tagged union type that encapsulates all possible embedding function configurations. It utilizes `Annotated` from `typing_extensions` to maintain clear type distinctions based on the `function_type` attribute.\n\n### ChromaDB Vector Memory Configuration Classes\n\n- **ChromaDBVectorMemoryConfig**: This is the primary configuration class for all types of ChromaDB memory implementations. It includes essential parameters like `client_type`, `collection_name`, and various metrics and thresholds for similarity searches. Notably, it integrates the `EmbeddingFunctionConfig`, allowing dynamic selection of the embedding function.\n\n- **PersistentChromaDBVectorMemoryConfig**: Inherits from `ChromaDBVectorMemoryConfig` and customizes it for persistent storage scenarios. It introduces an additional parameter, `persistence_path`, indicating where the data should be stored on the filesystem. This setup is ideal for applications requiring robust data persistence.\n\n- **HttpChromaDBVectorMemoryConfig**: This configuration is tailored for scenarios where the ChromaDB client operates via HTTP requests. Parameters like `host`, `port`, `ssl`, and optional `headers` are included to facilitate connection to a remote ChromaDB service. This is useful for cloud deployments or when accessing distributed vector databases.\n\n### Summary of Key Features and Functionalities\n\n- **Type Safety**: All configuration classes use Pydantic's features to ensure valid configurations.\n- **Flexibility**: Different embedding functions and client configurations enable users to adapt the memory handling to their specific use cases.\n- **Serialization**: Most configurations are designed to be easily serialized, aside from those involving custom functions, ensuring ease of deployment and persistent storage.\n- **Version Control**: Each class documents its version updates, providing clarity on feature availability and changes over time.\n\n## Usage Examples\n\nHere are a couple of usage examples for the configuration classes defined in this module:\n\n### Default Configuration Example\n```python\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemoryConfig\n\n# Default memory configuration with the default embedding function\ndefault_config = ChromaDBVectorMemoryConfig(\n    collection_name=\"my_collection\",\n    distance_metric=\"euclid\",\n    k=5,\n)\n```\n\n### OpenAI Configuration Example\n```python\nfrom autogen_ext.memory.chromadb import OpenAIEmbeddingFunctionConfig\n\n# Configuring OpenAI embedding function\nopenai_config = OpenAIEmbeddingFunctionConfig(\n    api_key=\"sk-...\",\n    model_name=\"text-embedding-3-small\"\n)\n```\n\n### Persistent Configuration Example\n```python\nfrom autogen_ext.memory.chromadb import PersistentChromaDBVectorMemoryConfig\n\n# Persistent memory configuration\npersistent_config = PersistentChromaDBVectorMemoryConfig(\n    collection_name=\"persistent_store\",\n    persistence_path=\"./persistent_storage\"\n)\n```\n\nOverall, this module serves as a critical foundation for managing embeddings and vector memory within applications that utilize ChromaDB, ensuring flexibility and ease of integration through well-defined configurations.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/chromadb/_chroma_configs.py"
  },
  {
    "code": false,
    "content": "# ChromaDB Vector Memory Documentation\n\n## Overview\n\nThe `ChromaDBVectorMemory` class provides a memory storage and retrieval mechanism using vector similarity search powered by ChromaDB. This implementation allows agents to recall contextual information during interactions by utilizing vector embeddings to find semantically relevant content. It supports both local and remote storage configurations and allows for the use of various embedding functions to suit different use cases.\n\n## Key Features\n\n- **Vector Similarity Search**: Leverages ChromaDB's capability to retrieve data based on semantic similarity.\n- **Multiple Configuration Types**: Supports local storage via `PersistentChromaDBVectorMemoryConfig` and remote storage using `HttpChromaDBVectorMemoryConfig`.\n- **Customizable Embedding**: Users can specify embedding functions through various configuration types, including options for using OpenAI models or Sentence Transformers.\n- **Contextual Memory Updates**: Capable of updating the conversation context based on memory query results, enhancing interaction relevance.\n\n## Class Structure and Methods\n\n### `ChromaDBVectorMemory`\n\n#### Initialization\n```python\ndef __init__(self, config: ChromaDBVectorMemoryConfig | None = None) -> None:\n```\nThe constructor initializes the `ChromaDBVectorMemory` instance with a given configuration. If no configuration is provided, it defaults to `PersistentChromaDBVectorMemoryConfig` with default settings. Attributes for the ChromaDB client and collection are also initialized here.\n\n#### Properties\n\n- **`collection_name`**: \n  This property retrieves the name of the ChromaDB collection, facilitating namespace management.\n\n### Helper Methods\n\n#### _create_embedding_function\n```python\ndef _create_embedding_function(self) -> Any:\n```\nThis method creates an embedding function based on the provided configuration. It raises exceptions if the embedding function type is unsupported or if dependencies are not installed. Supported configurations include default, Sentence Transformers, OpenAI embeddings, and custom embedding functions.\n\n#### _ensure_initialized\n```python\ndef _ensure_initialized(self) -> None:\n```\nEnsures that both the ChromaDB client and collection are initialized properly. If either component is uninitialized, it attempts to create them based on the provided configuration.\n\n#### _extract_text\n```python\ndef _extract_text(self, content_item: str | MemoryContent) -> str:\n```\nExtracts text from incoming content. It supports extraction from string, JSON, or various memory content types while raising errors for unsupported formats like images or incompatible data types.\n\n#### _calculate_score\n```python\ndef _calculate_score(self, distance: float) -> float:\n```\nTransforms ChromaDB distance metrics into a similarity score. This method supports different distance metrics including cosine similarity.\n\n## Core Functionalities\n\n### Query and Update Context\n\n#### `update_context`\n```python\nasync def update_context(self, model_context: ChatCompletionContext) -> UpdateContextResult:\n```\nThis asynchronous method updates the agent's context by querying memory based on the last message in the conversation. If relevant memory results are found, they are formatted and added to the model's context.\n\n#### `add`\n```python\nasync def add(self, content: MemoryContent, cancellation_token: CancellationToken | None = None) -> None:\n```\nAdds new content to the ChromaDB collection. It extracts text from the `MemoryContent`, prepares relevant metadata, and performs the addition to the appropriate storage.\n\n### Retrieval and Clearing Memory\n\n#### `query`\n```python\nasync def query(self, query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) -> MemoryQueryResult:\n```\nQueries the ChromaDB collection using the provided text input. It returns a list of `MemoryContent` objects representing relevant memory items. The query results are filtered based on a predefined score threshold.\n\n#### `clear`\n```python\nasync def clear(self) -> None:\n```\nClears all entries in the ChromaDB collection, if initialization was successful. This function is useful for resetting the memory state.\n\n### Resource Management\n\n#### `close`\n```python\nasync def close(self) -> None:\n```\nCleans up and releases any resources associated with the ChromaDB client and collection.\n\n#### `reset`\n```python\nasync def reset(self) -> None:\n```\nResets the ChromaDB client and collection. This action is controlled by an `allow_reset` flag in the configuration to prevent unintended resets.\n\n## Configuration Management\n\n### Serialization\n\n#### `_to_config`\n```python\ndef _to_config(self) -> ChromaDBVectorMemoryConfig:\n```\nSerializes the current memory configuration into a `ChromaDBVectorMemoryConfig` object, which can be useful for saving settings or for debugging purposes.\n\n#### `_from_config`\n```python\n@classmethod\ndef _from_config(cls, config: ChromaDBVectorMemoryConfig) -> Self:\n```\nDeserializes a given configuration into a `ChromaDBVectorMemory` object, facilitating the reconstruction of memory instances from saved configurations.\n\n## Example Usage\n\nThe accompanying example demonstrates how to use the `ChromaDBVectorMemory` to store and query user preferences in memory while facilitating an assistant's interaction, showcasing the addition of functionalities such as weather retrieval and temperature conversion.\n\nIn scenarios where advanced memory management is required, users are encouraged to extend the `ChromaDBVectorMemory` class and override methods like `update_context` to enhance functionality as per specific use cases.\n\nOverall, this class provides a comprehensive solution for managing contextual memory in conversational agents, optimizing performance, and enhancing user interactions by ensuring relevant data retrieval.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/chromadb/_chromadb.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module is part of a package and focuses on memory management through the implementation of a memory class named `Mem0Memory` and its configuration class, `Mem0MemoryConfig`. It utilizes a relative import to bring in these classes from a sibling module, `_mem0`.\n\n## Imports\n\n```python\nfrom ._mem0 import Mem0Memory, Mem0MemoryConfig\n```\n\nThis line imports the `Mem0Memory` and `Mem0MemoryConfig` classes from the `_mem0` module located in the same package. The use of the leading dot (`.`) indicates a relative import, which means that it will look for `_mem0` in the current package directory.\n\n## Exported Classes\n\n```python\n__all__ = [\n    \"Mem0Memory\",\n    \"Mem0MemoryConfig\",\n]\n```\n\nThis section defines the `__all__` list, which specifies the public interface of this module. When the module is imported using `from module import *`, only the names listed in `__all__` will be imported. This is useful for encapsulating the internal workings of the module while providing access to the necessary classes.\n\n### Mem0Memory Class\n\nThe `Mem0Memory` class likely handles the core functionality related to memory management. Although the specific implementation details are not provided in this snippet, one can infer its role involves allocation, deallocation, and manipulation of memory resources as per the design outlined in `_mem0`.\n\n### Mem0MemoryConfig Class\n\nThe `Mem0MemoryConfig` class is presumably utilized for configuration settings related to the `Mem0Memory` class. It might define parameters and options that control how memory management operates, such as size limits, allocation strategies, or initialization settings.\n\n## Conclusion\n\nThis module acts as a lightweight wrapper, exposing two crucial components, `Mem0Memory` and `Mem0MemoryConfig`, from the `_mem0` module. By structuring the code in this manner, it enhances modularity and maintainability. Users of this module can utilize the specified classes to manage memory effectively, without needing to delve into the underlying implementation details in `_mem0`.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/mem0/__init__.py"
  },
  {
    "code": false,
    "content": "# Mem0 Memory Component Documentation\n\n## Overview\n\nThe `Mem0Memory` class is part of the AutoGen framework that implements a memory system using Mem0.ai's APIs. This component allows users to store and retrieve conversational memories, thereby improving the interaction quality in applications like chatbots. The component can function using either cloud-based or local memory storage, based on the user's configuration.\n\n### Dependencies and Imports\n\nThe implementation relies on multiple modules including:\n\n- **Logging**: For logging error messages.\n- **UUID**: To generate unique identifiers when user IDs are not specified.\n- **Context Management**: To suppress error messages during memory operations.\n- **Datetime Manipulation**: To handle timestamps related to memory creation.\n- **Type Hinting**: To facilitate better code clarity and static typing.\n- **AutoGen Core**: Integrates with the primary Autogen system.\n- **Mem0 Memory Client**: Handles API interactions with the Mem0 memory service.\n\n## Memory Configuration Class\n\n### `Mem0MemoryConfig`\n\nA `BaseModel` derived class serves as a configuration schema for `Mem0Memory`:\n\n- **Attributes**:\n  - `user_id`: Unique identifier for the user. If not supplied, a UUID is generated.\n  - `limit`: Maximum results returned in memory queries.\n  - `is_cloud`: Boolean to determine if the Mem0 client is cloud-based.\n  - `api_key`: API key needed for cloud operations.\n  - `config`: Configuration for local memory storage.\n\nThis structure ensures that all necessary parameters are available for the `Mem0Memory` setup, streamlining the initialization process.\n\n## Memory Class Implementation\n\n### `Mem0Memory`\n\nThe main memory component class extends the `Memory`, `Component`, and `ComponentBase` classes. Its primary purpose is to offer an interface for storing and retrieving memory objects.\n\n#### Initialization\n\n- The constructor checks if the local storage configuration is provided when using a local Mem0 client.\n- Constructs a Mem0 client object based on whether the user opted for a cloud or local solution.\n\n### Properties\n\n- **`user_id`**: Returns the user ID used for memory operations.\n- **`limit`**: Returns the maximum number of results for memory queries.\n- **`is_cloud`**: Indicates whether the memory client operates in the cloud.\n- **`config`**: Returns the configuration used by the local client.\n\n## Main Functionalities\n\n### Adding Memory\n\n#### `add`\n\n- **Parameters**: `content` (memory data to be stored) and an optional `cancellation_token`.\n- **Function**: Stores content in memory, extracting and serializing it based on MIME types, and captures associated metadata.\n- **Error Handling**: Logs errors if the add operation fails while suppressing irrelevant logs.\n\n### Querying Memory\n\n#### `query`\n\n- **Parameters**: A string-based query or `MemoryContent`, an optional cancellation token, and additional query parameters.\n- **Function**: Searches for memories that match the provided query, returning a `MemoryQueryResult`.\n- **Error Handling**: Logs errors encountered during the querying process and returns an empty result if a failure occurs.\n\n### Updating Context\n\n#### `update_context`\n\n- **Parameters**: The `model_context`, which is updated with relevant memories based on the last conversation message.\n- **Function**: Retrieves messages from the context, queries memory using the last message, and appends relevant memories as a system message.\n- **Returns**: `UpdateContextResult` indicating the outcome and memories added.\n\n### Clearing Memory\n\n#### `clear`\n\n- **Function**: Deletes all stored memories associated with the current user.\n- **Error Handling**: Logs exceptions during the clear operation.\n\n### Resource Management\n\n#### `close`\n\n- **Function**: Designed for clean-up; a no-op for Mem0 clients as they manage their cleanup automatically.\n\n## Configuration Methods\n\n### `_from_config`\n\n- **Parameters**: A `Mem0MemoryConfig` instance.\n- **Function**: Creates a new `Mem0Memory` instance using the provided configuration.\n\n### `_to_config`\n\n- **Function**: Converts instance properties back into a configuration format compatible with `Mem0MemoryConfig`.\n\n## Example Usage\n\nUsers can initialize the `Mem0Memory` class for both cloud and local instances and interact with it to add and retrieve colorful memories throughout conversations. Here are a few sample interactions:\n\n```python\nasync def main() -> None:\n    memory = Mem0Memory(user_id=\"user123\", is_cloud=False, config={\"path\": \":memory:\"})\n    await memory.add(MemoryContent(content=\"User likes the color blue.\", mime_type=\"text/plain\"))\n    results = await memory.query(\"What color does the user like?\")\n    print(results)  # Output will show relevant memories\n```\n\n## Conclusion\n\nThe `Mem0Memory` component serves as an effective memory management system within the AutoGen framework, leveraging both cloud and local storage capabilities, ensuring efficient handling of conversational data through structured memory operations. This flexibility allows developers to create intelligent applications that can maintain contextual awareness throughout interactions.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/mem0/_mem0.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code snippet is a module for managing Redis memory configurations and operations through the definition of two key components: `RedisMemory` and `RedisMemoryConfig`. It appears to serve as the interface for importing these components from the current package, aiding users in accessing and utilizing memory functionality provided by Redis.\n\n## Imports\n\nThe first section of the code imports two classes from a local module called `_redis_memory`:\n\n```python\nfrom ._redis_memory import (\n    RedisMemory,\n    RedisMemoryConfig,\n)\n```\n\n### Classes Overview\n\n1. **RedisMemory**: \n   - This class likely encapsulates functionalities related to interaction with Redis memory. This may include operations such as storing, retrieving, and managing cached data using Redis as an in-memory data store.\n\n2. **RedisMemoryConfig**: \n   - This class likely serves as a configuration structure for the `RedisMemory` class. It may contain settings such as connection parameters (host, port), parameters for data serialization, or configuration options for memory management.\n\n## Exported Components\n\nThe `__all__` list defines the public interface of the module. It specifies the components that are intended to be exported when the module is imported:\n\n```python\n__all__ = [\n    \"RedisMemoryConfig\",\n    \"RedisMemory\",\n]\n```\n\n### Purpose of `__all__`\n\n- By defining `__all__`, the module stipulates that only `RedisMemoryConfig` and `RedisMemory` should be accessible for import when a user employs the statement `from module_name import *`. This is a common practice in Python for controlling the export of symbols and managing the namespace effectively.\n\n## Conclusion\n\nIn summary, this code snippet is a simple yet functional module that provides access to Redis memory management classes. By explicitly importing and exporting `RedisMemory` and `RedisMemoryConfig`, it encapsulates related functionalities while providing a clean interface for developers to utilize Redis in their applications. The design promotes modular development and scalability, allowing for efficient management of configurations and memory operations with Redis.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/redis/__init__.py"
  },
  {
    "code": false,
    "content": "# RedisMemory Documentation\n\n## Overview\n\nThe `RedisMemory` class provides a memory management solution utilizing Redis as a vector store, enhancing conversational agents with the ability to retrieve relevant information based on semantic similarity or sequential order. This system allows for the dynamic storage and retrieval of memory content, enabling more contextual and intelligent interactions in applications that require memory functionality.\n\n## Dependencies and Setup\n\nTo utilize the `RedisMemory` class, ensure that the `redis` library is installed along with the `redisvl` extensions. If these dependencies are missing, the class raises an `ImportError`, indicating how to install the necessary packages using pip:\n\n```bash\npip install autogen-ext[redisvl]\n```\n\nAdditionally, users can run a local instance of Redis using Docker or by installing Redis directly on their system through specified commands.\n\n## RedisMemoryConfig Class\n\n### Purpose\n\n`RedisMemoryConfig` is a configuration class extending `BaseModel` from Pydantic that encapsulates all configurable parameters for setting up Redis-based vector memory. The attributes within this class define how memory will be stored, indexed, and queried.\n\n### Key Attributes\n\n- `redis_url`: URL for the Redis instance.\n- `index_name`: Defines the name of the collection in Redis for storing messages.\n- `prefix`: A prefix for the keys in the Redis memory collection.\n- `sequential`: A boolean indicating whether memories should be returned in sequential order rather than based on semantic similarity.\n- `distance_metric`: Defines the type of distance metric used (e.g., cosine, inner product, or L2).\n- `algorithm`: Specifies the algorithm used for memory retrieval (flat or HNSW).\n- `top_k`: Maximum number of results to return in query responses.\n- `datatype`: Data type for vector embeddings in Redis.\n- `distance_threshold`: Minimum similarity score threshold to consider a memory relevant.\n- `model_name`: Name of the embedding model to be utilized for vectorization.\n\n## RedisMemory Class\n\n### Purpose\n\nThe `RedisMemory` class extends `Memory` and `Component` to establish a vector-based memory management system with Redis underlying as the store. It provides methods to add, query, and update memory based on user interactions, making it suitable for conversational agents that require contextual awareness.\n\n### Initialization\n\nWhen an instance of `RedisMemory` is created, it initializes its configuration using `RedisMemoryConfig`. Depending on whether the `sequential` attribute is set, it assigns the `message_history` to either a basic `MessageHistory` or a more complex `SemanticMessageHistory` that supports vector-based similarity searches.\n\n### Key Methods\n\n#### `update_context`\n\nThis asynchronous method updates the model context with relevant memory content based on the last message received. It queries the stored memories using the last message as the input and appends the results as a single system message in the model context.\n\n**Arguments:**\n- `model_context`: The context to update.\n\n**Returns:**\n- An `UpdateContextResult` containing memories that were added to the context.\n\n#### `add`\n\nThis method allows for the addition of new memory content into Redis. It accepts a `MemoryContent` object and handles various MIME types (text, JSON, Markdown) to store the content securely.\n\n**Arguments:**\n- `content`: The `MemoryContent` object to store.\n- `cancellation_token`: (Optional) A token for cancellation. Currently unused.\n\n#### `query`\n\nThe `query` method retrieves relevant memory content based on semantic vector similarity or the defined sequential order. It can handle both string queries and `MemoryContent` objects.\n\n**Arguments:**\n- `query`: A string or `MemoryContent` object for querying.\n- `cancellation_token`: (Optional) A token for cancellation.\n- `**kwargs`: Additional parameters to refine the query, such as `top_k` and `distance_threshold`.\n\n**Returns:**\n- A `MemoryQueryResult` object containing the relevant memory contents found.\n\n#### `clear`\n\nClears all entries from memory while preserving the resources of `RedisMemory`.\n\n#### `close`\n\nMethod to clean up resources associated with the Redis client, including deleting all memory entries.\n\n## Application in an Example\n\nThe example provided in the class documentation demonstrates how to use `RedisMemory` in conjunction with a conversational agent. It illustrates the initialization of the memory with preferences, the definition of a tool for fetching weather data, and the execution of the agent's conversation in an asynchronous context.\n\n- The memory is initialized, and preferences are added.\n- An `AssistantAgent` is created that utilizes `redis_memory` along with a model client.\n- The agent is tasked to report the weather, utilizing stored memory to customize its response based on user preferences.\n\nThis seamless integration showcases how `RedisMemory` can enrich user interactions by allowing agents to retrieve and utilize contextual memories effectively.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/memory/redis/_redis_memory.py"
  },
  {
    "code": false,
    "content": "# High-Level Overview of the Code\n\nThis document provides a structured overview of the provided source code. The code may contain functions, methods, or classes, and this description will analyze their roles and functionalities. \n\n## Purpose of the Code\n\nThe code appears to be designed for a specific task or functionality, likely focusing on data manipulation, processing, or analysis. It may operate on some input data and produce results based on certain computations or transformations. \n\n## Code Structure\n\nThe code is organized into functions and possibly classes, which serve to encapsulate specific behaviors or functionalities. Each component is analyzed below:\n\n### Function Definitions\n\n1. **Function Name**: \n   - **Purpose**: This function carries out a particular operation on the input data. It may involve calculations, data retrieval, or transformation.\n   - **Parameters**: The function accepts specific parameters that dictate its behavior. These parameters are presumably necessary for its operations.\n\n2. **Function Name**: \n   - **Purpose**: This function serves to handle different scenarios or conditions in the code. It might include error handling or conditional checks.\n   - **Use Case**: The function is invoked when certain criteria are met during execution.\n\n3. **Function Name**: \n   - **Purpose**: This function is likely involved in formatting or preparing output. It may convert data into a specific structure for further use or for presentation.\n   - **Output**: The output from this function is meant to be utilized elsewhere in the code, typically as a final result for end-users or systems.\n\n### Classes and Their Functionalities\n\nIf the code includes classes, they may serve the following roles:\n\n1. **Class Name**: \n   - **Description**: This class may represent a primary entity in the code, such as a data structure or a logical grouping of related functionalities.\n   - **Methods**: The class consists of methods that manipulate the instance variables, likely implementing core logic related to the class's purpose.\n\n2. **Class Name**: \n   - **Description**: This class might act as a utility or helper component, providing additional functionalities that support the main classes.\n   - **Collaboration**: Instances of this class could be used in conjunction with other classes, enhancing modularity and separation of concerns.\n\n## Step-by-Step Functionality\n\n### Step 1: Input Acquisition\n\nThe code begins with a mechanism for acquiring input data. This could involve reading from a file, receiving user input, or fetching data from an external source such as an API. \n\n### Step 2: Data Processing\n\nFollowing input acquisition, the code processes the data. The functions defined earlier are likely called to perform calculations, filter data, or transform it into the desired format. \n\n### Step 3: Conditional Logic\n\nThe processing stage may include conditional logic to handle different scenarios or errors. Specific paths are taken based on conditions evaluated during execution. \n\n### Step 4: Output Generation\n\nOnce processing is complete, the code generates output. This could involve formatting data for presentation or writing results back to a file. The previously described formatting function may be called at this stage to ensure output meets certain specifications.\n\n### Step 5: Error Handling\n\nIn the case of exceptions or unexpected results, the code likely incorporates error handling mechanisms. This ensures robustness, preventing the entire application from failing due to minor issues.\n\n### Step 6: Execution\n\nLastly, the code contains a main execution block, which orchestrates the flow from input acquisition to output generation. This block serves as the entry point for the entire script, ensuring a logical sequence of operations.\n\n## Conclusion\n\nIn summary, the code presents a structured approach to handling a specific task involving input processing and output generation. It leverages functions and possibly classes to enhance readability and maintainability while incorporating error handling to cover edge cases. The modular design facilitates understanding and future modifications, making it easier to extend functionality as needed.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation: Normalize Stop Reason Function\n\n## Overview\nThe provided code includes a function that normalizes stop reasons for a certain application. The function takes an input string that represents a stop reason and returns a standardized value, specifically of the type `FinishReasons`, from the `autogen_core.models` module. If the input is invalid or unknown, it defaults to returning \"unknown\".\n\n## Function: `normalize_stop_reason`\nHere is a breakdown of the `normalize_stop_reason` function:\n\n### Signature\n```python\ndef normalize_stop_reason(stop_reason: str | None) -> FinishReasons\n```\n\n#### Parameters\n- `stop_reason`: This parameter is either a string or `None`. It represents the stop reason that needs to be normalized.\n\n#### Returns\n- The function returns a value of type `FinishReasons`. This can be one of the predefined reasons defined in the `KNOWN_STOP_MAPPINGS` dictionary or defaults to \"unknown\" if the input does not match any known reasons.\n\n### Function Logic\n1. **Nil Check**: The function first checks if `stop_reason` is `None`. If so, it directly returns \"unknown\".\n\n2. **Normalization**: If the input string is not `None`, it converts the string to lowercase to ensure that the mapping is case insensitive.\n\n3. **Mapping Dictionary**: \n   - The function contains a mapping dictionary called `KNOWN_STOP_MAPPINGS`, which maps various string representations of stop reasons to their corresponding normalized values:\n     - \"stop\" -> \"stop\"\n     - \"length\" -> \"length\"\n     - \"content_filter\" -> \"content_filter\"\n     - \"function_calls\" -> \"function_calls\"\n     - \"end_turn\" -> \"stop\"\n     - \"tool_calls\" -> \"function_calls\"\n   \n   This allows for flexibility in the input strings by mapping different variations to a standard format.\n\n4. **Lookup and Return**: \n   - The function performs a lookup on `KNOWN_STOP_MAPPINGS` using the normalized string. \n   - If the `stop_reason` is found in the dictionary, the corresponding value is returned; otherwise, it defaults to returning \"unknown\".\n\n## Use Cases\nThe `normalize_stop_reason` function can be useful in applications where it's crucial to maintain consistency in the representations of various stop reasons. This might be particularly relevant in scenarios involving logging, monitoring, or any system where decision-making is driven by the nature of the underlying reasons for stops or terminations in processing.\n\n### Practical Example\n1. **Valid Input Case**: If the input string is \"STOP\", the function will normalize it to \"stop\" due to case insensitivity.\n  \n2. **Invalid Input Case**: If the input was \"timeout\" (which is not defined in the mapping), the function would return \"unknown\".\n\n3. **Nil Case**: If there is no input (i.e., `None`), it would also return \"unknown\".\n\n## Conclusion\nThe `normalize_stop_reason` function is a well-defined utility designed to standardize input-related stop reasons in a consistent manner, enhancing the overall robustness and reliability of the application\u2019s error handling or status reporting mechanisms. It effectively reduces potential discrepancies by mapping various input forms to a limited set of known outputs.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/_utils/normalize_stop_reason.py"
  },
  {
    "code": false,
    "content": "# Documentation for `parse_r1_content`\n\n## Overview\nThe `parse_r1_content` function is designed to analyze and extract specific information from a string formatted as an R1-style message. Specifically, it targets a section of content enclosed within `<think>` and `</think>` tags, allowing for the retrieval of both the extracted thought and the remaining content after the thought has been processed.\n\n## Function Purpose\n### `parse_r1_content(content: str) -> Tuple[str | None, str]`\nThis function accepts a single argument, `content`, which is expected to be a string containing text structured with `<think>` tags. Its return type is a tuple consisting of:\n- The extracted thought as a string if it exists, or `None` if not.\n- The leftover content string after removing the extracted thought.\n\n### Parameters\n- **content (str)**: A string input that contains potentially structured data including R1-style tagged thoughts.\n\n### Return Value\n- **Tuple**: A tuple where the first element can either be a string (the extracted thought) or `None`, and the second element is the remaining content string after the `<think>` section.\n\n## Function Logic\n1. **Finding Tags**: \n   - The function first attempts to locate the start (`<think>`) and end (`</think>`) tags within the provided content. It utilizes the `.find()` method on the string, which returns the index of the first occurrence or `-1` if the tag is not found.\n\n2. **Validation Checks**:\n   - If either tag is not found (i.e., `think_start` or `think_end` is `-1`), the function emits a warning.\n   - If the `</think>` closing tag precedes the `<think>` opening tag in the string, it also emits a warning about the incorrect ordering of the tags.\n\n3. **Extracting Thought**:\n   - Upon successful identification of both tags, the function extracts the text between these tags, trims any whitespace, and saves it as `thought`.\n\n4. **Handling Remaining Content**:\n   - The content following the `</think>` tag is captured and trimmed to remove any leading or trailing whitespace. This is the segment of content that comes after the extracted thought.\n\n5. **Return Statement**:\n   - Finally, the function returns a tuple comprising the extracted thought (or `None` if that was not found) and the adjusted remaining content.\n\n## Usage Scenario\nThis function can be particularly useful in context-aware applications such as chatbots or dialog systems where thoughts or reasoning might be encapsulated within specific tags. By efficiently parsing the content, the function facilitates further processing or display of both the thought process and any subsequent dialogue or information.\n\n## Warnings\nThe function utilizes the `warnings` module to issue warnings in scenarios where expected tags are not found or where the tag closure is improperly ordered. This is crucial for debugging and maintaining correct data handling, ensuring that users of the function are aware of potential issues with the content format.\n\n## Example\nWith an example input such as:\n```python\ncontent = \"Here is a thought: <think>This is my thought.</think> And then this follows.\"\n```\nThe function call `parse_r1_content(content)` would return:\n```python\n(\"This is my thought.\", \"And then this follows.\")\n```\nIn contrast, if the input content lacked properly ordered tags, it would return `(None, original_content)` and generate an appropriate warning.\n\n## Conclusion\nThe `parse_r1_content` function presents a straightforward yet effective method for extracting specific segments of structured string content. It handles errors gracefully through warnings, making it a robust tool for any text processing tasks that involve similar markup.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/_utils/parse_r1_content.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis module imports several classes and configurations related to the Anthropics API client. It organizes and provides these functionalities, presumably for use in a larger system that requires working with chat completion services, which may involve natural language processing. \n\n## Imports\n\nThe module imports components from two different modules:\n\n1. **Client Classes**: \n   - `AnthropicBedrockChatCompletionClient`\n   - `AnthropicChatCompletionClient`\n   - `BaseAnthropicChatCompletionClient`\n   \n   These classes are likely responsible for handling chat completion tasks. Each class may offer different levels or types of functionality related to user interactions with chat-based models.\n\n2. **Configuration Classes**: \n   - `AnthropicBedrockClientConfiguration`\n   - `AnthropicBedrockClientConfigurationConfigModel`\n   - `AnthropicClientConfiguration`\n   - `AnthropicClientConfigurationConfigModel`\n   - `BedrockInfo`\n   - `CreateArgumentsConfigModel`\n   \n   These classes are likely focused on configuring the clients, encompassing various settings, data structures, and metadata that the chat clients might need to operate correctly.\n\n## Purpose of Each Import\n\n### Client Classes\n\n- **AnthropicBedrockChatCompletionClient**: \n  This class likely extends the functionality of chat completion specifically designed for Bedrock, a service or feature provided by Anthropics. The specifics may involve unique parameters or additional capabilities catering to Bedrock scenarios.\n  \n- **AnthropicChatCompletionClient**:\n  This might serve as a more general or baseline implementation for chat completions. It's probable that this class includes core methods that outline standard procedures for interacting with chat models.\n\n- **BaseAnthropicChatCompletionClient**:\n  This class might act as a superclass (or abstract class) that defines common attributes and methods used by the other client classes. It likely sets the foundation for the other chat clients.\n\n### Configuration Classes\n\n- **AnthropicBedrockClientConfiguration**: \n  This configuration class may hold settings specifically tailored for the Bedrock chat client. It would define parameters necessary for initializing or modifying the behavior of the Bedrock-based client.\n  \n- **AnthropicBedrockClientConfigurationConfigModel**: \n  This model likely provides a structured format or schema for validating and managing the configuration settings associated with the Bedrock client.\n\n- **AnthropicClientConfiguration**: \n  This is probably a more general configuration schema applicable to the regular Anthropics chat client, managing settings common to all chat interactions.\n\n- **AnthropicClientConfigurationConfigModel**: \n  Similar to the Bedrock config model, this class might validate and enforce structure on the configuration settings of the general chat client.\n\n- **BedrockInfo**: \n  This class likely encapsulates meta-information about Bedrock services or capabilities, which could be important for users of the anthropic APIs to understand the available functionalities.\n\n- **CreateArgumentsConfigModel**: \n  This model may define the parameters required for creating requests to the chat completion clients, ensuring that any necessary arguments follow the expected format.\n\n## Exported Components\n\nThe module defines an `__all__` list that specifies which classes and configurations are publicly accessible when this module is imported elsewhere. This list includes:\n\n1. **Chat Client Classes**: \n   - `AnthropicChatCompletionClient`\n   - `AnthropicBedrockChatCompletionClient`\n   - `BaseAnthropicChatCompletionClient`\n\n2. **Configuration Classes**: \n   - `AnthropicClientConfiguration`\n   - `AnthropicBedrockClientConfiguration`\n   - `AnthropicClientConfigurationConfigModel`\n   - `AnthropicBedrockClientConfigurationConfigModel`\n   - `CreateArgumentsConfigModel`\n   - `BedrockInfo`\n\nThis organized structure helps maintain a clean interface for users of the module by explicitly stating what is available.\n\n## Conclusion\n\nIn summary, this code provides an organized wrapper around multiple classes and configurations aimed at interfacing with Anthropics-based chat completion services. By separating clients and their associated configurations, the module fosters a clear, maintainable approach to working with APIs that facilitate chat interactions. This modular design allows developers to use only the components they need, preserving clarity and enhancing usability within larger systems or applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/anthropic/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code implements a chat completion client for Anthropic's Claude models. It allows users to interact with the models via an API, sending messages, receiving responses, and handling functionalities such as tools and streaming responses. The primary components are classes that define configurations, message handling, and model capabilities, leveraging asynchronous programming for efficient API interactions.\n\n## Imports and Initialization\n\nThe code starts by importing various modules and classes from standard libraries, external libraries, and custom packages. Key libraries include:\n\n- `asyncio`: For asynchronous operations.\n- `base64`, `json`, `re`: For data serialization and formatting.\n- `logging`: For logging events.\n- Custom libraries such as `autogen_core` and `anthropic`, which provide essential classes and utilities.\n\n### Key Constants and Globals\n\nSeveral constants are defined:\n- **Message Parameters**: `anthropic_message_params`, `disallowed_create_args`, and `required_create_args` define the permissible parameters for creating messages and clients.\n- **Logging**: Two loggers are initialized for event and trace logging.\n\n## Utility Functions\n\nThe code includes various utility functions necessary for operations related to message creation, content transformation, and validation:\n\n- **Client Creation**: \n    - `_anthropic_client_from_config`: Creates an instance of `AsyncAnthropic` using filtered configuration parameters.\n    - `_create_args_from_config`: Extracts creation arguments from the configuration while ensuring that required and disallowed parameters are correctly handled.\n\n- **Type Handling Functions**:\n    - `type_to_role`: Maps different message types (system, user, assistant, tool) to corresponding roles.\n    - `get_mime_type_from_image`: Determines the MIME type of an image from its base64 representation.\n    - Other utility functions include content normalization and JSON extraction.\n\n## Message Conversion Functions\n\nSeveral functions transform different types of messages into a format compatible with the Anthropic API:\n\n- **UserMessage**: \n    - `user_message_to_anthropic`: Converts a user message to the required format, handling both text and images.\n  \n- **SystemMessage**: \n    - `system_message_to_anthropic`: Normalizes the format of system messages.\n  \n- **AssistantMessage**: \n    - `assistant_message_to_anthropic`: Converts assistant messages, managing tool invocation if present.\n  \n- **Tool Messages**: \n    - `tool_message_to_anthropic`: Formats results from function executions as user messages.\n\nThese functions encapsulate the logic for converting structured messages into a flat JSON-compatible structure used by the API.\n\n## Core Client Classes\n\n### BaseAnthropicChatCompletionClient\n\nThe `BaseAnthropicChatCompletionClient` class handles the initialization and configuration for chat completion, ensuring messages are formatted correctly for Anthropic's API:\n\n- **Initialization**: The constructor sets up the client, validates model information, and manages total and actual usage statistics.\n- **Message Serialization**: \n    - `_serialize_message`: Converts messages to a JSON-serializable format.\n  \n- **Message Management**: \n    - `_merge_system_messages`: Combines continuous system messages into one to prevent multiple entries.\n    - `_get_thinking_config`: Retrieves additional thinking configuration settings for API calls.\n\n### Chat Completion Methods\n\nThe `create` and `create_stream` methods are the main entry points for getting responses:\n\n- **create**: \n   - Handles message creation and submission to the API, including tool management and configurations.\n   - Validates if tools are provided and constructs the correct request format.\n   - Processes the response, extracting and simplifying the returned data into a usable format.\n\n- **create_stream**: \n   - Similar to `create`, but designed for streaming responses from the model.\n   - Handles real-time message processing via asynchronous consumption of response chunks.\n\n## Error Handling and Validations\n\nThe code employs multiple validation functions and error handling mechanisms:\n- Ensures the required parameters are present while creating clients and messages.\n- Validates names against a regex pattern ensuring they only have permissible characters.\n- Raises exceptions when the model's capabilities don't match the requested features (e.g., vision support).\n\n## Additional Features\n\nAdditional features include:\n- **Token Management**: The `count_tokens` and `remaining_tokens` methods estimate token usage, crucial for API limits.\n- **Logging**: The implementation includes logging for API calls, which can help in debugging and tracking performance.\n- **Configuration**: Using configuration models (like `AnthropicClientConfigurationConfigModel`) to encapsulate model settings and parameters.\n\n## Client Implementations\n\nTwo concrete implementations (`AnthropicChatCompletionClient` and `AnthropicBedrockChatCompletionClient`) extend the `BaseAnthropicChatCompletionClient`:\n- These clients adjust for specific models and environments, such as AWS Bedrock, providing the necessary API handling, configuration, and client initialization specific to their use cases.\n\n### Usage Example\n\nThe documentation within the class explains how to initialize and use these clients, demonstrating the simplicity of obtaining a response from the models using async functions. Users can plug their credentials and model configurations to interact with Anthropic\u2019s services effectively. \n\nOverall, the code provides a comprehensive framework for interacting with Anthropic's machine learning models, focusing on ease of use, extensibility, and robustness.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/anthropic/_anthropic_client.py"
  },
  {
    "code": false,
    "content": "# Model Information and Token Limits Documentation\n\n## Overview\nThe provided Python script defines a system for managing and querying information about various models from Anthropic's Claude family. It helps users retrieve model capabilities and token limits based on model names through specific functions.\n\n## Data Structures\n\n### Model Information Dictionary\nThe `_MODEL_INFO` dictionary is a mapping of model names to their respective capabilities, encapsulated in the `ModelInfo` dataclass. Each entry contains the following attributes:\n- **vision**: Indicates if the model can process visual inputs.\n- **function_calling**: Specifies if the model supports the function calling feature.\n- **json_output**: Denotes if the model can produce output in JSON format.\n- **family**: Identifies which family of models the specific model belongs to, categorized under `ModelFamily`.\n- **structured_output**: Indicates if the model supports structured output formats.\n- **multiple_system_messages**: Describes whether the model can handle multiple system messages in one request.\n\nThe dictionary includes various models, with specific entries detailing the most recent versions and their capabilities.\n\n### Model Token Limits Dictionary\nThe `_MODEL_TOKEN_LIMITS` dictionary holds information regarding the context window sizes (token limits) for the models. Each entry maps a model name to its maximum token limit, which determines how much input the model can process in one call.\n\n## Functions\n\n### `get_info(model: str) -> ModelInfo`\nThis function retrieves the `ModelInfo` for a specified model. The function performs the following tasks:\n1. **Exact Match Check**: It first checks if the model name exists in the `_MODEL_INFO` dictionary. If found, it returns the associated `ModelInfo`.\n   \n2. **Partial Match Check**: If an exact match is not found, it iterates over the keys of `_MODEL_INFO` and attempts to find a match for models that might share a base name with the provided model name. This is achieved using a string operation that splits on \"-2\".\n\n3. **Error Handling**: If neither an exact nor a partial match is found, the function raises a `KeyError`, indicating that the specified model is not recognized.\n\n### `get_token_limit(model: str) -> int`\nThis function is designed to retrieve the token limit for a specified model, executing similar logic as the `get_info` function:\n1. **Exact Match Check**: Initially checks if the model exists in `_MODEL_TOKEN_LIMITS`. If there is a direct match, it returns the corresponding token limit.\n\n2. **Partial Match Check**: If no exact match is found, it iterates over `_MODEL_TOKEN_LIMITS` to find a partial match based on the model's base name.\n\n3. **Default Value**: If the model is not identified after the checks, a default token limit of `100,000` is returned to ensure graceful handling of unrecognized model names.\n\n## Conclusion\nThis script thus serves as an essential utility in managing model information and token limits for the Claude model family. By providing functions to query this information, it simplifies the process of interacting with various models, ensuring that developers have easy access to crucial data for model usage and integration into larger systems. The intelligence built into the searching mechanisms allows for resilience against minor variations in model names, enhancing usability.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/anthropic/_model_info.py"
  },
  {
    "code": false,
    "content": "# Documentation for Configuration Models\n\nThis document provides a high-level overview of the configuration models defined in the provided source code, which primarily utilizes Python's type hints and Pydantic for data validation and serialization. The code appears to be tailored for configuring an AI model client that interacts with various features, including optional \"thinking\" capabilities and AWS Bedrock integration.\n\n## Overview of Model Capabilities and Configuration Types\n\nThe code defines multiple structures for configuring an AI model client through the use of TypedDicts and Pydantic models. These configurations aim to specify various parameters like model choice, token limits, and other operational characteristics. The flexibility in using both TypedDicts and Pydantic models allows for a robust set of definitions and ensures that data can be validated, enhancing reliability.\n\n### Response Format Definition\n\nAt the core of these configurations is the `ResponseFormat` TypedDict. This defines how responses from the AI model should be formatted. It requires a `type` field that can either be \"text\" or \"json_object\", allowing downstream components to handle responses appropriately based on their expected formats.\n\n```python\nclass ResponseFormat(TypedDict):\n    type: Literal[\"text\", \"json_object\"]\n```\n\n### Thinking Config Structure\n\nThe `ThinkingConfig` TypedDict configures a \"thinking mode,\" which can either be enabled or disabled. If enabled, it can specify a token budget, which limits how many tokens can be used for AI reasoning. This flexibility is useful for cost management and can help tailor interactions based on computational resource limits.\n\n```python\nclass ThinkingConfig(TypedDict, total=False):\n    type: Required[Literal[\"enabled\", \"disabled\"]]\n    budget_tokens: Optional[int]  # Required if type is \"enabled\"\n```\n\n### Create Arguments Definition\n\nThe `CreateArguments` TypedDict encapsulates the primary parameters needed to create a client request to the model. Crucial fields like `model`, `max_tokens`, and `temperature` allow for detailed customization of the AI's operational parameters. Additionally, optional fields such as `thinking` and `response_format` are included to further enhance the configuration capabilities.\n\n```python\nclass CreateArguments(TypedDict, total=False):\n    model: str\n    max_tokens: Optional[int]\n    temperature: Optional[float]\n    top_p: Optional[float]\n    top_k: Optional[int]\n    stop_sequences: Optional[List[str]]\n    response_format: Optional[ResponseFormat]\n    metadata: Optional[Dict[str, str]]\n    thinking: Optional[ThinkingConfig]\n```\n\n## Bedrock Information Structure\n\nThe code includes the `BedrockInfo` TypedDict, which is fundamental for applications that leverage AWS Bedrock models. This structure mandates essential credentials (access key, secret key, and session token) and the AWS region required for authentication. It helps encapsulate the necessary information for secure and authorized interactions with AWS services.\n\n```python\nclass BedrockInfo(TypedDict):\n    aws_access_key: Required[str]\n    aws_secret_key: Required[str]\n    aws_session_token: Required[str]\n    aws_region: Required[str]\n```\n\n### Client Configuration Structures\n\nThe `BaseAnthropicClientConfiguration` serves as a foundational structure that builds upon `CreateArguments`, integrating additional fields like `api_key`, `base_url`, and `timeout`. This lays the groundwork for specific client configurations while ensuring a common base.\n\n```python\nclass BaseAnthropicClientConfiguration(CreateArguments, total=False):\n    api_key: str\n    base_url: Optional[str]\n    model_capabilities: ModelCapabilities  # type: ignore\n    model_info: ModelInfo\n    timeout: Optional[float]\n    max_retries: Optional[int]\n    default_headers: Optional[Dict[str, str]]\n```\n\n### Specific Client Configuration Types\n\nThe `AnthropicClientConfiguration` extends `BaseAnthropicClientConfiguration`, allowing for more specialized functionalities, such as the inclusion of tools and methods for tool selection. This implies a richer interface for interacting with the AI service, offering features tailored for specific use cases.\n\n```python\nclass AnthropicClientConfiguration(BaseAnthropicClientConfiguration, total=False):\n    tools: Optional[List[Dict[str, Any]]]\n    tool_choice: Optional[Union[Literal[\"auto\", \"any\", \"none\"], Dict[str, Any]]]\n```\n\n## Enhanced Configuration for AWS Bedrock\n\nThe `AnthropicBedrockClientConfiguration` builds on the previous structure by integrating the `BedrockInfo` for clients that specifically need Bedrock support. This level of abstraction makes it easier to manage configurations for different environments or operational needs.\n\n```python\nclass AnthropicBedrockClientConfiguration(AnthropicClientConfiguration, total=False):\n    bedrock_info: BedrockInfo\n```\n\n### Pydantic Model Equivalents\n\nTo ensure strong data validation, the code introduces Pydantic models that correspond to the existing TypedDict structures. For example, `ThinkingConfigModel` and `CreateArgumentsConfigModel` offer the same functionalities as their TypedDict counterparts but with enhanced validation capabilities such as type checking and default values.\n\n```python\nclass ThinkingConfigModel(BaseModel):\n    type: Literal[\"enabled\", \"disabled\"]\n    budget_tokens: int | None = None  # Required if type is \"enabled\"\n```\n\n### Comprehensive Configuration Models\n\nThe `AnthropicBedrockClientConfigurationConfigModel` encapsulates the whole configuration structure, tying together the AWS credentials and various other operational parameters within a single cohesive unit. This design pattern promotes manageability and clarity when working with complex nested settings.\n\n```python\nclass AnthropicBedrockClientConfigurationConfigModel(AnthropicClientConfigurationConfigModel):\n    bedrock_info: BedrockInfoConfigModel | None = None\n```\n\n## Conclusion\n\nIn summary, the provided code defines versatile configuration models for an AI model client, with careful attention to the requirements for AWS Bedrock integration. By utilizing TypedDicts and Pydantic models, it supports a range of configurations while ensuring data integrity and ease of use. The logical structure facilitates both extensibility and maintainability, making it an effective solution for configuring AI model interactions.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/anthropic/config/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Azure AI Client Module\n\nThis module focuses on the integration of Azure AI functionalities into an application, providing an interface for chat completion features. It involves importing necessary components from other modules and defining accessible elements for external use.\n\n## Imports\n\nThe module includes two primary imports:\n\n1. **AzureAIChatCompletionClient**: This class likely encapsulates the functionality required to interact with Azure's AI chat service. This may include methods for initializing connection parameters, sending messages, and receiving responses from Azure's AI engine.\n\n2. **AzureAIChatCompletionClientConfig**: This class appears to be a configuration handler, providing the necessary settings to configure the `AzureAIChatCompletionClient`. This may include API keys, endpoint URLs, and other environment-specific settings.\n\n## Exported Classes\n\nThe module explicitly exports the following classes:\n\n- `AzureAIChatCompletionClient`: This class is utilized for making API calls to Azure's AI chat completion services, enabling developers to implement chat functionalities in their applications.\n\n- `AzureAIChatCompletionClientConfig`: This configuration class provides parameters needed to initialize the `AzureAIChatCompletionClient`. It ensures that clients have the required configurations to successfully connect and use the Azure services.\n\nBy using the `__all__` statement, the module restricts the exported names, which is a mechanism to enhance encapsulation and to define a public API for the module. Only the specified components are made available for import when the module is accessed, promoting cleaner usage and avoiding unintended exposure of internal classes or functions.\n\n## Purpose\n\nThe primary purpose of this module is to provide a neat and organized way of managing interactions with Azure's AI chat capabilities. By encapsulating the functionality and configuration into distinct classes, the module ensures easy integration and a clear interface for developers.\n\n## Expected Usage\n\n1. **Initialization**: Developers can start by creating a configuration instance using `AzureAIChatCompletionClientConfig`, supplying necessary credentials and parameters for the Azure service.\n\n2. **Client Creation**: Once the configuration is ready, a client can be instantiated using `AzureAIChatCompletionClient`, passing the configuration object as an argument.\n\n3. **Chat Functionality**: With the client set up, developers can utilize various methods provided by the `AzureAIChatCompletionClient` to implement chat features, such as sending user inputs to the Azure API and retrieving chat responses.\n\n## Summary\n\nThis module serves as a helpful interface for developers wishing to leverage Azure's AI capabilities within their own applications. It is designed to facilitate the setup of a chat client, enabling robust interaction with AI-driven chat functionalities in a streamlined manner. By clearly separating concerns through the use of class-based structures, the module promotes maintainability and scalability in software projects that incorporate Azure AI services.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/azure/__init__.py"
  },
  {
    "code": false,
    "content": "# Azure AI Chat Completion Client Documentation\n\n## Overview\nThis code snippet implements an asynchronous client for chat completions using models hosted on Azure AI Foundry or GitHub Models. It provides functionality to interact with these models, generate responses based on user input, and manage tools for enhanced interactions.\n\n## Imports and Dependencies\nThe script appears to rely on several external libraries and modules for core functionalities:\n\n- **asyncio**: For asynchronous programming, allowing operations to occur without blocking execution.\n- **logging**: For logging events and errors within the application.\n- **re**: For regular expression operations, particularly to validate or format names.\n- Various models and utilities from `autogen_core` and `azure.ai.inference`, which handle AI interactions and data structures.\n\n## Utility Functions\n\n### `_is_github_model(endpoint: str) -> bool`\nThis utility function checks whether the provided endpoint corresponds to a GitHub model endpoint. It returns `True` if the endpoint matches `GITHUB_MODELS_ENDPOINT`.\n\n### `convert_tools(tools: Sequence[Tool | ToolSchema]) -> List[ChatCompletionsToolDefinition]`\nThe `convert_tools` function translates a sequence of `Tool` or `ToolSchema` instances into `ChatCompletionsToolDefinition` instances compatible with Azure's completion client. The function iterates through each tool, modifies properties to fit the required format, and returns a list of converted tools.\n\n### Messaging Conversion Functions\nSeveral functions handle the conversion of messages to their corresponding Azure formats:\n1. **`_func_call_to_azure(message: FunctionCall) -> ChatCompletionsToolCall`**: Converts a `FunctionCall` to `ChatCompletionsToolCall`.\n2. **`_system_message_to_azure(message: SystemMessage) -> AzureSystemMessage`**: Converts a `SystemMessage` to `AzureSystemMessage`.\n3. **`_user_message_to_azure(message: UserMessage) -> AzureUserMessage`**: Converts a `UserMessage` to `AzureUserMessage`.\n4. **`_assistant_message_to_azure(message: AssistantMessage) -> AzureAssistantMessage`**: Converts an `AssistantMessage` to `AzureAssistantMessage`.\n5. **`_tool_message_to_azure(message: FunctionExecutionResultMessage) -> Sequence[AzureToolMessage]`**: Converts a tool message to its Azure equivalent.\n\n### `to_azure_message(message: LLMMessage) -> Sequence[AzureMessage]`\nThis function routes a given `LLMMessage` to the appropriate conversion function based on the message type (system, user, assistant, or tool).\n\n### Name Validation Functions\nTwo functions ensure name validity:\n1. **`normalize_name(name: str) -> str`**: Normalizes names for compatibility with LLM APIs by replacing invalid characters with underscores.\n2. **`assert_valid_name(name: str) -> str`**: Checks if the name contains valid characters and is of appropriate length, raising an error if the conditions are not met.\n\n## AzureAIChatCompletionClient Class\n\n### Class Definition\nThe `AzureAIChatCompletionClient` class extends `ChatCompletionClient`. It manages interactions with Azure\u2019s chat completion models and provides methods to create completions or streams of completions.\n\n### Constructor: `__init__(self, **kwargs: Unpack[AzureAIChatCompletionClientConfig])`\nThe constructor initializes the chat completion client with the specified configuration, which includes endpoint, credentials, and model information. It validates input fields, creates a completion client instance, and prepares necessary arguments for subsequent calls.\n\n### Configuration Validation Methods\n1. **`_validate_config(config: Dict[str, Any]) -> AzureAIChatCompletionClientConfig`**: Validates the configuration for required fields.\n2. **`_create_client(config: AzureAIChatCompletionClientConfig) -> ChatCompletionsClient`**: Instantiates the Azure `ChatCompletionsClient` with the validated configuration.\n3. **`_prepare_create_args(config: Mapping[str, Any]) -> Dict[str, Any]`**: Prepares arguments that are valid for the creation process based on the provided configuration.\n\n### Main Methods\n#### `async def create(self, messages: Sequence[LLMMessage], ...) -> CreateResult`\nThe `create` method generates chat completions by accepting a sequence of messages, optional tools, and configuration parameters. It validates input, converts messages to Azure\u2019s format, and handles asynchronous completion requests. After receiving the response, it logs the event and returns a `CreateResult`.\n\n#### `async def create_stream(self, messages: Sequence[LLMMessage], ...) -> AsyncGenerator[Union[str, CreateResult], None]`\nSimilar to `create`, this method streams chat completions, yielding responses as they arrive. This allows real-time updates to clients while maintaining log events. It handles tool calls similarly to the `create` method and uses `AsyncGenerator` for streaming responsive outputs.\n\n### Closing and Usage Methods\n#### `async def close(self) -> None`\nCloses the client and any associated resources when the interaction is completed.\n\n#### `def actual_usage(self) -> RequestUsage`\nReturns current usage statistics for this session.\n\n#### `def total_usage(self) -> RequestUsage`\nReturns cumulative token usage across all interactions.\n\n### Token Management Methods\nThese methods are placeholders and currently implement basic functionality:\n- **`def count_tokens(self, messages: Sequence[LLMMessage], ...) -> int`**\n- **`def remaining_tokens(self, messages: Sequence[LLMMessage], ...) -> int`**\n\n## Conclusion\nThe implemented client provides comprehensive access to Azure's chat models, allowing for flexible message handling via both synchronous and streaming modes. It outlines a structured approach for interacting with AI chat models while ensuring robust validation and conversion of data to match the requirements of Azure's endpoint specifications. This setup empowers developers to implement chatbot-like functionalities in various applications effectively.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/azure/_azure_ai_client.py"
  },
  {
    "code": false,
    "content": "# Azure AI Chat Completion Client Configuration\n\nThis document provides a high-level overview of the source code, which primarily defines data structures that are used for configuring clients interacting with Azure's AI services, particularly for chat completions. The code utilizes Python's type system with `TypedDict` for type safety and structure while integrating with Azure's AI inference models.\n\n## Imports\n\nThe code imports several classes and types from various libraries:\n\n- **Type Annotations:** \n  - `Any`, `Dict`, `List`, `Literal`, `Optional`, `TypedDict`, `Union` from the `typing` module for type definitions.\n  \n- **Azure AI Models:**\n  - `ModelInfo` from `autogen_core.models`.\n  - Several classes related to chat completions, such as `ChatCompletionsNamedToolChoice`, `ChatCompletionsToolChoicePreset`, and `ChatCompletionsToolDefinition` from the `azure.ai.inference.models`.\n\n- **Azure Credentials:**\n  - `AzureKeyCredential` and `AsyncTokenCredential` for managing authentication with Azure services.\n\n- **Constant Definition:**\n  - `GITHUB_MODELS_ENDPOINT` is defined as a constant URL that points to the models endpoint for GitHub's AI inference.\n\n## JsonSchemaFormat\n\n### Purpose\n\nThe `JsonSchemaFormat` class is a `TypedDict` that serves to define the schema for JSON data structures related to Azure AI models, allowing for both type checking and validation of these schemas in the client implementation.\n\n### Fields\n\n- **name (str):** The name of the schema.\n- **schema (Dict[str, Any]):** A dictionary representing the schema\u2019s structure.\n- **description (Optional[str]):** An optional description of what the schema represents.\n- **strict (Optional[bool]):** Indicates if the schema follows strict rules. \n\nThis structure will help to ensure the correct format and expectations when interacting with Azure's JSON data schemas.\n\n## AzureAIClientArguments\n\n### Purpose\n\nThe `AzureAIClientArguments` class is another `TypedDict` intended to encapsulate essential parameters required to create an AI client configured to connect with Azure services.\n\n### Fields\n\n- **endpoint (str):** The endpoint URL for the Azure AI service.\n- **credential (Union[AzureKeyCredential, AsyncTokenCredential]):** Authentication credentials needed to access the Azure service.\n- **model_info (ModelInfo):** Information about the specific AI model being used.\n\nThis class centralizes the parameters necessary for client initialization and will potentially streamline the client creation process.\n\n## AzureAICreateArguments\n\n### Purpose\n\nThe `AzureAICreateArguments` class is designed to define a more specific set of arguments for generating chat completions with Azure AI. This encapsulates various configurable parameters that can influence the behavior of the model.\n\n### Fields\n\n- **frequency_penalty (Optional[float]):** Adjusts the frequency of repeated tokens in the output.\n- **presence_penalty (Optional[float]):** Controls the presence of certain tokens to enrich conversation diversity.\n- **temperature (Optional[float]):** Affects randomness in the output; higher values yield more random outputs.\n- **top_p (Optional[float]):** Implements nucleus sampling to consider the most probable tokens.\n- **max_tokens (Optional[int]):** Limits the maximum number of tokens in the output.\n- **response_format (Optional[Literal[\"text\", \"json_object\"]]):** Specifies the format of the response from the model.\n- **stop (Optional[List[str]):** Tokens at which the generation should stop.\n- **tools (Optional[List[ChatCompletionsToolDefinition]]):** Define tools that the model can use during generation.\n- **tool_choice (Optional[Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]]):** Specifies which tool to use from the defined tools.\n- **seed (Optional[int]):** Sets a random seed for reproducibility.\n- **model (Optional[str]):** Allows specification of a different model if needed.\n- **model_extras (Optional[Dict[str, Any]]):** Additional parameters or configurations related to the model.\n\nThis structure affords users fine-grained control over the generation process, enabling customization based on the needs of the application.\n\n## AzureAIChatCompletionClientConfig\n\n### Purpose\n\nThe `AzureAIChatCompletionClientConfig` class combines the features of `AzureAIClientArguments` and `AzureAICreateArguments`. This unified `TypedDict` acts as an all-encompassing configuration object that can be used when initializing the Azure AI chat completion client.\n\n### Usage\n\nBy combining both sets of arguments, it simplifies the interface for client configuration. Developers can instantiate this configuration class to set up the client in a single step, encapsulating all necessary parameters for effective interaction with Azure\u2019s AI chat completion services.\n\n### Conclusion\n\nIn summary, the provided source code captures the various data structures and parameters necessary for configuring a client that interacts with Azure's AI services, particularly for generating chat completions. Through the use of `TypedDict`, it enforces type safety and clarity, which aids in building robust applications leveraging AI functionalities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/azure/config/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface for importing specific components related to chat completion caching from a submodule called `_chat_completion_cache`. It specifies which elements are publicly accessible when the module is imported. The primary components included in the module are `CHAT_CACHE_VALUE_TYPE` and `ChatCompletionCache`.\n\n## Imports\n\n```python\nfrom ._chat_completion_cache import CHAT_CACHE_VALUE_TYPE, ChatCompletionCache\n```\n\nThis line imports two specific items from the `_chat_completion_cache` module:\n- **CHAT_CACHE_VALUE_TYPE**: This is likely a constant that defines the type of values stored in a cache related to chat completions.\n- **ChatCompletionCache**: This is presumably a class or data structure that manages the storage, retrieval, or manipulation of cached chat completion data.\n\n## Public API Declaration\n\n```python\n__all__ = [\n    \"CHAT_CACHE_VALUE_TYPE\",\n    \"ChatCompletionCache\",\n]\n```\n\nThe `__all__` declaration is used to specify which attributes and methods should be available for import when the module is imported using the `from module_name import *` syntax. Here, it indicates that only `CHAT_CACHE_VALUE_TYPE` and `ChatCompletionCache` are intended for public use. Any elements not included in this list are considered private and will not be accessible with the wildcard import.\n\n## Component Details\n\n### CHAT_CACHE_VALUE_TYPE\n\n- **Purpose**: This constant is likely defined to standardize the type of values that can be stored in the chat completion cache. \n- **Functionality**: It aids in ensuring consistency and type safety within the caching system by preventing incompatible types from being stored.\n\n### ChatCompletionCache\n\n- **Purpose**: The `ChatCompletionCache` class is intended to handle the intricacies of caching chat completions. \n- **Functionality**: It likely provides methods for adding, retrieving, and managing cached data related to chat responses, enhancing performance through quicker data retrieval instead of recalculating responses.\n\n## Summary\n\nIn summary, this module establishes a cohesive interface for chat completion caching by selectively exposing critical components. By importing `CHAT_CACHE_VALUE_TYPE` and `ChatCompletionCache`, the module allows other parts of a system to efficiently manage and use cached chat data while restricting access to internal functionalities. This design choice fosters good software practices, maintaining encapsulation and modularity in the codebase.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/cache/__init__.py"
  },
  {
    "code": false,
    "content": "# ChatCompletionCache\n\n## Overview\nThe `ChatCompletionCache` class acts as a wrapper around a core `ChatCompletionClient`. Its primary functionality is to facilitate result caching for chat completion tasks, thus reducing repeated calls to the underlying client and allowing for quicker response times. This benefits performance and minimizes consumption of tokens by leveraging a cache store. The cache is designed to handle results from synchronous and asynchronous calls, catering to individual or streaming operations.\n\n## Dependencies and Imports\nThe code imports several modules, including:\n\n- `hashlib` and `json` for generating hash keys and handling JSON serialization.\n- Various classes and functions from `autogen_core`, which seems to provide foundational components for the class, such as caching and message handling.\n- `pydantic`, used for data validation through models.\n- Standard type annotations to enhance code clarity and maintainability.\n\n## Configuration\n### ChatCompletionCacheConfig\nThe `ChatCompletionCacheConfig` class serves as a data model to encapsulate configuration settings for the `ChatCompletionCache`. It contains:\n\n- `client`: This holds the actual `ChatCompletionClient`.\n- `store`: An optional cache store that can be used to cache results.\n\n## Class Structure\n### ChatCompletionCache\nThe `ChatCompletionCache` inherits from `ChatCompletionClient` and `Component`, making it a specialized client component capable of caching responses based on sender-defined configurations.\n\n#### Initialization\n- The constructor sets up the `client` and `store`, using an in-memory store as the default if none is provided.\n- Establishes the structure of the cache, preparing it to retrieve results quickly.\n\n## Caching Mechanisms\n### _check_cache Method\nThe `_check_cache` method is a private helper function that:\n\n1. Takes in parameters such as `messages`, `tools`, and `extra_create_args`.\n2. Serializes this information to create a unique cache key using SHA256.\n3. Attempts to fetch the cached result using the generated key.\n4. Handles various formats of cached results, including dictionaries and strings, reconstructing them as necessary.\n\nThis method crucially reduces redundant calls to the underlying client by checking whether a result for the given parameters already exists in the cache.\n\n### Main API Functions\n#### create Method\nThe `create` method provides a cached version of the standard `create` function. It performs the following steps:\n\n1. Checks if a cached response is available by calling `_check_cache`.\n2. If a valid cached result exists, it marks it as cached and returns it.\n3. If there is no cached result, it calls the original client's `create` method and stores the result in the cache.\n\nBy doing so, it provides a seamless experience for end-users, allowing quick access to previously computed responses.\n\n#### create_stream Method\nSimilar to the `create` method, `create_stream` serves to facilitate streaming responses while checking for cached results. It behaves in much the same manner, attempting to yield items from cache when available, or streaming from the original client when necessary.\n\n### Additional Features\n- **close() Method**: This method allows for graceful shutdown of the instance, ensuring any necessary cleanup is conducted.\n- **actual_usage() and total_usage() Methods**: These provide insights into usage statistics of the underlying client, relevant for monitoring and optimization.\n- **Token Management Methods**: Functions such as `count_tokens()` and `remaining_tokens()` provide utilities for managing and tracking token usage, essential for users of token-based models.\n\n### Properties\nThe class includes several properties, such as `capabilities` and `model_info`, both of which make it easy to access metadata about the underlying model being used without directly querying the original client.\n\n### Configuration Management\n- **_to_config()**: Converts the current instance back into configuration format, allowing for easy serialization and storage.\n- **_from_config()**: A class method that creates an instance of `ChatCompletionCache` from a given configuration object.\n\n## Usage Examples\nThe documentation provides several usage patterns showcasing different caching solutions, including disk caching and Redis caching. Users can quickly set up their cache with the relevant initialization patterns in both asynchronous API contexts with streaming capabilities.\n\n- **Disk Cache Example**: Users can implement temporary disk caching using a simplistic example that initializes the cache and client, executes a sample message, and retrieves results from cache on subsequent calls.\n  \n- **Redis Cache Example**: The same setup translates easily to Redis caching through another illustrative example, indicating usage flexibility.\n\n## Conclusion\nThe `ChatCompletionCache` class is a robust tool designed to enhance the performance of chat-based applications by allowing for efficient result caching. With clear interfaces for managing cached operations, the design promotes easy integration and improved user experiences.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/cache/_chat_completion_cache.py"
  },
  {
    "code": false,
    "content": "# Code Overview\n\nThis code snippet is a part of a Python module that manages the importation of a client for interacting with a language model, likely a variant of OpenAI's language model. It specifically handles the import of `LlamaCppChatCompletionClient` and manages dependency issues if they arise.\n\n## Dependency Import\n\nThe code begins by attempting to import `LlamaCppChatCompletionClient` from a local module named `._llama_cpp_completion_client`. This pattern suggests the use of a relative import, indicating that this module is part of a package where the specified module is located in the same directory or a subdirectory.\n\n```python\ntry:\n    from ._llama_cpp_completion_client import LlamaCppChatCompletionClient\n```\n\nIn this try block, if the import succeeds, it will allow the use of `LlamaCppChatCompletionClient` in the current module. In Python, using relative imports helps organize code logically within packages, promoting better encapsulation and reuse.\n\n## Error Handling\n\nIf the specified import fails due to the absence of the necessary module or package, an `ImportError` is raised:\n\n```python\nexcept ImportError as e:\n    raise ImportError(\n        \"Dependencies for Llama Cpp not found. \"\n        \"Please install llama-cpp-python: \"\n        \"pip install autogen-ext[llama-cpp]\"\n    ) from e\n```\n\nThis block captures the original ImportError exception (`e`) and raises a new ImportError with a more informative message for the user. The message indicates the required package `llama-cpp-python` and provides the pip command to install it. This error handling is crucial for user experience, as it guides users towards resolving issues related to missing dependencies.\n\n## Exporting Interfaces\n\nFollowing the import statements and error handling, the code specifies the public interface of the module using the `__all__` variable:\n\n```python\n__all__ = [\"LlamaCppChatCompletionClient\"]\n```\n\nThis statement defines which names will be exported when the module is imported with the wildcard (`*`). By including `LlamaCppChatCompletionClient` in `__all__`, the module indicates that this class or function is intended for public use, while other names in the module (if any) should remain private. This practice enhances encapsulation and helps prevent namespace pollution in client code.\n\n## Summary of Purpose\n\nThe primary purpose of this code is to facilitate the importing of a specific client that acts as an interface to a chat completion model from the Llama library. It ensures that users of the code will be promptly informed about dependency issues and provides guidance on how to resolve these issues through package installation. The use of an explicit public interface makes it clear what features are available for use outside of the module, promoting better software design principles.\n\nIn conclusion, this code serves an essential role in a larger application, setting up the environment correctly by managing dependencies and specifying accessible classes or functions, thus streamlining the usability of the language model's features.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/llama_cpp/__init__.py"
  },
  {
    "code": false,
    "content": "# LlamaCppChatCompletionClient Documentation\n\n## Overview\nThis Python module implements an asynchronous client for interacting with language models based on the Llama architecture. It provides utilities for normalizing data, validating names, and converting tools into a format usable by the underlying model. It offers a client class (`LlamaCppChatCompletionClient`) that allows users to send messages to the model and receive completions, including handling function calls.\n\n## Imports and Logging\nThe module begins by importing essential libraries for asynchronous operations, logging, regex processing, and type handling. It also imports various types and classes from `autogen_core` and `llama_cpp`, which are crucial for interacting with the neural network models. A logger is initialized to capture events related to the interactions with the language model.\n\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)  # initialize logger\n```\n\n## Utility Functions\n### `normalize_stop_reason`\nThis function takes a string representing a stopping reason and normalizes it to match predefined acceptable values. If the input isn't recognized, it defaults to \"unknown\".\n\n```python\ndef normalize_stop_reason(stop_reason: str | None) -> FinishReasons:\n    ...\n```\n\n### `normalize_name`\nThis function sanitizes function names by replacing invalid characters with underscores and limits the length to 64 characters. It is used mainly to ensure the names comply with the API requirements.\n\n```python\ndef normalize_name(name: str) -> str:\n    ...\n```\n\n### `assert_valid_name`\nThis function validates that a given name only contains acceptable characters (`a-z`, `A-Z`, `0-9`, `_` and `-`) and is no longer than 64 characters. A `ValueError` is raised if the name is invalid.\n\n```python\ndef assert_valid_name(name: str) -> str:\n    ...\n```\n\n### `convert_tools`\nThe `convert_tools` function takes a sequence of tools or tool schemas and converts them into a list of `ChatCompletionTool` objects. It also checks that all tools have valid names by leveraging `assert_valid_name`.\n\n```python\ndef convert_tools(\n    tools: Sequence[Tool | ToolSchema],\n) -> List[ChatCompletionTool]:\n    ...\n```\n\n## Class Definitions\n### `LlamaCppParams`\nThis class uses `TypedDict` to define parameters necessary for initializing a Llama model, including paths to model files, GPU settings, and other parameters affecting model performance.\n\n```python\nclass LlamaCppParams(TypedDict, total=False):\n    ...\n```\n\n### `LlamaCppChatCompletionClient`\nThe main class of this module, `LlamaCppChatCompletionClient`, enables interaction with Llama-based models for chat completions.\n\n#### Constructor\nThe constructor accepts initialization parameters, including paths to models and various configuration settings. It validates model info and initializes the model accordingly.\n\n```python\ndef __init__(self, model_info: Optional[ModelInfo] = None, **kwargs: Unpack[LlamaCppParams]) -> None:\n    ...\n```\n\n#### `create` Method\nThis asynchronous method generates completions based on a sequence of messages. It transforms the input messages into a format suitable for the model, invokes the model's completion function, and parses the response into a standardized format.\n\n```python\nasync def create(\n    self,\n    messages: Sequence[LLMMessage],\n    ...\n) -> CreateResult:\n    ...\n```\n\n#### `create_stream` Method\nA placeholder for an asynchronous streaming completion method. Currently, it raises a `NotImplementedError` as streaming functionality isn't yet supported.\n\n```python\nasync def create_stream(\n    self,\n    messages: Sequence[LLMMessage],\n    ...\n) -> AsyncGenerator[Union[str, CreateResult], None]:\n    ...\n```\n\n#### Supporting Methods\n- **Utility Methods**: The class includes methods like `actual_usage`, `count_tokens`, and `remaining_tokens` to facilitate tracking and management of token use.\n- **Close Method**: This method ensures that the resources held by the client are released properly.\n\n```python\nasync def close(self) -> None:\n    ...\n```\n\n## Logging and Event Handling\nThroughout the class, the logger captures events related to the messages sent and responses received from the model. This provides insights for debugging and monitoring the overall model interaction.\n\n## Error Handling\nThe code uses exceptions to manage errors related to invalid input, processing issues, and model response types. This helps maintain robustness and facilitates easier debugging when interacting with external models.\n\n## Summary\nThis module provides a comprehensive solution for interacting with Llama-based language models, focusing on asynchronous operations, input validation, and response handling. The structure is designed for easy extension and integration with other components in a broader application, specifically in a conversational AI context.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/llama_cpp/_llama_cpp_completion_client.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code snippet defines a module in Python, primarily focusing on importing specific classes and setting up the public API of the module.\n\n## Imports\n\nThe code imports classes from local modules:\n- **OllamaChatCompletionClient**: Presumably a client class for handling chat completion tasks, likely interacting with an AI model to process chat inputs.\n- **BaseOllamaClientConfigurationConfigModel**: A configuration model class that seems to serve as a base for setting various client configurations within the Ollama framework.\n- **CreateArgumentsConfigModel**: Another configuration model class, potentially used to define arguments needed when creating a session or initiating a chat with the client.\n\nThese imports suggest that the module is part of a larger package dealing with a chat or AI completion service.\n\n## Public API Definition\n\nThe `__all__` list is defined to explicitly declare which components of the module should be accessible when using the `from module import *` syntax. This is important for controlling the module's public interface and improving encapsulation. The following components are included:\n\n- **OllamaChatCompletionClient**: Functionality related to chat-based interactions.\n- **BaseOllamaClientConfigurationConfigModel**: Configuration data representation.\n- **CreateArgumentsConfigModel**: Data handling for argument creation.\n\nThis setup ensures that only specific classes are exposed, keeping the module's internal details private and allowing users to interact with the component functionality without needing to understand its intricate workings.\n\n## Purpose and Role of Classes\n\n### OllamaChatCompletionClient\n\nThis class is likely responsible for managing interactions with chat completion services. It could include methods for sending chat messages, receiving responses, and handling session states. The naming convention indicates that it mainly deals with utilizing completion models for chat applications.\n\n### BaseOllamaClientConfigurationConfigModel\n\nAs a base configuration model, this class likely provides foundational settings that can be extended or modified by other configuration classes. It is designed to ensure consistency in the configuration handling mechanisms across different instances or types of clients.\n\n### CreateArgumentsConfigModel\n\nThis model is expected to encapsulate the arguments required to create a session or initiate actions with the chat client. It probably includes validation logic or default settings that streamline the process of setting up interactions with the system.\n\n## Summary\n\nIn summary, this code snippet is structured to facilitate a modular design where specific functionalities of an Ollama-based chat client can be accessed. By neatly organizing imports and defining a public interface, the module enhances usability and keeps the underlying implementation abstracted away from the user, promoting better maintenance and scalability of the code.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/ollama/__init__.py"
  },
  {
    "code": false,
    "content": "# Model Information and Token Limit Management\n\nThis code manages a collection of AI model definitions, providing information about their capabilities and token limits. It imports necessary data types and classes, defines constant dictionaries for model specifications, and includes functions to retrieve model-related information.\n\n## Imports\n\n```python\nfrom typing import Dict\nfrom autogen_core.models import ModelFamily, ModelInfo\n```\n\nThe code begins by importing `Dict` from the `typing` module, allowing for type hinting of dictionary structures. It also imports `ModelFamily` and `ModelInfo` from the `autogen_core.models` module, suggesting that the project involves AI models, where these classes likely encapsulate structural or metadata-related functionality for models.\n\n## Model Information Dictionary\n\n```python\n_MODEL_INFO: Dict[str, ModelInfo] = {\n    ...\n}\n```\n\n### Purpose\nThe `_MODEL_INFO` dictionary contains detailed configurations for multiple AI models. Each model is represented as a key with its associated value being another dictionary that holds specific attributes about the model.\n\n### Attributes\nEach model entry includes the following attributes:\n\n- **vision**: A boolean indicating if the model has vision capabilities.\n- **function_calling**: A boolean indicating if the model can perform function calling.\n- **json_output**: A boolean indicating support for structured JSON output.\n- **family**: An enumeration type `ModelFamily` indicating the family category of the model.\n- **structured_output**: A boolean indicating if structured output is enabled.\n\n### Models\nExamples of models included are `all-minilm`, `codellama`, and `llama2`, with most having attributes primarily set to indicate no vision or function calling capabilities.\n\n## Model Token Limits Dictionary\n\n```python\n_MODEL_TOKEN_LIMITS: Dict[str, int] = {\n    ...\n}\n```\n\n### Purpose\nThis dictionary outlines token limits for the models defined in `_MODEL_INFO`. Each model key maps to an integer that denotes its maximum token processing capacity.\n\n### Token Capacity\nThe token limits range from as low as 256 tokens (e.g., `all-minilm`) to extremely large capacities (e.g., `command-r`, which has an impressive limit of 131072 tokens). This provides key information that assists in ensuring that inputs do not exceed model capabilities.\n\n### Inconsistencies\nThe comments associated with some models indicate that there might be discrepancies in the provided metadata, raising alertness about potential inaccuracies.\n\n## Utility Functions\n\n### `resolve_model_class`\n\n```python\ndef resolve_model_class(model: str) -> str:\n    return model.split(\":\")[0]\n```\n\n#### Purpose\nThis function takes a model identifier (which may include additional qualifiers separated by a colon) and returns the base name of the model.\n\n#### Role\nIt helps in standardizing model queries, allowing for efficient look-ups against the model information and token limits dictionaries.\n\n### `get_info`\n\n```python\ndef get_info(model: str) -> ModelInfo:\n    resolved_model = resolve_model_class(model)\n    return _MODEL_INFO[resolved_model]\n```\n\n#### Purpose\nThis function retrieves model information for a specified model.\n\n#### Role\nBy first resolving the model class name and then returning the corresponding information from `_MODEL_INFO`, this function acts as a central point for gathering data about models.\n\n### `get_token_limit`\n\n```python\ndef get_token_limit(model: str) -> int:\n    if model in _MODEL_TOKEN_LIMITS:\n        return _MODEL_TOKEN_LIMITS[model]\n    else:\n        resolved_model = resolve_model_class(model)\n        return _MODEL_TOKEN_LIMITS[resolved_model]\n```\n\n#### Purpose\nThis function provides the maximum token limit for a specified model.\n\n#### Role\nIt checks if the model is directly available in the `_MODEL_TOKEN_LIMITS` dictionary; if not, it resolves to the base model name and retrieves the limit. This dual approach ensures that model look-ups remain flexible and robust.\n\n## Summary\n\nIn summary, this code serves as a centralized management system for handling different AI model specifications and their inherent limits. By utilizing dictionaries for model information and token constraints, coupled with utility functions for querying, it provides an efficient and organized framework for accessing model capabilities. The comments indicate ongoing efforts to refine these definitions, potentially enhancing the accuracy of the data presented.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/ollama/_model_info.py"
  },
  {
    "code": false,
    "content": "# Ollama Chat Completion Client Documentation\n\n## Overview\n\nThe code defines a client for interacting with Ollama, a platform for chat-based large language models (LLMs). This client manages communication between the user and the models, handles processing of messages, manages state, and provides capability to facilitate both synchronous and asynchronous interactions.\n\nThe primary class involved is `OllamaChatCompletionClient`, which extends `BaseOllamaChatCompletionClient`. This class centralizes the functionality needed for chat completions, such as sending messages, receiving responses, managing configurations, and calculating usage statistics.\n\n## Dependencies and Imports\n\nThe code begins with necessary imports from standard libraries, third-party libraries, and local modules:\n\n- **Standard Libraries**: `asyncio`, `inspect`, `json`, and others for basic functionality and data handling.\n- **Third-party Libraries**: Libraries like `tiktoken`, `autogen_core`, and `ollama` provide specific functionalities for token management, logging, and interfacing with the Ollama client.\n- **Type Hints and Data Classes**: `dataclass` and typing hints for clarity and type checking throughout the code.\n\n## Client Configuration Functions\n\n### `_ollama_client_from_config`\n\nThis helper function takes in a configuration dictionary and constructs an `AsyncClient` object from it. It filters the configuration to extract only valid initialization keys pertaining to the Ollama client, which currently only includes \"host\".\n\n### `_create_args_from_config`\n\nThis function processes a provided configuration dictionary to extract valid arguments for creating chat requests. It handles recognized keys, manages options, and warns about deprecated configurations. The output is a dictionary of parameters ready to be used in requests.\n\n## Message Conversion and Token Management\n\n### `to_ollama_type`\n\nThe `to_ollama_type` function converts instances of different message types (`SystemMessage`, `UserMessage`, `AssistantMessage`, etc.) into a format compatible with the Ollama API. Each message type has a distinct processing approach to ensure that the content and structure align with Ollama's expectations.\n\n### `calculate_vision_tokens`\n\nThis function calculates the number of tokens required for image inputs. It accounts for token limits based on image size, aspect ratio adjustments, and defined constants that govern how tokens are assigned.\n\n### `count_tokens_ollama`\n\nThis utility function counts tokens for LLM messages, accounting for both the messages and associated tools. It uses the `tiktoken` library to ensure accurate counting and includes logging for mismatched models.\n\n## Ollama Chat Client Class\n\n### `BaseOllamaChatCompletionClient`\n\nA base client class that wraps around the Ollama API. It initializes with parameters related to the model being used and exposes methods to create messages and manage their configurations.\n\n### `OllamaChatCompletionClient`\n\nThis class extends the base client and includes primary functionalities for creating chat completions. Significant methods include:\n\n- **`create`**: Sends a list of messages to the Ollama model and retrieves the response. It processes configuration arguments dynamically, logs usage for analysis, and handles errors related to tool usage and response formats.\n  \n- **`create_stream`**: Similar to `create`, but this method provides a stream of responses, yielding parts of the output in real-time. It manages state across multiple chunks and accumulates responses.\n\n- **`actual_usage`** and **`total_usage`**: Methods that return the usage statistics for monitoring purpose.\n\n### State Management\n\nThe Ollama client maintains local state to track the total and actual token usage across calls made to the LLM. The usage is updated based on responses received from the API, allowing for effective quota management.\n\n## Error Handling and Validation\n\nFunctions such as `assert_valid_name` ensure that input names comply with set rules, which helps prevent errors in API calls. The code includes various checks and logs warnings regarding deprecated features and incorrect configurations.\n\n## Example Usage\n\nThe documentation provides examples to illustrate how to instantiate the `OllamaChatCompletionClient`, including how to send a message to the model and specify structured output formats using Pydantic models.\n\n```python\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\nfrom autogen_core.models import UserMessage\n\nollama_client = OllamaChatCompletionClient(model=\"llama3\")\nresult = await ollama_client.create(\n    [UserMessage(content=\"What is the capital of France?\", source=\"user\")]\n)\nprint(result)\n```\n\n## Conclusion\n\nThis code serves as a comprehensive client to interact with Ollama's chat models effectively. By encapsulating method definitions, state management, and response handling, it provides a robust interface for developers looking to integrate conversational AI into their applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/ollama/_ollama_client.py"
  },
  {
    "code": false,
    "content": "# Documentation of Ollama Client Configuration Code\n\nThis document provides a high-level overview of the provided source code, which defines classes and models for configuring clients that interact with the Ollama service.\n\n## Overview\n\nThe code begins by importing essential components from various libraries, specifically the `autogen_core` and `pydantic` modules. It utilizes type hints from the `typing` and `typing_extensions` libraries to enhance code clarity and ensure type safety. The primary goal of this code is to define configurations for creating requests to the Ollama API.\n\n### Imports\n\n- `Any`, `Mapping`, `Optional`, and `Union` are imported from the `typing` module to facilitate advanced type hinting.\n- `ModelCapabilities` and `ModelInfo` are imported from `autogen_core.models`, suggesting that the code will utilize properties related to the capabilities and information of models used within the Ollama client.\n- `Options` is imported from `ollama` to manage client options.\n- `BaseModel` is imported from `pydantic` to establish data validation and settings management.\n- `TypedDict` is imported to define a dictionary with a specified structure.\n\n## Class Definitions\n\n### CreateArguments\n\n```python\nclass CreateArguments(TypedDict, total=False):\n    model: str\n    host: Optional[str]\n    response_format: Any\n```\n\n`CreateArguments` is a `TypedDict` that specifies the structure of the arguments used during the creation of requests to the Ollama service. The keys defined include:\n\n- **model**: A required string indicating the model to be used.\n- **host**: An optional string that specifies the server host address.\n- **response_format**: An optional field that can accept any type, which likely will indicate the format of the response expected from the server.\n\n### BaseOllamaClientConfiguration\n\n```python\nclass BaseOllamaClientConfiguration(CreateArguments, total=False):\n    follow_redirects: bool\n    timeout: Any\n    headers: Optional[Mapping[str, str]]\n    model_capabilities: ModelCapabilities\n    model_info: ModelInfo\n    options: Optional[Union[Mapping[str, Any], Options]]\n```\n\n`BaseOllamaClientConfiguration` extends `CreateArguments`, adding additional configuration options necessary for the Ollama client. The additional fields are as follows:\n\n- **follow_redirects**: A boolean indicating whether the client should follow HTTP redirects (`default` implementation not specified).\n- **timeout**: An optional field to specify the timeout duration for requests.\n- **headers**: Optional HTTP headers to send with the request, structured as a mapping of string keys and values.\n- **model_capabilities**: A field for defining the capabilities of the model, using `ModelCapabilities` to determine the supported functionality.\n- **model_info**: Provides details about the model using `ModelInfo`.\n- **options**: An optional field that can accept either a mapping of string arguments or an `Options` object.\n\n## Pydantic Models\n\n### CreateArgumentsConfigModel\n\n```python\nclass CreateArgumentsConfigModel(BaseModel):\n    model: str\n    host: str | None = None\n    response_format: Any = None\n```\n\n`CreateArgumentsConfigModel` is a Pydantic model that corresponds to `CreateArguments`. It validates the input for the creation arguments and provides default values for optional fields. The fields are:\n\n- **model**: Required field similar to `CreateArguments`.\n- **host**: Optional with a default value of `None`.\n- **response_format**: Accepts any type with a default value of `None`.\n\n### BaseOllamaClientConfigurationConfigModel\n\n```python\nclass BaseOllamaClientConfigurationConfigModel(CreateArgumentsConfigModel):\n    follow_redirects: bool = True\n    timeout: Any = None\n    headers: Mapping[str, str] | None = None\n    model_capabilities: ModelCapabilities | None = None\n    model_info: ModelInfo | None = None\n    options: Mapping[str, Any] | Options | None = None\n```\n\n`BaseOllamaClientConfigurationConfigModel` extends `CreateArgumentsConfigModel` to provide a comprehensive configuration schema for the Ollama client. The attributes include:\n\n- **follow_redirects**: Set to `True` by default, allowing the client to follow redirects.\n- **timeout**: Default value is `None`, which means no custom timeout is applicable unless specified.\n- **headers**: Optional corresponds to the headers that might be sent with tasks.\n- **model_capabilities** and **model_info**: Both are optional fields, allowing for more flexible use of model capabilities and information.\n- **options**: Provides flexibility in passing additional options, either as a mapping or an `Options` instance.\n\n## Summary\n\nThis code implements configuration management for a client that communicates with the Ollama service. The overall architecture allows for flexible and type-checked configurations using TypedDicts and Pydantic models. By structuring the arguments and client settings in this manner, developers can easily instantiate and manage API requests while ensuring that configurations are validated against expected types, which enhances reliability in communications with the service.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/ollama/config/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis Python module appears to be part of a package designed for integrating with OpenAI's API, specifically for chat completion features. It imports necessary components, defines available API classes and configurations, and sets up an `__all__` declaration that specifies what is publicly accessible.\n\n## Imports\n\n### Message Transformation\n\n```python\nfrom . import _message_transform\n```\nThe module imports `_message_transform`, which likely contains functions or classes responsible for transforming messages before sending them to or after receiving them from the OpenAI API. This might include encoding, formatting, or handling message structures.\n\n### OpenAI Client Classes\n\n```python\nfrom ._openai_client import (\n    AZURE_OPENAI_USER_AGENT,\n    AzureOpenAIChatCompletionClient,\n    BaseOpenAIChatCompletionClient,\n    OpenAIChatCompletionClient,\n)\n```\nHere, multiple classes and constants related to OpenAI clients are imported:\n\n- **`AZURE_OPENAI_USER_AGENT`**: A constant that presumably defines the user agent string for requests made to Azure's OpenAI service. This string may help the server identify the source of the requests.\n  \n- **`AzureOpenAIChatCompletionClient`**: A specialized client for interacting with Azure's version of OpenAI's Chat Completion API. This class likely implements Azure-specific features or configurations.\n\n- **`BaseOpenAIChatCompletionClient`**: A base class that provides shared functionality for OpenAI chat completion clients. This class may define generic methods for sending requests to the API and processing responses.\n\n- **`OpenAIChatCompletionClient`**: This client interacts with OpenAI's native chat completion API. It may differ from the Azure client in configuration or capabilities, tailored for direct OpenAI API usage.\n\n### Configuration Models\n\n```python\nfrom .config import (\n    AzureOpenAIClientConfigurationConfigModel,\n    BaseOpenAIClientConfigurationConfigModel,\n    CreateArgumentsConfigModel,\n    OpenAIClientConfigurationConfigModel,\n)\n```\nSeveral configuration models are imported, which are likely used to manage and validate configurations for the chat completion clients:\n\n- **`AzureOpenAIClientConfigurationConfigModel`**: A model that defines the configuration settings specifically required for the Azure OpenAI client. This might include required fields like API keys or endpoint URLs.\n\n- **`BaseOpenAIClientConfigurationConfigModel`**: A general base configuration model that provides shared settings and validations for OpenAI clients.\n\n- **`CreateArgumentsConfigModel`**: A configuration model that might govern the arguments needed when creating new instances of clients or making chat completion requests.\n\n- **`OpenAIClientConfigurationConfigModel`**: A model that outlines the configuration settings specifically for the OpenAI Chat Completion client.\n\n## Public API Exposure\n\n```python\n__all__ = [\n    \"OpenAIChatCompletionClient\",\n    \"AzureOpenAIChatCompletionClient\",\n    \"BaseOpenAIChatCompletionClient\",\n    \"AzureOpenAIClientConfigurationConfigModel\",\n    \"OpenAIClientConfigurationConfigModel\",\n    \"BaseOpenAIClientConfigurationConfigModel\",\n    \"CreateArgumentsConfigModel\",\n    \"AZURE_OPENAI_USER_AGENT\",\n    \"_message_transform\",\n]\n```\nThe `__all__` list defines the public API of this module. By including these names, the module controls what is imported when a user does `from module_name import *`. This encapsulation helps in hiding internal functions or classes that are not intended for public use.\n\n### Key Components in `__all__`\n\n- **Client Classes**: The inclusion of all client classes (`OpenAIChatCompletionClient`, `AzureOpenAIChatCompletionClient`, and `BaseOpenAIChatCompletionClient`) suggests that this module is focused on providing user capabilities to interface with chat completion services offered by both OpenAI and Azure.\n\n- **Configuration Models**: The configuration models listed enable users to set up clients efficiently, controlling various settings.\n\n- **Utility Functions or Constants**: The inclusion of `_message_transform` indicates that there are functionalities for handling message formats, crucial for maintaining proper communication with the APIs.\n\n## Conclusion\n\nIn summary, this module serves as a foundational part of a package that lets developers interact with OpenAI's chat completion services. It organizes various client classes and configuration models, making it easier to configure API interactions. This setup allows for a structured approach to managing requests and responses when using OpenAI's capabilities, whether on their native platform or via Azure.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/__init__.py"
  },
  {
    "code": false,
    "content": "# `_message_transform.py` Documentation\n\nThis documentation provides an overview and detailed explanation of the `_message_transform.py` module, highlighting its purpose, functionality, and key components.\n\n## AutoGen Modular Transformer Pipeline\n\nThe `_message_transform.py` module implements a flexible and modular pipeline designed for transforming instances of `LLMMessage` into message formats compatible with specific SDKs, such as OpenAI's `ChatCompletionMessageParam`. This modular approach enhances code organization and reduces redundancies in message processing across varying model SDKs, thereby improving maintainability and compatibility.\n\n---\n\n### \ud83d\udccc Background\n\nPrior to the introduction of this module, message adaptation was inconsistently managed across various model clients, leading to numerous compatibility issues and code duplication. The unified transformer pipeline introduced in this module addresses these shortcomings by standardizing the transformation logic, thus fostering extensibility to support a variety of models like Gemini, Claude, and others.\n\n---\n\n### \ud83c\udfaf Key Concepts\n\n1. **Transformer Function**:\n   A function dedicated to transforming specific fields of an `LLMMessage`, such as `content`, `name`, or `role`, into keyword arguments suited for SDK consumption.\n\n2. **Transformer Pipeline**:\n   A structured sequence of transformer functions that can be composed using the `build_transformer_func` method, allowing for customization and flexibility in message processing.\n\n3. **Transformer Map**:\n   A dictionary mapping the types of `LLMMessage` (System, User, Assistant) to their respective transformers for specific models, facilitating the appropriate message handling based on message type.\n\n4. **Conditional Transformer**:\n   This function determines the appropriate transformer pipeline at runtime based on the content of the message or specific runtime conditions, enabling dynamic message handling.\n\n---\n\n### \ud83e\uddea Example: Basic Flow\n\n```python\nfrom autogen_ext.models.openai._message_transform import get_transformer\nfrom autogen.types import AssistantMessage\n\nllm_message = AssistantMessage(name=\"a\", thought=\"Let's go!\")\ntransformer = get_transformer(\"openai\", \"gpt-4\", type(llm_message))\nsdk_message = transformer(llm_message, context={})\nprint(sdk_message)\n```\n\nIn this example, an `AssistantMessage` is created, and a suitable transformer is retrieved for the OpenAI `gpt-4` model. The transformer is then applied to the `LLMMessage`, resulting in an SDK-formatted message.\n\n---\n\n### \ud83e\uddf0 Example: Define Transformer Functions\n\n```python\ndef _set_role(role: str):\n    def fn(message, context):\n        return {\"role\": role}\n\n    return fn\n\n\ndef _set_content_from_thought(message, context):\n    return {\"content\": message.thought or \" \"}\n\nbase_user_transformer_funcs = [_set_role(\"user\"), _set_content_from_thought]\n```\n\nIn this snippet, two transformer functions are defined. The first, `_set_role`, assigns a specified role to the message, while the second, `_set_content_from_thought`, extracts the content from the message's `thought` property.\n\n---\n\n### \ud83d\udee0\ufe0f Example: Build and Register Transformer Map\n\n```python\nfrom autogen_ext.models.utils import build_transformer_func, register_transformer\nfrom openai.types.chat import ChatCompletionUserMessageParam\nfrom autogen.types import UserMessage, SystemMessage, AssistantMessage\n\nuser_transformer = build_transformer_func(\n    funcs=base_user_transformer_funcs, message_param_func=ChatCompletionUserMessageParam\n)\n\nMY_TRANSFORMER_MAP = {UserMessage: user_transformer, SystemMessage: ..., AssistantMessage: ...}\n\nregister_transformer(\"openai\", \"mistral-7b\", MY_TRANSFORMER_MAP)\n```\n\nThis example illustrates how to create a complete transformer function by assembling defined transformer functions into a coherent pipeline, which is then registered with the transformer map for a specific model (in this case, Mistral).\n\n---\n\n### \ud83d\udd01 Conditional Transformer Example\n\n```python\nfrom autogen_ext.models.utils import build_conditional_transformer_func\n\ndef condition_func(message, context):\n    return \"multimodal\" if isinstance(message.content, dict) else \"text\"\n\nuser_transformers = {\n    \"text\": [_set_content_from_thought],\n    \"multimodal\": [_set_content_from_thought],  # could be different logic\n}\n\nconditional_user_transformer = build_conditional_transformer_func(\n    funcs_map=user_transformers,\n    message_param_func_map=message_param_funcs,\n    condition_func=condition_func,\n)\n```\n\nIn this example, a conditional transformer function is constructed based on the type of content (text or multimodal). Depending on the condition, different transformation logic can be applied to user messages.\n\n---\n\n### \ud83d\udce6 Design Principles\n\n- **DRY and Composable**: Aims to reduce redundancy by promoting the reuse of transformer functions across various message types and models.\n  \n- **Model-specific Overrides**: Allows easy customization of transformers specific to each model without requiring extensive codebase changes or forking.\n\n- **Explicit Separation of Concerns**: Clearly distinguishes between transformation logic and SDK builder logic, leading to improved code clarity.\n\n- **Future Extensibility**: Designed with extensibility in mind, to facilitate the addition of new models and adaptations in the future, such as those from Claude, Gemini, and Alibaba.\n\n---\n\n### \ud83d\udcce Reference\n\nThis module's functionality and structure originated from [PR #6063](https://github.com/microsoft/autogen/pull/6063), marking a significant development in enhancing message transformation capabilities within the AutoGen framework.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_message_transform.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis module offers functionalities to manage and retrieve information related to various AI models available in the market, specifically from providers like OpenAI, Anthropic, and Llama. It features logging, configuration mappings, and several functions to resolve model names, obtain model information, and retrieve token limits.\n\n## Imports and Logger Setup\n\n### Imports\nThe code imports specific modules necessary for its functionality:\n\n- **`logging`**: For logging events and debugging information.\n- **`Dict` from `typing`**: To specify that certain variables will store dictionaries with a specific key and value types.\n\n### Logger Initialization\nTwo loggers are initialized using pre-defined names, likely for event logging and trace logging. These loggers help in tracking the operations and debugging if needed:\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\ntrace_logger = logging.getLogger(TRACE_LOGGER_NAME)\n```\n\n## Model Pointers Configuration\n\n### `_MODEL_POINTERS`\nThis dictionary maps user-friendly model names to specific versions of AI models. Each key corresponds to a general model name while each value indicates the specific version of the model:\n```python\n_MODEL_POINTERS = {\n    \"o4-mini\": \"o4-mini-2025-04-16\",\n    ...\n}\n```\nThe models are categorized by their respective providers such as OpenAI, Anthropic, and Llama, enabling easy reference to the latest versions.\n\n## Model Information Schema\n\n### `_MODEL_INFO`\nThis dictionary holds detailed configurations for specific models, including attributes such as:\n\n- **`vision`**: Indicates if the model can process images.\n- **`function_calling`**: Specifies if the model supports function calling.\n- **`json_output`**: Indicates whether the model can produce JSON-formatted output.\n- **`family`**: Classifies the model into its corresponding family (e.g., GPT, Claude).\n- **`structured_output`**: States if the model supports structured output.\n- **`multiple_system_messages`**: Denotes if multiple system messages can be handled.\n\nExample entries are structured as follows:\n```python\n_MODEL_INFO: Dict[str, ModelInfo] = {\n    \"gpt-4o-mini-search-preview-2025-03-11\": {\n        \"vision\": False,\n        ...\n    },\n    ...\n}\n```\n\n## Model Token Limits\n\n### `_MODEL_TOKEN_LIMITS`\nThis dictionary catalogs the token limits for different AI models, specifying the maximum number of tokens each model can process in a single request:\n```python\n_MODEL_TOKEN_LIMITS: Dict[str, int] = {\n    \"o4-mini-2025-04-16\": 200000,\n    ...\n}\n```\n\nThis feature helps developers understand the constraints of various models and manage input sizes accordingly.\n\n## API Base URLs\n\n### Definitions\nConstants are defined for the base URLs of various APIs corresponding to different model providers:\n```python\nGEMINI_OPENAI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\nANTHROPIC_OPENAI_BASE_URL = \"https://api.anthropic.com/v1/\"\nLLAMA_API_BASE_URL = \"https://api.llama.com/compat/v1/\"\n```\nThese URLs will likely be used in API calls to interact with the respective services.\n\n## Functions\n\n### `resolve_model`\nThis function translates a user-friendly model name into its corresponding version. It checks the `_MODEL_POINTERS` dictionary and returns the resolved model name:\n```python\ndef resolve_model(model: str) -> str:\n    if model in _MODEL_POINTERS:\n        return _MODEL_POINTERS[model]\n    return model\n```\nIf the model isn\u2019t found, it returns the original model name provided.\n\n### `get_info`\nThe `get_info` function retrieves metadata about a specified model:\n```python\ndef get_info(model: str) -> ModelInfo:\n```\n1. It calls `resolve_model` to find the corresponding version.\n2. It looks up the model information in `_MODEL_INFO`.\n3. If the model is not valid, it raises a `ValueError`.\n4. If the family is unknown, it logs a warning.\n\n### `get_token_limit`\nThis function fetches the token limit for a specified model:\n```python\ndef get_token_limit(model: str) -> int:\n    resolved_model = resolve_model(model)\n    return _MODEL_TOKEN_LIMITS[resolved_model]\n```\nIt similarly uses `resolve_model` to get the appropriate model name and then retrieves the token limit from `_MODEL_TOKEN_LIMITS`.\n\nIn summary, this module offers structured access to the characteristics and limitations of various AI models, streamlining the process for developers working with these technologies.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_model_info.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis module primarily serves as a client for interacting with OpenAI's chat completion capabilities. It provides asynchronous methods to create chat completions using both OpenAI's API and Azure OpenAI services. The code includes functions for configuration, message handling, token counting, and creating results based on user inputs and tool interactions.\n\n## Imports and Configuration\n\n### Imports\n\nThe module begins by importing several libraries and components necessary for its functionality, including:\n\n- Standard libraries such as `asyncio`, `inspect`, and `json`.\n- External libraries like `openai` for accessing OpenAI's API and `tiktoken` for token counting.\n- Internal components from `autogen_core`, which likely provide models for messages, tools, and logging events.\n\n### Configuration Variables\n\nSeveral configuration variables are defined to handle API initialization, such as `openai_init_kwargs` and `AZURE_OPENAI_USER_AGENT`, which are used to track kwargs for OpenAI and Azure clients, respectively.\n\n## Client Configuration Functions\n\nThe code includes utility functions to create OpenAI and Azure clients from a given configuration.\n\n```python\ndef _azure_openai_client_from_config(config: Mapping[str, Any]) -> AsyncAzureOpenAI:\n    ...\ndef _openai_client_from_config(config: Mapping[str, Any]) -> AsyncOpenAI:\n    ...\n```\n\nThese functions extract the relevant parameters from the configuration dictionary and instantiate the respective async clients. They also manage headers and user agent strings.\n\n## Argument Creation Function\n\n```python\ndef _create_args_from_config(config: Mapping[str, Any]) -> Dict[str, Any]:\n    ...\n```\n\nThis function validates the provided arguments against required and disallowed arguments, ensuring that all necessary parameters for API calls are present.\n\n## Message Type Handling and Conversion\n\n### Message Role and Type Identification\n\nThe code defines a utility function to identify message types and convert them to the appropriate OpenAI type, allowing for seamless integration between custom message structures and the OpenAI API format.\n\n```python\ndef type_to_role(message: LLMMessage) -> ChatCompletionRole:\n    ...\ndef to_oai_type(\n    message: LLMMessage, \n    ...\n) -> Sequence[ChatCompletionMessageParam]:\n    ...\n```\n\nThese utility functions ensure that the different types of messages (`SystemMessage`, `UserMessage`, etc.) are correctly formatted for API communication.\n\n## Token Calculation\n\n### Token Management\n\nThe code includes a detailed calculation function for determining the number of tokens required for API calls based on messages and optional images:\n\n```python\ndef calculate_vision_tokens(image: Image, detail: str = \"auto\") -> int:\n    ...\ndef count_tokens_openai(\n    messages: Sequence[LLMMessage],\n    ...\n) -> int:\n    ...\n```\n\nThese functions handle token counts by taking the dimensions of images into account and determining how many tokens various message parts contribute.\n\n## Main Client Class: `BaseOpenAIChatCompletionClient`\n\n### Constructor and Initialization\n\n```python\nclass BaseOpenAIChatCompletionClient(ChatCompletionClient):\n    def __init__(\n        self,\n        client: Union[AsyncOpenAI, AsyncAzureOpenAI],\n        ...\n    ):\n        ...\n```\n\nThis base client class receives an OpenAI or Azure client and various parameters for message handling. It initializes internal state variables, including those related to model capabilities and usage tracking.\n\n### Message Processing and Creation\n\nThe methods within this class handle the process of creating chat completions from user messages, managing tools for enhanced capabilities, and issuing API requests.\n\n```python\nasync def create(self, messages: Sequence[LLMMessage], ...) -> CreateResult:\n    ...\n```\n\nThis core function processes messages, tools, and any additional parameters to format a request for the OpenAI API. It handles responses, usage tracking, and logging of events.\n\n### Streaming Support\n\nThe class includes support for streaming responses from OpenAI's API, allowing for incremental updates as data comes in:\n\n```python\nasync def create_stream(\n    self, \n    messages: Sequence[LLMMessage], \n    ...\n) -> AsyncGenerator[Union[str, CreateResult], None]:\n    ...\n```\n\nThis method emits string chunks and final messages, effectively allowing real-time interaction with the API.\n\n## Specialized Client Classes\n\n### OpenAI and Azure Client Classes\n\nTwo specialized classes extend the base client to support OpenAI and Azure-specific features:\n\n```python\nclass OpenAIChatCompletionClient(BaseOpenAIChatCompletionClient, ...):\n    ...\nclass AzureOpenAIChatCompletionClient(BaseOpenAIChatCompletionClient, ...):\n    ...\n```\n\nThese classes handle additional authentication and configuration specifics unique to OpenAI and Azure platforms, ensuring that the respective API calls are structured correctly.\n\n## Error Handling and Validation\n\n### Validation and Warnings\n\nThe code includes extensive validation checks to ensure configurations and parameters are appropriate for the intended API calls. Warnings are issued in various scenarios, such as when deprecated parameters are used or when expected message formats are not adhered to.\n\n### Logging Operations\n\nThroughout the code, instances of `logger.info()` and similar calls capture events, usage statistics, and potential issues with API interaction, providing insight into client behavior during execution.\n\n## Conclusion\n\nIn summary, this module acts as a comprehensive interface to OpenAI's chat completion systems, providing developers with the ability to easily configure, create, and manage interactive AI messages. The design emphasizes flexibility and thoroughness, ensuring that it can handle a variety of input types while managing complexities like token counting and API-specific configurations.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis Python module is part of a larger codebase and facilitates the registration and management of message transformers, which likely transform or process data associated with messages. It specifically imports functionalities and types needed for defining, registering, and applying these transformers.\n\n## Imports\n\nThe module imports several constants and functions from the `registry` and `types` submodules:\n\n### From `registry`\n\n- **MESSAGE_TRANSFORMERS**: Presumably a collection or list of currently registered transformers.\n- **build_conditional_transformer_func**: A function designed to create transformer functions that apply certain transformations based on specified conditions.\n- **build_transformer_func**: A utility for building transformer functions without conditional logic.\n- **get_transformer**: A function to retrieve a transformer from the registered transformers.\n- **register_transformer**: A function to register a new transformer, allowing it to be used within the application.\n\n### From `types`\n\n- **LLMMessageContent**: Likely a data type that defines the structure or schema for message content associated with a transformer.\n- **MessageParam**: A type representing the parameters that transformers might accept for processing messages.\n- **TransformerFunc**: This is presumably a type alias for functions that define how message transformations are executed.\n- **TransformerMap**: A type that may describe the relationship between different transformers and their identifiers.\n- **TrasformerReturnType**: Presumably a type that outlines the expected return type of transformer functions.\n\n## Purpose of Functions and Types\n\n### Functionality\n\nThis module serves as a registry for message transformers, enabling flexible message processing in a structured manner. Each of the imported functions and types plays a critical role in the functionality of the module.\n\n- **register_transformer**: Allows the addition of new transformers, promoting code modularity and enhancing extensibility. This is crucial for adding new transformations without altering the existing codebase significantly.\n  \n- **get_transformer**: Enables the retrieval of a specific transformer based on provided identifiers, allowing for selective application of transformations based on specific message content or parameters.\n\n- **build_transformer_func**: Facilitates the creation of transformer functions by possibly providing boilerplate or standard patterns that ensure consistency across different transformers.\n\n- **build_conditional_transformer_func**: Enhances the functionality of message transformations by allowing for branch logic \u2014 enabling different transformations to be applied based on specific conditions defined by user input or message content.\n\n### Types\n\n- **LLMMessageContent and MessageParam**: These types are likely used to enforce structure around the data that is being processed, ensuring that transformers can rely on consistent input formats.\n\n- **TransformerFunc and TransformerMap**: They help define what a transformation function should look like and how transformers are mapped and organized within the module.\n\n- **TrasformerReturnType**: This type serves as an expected return type for the transformers, providing clarity and type safety when carrying out message transformations.\n\n## Exposed API\n\nThe module explicitly lists its public API through the `__all__` variable, promoting a clean interface for users of this module. Developers can easily see what components are intended for use outside of the module, which aids in modular design and usability.\n\n### List Explanation\n\n- The inclusion of `register_transformer`, `get_transformer`, `build_transformer_func`, and `build_conditional_transformer_func` indicates that registration and retrieval of transformers, as well as their creation, are core functionalities.\n- Constants like `MESSAGE_TRANSFORMERS` provide immediate access to currently registered transformers, supporting dynamic interaction with the messages.\n- The various types (`TransformerMap`, `TransformerFunc`, etc.) ensure that there is a clear contract for what inputs and outputs are expected, making the module more robust and easier to maintain.\n\n## Conclusion\n\nIn summary, this module serves as a foundational component for managing message transformers effectively within a larger application. By providing an interface for registration, retrieval, and creation of transformer functions while enforcing type safety, it promotes modularity, maintainability, and extensibility. The clear structure and defined types enhance the robustness of message processing capabilities, allowing developers to focus on business logic without worrying about the underlying transformation mechanics.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_transformation/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview of Transformer Registration and Management System\n\nThis code defines a framework for managing transformer functions that convert messages between different formats, specifically for various pre-trained language models (LLMs). The key components include defining transformer functions, registering them for specific model families, and retrieving them based on API and model inputs.\n\n## Global Variables\n\n### MESSAGE_TRANSFORMERS\nA global dictionary that uses `defaultdict` to hold mappings of model families to their associated message transformers. The keys are API identifiers, and the values are dictionaries that map model family names (e.g., \"gpt-4o\", \"gemini-1.5-flash\") to a dictionary of transformers for different message types.\n\n## Transformer Function Builders\n\n### build_transformer_func\n\nThis function constructs a combined transformer function from a list of transformer functions. \n\n- **Parameters:**\n  - `funcs`: A list of transformer functions that accept an `LLMMessage` and a context dictionary.\n  - `message_param_func`: A function that accepts keyword arguments and constructs a message parameter.\n  \n- **Returns:** A function decorated as `transformer_func` which aggregates the results of all provided `funcs` and returns a list containing the output of `message_param_func`.\n\n#### Purpose\nThis function supports versatile transformation pipelines that enable future extensibility for modifying message parameters.\n\n### build_conditional_transformer_func\n\nSimilar to `build_transformer_func`, this function creates a transformer that selects between multiple functions based on specific conditions.\n\n- **Parameters:**\n  - `funcs_map`: A dictionary mapping conditions to lists of transformer functions.\n  - `message_param_func_map`: A dictionary mapping conditions to specific message parameter functions.\n  - `condition_func`: A function that evaluates the message and context to determine which condition to apply.\n  \n- **Returns:** A `transformer` function that executes the appropriate transformer functions based on the determined condition and merges their outputs.\n\n#### Purpose\nThis allows for dynamic transformation pipelines based on the content or context of messages, enhancing the flexibility of the system.\n\n## Transformer Registration\n\n### register_transformer\n\nA function that registers a mapping of transformer functions for a specified model family under a given API identifier.\n\n- **Parameters:**\n  - `api`: String identifier for the API.\n  - `model_family`: The family of models for which the transformers are being registered.\n  - `transformer_map`: A mapping of message types to their corresponding transformer functions.\n\n#### Purpose\nThis function enables the dynamic addition of new transformations for various message types, allowing the system to adapt to new models or requirements.\n\n## Model Family Identification\n\n### _find_model_family\n\nA private function that determines the best matching model family based on prefix matching of the model name.\n\n- **Parameters:** \n  - `api`: The API identifier.\n  - `model`: The specific model name to find a match for.\n\n- **Returns:** A string representing the best matching model family.\n\n#### Purpose\nThis function ensures that the system can intelligently handle model names and provide an appropriate transformer even when the exact model family is not known.\n\n## Transformer Retrieval\n\n### get_transformer\n\nThis function retrieves the transformer map associated with a specified API and model family.\n\n- **Parameters:**\n  - `api`: The API identifier.\n  - `model`: The specific model name.\n  - `model_family`: The family of the model.\n\n- **Returns:** The transformer map for the corresponding model family.\n\n#### Purpose\nThis function provides a consistent interface for accessing transformers, with flexibility built-in for enhancements such as logging, versioning, and fallback mechanisms for unknown model families.\n\n## Conclusion\n\nIn summary, this code provides a flexible and extensible system for registering and managing transformers for different model families of LLMs. With a focus on modularity and adaptability, the functions defined allow for dynamic handling of message transformations based on both conditions and model specifications. Overall, the implementation supports the evolving needs of applications that leverage LLMs by making it easier to manage and apply various transformations to inputs and outputs.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_transformation/registry.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code snippet defines several type aliases and data structures that are likely used to manage and transform message data in a conversational AI context. These definitions enable better type-checking and clarity throughout the code that will implement or utilize these structures and functions.\n\n## Imports\n\nThe code begins by importing key components necessary for type definitions and functionality:\n\n- **Typing Imports**: It imports `Any`, `Callable`, `Dict`, `List`, `Sequence`, `Type`, and `Union` from the `typing` module. These are used for providing type hints to enhance the readability and maintainability of the code.\n  \n- **From `autogen_core`**: It imports `FunctionCall`, `Image`, `LLMMessage`, and `FunctionExecutionResult`, which seem to be central to the message handling and function execution within a chatbot or conversational AI framework.\n\n- **From `openai.types.chat`**: It imports `ChatCompletionMessageParam` which indicates that this code may be targeting the OpenAI API or a similar chat service.\n\n## Type Aliases\n\nThe code defines several type aliases, which are shorthand representations for complex data types that improve clarity.\n\n### MessageParam\n\n```python\nMessageParam = Union[ChatCompletionMessageParam]\n```\n\n- **Purpose**: This alias defines `MessageParam` as a union type. Specifically, it currently allows for the type `ChatCompletionMessageParam`. This could be extended in the future to include additional message parameters if needed.\n\n### TrasformerReturnType\n\n```python\nTrasformerReturnType = Sequence[MessageParam]\n```\n\n- **Purpose**: This alias specifies that a transformer function will return a sequence of `MessageParam`, indicating that it can yield multiple message parameters as output.\n\n### TransformerFunc\n\n```python\nTransformerFunc = Callable[[LLMMessage, Dict[str, Any]], TrasformerReturnType]\n```\n\n- **Purpose**: This represents a callable type (function) that takes an `LLMMessage` and a dictionary of parameters (of any type) as inputs and returns a `TrasformerReturnType`. It outlines how a function designed to transform messages should be structured.\n\n### TransformerMap\n\n```python\nTransformerMap = Dict[Type[LLMMessage], TransformerFunc]\n```\n\n- **Purpose**: This defines a dictionary where the keys are types derived from `LLMMessage` and the values are functions that transform these messages. This structure allows for an organized way of mapping specific message types to their respective transformation functions.\n\n### LLMMessageContent\n\n```python\nLLMMessageContent = Union[\n    str,  # For SystemMessage.content\n    List[Union[str, Image]],  # For UserMessage.content\n    List[FunctionCall],  # For AssistantMessage.content\n    List[FunctionExecutionResult],  # For FunctionExecutionResultMessage.content\n]\n```\n\n- **Purpose**: This alias consolidates various types of content that can be included in messages from different message types. It allows for flexibility in handling various content formats:\n  - **System Messages**: Simple string content.\n  - **User Messages**: List of strings or images.\n  - **Assistant Messages**: List of `FunctionCall` objects, indicating actions the assistant might take.\n  - **Function Execution Results**: List of results from executed functions.\n\n## Summary\n\nIn summary, this code primarily serves as a foundation for type management in a chatbot or conversational AI application. By defining aliases and callable types, it helps ensure that developers have clear expectations for the structure and purpose of functions working with messages in the system. This kind of organization is crucial for maintaining code that is extensible and understandable over time, especially in complex systems that manipulate various message types and their contents.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_transformation/types.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script defines a function called `assert_valid_name` which is responsible for validating names composed of alphanumeric characters and specific symbols. It ensures that names conform to defined restrictions and raises exceptions if the names provided do not meet these criteria. This function is likely designed for pre-validation of user inputs or configurations, ensuring they are suitable for subsequent processing, possibly in relation to API interactions.\n\n## Function: `assert_valid_name`\n\n### Purpose\n\nThe `assert_valid_name` function checks whether a given name adheres to certain predefined formatting rules. It is intended to prevent the acceptance of invalid names that could lead to issues in processing or API interactions.\n\n### Parameters\n\n- **name (str)**: The name to be validated. It is expected to be a string that the user provides.\n\n### Function Logic\n\n1. **Regular Expression Validation**: \n   - The function utilizes a regular expression (regex) to ensure that the name consists solely of letters (both uppercase and lowercase), numbers, underscores (`_`), and hyphens (`-`). \n   - If the name contains any characters outside of this set, a `ValueError` is raised with a descriptive message indicating the invalid nature of the name.\n\n2. **Length Check**: \n   - After passing the regex test, the function checks the length of the name to ensure that it does not exceed 64 characters. \n   - Should the name be longer than 64 characters, another `ValueError` is raised, specifying that the name must be less than 64 characters.\n\n3. **Return Valid Name**: \n   - If the name passes both checks (regex and length), the function returns the validated name.\n\n### Error Handling\n\nThe function employs error handling by raising `ValueError` exceptions when invalid names are detected. This enhances robustness and allows the calling code to handle these exceptions appropriately, potentially logging errors or prompting user corrections.\n\n### Use Cases\n\n- The function could be used in environments where names are dynamically generated or input by users, such as in user account creation, configuration files, or in API requests.\n- It is particularly beneficial in situations where invalid input could lead to failure in further processing, such as calling external APIs that may have strict naming guidelines.\n\n### Integration with Other Functions\n\nThe docstring mentions a related function, `_normalize_name`, implying that names may undergo additional processing prior to being used (likely to sanitize or adapt them for specific contexts). However, this function is not defined in the provided code.\n\n## Conclusion\n\nIn summary, the `assert_valid_name` function plays a critical role in input validation for names in a system that requires strict adherence to naming conventions. By ensuring that names are in a valid format and of acceptable length, it mitigates risks associated with improperly formatted names, contributing to the overall reliability and stability of the application utilizing this function.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/_utils.py"
  },
  {
    "code": false,
    "content": "# Documentation for OpenAI Client Configuration Code\n\nThis document provides a detailed analysis of the provided source code, which defines various configurations and data models used for interfacing with OpenAI's API as well as Azure's OpenAI service. The source code utilizes Python's type hints and the `pydantic` library for data validation and management of structured data through typed dictionaries and models.\n\n## Overview of Imports\n\nThe code imports several modules and classes including:\n\n- **Typing Types**: Types such as `Awaitable`, `Callable`, `Dict`, `List`, `Literal`, `Optional`, and `Union` are used for type hinting throughout the code, enabling better static type checking.\n- **Pydantic**: The `BaseModel` class and `SecretStr` are imported from the `pydantic` library to define and validate complex data structures.\n- **autogen_core**: This module is expected to contain classes like `ComponentModel`, `ModelCapabilities`, and `ModelInfo` that are likely related to the configuration of AI models.\n\n## TypedDict Definitions\n\n### JSONSchema\n\nThe `JSONSchema` class is defined as a TypedDict that describes the format of a JSON schema response:\n\n- **Attributes**:\n  - `name`: Specifies the name of the response format.\n  - `description`: Provides a brief explanation of the response format's purpose.\n  - `schema`: Contains the actual JSON schema definition.\n  - `strict`: A boolean indicating whether strict adherence to the schema is required.\n\nThis structure ensures that the response format can be customized and validated correctly, following JSON Schema standards.\n\n### ResponseFormat\n\nThe `ResponseFormat` TypedDict defines three potential response types that the OpenAI client can handle:\n\n- **Attributes**:\n  - `type`: Specifies the response type, with allowed values being `text`, `json_object`, or `json_schema`.\n  - `json_schema`: An optional field for when the response type is `json_schema`, linking it to the `JSONSchema` TypedDict.\n\n## StreamOptions and CreateArguments\n\n### StreamOptions\n\nThe `StreamOptions` TypedDict includes:\n\n- **Attributes**:\n  - `include_usage`: A boolean that determines whether to include usage statistics in the response.\n\nThis dictionary caters to configuration options for data streaming from the OpenAI model.\n\n### CreateArguments\n\nThe `CreateArguments` TypedDict encapsulates various parameters that configure the API request:\n\n- **Attributes**:\n  - Includes fields such as `frequency_penalty`, `logit_bias`, `max_tokens`, and others that influence how the AI model generates outputs.\n  - The `response_format` field links to the `ResponseFormat` TypedDict, indicating the type of output expected.\n  \nThis structured approach allows for flexible configurations when creating requests to the OpenAI service.\n\n## Client Configuration Classes\n\n### BaseOpenAIClientConfiguration\n\nThis class extends `CreateArguments` and includes core parameters for API configuration:\n\n- **Attributes**:\n  - Fields like `model`, `api_key`, `timeout`, `max_retries`, and other model-specific options are also included here, providing a comprehensive configuration model for OpenAI clients.\n\n### OpenAIClientConfiguration and AzureOpenAIClientConfiguration\n\nBoth classes extend the `BaseOpenAIClientConfiguration` to cater to specific aspects of using OpenAI's API or Azure's OpenAI deployment:\n\n- **OpenAIClientConfiguration**:\n  - Adds fields like `organization` and `base_url` which are relevant for OpenAI's API calls.\n  \n- **AzureOpenAIClientConfiguration**:\n  - Introduces Azure-specific fields `azure_endpoint`, `azure_deployment`, and others to manage configuration for Azure's OpenAI service.\n\n## Pydantic Models\n\nThe latter part of the code defines Pydantic models that are equivalent to the previously defined TypedDicts, enabling easy validation and creation of configuration objects:\n\n### CreateArgumentsConfigModel\n\nThis model mirrors the `CreateArguments` TypedDict and uses Pydantic's features for automatic data validation and handling defaults.\n\n### Configuration Models\n\nSubsequent models like `BaseOpenAIClientConfigurationConfigModel`, `OpenAIClientConfigurationConfigModel`, and `AzureOpenAIClientConfigurationConfigModel` extend their respective TypedDict counterparts, inheriting fields and ensuring that each configuration is validated according to the rules defined in Pydantic.\n\n- **Attributes**:\n  - They include field types and default values, facilitating validation and ensuring the configurations are properly structured before use.\n\n## Conclusion\n\nThe code presented establishes a comprehensive framework for managing configurations for the OpenAI API and Azure's OpenAI deployment. By using `TypedDict`, `Pydantic`, and type hints extensively, it helps maintain structured data integrity while providing flexibility for users when defining request parameters and handling responses. This model-centric approach not only aids in clarity and maintainability of the code but also simplifies the task of creating requests to the AI services.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/openai/config/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface for managing the export of the `ReplayChatCompletionClient` class from the `_replay_chat_completion_client` submodule. By utilizing the `__all__` variable, it defines the public API of the module, indicating which components can be accessed when the module is imported.\n\n## Import Statement\n\n```python\nfrom ._replay_chat_completion_client import ReplayChatCompletionClient\n```\n\nThis line imports the `ReplayChatCompletionClient` class from the `_replay_chat_completion_client` module, which is presumably located within the same package. This indicates that the code is organized in a package structure, and the underscore prefix suggests that `_replay_chat_completion_client` is intended for internal use while providing its class to other modules through this interface.\n\n## Public API Definition\n\n```python\n__all__ = [\n    \"ReplayChatCompletionClient\",\n]\n```\n\nThe `__all__` list specifies that `ReplayChatCompletionClient` is the only public component of this module. When another module imports this module using `from module_name import *`, only the items in the `__all__` list will be imported. This is a common convention used to control the visibility of attributes and classes, helping to prevent namespace pollution and maintain encapsulation.\n\n## Purpose of `ReplayChatCompletionClient`\n\nWhile the specific contents of `ReplayChatCompletionClient` are not visible in the provided code snippet, we can infer its purpose based on the naming conventions. Typically, a class named `ReplayChatCompletionClient` suggests that it may be responsible for handling chat completion functionalities, possibly simulating or replaying previous chat interactions in a system involving interactive messaging or chatbots.\n\n## Summary of Module Functionality\n\n1. **Single Class Export**: The module primarily focuses on exporting a single client class to be used elsewhere, maintaining a streamlined interface for users of the module.\n\n2. **Encapsulation**: By using an underscore in the module name of the class, the design encourages developers to understand that `_replay_chat_completion_client` may contain implementation details not intended for public use.\n\n3. **Clear API Structure**: The use of `__all__` provides clarity on what functionalities are publicly exposed, adhering to best practices for modular programming.\n\n4. **Facilitation of Chat Functions**: While we do not see the implementation, the `ReplayChatCompletionClient` is likely designed to encapsulate various operations related to replaying or managing chat completions, hinting at functionality that deals with enhancing user interactions in chat applications. \n\n5. **Modular Design**: This pattern suggests a modular design, where individual units like `ReplayChatCompletionClient` could be more easily tested or replaced as the application evolves.\n\n6. **Potential extensibility**:  If additional features or classes are developed in the `_replay_chat_completion_client`, they can be easily integrated into this module, keeping the public interface neat and focused.\n\n7. **Intended Usage**: In practical terms, a user or developer importing this module would likely use the `ReplayChatCompletionClient` to instantiate an object that facilitates chat-based functions, such as replaying scenarios or interactions, though the specifics would depend on the implementation details of that class.\n\n8. **Industry Relevance**: This kind of module structure is relevant in industries that utilize interactive applications, such as customer support, gaming, and conversational AI, where chat completion and user engagement are crucial.\n\nBy serving as an organized entry point to the `ReplayChatCompletionClient`, this module plays a critical role in the larger system it is part of, enhancing modularity and promoting clean coding practices.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/replay/__init__.py"
  },
  {
    "code": false,
    "content": "# ReplayChatCompletionClient Documentation\n\n## Overview\n\nThe `ReplayChatCompletionClient` is a specialized class that simulates a chat completion client by replaying a predefined list of responses. It is part of the `autogen` framework and is built to protect valuable resources by allowing developers to define static responses. This is particularly useful in testing and development, where dynamic model responses are unnecessary or when wanting to simulate specific interaction patterns.\n\n## Configuration Class\n\n### `ReplayChatCompletionClientConfig`\n\nThis class extends `BaseModel` and serves as the configuration schema for the `ReplayChatCompletionClient`. It accepts the following parameters:\n\n- **`chat_completions`**: A sequence of predefined responses that can either be strings or `CreateResult` objects.\n- **`model_info`**: Optional metadata about the model, encapsulated in a `ModelInfo` object.\n\n### Purpose\nThe configuration class helps in initializing the replay client without tight coupling to the data structure used for chats and responses, providing flexibility when creating instances of the client.\n\n## Main Class\n\n### `ReplayChatCompletionClient`\n\nThis class inherits from `ChatCompletionClient` and `Component`, enabling it to act and configure in environments where component-based architecture is present. \n\n#### Initialization \nThe constructor accepts a sequence of chat completions and optional model information. When instantiated, it sets up variables to track state, usage statistics, and potential token limits.\n\n### Key Features\n\n- **Replay Responses**: It can return predefined messages, either as a single completion or through streaming.\n- **State Management**: It maintains an internal index to track which response to send next and provides a reset mechanism to restart the reply sequence.\n- **Token Tracking**: Implements mechanisms to track token usage for prompts and completions, simulating a realistic usage scenario for planning quotas or limits.\n\n## Methods\n\n### `create`\n\nThis asynchronous method generates the next chat completion based on the input messages. Key features include:\n\n- Returning a response from the `chat_completions` list, identified by the current index.\n- Updating usage statistics, including token counts for prompts and completions.\n- Handling both string responses and `CreateResult` objects, encapsulating additional response metadata.\n\n#### Raises:\n- Raises a `ValueError` when there are no more responses available in the predefined list.\n\n### `create_stream`\n\nThis asynchronous method allows responses to simulate streaming behavior, yielding individual tokens before returning the complete response. It maintains the same logic as `create` for determining responses and usage, but outputs token-by-token.\n\n### `close`\n\nA placeholder method for cleanup when closing the client. As the class is a mock, it currently does nothing.\n\n### `reset`\n\nResets the state of the client to its initial state, clearing usage statistics and resetting the current response index. This method is essential for re-testing or replaying sequences without needing to re-instantiate the client.\n\n### `actual_usage` and `total_usage`\n\nThese properties return the current token usage for the last call made and the total token usage accumulated over time, respectively. \n\n### `count_tokens` and `remaining_tokens`\n\nThese methods compute the number of tokens used in a given sequence of messages. They can be useful to evaluate if the client has enough tokens left to process further messages.\n\n### Internal Utility Methods\n\n- **`_tokenize`**: An internal method responsible for splitting messages into tokens and counting them. It supports both single messages and sequences, including basic error handling for message content.\n- **`_update_total_usage`**: Updates the total usage statistics based on the current call's usage metrics.\n\n## Properties\n\n### `capabilities`\n\nThis deprecated property warns developers that it is advisable to use the `model_info` property instead, which provides a detailed structure of the model's capabilities through the underlying `ModelInfo` object.\n\n### `model_info`\n\nReturns model-specific information, including features like vision capabilities or function calling.\n\n## Configuration Methods\n\n### `_to_config`\n\nThis method converts the current client state back to a `ReplayChatCompletionClientConfig` object, facilitating storage or transmission of configuration settings.\n\n### `_from_config`\n\nA class method that creates a new `ReplayChatCompletionClient` instance from a given configuration object. This allows for the re-creation of client states in a structured way.\n\n## Example Usage\n\nThe documentation includes various code snippets that demonstrate how to use the `ReplayChatCompletionClient`, such as creating a simple chat or using the reset functionality:\n\n```python\nasync def example():\n    chat_completions = [\n        \"Hello, how can I assist you today?\",\n        \"I'm happy to help with any questions you have.\",\n    ]\n    client = ReplayChatCompletionClient(chat_completions)\n    messages = [UserMessage(content=\"What can you do?\", source=\"user\")]\n    response = await client.create(messages)\n```\n\nThis showcases the client's straightforward design, emphasizing ease of use for both synchronous and asynchronous response handling.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/replay/_replay_chat_completion_client.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for the Chat Completion Adapter\n\n## Overview\n\nThis small module essentially sets up an interface to expose the `SKChatCompletionAdapter`. It imports this specific class from a relative module and specifies it for public access within the module's namespace. \n\n## Imports\n\n```python\nfrom ._sk_chat_completion_adapter import SKChatCompletionAdapter\n```\n\nIn this line, the module imports the `SKChatCompletionAdapter` from a sibling module named `_sk_chat_completion_adapter`. This indicates that the functionalities of the `SKChatCompletionAdapter` will be utilized, possibly to handle chat completions in some capacity. The use of a relative import suggests that this module is part of a larger package structure.\n\n## Public Interface\n\n```python\n__all__ = [\"SKChatCompletionAdapter\"]\n```\n\nThe `__all__` list defines the public API of the module, indicating which components should be exposed when the module is imported. By including `SKChatCompletionAdapter` in this list, it suggests that this class is intended for use outside the module, serving as the primary interface for the users of the package.\n\n## Purpose of SKChatCompletionAdapter\n\nAlthough the class definition of `SKChatCompletionAdapter` is not provided in the given code, we can infer its role based on its name and context. The name suggests that it is an adapter for handling chat completion tasks, likely wrapping around some underlying functionality for generating or managing chat-based interactions.\n\n## Potential Use Cases\n\n1. **Chatbot Development**: The `SKChatCompletionAdapter` could be used in the creation of AI bots that assist users by generating responses based on chat input.\n2. **Natural Language Processing**: It may be employed in applications that require understanding and generating human-like responses in conversational settings.\n3. **Integration with Other Systems**: Given that it is an adapter, it might also serve to integrate chat functionalities with different systems or services, potentially converting data formats or managing API requests.\n\n## Expected Functionality\n\nWhile the exact functionality of the `SKChatCompletionAdapter` class is not detailed, one can expect it to include features such as:\n\n- **Response Generation**: The ability to generate meaningful responses based on user input through algorithms or models.\n- **Context Management**: Maintaining the context of conversation to provide coherent and context-aware replies.\n- **API Interfacing**: Facilitating communication between frontend applications and backend services that handle processing requests for chat completions.\n\n## Conclusion\n\nThis module plays an essential role in establishing public access to the `SKChatCompletionAdapter` class and forms a part of a package that likely supports functionalities related to chat interfaces. By using relative imports and defining its public API, it ensures the proper encapsulation and accessibility of its components, laying the groundwork for building advanced conversational systems.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/semantic_kernel/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for SKChatCompletionAdapter\n\n## Overview\n\nThe `SKChatCompletionAdapter` acts as a bridge that enables the integration of various Semantic Kernel model clients into the Autogen framework, specifically as a `ChatCompletionClient`. This facilitates seamless interactions between Autogen agents and AI models from different providers like Azure OpenAI, Google Gemini, and Ollama. It helps in managing chat completions, function calls, and tool invocations while ensuring compatibility with Autogen's tools and plugins.\n\n## Dependencies & Imports\n\nThe adapter uses several components from the `autogen_core`, `semantic_kernel`, and `pydantic` libraries to manage its functionality:\n\n1. **Logging**: Standard logging setup for event capturing.\n2. **JSON Handling**: Utilities for serialization of data, especially Pydantic models.\n3. **Type Hinting**: Provides type definitions for better code clarity and error checking.\n4. **Model Handling**: Integrates various models from `autogen_core.models`, aiding in data validation and model configuration.\n\n## Function: `ensure_serializable`\n\nThis function ensures that the provided Pydantic model can be serialized into JSON format. If direct serialization fails (due to non-serializable fields), it employs a workaround that converts the model's attributes into a dictionary, allowing successful serialization. This function helps maintain data integrity when communicating with APIs or logging systems.\n\n## Class: `SKChatCompletionAdapter`\n\n### Constructor\n\nThe constructor initializes several parameters essential for creating a chat completion session:\n\n- **`sk_client`**: The specific Semantic Kernel client that interfaces directly with an AI model.\n- **`kernel`**: A Semantic Kernel instance that's crucial for executing requests.\n- **`prompt_settings`**: Settings for how prompts are executed, with options for defaults and overrides.\n- **`model_info`**: Contains information about the model's capabilities, such as whether it supports function calling or JSON output.\n- **`service_id`**: An optional identifier for better context when calling various services.\n\n### Method: `_convert_to_chat_history`\n\nThis method transforms a sequence of `LLMMessage` (Autogen's message format) into `ChatHistory`, which is a format understood by the Semantic Kernel. The conversion accounts for various message types, ensuring that system, user, and assistant messages are structured correctly, including function calls and their results when applicable.\n\n### Method: `_build_execution_settings`\n\nThis method constructs the `PromptExecutionSettings` for the Semantic Kernel client. The settings are based on both default prompt settings and any tools that may be invoked during the chat. This is crucial as it dictates how the chat completion behaves, including how tools are permitted to operate.\n\n### Method: `create`\n\nThis is a primary method responsible for generating chat completions. The method accepts various arguments, including:\n\n- **`messages`**: The previous chat messages that inform the context for the current request.\n- **`tools`**: Any auxiliary tools that might be invoked as part of the conversation.\n- **`tool_choice`**: Defines whether tools are required, auto-invoked, or not used.\n- **`extra_create_args`**: Additional parameters for customizing behavior based on specific needs.\n- **`cancellation_token`**: Allows for request cancellation if needed.\n\nThe function orchestrates communication with the Semantic Kernel client and handles responses, such as processing any tool calls that may occur during the interaction.\n\n### Method: `create_stream`\n\nAn asynchronous generator that creates a streaming chat completion. This allows the client to yield responses progressively, which is particularly useful in conversational applications where responses can be elongated or need to simulate real-time interactions. The method keeps track of function calls and manages the completion of those calls, yielding control back to the calling function as data becomes available.\n\n### Utility Methods\n\n- **`_merge_function_call_content`**: Merges arguments from new function calls into existing calls, ensuring that no data is lost even if responses come in chunks or the same call is updated multiple times.\n- **`actual_usage`**: Provides the current token usage statistics, which is vital for managing quotas and understanding how much of the model's capacity is used.\n- **`remaining_tokens`**: Computes how many tokens remain available for use when interacting with the AI model based on known limits.\n  \n## Conclusion\n\nThe `SKChatCompletionAdapter` class is a sophisticated component designed for enhancing interaction with various Semantic Kernel AI models within an Autogen framework. It provides necessary abstraction and utility for efficient communication, including message structuring, tool synchronization, and comprehensive completion management. Through its robust design, it enables developers to create adaptive and interactive AI-driven applications leveraging existing AI models from multiple providers.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/models/semantic_kernel/_sk_chat_completion_adapter.py"
  },
  {
    "code": false,
    "content": "It seems you haven't provided any source code for analysis. Please share the code you would like me to analyze, and I'll create the documentation as requested.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/__init__.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module is a part of a package designed to implement gRPC (Google Remote Procedure Call) runtime features. It provides an interface for invoking methods on remote servers and handling communications between clients and servers with a focus on worker agents.\n\n## Imports and Dependencies\n\nAt the beginning of the module, three components are imported from submodules:\n\n- `GrpcWorkerAgentRuntime`: Likely serves as the core runtime class for gRPC worker agents.\n- `GrpcWorkerAgentRuntimeHost`: Presumably represents the host environment that runs the worker agents.\n- `GrpcWorkerAgentRuntimeHostServicer`: Most likely acts as a service handler, processing incoming gRPC requests and managing interactions between clients and the worker agents.\n\nThe module attempts to import the `grpc` library, which is required for gRPC functionality. If the `grpc` module is not found, an `ImportError` is raised, advising the user to install the relevant package using pip. This ensures that all necessary dependencies are in place before executing the module's functionalities.\n\n## Error Handling\n\nThe error handling around the import of the `grpc` library is an essential part of this module. It informs developers of the specific installation requirements for utilizing the gRPC runtime. The usage of `type: ignore` in the import statement indicates that type checkers should ignore any potential issues with the import, allowing for flexibility in the codebase during static analysis.\n\n## Exported Components\n\nIn the `__all__` list, the module specifies which objects are publicly accessible when importing from this module. The components included are:\n\n- `GrpcWorkerAgentRuntime`\n- `GrpcWorkerAgentRuntimeHost`\n- `GrpcWorkerAgentRuntimeHostServicer`\n\nThis export mechanism indicates the intended public API of the module, allowing users to understand which classes or functions they can work with without delving into the entire codebase.\n\n## Summary\n\nIn summary, this module serves as an initialization point for the gRPC runtime environment within a broader application. It establishes dependencies, handles errors regarding those dependencies, and defines the public interface for the classes that manage worker agents and their hosting environments. \n\nBy encapsulating the functional aspects related to gRPC in this way, the module aims to provide a clean and clear entry point for developers implementing remote procedure calls in their applications using worker agent models.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/__init__.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Source Code\n\nThis piece of source code is a configuration script intended to define various constants used in the context of a distributed runtime environment, possibly leveraging gRPC (Google Remote Procedure Call) for communication between agents or services. The code outlines specific attributes and message kinds that facilitate the management of data and settings necessary for operation in a distributed system.\n\n## Constants and Configuration Strings\n\n### GRPC Import Error Message\n\n```python\nGRPC_IMPORT_ERROR_STR = (\n    \"Distributed runtime features require additional dependencies. Install them with: pip install autogen-core[grpc]\"\n)\n```\n\nThe constant `GRPC_IMPORT_ERROR_STR` provides an error message that is likely displayed when certain gRPC-related dependencies are not installed. This is a crucial instructional message for users or developers who are attempting to use distributed runtime features but have not correctly set up their environment. The message suggests a specific command to install the missing dependencies, guiding them toward resolving the issue.\n\n### Data Content Type and Schema Attributes\n\n```python\nDATA_CONTENT_TYPE_ATTR = \"datacontenttype\"\nDATA_SCHEMA_ATTR = \"dataschema\"\n```\n\nThese two constants, `DATA_CONTENT_TYPE_ATTR` and `DATA_SCHEMA_ATTR`, seem to represent attribute names that can be used for data metadata. `DATA_CONTENT_TYPE_ATTR` likely denotes the type of content being handled (such as JSON, XML, etc.), while `DATA_SCHEMA_ATTR` signifies the schema or structure these data instances adhere to. These attributes would be especially useful for ensuring data integrity and compatibility between different components of the system.\n\n### Agent Sender Attributes\n\n```python\nAGENT_SENDER_TYPE_ATTR = \"agagentsendertype\"\nAGENT_SENDER_KEY_ATTR = \"agagentsenderkey\"\n```\n\nThe constants `AGENT_SENDER_TYPE_ATTR` and `AGENT_SENDER_KEY_ATTR` are defined to categorize messages based on the agent that sends them. The `AGENT_SENDER_TYPE_ATTR` may specify the type of sender (e.g., a specific service or component), while `AGENT_SENDER_KEY_ATTR` would serve as a unique identifier or key for the sender agent. This enables better tracing and management of messages in a distributed environment.\n\n### Message Kind Attributes\n\n```python\nMESSAGE_KIND_ATTR = \"agmsgkind\"\nMESSAGE_KIND_VALUE_PUBLISH = \"publish\"\nMESSAGE_KIND_VALUE_RPC_REQUEST = \"rpc_request\"\nMESSAGE_KIND_VALUE_RPC_RESPONSE = \"rpc_response\"\nMESSAGE_KIND_VALUE_RPC_ERROR = \"error\"\n```\n\nIn this section, `MESSAGE_KIND_ATTR` represents the type of message being sent or received within the system. The accompanying constants (`MESSAGE_KIND_VALUE_PUBLISH`, `MESSAGE_KIND_VALUE_RPC_REQUEST`, `MESSAGE_KIND_VALUE_RPC_RESPONSE`, `MESSAGE_KIND_VALUE_RPC_ERROR`) define the various types of messages that can be utilized. \n\n- `MESSAGE_KIND_VALUE_PUBLISH` indicates a standard publish operation, likely used for broadcasting information.\n- `MESSAGE_KIND_VALUE_RPC_REQUEST` identifies messages that are requests to invoke a remote procedure or service.\n- `MESSAGE_KIND_VALUE_RPC_RESPONSE` denotes the messages receiving responses from such requests.\n- `MESSAGE_KIND_VALUE_RPC_ERROR` signals that an error occurred during any RPC call, facilitating error handling and debugging.\n\n## Summary of Interactions and Usages\n\nThe constants defined in this script collectively help in the orchestration of operations in a distributed system utilizing gRPC. They play a significant role in ensuring that the components involved can communicate effectively, understand the structure and content of data being exchanged, and can adequately respond to or report errors.\n\nBy categorizing message types and sender attributes, the code establishes a clear protocol for inter-agent communication, which is essential for any robust distributed application. This structured approach aids developers in maintaining clarity and organization within their messaging systems, ultimately contributing to smoother operations and easier troubleshooting when issues arise. \n\nOverall, while this script does not include functional programming elements such as functions or classes, the constants act as foundational building blocks that other components of the system may rely on for their functionalities related to gRPC and distributed operations. The design reflects common practices in defining shared configurations and constants in software engineering.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_constants.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code snippet defines a type alias for a specific data structure in Python, facilitating type hinting in a codebase that aims to utilize `gRPC` with a focus on asynchronous I/O (Aio). Below, the components of the code are detailed for clarity.\n\n## Type Alias Definition\n\n### `ChannelArgumentType`\n\n```python\nChannelArgumentType = Sequence[Tuple[str, Any]]\n```\n\n- **Purpose**: This line creates a type alias called `ChannelArgumentType`. This alias is intended to simplify type annotations and enhance code readability by encapsulating a complex type definition.\n  \n- **Structure**: \n  - The type `ChannelArgumentType` is defined as a `Sequence` of `Tuple` elements, where each `Tuple` consists of:\n    - A `str` type as the first element.\n    - An `Any` type as the second element.\n  \n- **Implication**: This structure typically represents a list or a similar collection type that holds several pairs of settings or options relevant to a channel in a gRPC application. The first element in each pair signifies the name of a channel argument, while the second element accommodates any type of value associated with it (since `Any` allows for flexibility).\n\n## Context and Usage\n\n### redefinition aspect\n\n- **Context**: The comment preceding the type alias highlights that this particular definition is a workaround. The alias was necessary due to type-checking issues (`mypy` errors) encountered when attempting to use the original type from `grpc.aio._typing`.\n  \n- **Significance**: By re-defining `ChannelArgumentType`, the author aims to avoid complications that may arise from the gRPC's internal type definitions, thereby maintaining compatibility with type checkers like `mypy`.\n\n## Benefits of Type Aliases\n\n- **Readability**: By assigning a descriptive name to a tuple structure, the code achieves better readability. Developers can intuitively understand that `ChannelArgumentType` pertains to channel arguments without dealing with the complexity of the type definition each time.\n\n- **Maintainability**: If the underlying structure needs to change or be updated, only the type alias definition would require modification, leaving the rest of the codebase intact and consistent.\n\n- **Type Safety**: Utilizing type aliases allows for better type checking and ensures that developers know what data structures to expect and how to interact with them.\n\n## Conclusion\n\nIn summary, this code snippet introduces a helpful type alias for managing channel arguments in a gRPC asynchronous context. The development environment benefit from this abstraction includes enhanced clarity, improved maintainability, and adequate type safety, while addressing specific issues presented by `mypy` and existing type definitions. The overall design intention is to streamline and correct type handling in a potentially complex gRPC application.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_type_helpers.py"
  },
  {
    "code": false,
    "content": "# Subscription Protocol Conversion\n\nThis module provides functionality to convert between subscription objects used in a specific application (represented by the `Subscription`, `TypePrefixSubscription`, and `TypeSubscription` classes from the `autogen_core` package) and their protocol buffer representations (defined in `agent_worker_pb2`).\n\n## Overview\n\nThe code defines two primary functions for handling the serialization and deserialization of subscription objects: `subscription_to_proto` and `subscription_from_proto`. These functions facilitate the conversion of subscriptions used in code to their protocol buffer equivalents and vice versa. This is particularly useful for applications that need to communicate subscription data over a network or between different components.\n\n### Dependencies\n\nThe code imports several classes from the `autogen_core` package:\n- `Subscription`\n- `TypePrefixSubscription`\n- `TypeSubscription`\n\nAdditionally, it imports the protocol definition `agent_worker_pb2` from a specific local module. The `agent_worker_pb2` module contains the protocol buffer messages that correspond to the subscription types.\n\n## `subscription_to_proto`\n\n### Purpose\n\nThe `subscription_to_proto` function is responsible for converting an instance of a subscription object into its protocol buffer format.\n\n### Parameters\n\n- **subscription (Subscription)**: An instance of `Subscription` which can be either a `TypeSubscription` or a `TypePrefixSubscription`.\n\n### Functionality\n\nThe function uses a `match` statement to determine the type of the incoming subscription:\n1. **TypeSubscription**: If the subscription is of the `TypeSubscription` class, it extracts the `topic_type`, `agent_type`, and `id`, and constructs a `agent_worker_pb2.Subscription` object, initializing it with the appropriate `TypeSubscription`.\n2. **TypePrefixSubscription**: If the subscription is of the `TypePrefixSubscription` class, it similarly constructs a `agent_worker_pb2.Subscription`, but initializes it with a `TypePrefixSubscription`.\n3. **Error Handling**: If the subscription type is not recognized, it raises a `ValueError`, indicating that the subscription type is unsupported.\n\n### Return Value\n\nThe function returns an `agent_worker_pb2.Subscription` object representing the serialized form of the original `Subscription`.\n\n## `subscription_from_proto`\n\n### Purpose\n\nThe `subscription_from_proto` function is aimed at converting a protocol buffer subscription message back into an application-specific subscription object.\n\n### Parameters\n\n- **subscription (agent_worker_pb2.Subscription)**: An instance of a subscription in its protocol buffer format.\n\n### Functionality\n\nThe function uses the `WhichOneof` method to identify which type of subscription message it is dealing with:\n1. **typeSubscription**: If the message contains a type subscription, it extracts the `topic_type`, `agent_type`, and `id` from the `typeSubscription` field and constructs a corresponding `TypeSubscription` object.\n2. **typePrefixSubscription**: If it is a type prefix subscription, the function extracts relevant data to create a `TypePrefixSubscription` object.\n3. **Error Handling**: If the message does not match either of the known subscription types or is missing entirely, a `ValueError` is raised, indicating the invalid subscription message.\n\n### Return Value\n\nThe function returns either a `TypeSubscription` or `TypePrefixSubscription` object, depending on the content of the provided protocol buffer message.\n\n## Summary\n\nThis module effectively bridges the gap between high-level application logic and low-level protocol buffer messaging, providing essential methods to translate subscription formats in both directions. The use of pattern matching allows the code to be both expressive and concise, ensuring that different subscription types can be handled efficiently while maintaining proper error handling for unsupported cases.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_utils.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code defines an asynchronous framework for managing remote agents using gRPC as the communication protocol. It includes classes for establishing a connection to a host, sending and receiving messages, and managing the lifecycle of agent runtimes. The primary classes involved are `HostConnection` and `GrpcWorkerAgentRuntime`, which together facilitate the operation of agents that can communicate over a defined protocol.\n\n## HostConnection Class\n\n### Purpose\nThe `HostConnection` class manages the connection to a gRPC server where agents communicate. It encapsulates the logic for sending and receiving messages asynchronously.\n\n### Key Features\n- **Connection Management**: It initiates a gRPC channel and a stub to communicate with the server. \n- **Message Queues**: The class handles two queues\u2014one for messages to send and another for messages received.\n- **Asynchronous Operations**: It utilizes `asyncio` tasks to handle reading from the gRPC stream continuously, ensuring non-blocking operations.\n\n### Important Methods and Properties\n- `from_host_address`: A class method that establishes a connection to a given host address and creates an instance of `HostConnection`.\n- `send(message)`: Places a message in the send queue to be sent to the host.\n- `recv()`: Retrieves a message from the receive queue.\n- `_connect()`: Establishes the streaming connection and maintains an asynchronous read loop.\n\n## QueueAsyncIterable Class\n\n### Purpose\nThe `QueueAsyncIterable` class acts as a bridge to convert an `asyncio.Queue` into an asynchronous iterable, facilitating the reading of messages from the queue in a gRPC streaming context.\n\n### Functionality\n- Implements `AsyncIterable` and `AsyncIterator` to allow iteration over items in the queue as they arrive.\n- The `__anext__()` method retrieves messages asynchronously using `queue.get()`.\n\n## GrpcWorkerAgentRuntime Class\n\n### Purpose\nThe `GrpcWorkerAgentRuntime` class is the main component responsible for running agents in a distributed environment using gRPC. It interacts with the host and processes incoming and outgoing messages to manage agent lifecycles.\n\n### Key Features\n- **Agent Management**: The class provides methods for registering agent factories and instances, and managing subscriptions.\n- **Message Handling**: It processes different types of messages (RPC requests, responses, and events) and manages the serialization and deserialization of message payloads.\n\n### Important Methods\n- `start()`: Initializes the runtime and connects to the host.\n- `stop()`: Terminates the runtime and cleans up resources.\n- `_run_read_loop()`: Continuously listens for incoming messages from the host and delegates processing based on message type.\n- `send_message()`: Asynchronously sends a message to a designated agent.\n- `publish_message()`: Publishes a message to a topic.\n\n## Messaging and Serialization\n\n### Message Types\nThe code utilizes Protobuf for message serialization, supporting types such as `RpcRequest`, `RpcResponse`, and `CloudEvent`. This enables cross-language communication among agents.\n\n### Serialization Registry\nA `SerializationRegistry` is employed to manage the serialization and deserialization of different message payloads. It determines how messages are serialized based on their type.\n\n## Error Handling and Logging\n\n### Logging\nThe code utilizes the `logging` module to log various events and messages during execution, aiding in debugging and monitoring.\n\n### Error Handling\nThe runtime is designed to manage exceptions gracefully, with mechanisms in place to handle errors that may arise during message processing or connection management. Callbacks are used to raise exceptions related to background tasks.\n\n## Agent Management\n\n### Registration\nThe runtime supports the registration of agent types and their associated factories. This allows flexible instantiation of agents depending on the defined runtime.\n\n### Requests and Responses\nA structured process manages requests and responses, ensuring that communication between agents and the runtime is handled according to the defined protocol, enabling features like callbacks for results.\n\n## Subscription Management\n\n### Subscribing to Topics\nAgents can register for subscriptions to specific topics, allowing them to receive messages relevant to those topics. The `SubscriptionManager` class helps manage these subscriptions internally.\n\n### Adding and Removing Subscriptions\nMethods like `add_subscription()` and `remove_subscription()` facilitate dynamic topic subscription management during the agent's lifecycle.\n\n## Signal Handling and Graceful Shutdown\n\n### Handling System Signals\nThe runtime includes mechanisms to listen for system signals (e.g., SIGTERM, SIGINT) for graceful shutdown, utilizing `asyncio.Event` to manage the shutdown process seamlessly.\n\n## Conclusion\n\nIn summary, this code provides a robust asynchronous architecture for managing remote agents, encapsulating the complexities of communication via gRPC, message handling, and agent lifecycle management. It lays the groundwork for building distributed systems using agents that communicate and interact based on defined protocols and message types.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_worker_runtime.py"
  },
  {
    "code": false,
    "content": "# GrpcWorkerAgentRuntimeHost Documentation\n\nThis module implements an asynchronous gRPC server using Python's asyncio framework. It facilitates the creation, management, and graceful termination of the gRPC server, which is designed to serve requests through a specified address. Below is a structured overview of the key components and functionality provided by the `GrpcWorkerAgentRuntimeHost` class.\n\n## Imports and Constants\n\nThe code begins by importing necessary libraries such as `asyncio`, `logging`, and `signal`, as well as some types and constants used within the class. Notably, it attempts to import the `grpc` library and raises a custom error if the import fails, ensuring that the program fails gracefully when dependencies are missing.\n\n```python\ntry:\n    import grpc\nexcept ImportError as e:\n    raise ImportError(GRPC_IMPORT_ERROR_STR) from e\n```\n\n## Class: GrpcWorkerAgentRuntimeHost\n\n### Purpose\n\nThe `GrpcWorkerAgentRuntimeHost` class encapsulates the functionality to create and manage a gRPC server which responds to requests as defined by the `GrpcWorkerAgentRuntimeHostServicer`. This class handles server initialization, starting and stopping the server, and graceful shutdowns upon receiving specific operating system signals.\n\n### Initialization\n\n#### `__init__(self, address: str, extra_grpc_config: Optional[ChannelArgumentType] = None)`\n\nThe constructor initializes the gRPC server with the provided address and optional configuration parameters. During initialization, it also sets up the servicer to process the incoming gRPC calls:\n\n- `self._server`: The gRPC server instance created asynchronously.\n- `self._servicer`: An instance of `GrpcWorkerAgentRuntimeHostServicer` which processes requests.\n- The server listens for incoming connections at the specified address.\n\n### Starting the Server\n\n#### `start(self)`\n\nThis method initiates the gRPC server by creating an asynchronous task that runs the `_serve` method. If the server is already running, an exception is raised to prevent multiple instances from running simultaneously.\n\n- **Error Handling**: It raises a `RuntimeError` if the server is already started.\n\n### Serving Requests\n\n#### `_serve(self)`\n\nThis is an internal asynchronous method that manages the server lifecycle:\n\n- It starts the gRPC server and logs its address.\n- The server will continue running until termination is requested.\n\n### Stopping the Server\n\n#### `stop(self, grace: int = 5)`\n\nThis asynchronous method stops the running gRPC server. It first checks if the server is running, and if so, it initiates termination:\n\n- It calls `stop()` on the gRPC server with an optional grace period to allow ongoing requests to complete.\n- It then cancels the serving task and logs that the server has stopped.\n\n### Graceful Shutdown with Signal Handling\n\n#### `stop_when_signal(self, grace: int = 5, signals: Sequence[signal.Signals] = (signal.SIGTERM, signal.SIGINT))`\n\nThis method allows the server to shut down gracefully upon receiving specific signals (like `SIGTERM` and `SIGINT`). It sets up a signal handler that waits for an event indicating the shutdown request.\n\n- **Signal Setup**: It utilizes the running event loop to register signal handlers for graceful shutdown.\n- It waits for the shutdown event to occur before calling the `stop` method to terminate the server.\n\n## Conclusion\n\nThe `GrpcWorkerAgentRuntimeHost` class provides a comprehensive solution for managing a gRPC server in an asynchronous environment. It includes methods for starting and stopping the server, ensuring that both initiation and termination are handled cleanly, including support for graceful shutdowns in response to operating system signals. This design follows best practices for asynchronous programming, making it suitable for production environments where reliable and efficient service management is crucial.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_worker_runtime_host.py"
  },
  {
    "code": false,
    "content": "# gRPC Worker Agent Runtime Host\n\nThis document provides a high-level overview of a gRPC-based message delivery system designed for agent communication within a concurrent environment. It outlines the core functionalities, classes, and methods in the codebase, which leverages asyncio for asynchronous operations and gRPC for remote procedure calls.\n\n## Overview\n\nThe primary role of this code is to facilitate communication between clients (agents) through a gRPC service, enabling agents to send and receive messages. The code includes classes and methods to manage connections, handle messages, and maintain subscription management. Error handling is also integrated to ensure robust communication.\n\n## Key Components\n\n### Imports and Dependencies\n\nThe code begins with the importation of various modules including:\n- **Asyncio**: For managing asynchronous I/O operations.\n- **Grpc**: For implementing the gRPC functionalities.\n- **Logging**: To capture runtime events and errors.\n- **ABC and abstractmethod**: To define abstract base classes and enforce method implementation in derived classes.\n\n### Helper Functions\n\n- **metadata_to_dict**: Converts gRPC metadata into a dictionary format, returning an empty dictionary if metadata is None.\n- **get_client_id_or_abort**: Checks for a \"client-id\" in the gRPC invocation metadata; if not present, it aborts the operation with an error.\n\n## Channel Connection Classes\n\n### Abstract Class: ChannelConnection\n\nThe `ChannelConnection` class serves as a base class to handle asynchronous message transmission. It maintains a queue for outgoing messages and defines an abstract method for message handling:\n- **Initialization**: Creates an eviction queue for outgoing messages and starts a task to listen for incoming requests.\n- **_receive_messages**: A private async method that processes incoming messages from the `request_iterator`.\n\n### Derived Class: CallbackChannelConnection\n\nThis class extends `ChannelConnection` to manage incoming messages by invoking a user-defined callback upon receipt:\n- **_handle_message**: Implements the abstract method by calling a handle callback with the message received.\n\n## gRPC Worker Agent Service\n\n### Class: GrpcWorkerAgentRuntimeHostServicer\n\nThe main class implementing the gRPC service, providing various endpoints for client interactions:\n- **Data and Control Connections**: Separate dictionaries to manage data and control connection instances for different client IDs.\n- **Subscription Management**: Holds subscriptions for agents, allowing event-driven communication.\n\n### Key Methods\n\n1. **OpenChannel**: Establishes a channel for data communication. It validates client ID, processes incoming messages, and manages disconnection cleanup.\n2. **OpenControlChannel**: Sets up a control channel to handle control messages in a similar manner to `OpenChannel`.\n3. **_on_client_disconnect**: Responsible for handling client disconnection events, cleaning up associated resources, and logging relevant information.\n\n### Message Processing Methods\n\n- **_receive_message**: Processes different types of messages (requests, responses, events) and delegates them to their respective processing methods.\n- **_receive_control_message**: Routes control messages to the appropriate target client based on the specified destination within the message.\n- **_process_request**: Handles incoming RPC requests, routes them to the target agent, and prepares for future responses.\n\n### Subscription Management Methods\n\n- **RegisterAgent**: Records an agent type associated with a client ID and ensures that each type is uniquely registered.\n- **AddSubscription**: Adds a subscription for a client based on a request and updates the internal subscription manager.\n- **RemoveSubscription**: Removes a subscription identified by its ID from the subscription manager.\n- **GetSubscriptions**: Retrieves the list of current subscriptions for a client.\n\n## Error Handling and Logging\n\nThe code uses logging extensively to provide insight into various stages of message processing and connection management:\n- Uses logging to record successful connections, errors during message routing, and any issues encountered during subscriptions.\n- Employs exception handling mechanisms to ensure that the service behaves predictably when invalid data or conditions are encountered.\n\n## Conclusion\n\nThis gRPC worker agent runtime implementation is structured to facilitate efficient, asynchronous communication between agents in a defined environment. By employing abstract classes for extensibility and defining clear message routing and handling logic, this architecture allows for scalable and maintainable agent interactions. The system's robust error handling and logging further contribute to its reliability and ease of troubleshooting during development and deployment.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/_worker_runtime_host_servicer.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: `autogen_ext.runtimes.grpc.protos`\n\n## Overview\n\nThe `autogen_ext.runtimes.grpc.protos` module is designed to facilitate communication between agents and workers using Google Protobuf (Protocol Buffers). Protocol Buffers is a method developed by Google for serializing structured data, which allows for efficient and robust data exchange between different components of a system. This module likely defines a set of message types that are used to encode the data communicated between agents and workers in a system that makes use of gRPC (Google Remote Procedure Calls).\n\n## Purpose \n\nThe primary purpose of this module is to provide well-defined data structures that represent the messages exchanged in a client-server architecture, specifically tailored for an asynchronous agent-worker environment. Utilizing Protobufs allows for easier versioning and backward compatibility, which is beneficial in systems that may evolve over time.\n\n## Key Features\n\n1. **Protobuf Integration**: The module probably contains Protobuf definitions that specify how data should be formatted for transmission. This standardization makes it easier for different services to interoperate without ambiguity.\n\n2. **Efficient Serialization**: Protobuf is known for its efficient serialization and deserialization capabilities, which are crucial in high-performance applications that rely on rapid communication.\n\n3. **Type Safety**: By using Protobuf, the module benefits from a strong type system, reducing the chances of data corruption and ensuring that the messages conform to expected schemas.\n\n## Classes and Structures\n\nWhile the initial description does not specify exact classes or message types defined in the module, it is typical for such a module to include the following:\n\n- **Messages**: These are the main data structures defined using Protobuf syntax. Each message would likely represent a specific set of data that an agent or worker can send or receive.\n\n- **Enumerations**: Enums may be used to define a set of named values, which can help in clarifying the possible states or types of messages that can be exchanged.\n\n## Role in Communication\n\nThe defined Protobuf classes enable agents and workers to perform the following:\n\n- **Request and Response Handling**: Each message type can represent either a request from an agent or a response from a worker, allowing for a clear and organized communication protocol.\n\n- **Error Reporting**: Error messages may also be defined, allowing agents or workers to communicate failures or exceptions in a standardized manner.\n\n## Interactions with gRPC\n\nThe integration with gRPC provides the following benefits:\n\n- **Scalability**: gRPC's ability to manage multiple simultaneous connections allows for scalable applications that can handle many agents and workers communicating at the same time.\n\n- **Streaming Support**: The use of streaming with gRPC enables efficient communication, particularly for scenarios requiring continuous data feeds or real-time updates.\n\n## Typical Usage Patterns\n\n1. **Define Protobuf Messages**: Developers would typically start by defining the required message structures in a `.proto` file located within this module.\n\n2. **Generate Code**: Using the Protobuf compiler, the `.proto` definitions are compiled into language-specific classes, which can then be directly used in the agent and worker implementations.\n\n3. **Implement Servers and Clients**: The generated classes are utilized within the gRPC server and client implementations, allowing them to easily encode and decode messages during communication.\n\n## Summary\n\nIn summary, the `autogen_ext.runtimes.grpc.protos` module plays a crucial role in the communication framework between agents and workers by leveraging Google Protobuf for efficient message serialization. This setup enhances performance and maintainability in an agent-worker architecture, relying on gRPC for robust communication. The carefully defined message types facilitate a clear, consistent, and type-safe method for exchanging data, contributing to the overall effectiveness of the application system.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/__init__.py"
  },
  {
    "code": false,
    "content": "# agent_worker.proto: Protocol Buffers Description\n\nThis document outlines the generated code from the Protocol Buffers (protobuf) file named `agent_worker.proto`, which describes a gRPC service for managing agents in a system. This includes defining protocols for communication between agents and possibly other components in a distributed architecture.\n\n## Overview\n\nThe `agent_worker.proto` file defines a schema that enables different types of agents to communicate with one another through an RPC (Remote Procedure Call) mechanism using protobuf messages. It includes several message types and RPC service definitions that facilitate requests and responses between agents.\n\n## Key Components\n\n### Messages\n\nThe following key message types are defined in this schema:\n\n1. **AgentId**\n    - Represents the unique identifier for an agent.\n    - Fields:\n        - `type`: The type of the agent (string).\n        - `key`: A secondary identifier (string).\n\n2. **Payload**\n    - Carries the payload for RPC requests.\n    - Fields:\n        - `data_type`: The type of data (string).\n        - `data_content_type`: Content type of the data (string).\n        - `data`: The actual data being transmitted (bytes).\n\n3. **RpcRequest**\n    - Encapsulates a remote procedure call request.\n    - Fields:\n        - `request_id`: A unique identifier for the request (string).\n        - `source`: The source agent's Id.\n        - `target`: The target agent's Id.\n        - `method`: The method to be invoked (string).\n        - `payload`: The associated payload.\n\n4. **RpcResponse**\n    - Represents the response to a RPC request.\n    - Fields:\n        - `request_id`: Corresponding request identifier (string).\n        - `payload`: Response payload.\n        - `error`: An error message if applicable.\n\n5. **Subscription Types**\n    - Include `TypeSubscription`, `TypePrefixSubscription`, and `Subscription`, which specify details for subscribing to various agent types or events.\n\n6. **State Management Messages**\n    - Include `SaveStateRequest`, `SaveStateResponse`, `LoadStateRequest`, and `LoadStateResponse` to manage the state of agents.\n\n7. **Control and Messaging**\n    - Involves messages like `ControlMessage` and `Message`, which define the structure for controls and general messages exchanged between agents.\n\n## RPC Service Definitions\n\nThe main RPC service defined within the file is **AgentRpc**, which consists of several RPC methods:\n\n- **OpenChannel**\n    - Accepts a `Message` and returns a `Message`.\n  \n- **OpenControlChannel**\n    - Manages a control channel and also accepts and returns `ControlMessage` instances.\n  \n- **RegisterAgent**\n    - Registers a new agent type, accepting a `RegisterAgentTypeRequest` and returning a `RegisterAgentTypeResponse`.\n\n- **AddSubscription**\n    - Manages subscriptions related to agents, processing `AddSubscriptionRequest` and providing `AddSubscriptionResponse`.\n\n- **RemoveSubscription**\n    - Allows for the removal of an existing subscription.\n\n- **GetSubscriptions**\n    - Retrieves the current subscriptions in the system, responding with `GetSubscriptionsResponse`.\n\n## Serialization\n\nThe schema uses Protocol Buffers, a language-agnostic method of serializing structured data. The generated code allows seamless communication between services, as all message types created in the `agent_worker.proto` can be serialized and deserialized easily through the corresponding Python classes.\n\n## Important Annotations\n\nThe generated file contains specific annotations and metadata relevant for internal or external usage, including:\n- Versioning information for protobuf.\n- Options for RPC metadata serialization.\n\nThe metadata entries tied to RPC methods enhance the extensibility and traceability of requests and responses, ensuring clarity in their interactions.\n\n## Conclusion\n\nOverall, the `agent_worker.proto` file provides a comprehensive schema for creating, managing, and communicating with autonomous agents in a networked environment. The careful design allows agents to perform tasks by sending requests and receiving responses through various methods, while also managing subscriptions and states efficiently.\n\nBy implementing a gRPC service with Protocol Buffers, this schema facilitates robust inter-agent communication that can be expanded or modified as needed for future requirements.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/agent_worker_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Python Client and Server Implementation Documentation\n\nThis document provides a high-level overview of the generated gRPC client and server classes based on the Protocol Buffers (protobuf) service definitions, specifically focusing on the `AgentRpc` service.\n\n## Overview\n\nThe code defines a gRPC service (`AgentRpc`) that is structured for communication between clients and servers using remote procedure calls (RPCs). The service is designed to handle various operations related to agents, such as opening channels, registering agents, and managing subscriptions. This generated code primarily interacts with protobuf message types defined in the `agent_worker_pb2` module.\n\n## Version Compatibility Check\n\nAt the start of the script, the code checks for compatibility between the installed `grpc` package version and the version specified in the generated code (`GRPC_GENERATED_VERSION`). If there is a version mismatch that makes the code incompatible, a `RuntimeError` is raised, instructing the user to either upgrade or downgrade their grpc module or the generated code.\n\n```python\ntry:\n    from grpc._utilities import first_version_is_lower\n    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n    _version_not_supported = True\n```\n\n## Class: `AgentRpcStub`\n\nThe `AgentRpcStub` class is the client-side representation of the gRPC service. This class contains methods that provide the functionality for making calls to the server. Each method corresponds to a specific RPC defined in the service.\n\n### Methods in `AgentRpcStub`\n\n1. **OpenChannel**\n   - Supports a stream of requests and responses.\n   - Serializes `Message` objects for requests and deserializes them from responses.\n\n2. **OpenControlChannel**\n   - Similar to `OpenChannel`, but for control messages.\n   - Utilizes `ControlMessage` for request serialization and response deserialization.\n\n3. **RegisterAgent**\n   - Handles the registration of agents by sending `RegisterAgentTypeRequest` and expecting a `RegisterAgentTypeResponse`.\n\n4. **AddSubscription**\n   - Allows adding subscriptions using `AddSubscriptionRequest` and listens for `AddSubscriptionResponse`.\n\n5. **RemoveSubscription**\n   - Similar to `AddSubscription`, but functions to remove existing subscriptions.\n\n6. **GetSubscriptions**\n   - Retrieves current subscriptions through a `GetSubscriptionsRequest`, returning a `GetSubscriptionsResponse`.\n\n```python\nclass AgentRpcStub(object):\n    ...\n    def __init__(self, channel):\n        ...\n```\n\n## Class: `AgentRpcServicer`\n\nThe `AgentRpcServicer` class serves as the server-side implementation for each of the RPC methods defined in the `AgentRpc` service. Currently, all methods raise a `NotImplementedError`, indicating that they must be implemented by the user of the generated code.\n\n### Methods in `AgentRpcServicer`\n\n1. **OpenChannel**\n   - This is intended to handle incoming requests for establishing a channel but is yet to be implemented.\n\n2. **OpenControlChannel**\n   - Designed to manage control channel requests, this method also remains unimplemented.\n\n3. **RegisterAgent**\n   - Should register an agent based on the incoming request but hasn't been implemented.\n\n4. **AddSubscription**\n   - Intended to add a subscription when implemented.\n\n5. **RemoveSubscription**\n   - This method is meant to remove a subscription based on client requests.\n\n6. **GetSubscriptions**\n   - A placeholder for retrieving current subscriptions, awaiting implementation.\n\n```python\nclass AgentRpcServicer(object):\n    ...\n```\n\n## Function: `add_AgentRpcServicer_to_server`\n\nThis function registers the `AgentRpcServicer` with a gRPC server instance, defining the behavior of each RPC method. Handlers for each method are created and added to the server's RPC methods.\n\n### Functionality\n\n- Maps each RPC method to the respective function in the `servicer` instance.\n- Utilizes gRPC method handlers for setting how requests and responses are serialized and deserialized.\n\n```python\ndef add_AgentRpcServicer_to_server(servicer, server):\n    ...\n```\n\n## Class: `AgentRpc`\n\nThe `AgentRpc` class provides static methods that enable client calls to the service. Each method corresponds to the RPCs defined in the service and abstracts away the details of making a gRPC call. This class is marked as experimental, indicating it may undergo changes.\n\n### Static Methods in `AgentRpc`\n\n1. **OpenChannel**\n   - Initiates a stream-stream call to open a channel.\n\n2. **OpenControlChannel**\n   - Similar in functionality to `OpenChannel`, but tailored for control messages.\n\n3. **RegisterAgent**\n   - Facilitates the registration of an agent with a unary-unary call.\n\n4. **AddSubscription**\n   - Implements adding a subscription through a unary-unary RPC call.\n\n5. **RemoveSubscription**\n   - Sends a request to remove a subscription.\n\n6. **GetSubscriptions**\n   - Calls to retrieve subscriptions from the server.\n\n```python\nclass AgentRpc(object):\n    ...\n```\n\n## Conclusion\n\nThis code serves as a boilerplate for implementing a gRPC service that manages agent interactions. While essential components and structures are present, users are expected to implement the actual business logic behind each method in the `AgentRpcServicer`. The stub class provides a convenient way for clients to interact with the service. Proper version management ensures compatibility with the underlying gRPC library.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/agent_worker_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "# CloudEvent Protocol Buffers Definition\n\nThis document provides a high-level description of the generated Protocol Buffers code for CloudEvent. The code defines a structured format for various attributes and data related to CloudEvents, facilitating interoperability across different systems.\n\n## Overview of CloudEvent\n\nThe CloudEvent consists of several fields that capture essential metadata and information about events. It is defined in the `cloudevent.proto` file and contains both standard fields and custom attributes that can be added as key-value pairs. The design follows Protocol Buffers conventions to ensure efficient serialization and deserialization.\n\n### Key Components\n\n1. **CloudEvent Message**:\n   - This is the primary structure that encapsulates a CloudEvent. It includes specific attributes such as `id`, `source`, and `type`, as well as several data formats for the event content.\n\n2. **Field Descriptions**:\n   - **`id`**: A unique identifier for the event, represented as a string.\n   - **`source`**: The origin of the event, specified as a string.\n   - **`spec_version`**: Version of the CloudEvent specification being used, also a string.\n   - **`type`**: The type of event, given as a string.\n\n3. **Attributes Field**:\n   - The `attributes` field is an array of key-value pairs. Each entry in this field is an instance of `AttributesEntry`, which allows extensibility by adding custom attributes.\n\n4. **Data Fields**:\n   - CloudEvent can have multiple formats for the data it carries:\n     - **`binary_data`**: Raw data in binary format.\n     - **`text_data`**: String data.\n     - **`proto_data`**: Structured data encapsulated in a `google.protobuf.Any`.\n\n### AttributesEntry Structure\n\nThe `AttributesEntry` is a nested message within `CloudEvent`:\n\n- **`key`**: A string that serves as the identifier for the attribute.\n- **`value`**: An instance of `CloudEventAttributeValue`, which defines the specifics of the value type (boolean, integer, string, etc.).\n\n## CloudEventAttributeValue Types\n\nThe `CloudEventAttributeValue` message provides a container for various value types that attributes can hold. It includes the following fields:\n\n1. **`ce_boolean`**: A field that can hold a boolean value.\n2. **`ce_integer`**: An integer value field.\n3. **`ce_string`**: A string value field.\n4. **`ce_bytes`**: A binary data representation.\n5. **`ce_uri`**: A URI as a string.\n6. **`ce_uri_ref`**: A reference to another URI.\n7. **`ce_timestamp`**: A timestamp using Google\u2019s `Timestamp` message.\n\n### Serialization Information\n\nThe code contains necessary meta-information regarding the serialization of the messages using Protocol Buffers, including versioning details.\n\n- **Runtime Version Checking**: The code performs validation of the Protobuf runtime version to ensure compatibility.\n- **Descriptor Build**: It constructs message and enum descriptors dynamically from the serialized file.\n\n## Usage Considerations\n\nThis generated code is intended for automatic generation from the `.proto` file and should be treated as a system component rather than manual code to be altered. The code relies on Protocol Buffers classes and methods for serialization, and any customization should be done at the `.proto` file level.\n\n### Namespace and Dependency Management\n\n- The CloudEvent is defined under the namespace `io.cloudevents.v1`, which helps to categorize the messages appropriately.\n- The implementation relies on external dependencies from Google\u2019s Protocol Buffers library (e.g., `google/protobuf/any.proto` and `google/protobuf/timestamp.proto`).\n\n### Conclusion\n\nThe structure defined in `cloudevent.proto` provides a robust framework for modeling and transmitting CloudEvents across services and platforms. By leveraging Protocol Buffers, it ensures efficient data interchange, accommodating various formats and extensibility through attributes. This makes it suitable for modern cloud-native applications and event-driven architectures.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/cloudevent_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Python Generated Code Documentation\n\n## Overview\nThis Python script is a generated component of a gRPC service implementation. It is specifically structured to handle client and server classes that correspond to services defined via Protocol Buffers (protobuf). The warnings included in the script emphasize the importance of not editing it manually as it is generated automatically by the `gRPC` protocol compiler.\n\n## Imports\n\n```python\nimport grpc\nimport warnings\n```\n\nThe script imports two essential modules:\n- `grpc`: This is the primary module for working with gRPC in Python, providing the necessary functionalities for RPC (Remote Procedure Call) communication.\n- `warnings`: This module is used in Python to issue warnings to alert the user about deprecated features or potential issues in the code.\n\n## Version Checks\n### Constant Definitions\n\n```python\nGRPC_GENERATED_VERSION = '1.70.0'\nGRPC_VERSION = grpc.__version__\n_version_not_supported = False\n```\n\nIn this section, two constants are defined:\n- `GRPC_GENERATED_VERSION`: This specifies the minimum gRPC version required by the generated code. In this case, it is `1.70.0`.\n- `GRPC_VERSION`: This retrieves the currently installed version of the gRPC package.\n\nA boolean variable `_version_not_supported` is initialized to `False`, which will later be used to mark if the installed gRPC version is compatible with the generated code.\n\n### Version Compatibility Check\n\n```python\ntry:\n    from grpc._utilities import first_version_is_lower\n    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n    _version_not_supported = True\n```\n\nThis section attempts to import a utility function, `first_version_is_lower`, which compares the installed gRPC version with the generated version. If the import succeeds, the comparison is made to check if the installed version is less than the required version.\n\nIf the import fails (which indicates that the utility may not be available in the installed version), `_version_not_supported` is set to `True` to indicate that the version is incompatible.\n\n## Error Handling\n### Raising Runtime Errors\n\n```python\nif _version_not_supported:\n    raise RuntimeError(\n        f'The grpc package installed is at version {GRPC_VERSION},'\n        + f' but the generated code in cloudevent_pb2_grpc.py depends on'\n        + f' grpcio>={GRPC_GENERATED_VERSION}.'\n        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n    )\n```\n\nThis final section of the script checks the `_version_not_supported` flag. If it is `True`, a `RuntimeError` is raised. The error message provides detailed information about the version mismatch, suggesting possible corrective actions:\n- Upgrading the `grpc` module to meet the minimum required version.\n- Downgrading the generated code if the current version is incompatible.\n\n## Summary\nThis script serves as a foundational piece for ensuring that the gRPC service implemented in Python operates correctly by enforcing version compatibility between the installed gRPC library and the generated service files. It handles version checks gracefully, ensuring that developers are alerted to any issues that may arise due to incompatible versions, thus maintaining the integrity of the gRPC service implementations.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/runtimes/grpc/protos/cloudevent_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "It seems that there isn't any code provided for analysis. If you have a piece of source code you'd like me to analyze, please share it, and I'd be happy to help!",
    "filename": "python/packages/autogen-ext/src/autogen_ext/teams/__init__.py"
  },
  {
    "code": false,
    "content": "# MagenticOne Documentation\n\n## Overview\n\nMagenticOne is a specialized multi-agent system designed to solve complex tasks by combining various agents such as FileSurfer, WebSurfer, Coder, and Executor. This system aims to automate and handle more complicated tasks typically found in human work life. Built on a multi-agent architecture, it relies on an Orchestrator agent to manage task flow, delegate subtasks, and monitor overall progress. This architecture allows MagenticOne to work efficiently across various domains, including web browsing and file interactions.\n\nFor practical guidance on installation, consult the installation instructions, which include the following command:\n\n```bash\npip install \"autogen-ext[magentic-one]\"\n```\n\n## Class Definition: MagenticOne\n\n### Purpose\n\nThe `MagenticOne` class extends the `MagenticOneGroupChat` class and serves as the core engine for the multi-agent interactions. It integrates various agents responsible for different functionalities within its architecture while allowing for configurations like human-in-the-loop (HIL) mode, code execution approval, and selection of code executors.\n\n### Constructor Parameters\n\n- **client** (`ChatCompletionClient`): This parameter is mandatory and represents the client used for model interactions.\n  \n- **hil_mode** (`bool`, optional): If set to `True`, enables the UserProxyAgent, allowing for human input during execution.\n\n- **input_func** (`InputFuncType`, optional): A function defining how user input is processed in HIL mode.\n\n- **code_executor** (`CodeExecutor`, optional): It defines how code execution will be handled. If not provided, the default options will be utilized.\n\n- **approval_func** (`ApprovalFuncType`, optional): A function to manage approval for code execution. If none is provided, code executes without any approval checks.\n\nWarnings about potential risks associated with running agents in a digital environment emphasize the importance of safe practices like using containers, virtual environments, and logging.\n\n## Internal Validation\n\n### `_validate_client_capabilities`\n\nThis method ensures the `client` provided to `MagenticOne` possesses certain capabilities necessary for effective functioning. It checks for:\n\n- **function_calling**: Ensures the system can call functions based on model reasoning.\n  \n- **json_output**: Verifies that any output from the model conforms to a JSON format.\n\nWarnings are raised if the client lacks these capabilities, ensuring that users are aware of potential limitations.\n\n## Agent Components\n\nMagenticOne integrates several specialized agents:\n\n1. **Orchestrator**: Manages the overall task execution workflow, ensuring tasks are broken down into manageable subtasks and overseen until completion.\n\n2. **WebSurfer**: Capable of controlling a web browser and extracting necessary data from webpages.\n\n3. **FileSurfer**: Facilitates interaction with markdown-based file systems, allowing the reading and navigation of local files.\n\n4. **Coder**: Focused on writing and analyzing code, as well as generating new programming artifacts based on input from other agents.\n\n5. **ComputerTerminal**: This agent allows executing the Coder's programs and installing new programming libraries.\n\nTogether, these agents empower MagenticOne to complete a diverse range of open-ended tasks autonomously.\n\n## Usage Examples\n\n### Autonomous Task Completion\n\nUsers can run tasks without HIL mode by initializing a `MagenticOne` instance with a predefined task and executing it as shown below:\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_agentchat.ui import Console\n\nasync def example_usage():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    m1 = MagenticOne(client=client)  \n    task = \"Write a Python script to fetch data from an API.\"\n    result = await Console(m1.run_stream(task=task))\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```\n\n### Human-in-the-loop Mode\n\nTo enable user oversight, the `MagenticOne` instance can be configured with HIL mode. This allows for user interaction and code execution approval:\n\n```python\nasync def example_usage_hil():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=True,\n            input_func=user_input_func,\n            code_executor=code_executor,\n            approval_func=approval_func\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_hil())\n```\n\n### Approval without HIL Mode\n\nIt is also possible to run tasks with code execution approval while not using HIL:\n\n```python\nasync def example_usage_with_approval():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=False,\n            code_executor=code_executor,\n            approval_func=approval_func\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_with_approval())\n```\n\n## Recommendations and References\n\nThe documentation contains a list of precautions to safely use MagenticOne, including the use of containers, virtual environments, and human supervision during execution to mitigate risks. Also included is a reference to an academic paper detailing the system's underlying architecture and capabilities.\n\n### References\n\n```bibtex\n@article{fourney2024magentic,\n    title={Magentic-one: A generalist multi-agent system for solving complex tasks},\n    author={Fourney, Adam and Bansal, Gagan and others},\n    journal={arXiv preprint arXiv:2411.04468},\n    year={2024},\n    url={https://arxiv.org/abs/2411.04468}\n}\n``` \n\nThis documentation provides a comprehensive overview of MagenticOne, detailing its purpose, functionality, and best practices for usage.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/teams/magentic_one.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module is part of a larger package that appears to be focused on integrating with Azure AI's search capabilities. The primary purpose of this code is to organize imports from submodules and define the public API of the module.\n\n## Import Statements\n\nThe module imports several classes and functions from two specific submodules:\n\n1. **From `._ai_search`:**  \n   - **`AzureAISearchTool`:** Likely a tool for interacting with Azure's AI search functionalities.\n   - **`BaseAzureAISearchTool`:** A base class that provides foundational features for Azure AI search tools.\n   - **`SearchQuery`:** A class that probably implements a structure for crafting search queries.\n   - **`SearchResult`:** A class that likely represents an individual result from a search.\n   - **`SearchResults`:** This might represent a collection of `SearchResult` instances.\n   - **`VectorizableTextQuery`:** A class that may allow translating text queries into vector format for enhanced search capabilities.\n\n2. **From `._config`:**  \n   - **`AzureAISearchConfig`:** A configuration class, presumably for setting up parameters and options needed to interact with Azure AI search services.\n\n## Public API Definition\n\nThe `__all__` list defines the public interface of the module. This is a convention in Python that specifies which names will be considered public when the module is imported using the wildcard import syntax (i.e., `from module import *`). By including specific classes in the `__all__` list, the developer is signaling to other developers which components are intended for public use.\n\nThe items included in the `__all__` list are as follows:\n\n- `AzureAISearchTool`\n- `BaseAzureAISearchTool`\n- `SearchQuery`\n- `SearchResult`\n- `SearchResults`\n- `AzureAISearchConfig`\n- `VectorizableTextQuery`\n\nThis selection suggests that these classes are integral to the functionality the module provides and are likely the primary components that users of the package will interact with.\n\n## Potential Usage Scenarios\n\nWhile the module does not directly implement any functionality on its own, the imported classes suggest a variety of use cases involving Azure AI search:\n\n- **Creating Search Tools:** Users may create search tools based on the `AzureAISearchTool` and its base class.\n- **Constructing Queries:** The `SearchQuery` class allows users to construct customized search queries that can be executed against Azure AI's services.\n- **Handling Search Results:** Through the `SearchResult` and `SearchResults` classes, users can effectively manage and manipulate search outcomes.\n- **Configuration Management:** The `AzureAISearchConfig` class suggests that there can be various configuration settings which can be applied to tailor the search functionality to specific needs.\n- **Utilizing Text Queries as Vectors:** By using the `VectorizableTextQuery`, users can explore more advanced search techniques that leverage vector representations of text, which can improve search accuracy and relevance.\n\n## Conclusion\n\nThis module provides a foundation for integrating Azure AI capabilities into Python applications focused on search functionalities. By importing and organizing essential components\u2014such as search tools, query structures, and configuration management\u2014the module sets the stage for developers to build applications that can leverage Azure's advanced search features efficiently. The careful organization and documentation of the public API aid in enhancing usability and understanding of the module's purpose and capabilities.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/azure/__init__.py"
  },
  {
    "code": false,
    "content": "# Azure AI Search Tool Documentation\n\n## Overview\n\nThis document provides an overview of the Azure AI Search Tool, which is a framework for interacting with Azure AI Search services. The code defines a set of classes and protocols that facilitate full-text and vector searches within Azure search indexes, enabling enhanced data retrieval capabilities through embedding generation and various query types. It lays out structured configurations for implementing various search functionalities and includes methods to process queries asynchronously.\n\n## Key Classes and Their Responsibilities\n\n### 1. **SearchQuery**\n\n```python\nclass SearchQuery(BaseModel):\n```\n\n- **Purpose**: Represents the parameters used in a search operation.\n- **Description**: This class encapsulates a search query string, simplifying the interface for the user by consolidating additional parameters during tool creation rather than at the query time.\n\n### 2. **SearchResult**\n\n```python\nclass SearchResult(BaseModel):\n```\n\n- **Purpose**: Defines a single search result returned from a query.\n- **Attributes**:\n  - `score`: A float indicating the relevancy score of the search result.\n  - `content`: A dictionary holding the document content.\n  - `metadata`: Additional information about the document.\n  \n### 3. **SearchResults**\n\n```python\nclass SearchResults(BaseModel):\n```\n\n- **Purpose**: Serves as a container for multiple `SearchResult` instances.\n- **Attributes**:\n  - `results`: A list of `SearchResult` instances.\n\n### 4. **BaseAzureAISearchTool**\n\n```python\nclass BaseAzureAISearchTool(...):\n```\n\n- **Purpose**: An abstract base class for implementing Azure AI Search tools.\n- **Responsibilities**:\n  - Manages initialization and configuration of Azure Search clients.\n  - Provides a common interface and handles authentication with Azure services using various credential types.\n  - Includes methods to process credentials, close clients, and execute searches.\n\n### 5. **AzureAISearchTool**\n\n```python\nclass AzureAISearchTool(EmbeddingProviderMixin, BaseAzureAISearchTool):\n```\n\n- **Purpose**: A concrete implementation of search functionality, designed for ease of use with Azure AI Search.\n- **Responsibilities**:\n  - Offers factory methods for creating tools for different search types including full-text, vector, and hybrid search.\n  - Provides detailed configuration options for embedding generation, caching, and API interaction.\n  \n## Protocols and Mixins\n\n### 1. **EmbeddingProvider**\n\n```python\nclass EmbeddingProvider(Protocol):\n```\n\n- **Description**: Defines the asynchronous interface for generating embedding vectors from text.\n  \n### 2. **EmbeddingProviderMixin**\n\n```python\nclass EmbeddingProviderMixin:\n```\n\n- **Purpose**: Provides embedding generation functionality, utilizing Azure OpenAI or OpenAI SDK.\n- **Key Methods**:\n  - `_get_embedding`: Asynchronously generates an embedding vector for a given query, raising exceptions on configuration issues or failure to load the required SDKs.\n\n## Core Functionalities and Methods\n\n### `run`\n\n```python\nasync def run(self, ...):\n```\n\n- **Description**: Executes a search operation against the configured Azure AI Search index.\n- **Parameters**: Accepts either a string, dictionary, or `SearchQuery` instance as the search query.\n- **Returns**: A `SearchResults` object containing the resulting documents.\n- **Error Handling**: Thoroughly checks and raises exceptions for authentication, empty queries, and any other issues encountered during search.\n\n### `_get_client`\n\n```python\nasync def _get_client(self) -> SearchClient:\n```\n\n- **Purpose**: Initializes and returns a `SearchClient` for interacting with the Azure AI Search service.\n- **Error Handling**: Validates the index existence and manages authentication errors.\n\n### `_process_credential`\n\n```python\ndef _process_credential(self, ...):\n```\n\n- **Purpose**: To handle and convert credential data types to the correct format required by the Azure SDK.\n- **Input Handling**: Accepts either a credential object, dictionary, or raises appropriate errors for unsupported types.\n\n## Search Types and Factory Methods\n\n### Full-Text and Vector Search\n\n- **Factory Methods**: \n  - `create_full_text_search`: Initializes for keyword, Lucene syntax, or semantic searches.\n  - `create_vector_search`: Configured specifically for vector-based similarity search.\n  - `create_hybrid_search`: Combines text and vector searches, enabling complex retrieval strategies.\n\n### Caching Mechanism\n\n- **Caching**: The tool supports optional caching of search results to enhance efficiency. Each cached entry is identified by a unique key created from search parameters, which can reduce redundant API calls.\n\n## Error Handling\n\nThe tool robustly handles various exceptions, such as authentication failures, missing configuration attributes, or index-related errors. This ensures that users are provided with helpful feedback should something go wrong during their operations.\n\n## Conclusion\n\nThe Azure AI Search Tool provides a comprehensive framework for efficiently querying Azure-based search indexes leveraging advanced functionalities such as semantic search and embedding generation. With its clear structure, extensibility, and error management, it enables developers to build intelligent search capabilities in their applications seamlessly.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/azure/_ai_search.py"
  },
  {
    "code": false,
    "content": "# Azure AI Search Configuration Module\n\nThis module outlines the configuration settings necessary for effectively utilizing the Azure AI Search tool. It encompasses various configurations related to authentication, search behavior, caching, and embedding functionalities. The module leverages the Pydantic library for data validation and management.\n\n## Overview of the Module\n\nThe primary class defined within this module is `AzureAISearchConfig`, which serves as a configuration container for the different aspects of the Azure AI Search service. This class validates and stores parameters essential for establishing the connection and executing search queries.\n\n### Key Features\n- **Authentication**: Includes support for Azure API keys and token-based authentication.\n- **Search Options**: Allows users to define various types of search queries, such as simple, full-text, semantic, and vector searches.\n- **Result Management**: Provides options to specify which fields should be returned in the search results and to implement caching strategies.\n- **Embedding Options**: Supports configurations necessary for integrating OpenAI's embeddings and Azure's embedding services.\n\n## Class: AzureAISearchConfig\n\n### Definition\n\n```python\nclass AzureAISearchConfig(BaseModel):\n```\n\nThis class is derived from the Pydantic `BaseModel`, facilitating data validation and type checking.\n\n### Attributes\n\n- **name**: A string specifying the name of the configuration instance.\n- **description**: An optional string for a brief description of the tool's purpose.\n- **endpoint**: A string indicating the Azure AI Search service URL.\n- **index_name**: A string naming the search index to query.\n- **credential**: An instance of either `AzureKeyCredential` or `AsyncTokenCredential` for authentication.\n- **api_version**: A string representing the API version, defaulting to \"2023-10-01-preview\".\n- **query_type**: A string indicating the search query type, which can be \"simple\", \"full\", \"semantic\", or \"vector\".\n- **search_fields**: An optional list of strings specifying which fields to search through.\n- **select_fields**: An optional list of strings defining the fields to return in the results.\n- **vector_fields**: An optional list of strings specifying fields used in vector searches.\n- **top**: An optional integer controlling the maximum number of search results to return.\n- **filter**: An optional string for OData filter expressions to refine search results.\n- **semantic_config_name**: An optional string for specifying the semantic configuration name when needed.\n- **enable_caching**: A boolean flag to determine if caching should be enabled for search results.\n- **cache_ttl_seconds**: An integer signifying how long results should stay in the cache.\n- **embedding_provider**: An optional string that names the provider for client-side embeddings.\n- **embedding_model**: An optional string defining the model name for embeddings.\n- **openai_api_key**: An optional string for the OpenAI API key.\n- **openai_api_version**: An optional string for the API version specific to Azure OpenAI embeddings.\n- **openai_endpoint**: An optional string for the endpoint URL of Azure OpenAI embeddings.\n\n## Validations and Constraints\n\n### Endpoint Validation\n\n```python\n@field_validator(\"endpoint\")\ndef validate_endpoint(cls, v: str) -> str:\n```\n\nThis method ensures that the provided `endpoint` is a valid URL format, throwing an error if it does not begin with `http://` or `https://`.\n\n### Query Type Normalization\n\n```python\n@field_validator(\"query_type\")\ndef normalize_query_type(cls, v: QueryTypeLiteral) -> QueryTypeLiteral:\n```\n\nThis method normalizes the `query_type` to ensure valid values are used. It converts any input of \"fulltext\" to the standard \"full\".\n\n### Top Validation\n\n```python\n@field_validator(\"top\")\ndef validate_top(cls, v: Optional[int]) -> Optional[int]:\n```\n\nThis validator guarantees that if a value is provided for `top`, it must be a positive integer.\n\n### Interdependent Field Validation\n\n```python\n@model_validator(mode=\"after\")\ndef validate_interdependent_fields(self) -> \"AzureAISearchConfig\":\n```\n\nThis method checks the logical dependencies among fields. For instance, if a semantic search is requested, a semantic configuration name must be specified. Likewise, it validates that if vector fields are requested, they must be provided.\n\n## Example Usages\n\nThe configuration class can be instantiated in various ways based on the type of search desired. For example:\n\n### Basic Configuration for Full-Text Search\n\n```python\nconfig = AzureAISearchConfig(\n    name=\"doc-search\",\n    endpoint=\"https://your-search.search.windows.net\",\n    index_name=\"<your-index>\",\n    credential=AzureKeyCredential(\"<your-key>\"),\n    query_type=\"simple\",\n    search_fields=[\"content\", \"title\"],\n    top=5,\n)\n```\n\n### Vector Search Configuration with Azure OpenAI Embeddings\n\n```python\nvector_config = AzureAISearchConfig(\n    name=\"vector-search\",\n    endpoint=\"https://your-search.search.windows.net\",\n    index_name=\"<your-index>\",\n    credential=AzureKeyCredential(\"<your-key>\"),\n    query_type=\"vector\",\n    vector_fields=[\"embedding\"],\n    embedding_provider=\"azure_openai\",\n    embedding_model=\"text-embedding-ada-002\",\n    openai_endpoint=\"https://your-openai.openai.azure.com\",\n    openai_api_key=\"<your-openai-key>\",\n    top=5,\n)\n```\n\n### Hybrid Search with Semantic Ranking\n\n```python\nhybrid_config = AzureAISearchConfig(\n    name=\"hybrid-search\",\n    endpoint=\"https://your-search.search.windows.net\",\n    index_name=\"<your-index>\",\n    credential=AzureKeyCredential(\"<your-key>\"),\n    query_type=\"semantic\",\n    semantic_config_name=\"<your-semantic-config>\",\n    search_fields=[\"content\", \"title\"],\n    vector_fields=[\"embedding\"],\n    embedding_provider=\"openai\",\n    embedding_model=\"text-embedding-ada-002\",\n    openai_api_key=\"<your-openai-key>\",\n    top=5,\n)\n```\n\n## Summary\n\nThe `AzureAISearchConfig` class provides a robust and validated interface for configuring Azure AI Search tools. With clear validation mechanisms, interdependencies, and a flexible structure, this configuration module is crucial for successfully executing diverse search queries across the Azure platform.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/azure/_config.py"
  },
  {
    "code": false,
    "content": "# Code Overview\n\nThis code snippet is a Python module that deals with the execution of Python code in a controlled manner. It imports specific classes or functions from a module named `._code_execution` and makes them available for use in the current module's namespace.\n\n## Import Statements\n\nThe main purpose of this code is to import and expose functionalities related to code execution. Here's a breakdown of the imported components:\n\n1. **CodeExecutionInput**: Presumably, this class or function is used to encapsulate the inputs needed for executing Python code. The specific attributes or methods of this class would define how the input is structured or managed.\n\n2. **CodeExecutionResult**: This class or function likely represents the outcome of a code execution operation. It may include information about the execution result, such as whether it was successful, any returned values, and any errors that occurred.\n\n3. **PythonCodeExecutionTool**: This appears to be a utility or helper class designed for executing Python code. It probably contains methods that facilitate the process of running a code snippet, handling input, and generating the output.\n\n## `__all__` Declaration\n\nThe `__all__` variable defines a list of public objects of the module. By including:\n\n```python\n__all__ = [\"CodeExecutionInput\", \"CodeExecutionResult\", \"PythonCodeExecutionTool\"]\n```\n\nthe module specifies that only these three components should be considered part of its public API when the module is imported with `from module_name import *`. This is a way to control what is accessible to users of the module, promoting encapsulation and reducing potential conflicts with other imports.\n\n## Purpose and Role of Components\n\n### CodeExecutionInput\n\n- **Purpose**: Encapsulates input data relevant to Python code execution. This might involve user-provided code snippets, parameters, and context required for execution.\n- **Role**: Ensures that all necessary data is bundled into a single manageable entity, which can simplify the handling of execution requests.\n\n### CodeExecutionResult\n\n- **Purpose**: Captures and formats the output generated from executing the provided Python code.\n- **Role**: Provides a structured way to return execution details, enhancing error handling and debugging capabilities by containing relevant execution metadata.\n\n### PythonCodeExecutionTool\n\n- **Purpose**: Serves as the main handler for executing Python code, managing the process of taking inputs, running the code, and outputting results.\n- **Role**: Implements core execution logic and interfaces, making it easier to execute and manage Python code in various contexts, such as scripts or interactive environments.\n\n## Summary of Module Functionality\n\nIn summary, this module streamlines the execution of Python code by providing well-defined classes to handle input and output. It enables encapsulated interaction with Python code, allowing for execution in a controlled environment. This design promotes maintainability and clarity, adhering to good software engineering practices. By defining an explicit public interface, it allows users to understand which components are meant to be used, enhancing usability and reducing complexity. \n\nOverall, these components likely contribute to a broader system intended for dynamic code execution, such as an educational tool, a sandbox environment for code testing, or an integrated development environment (IDE) feature.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/code_execution/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Python Code Execution Tool\n\n## Overview  \nThe `PythonCodeExecutionTool` is a component designed for executing Python code blocks within various environments. It acts as a bridge between the user\u2019s code input and the designated code execution environment. Through this tool, users can execute arbitrary Python code and receive output, including both the success status of the execution and any resulting outputs.\n\n## Key Classes and Their Purpose  \n\n### CodeExecutionInput  \n```python\nclass CodeExecutionInput(BaseModel):\n    code: str = Field(description=\"The contents of the Python code block that should be executed\")\n```\n- **Purpose**: This class defines the input structure required by the `PythonCodeExecutionTool`. It encapsulates the code that the user wishes to execute, ensuring that the provided input adheres to the specified structure.\n- **Attributes**:\n  - `code`: A string that holds the Python code to be executed.\n\n### CodeExecutionResult  \n```python\nclass CodeExecutionResult(BaseModel):\n    success: bool\n    output: str\n\n    @model_serializer\n    def ser_model(self) -> str:\n        return self.output\n```\n- **Purpose**: This class defines the output structure for the execution results. It encapsulates the success status and the produced output.\n- **Attributes**:\n  - `success`: A boolean indicating whether the code execution was successful.\n  - `output`: A string that contains the output resulting from executing the provided Python code.\n- **Method**:\n  - `ser_model`: A serializer method that returns the output as a string representation.\n\n### PythonCodeExecutionToolConfig  \n```python\nclass PythonCodeExecutionToolConfig(BaseModel):\n    executor: ComponentModel\n    description: str = \"Execute Python code blocks.\"\n```\n- **Purpose**: This class is responsible for configuring the `PythonCodeExecutionTool`. It holds configuration details for how the tool should operate.\n- **Attributes**:\n  - `executor`: The external code execution component that will handle executing the provided code blocks.\n  - `description`: A string providing a brief description of the tool\u2019s functionality.\n\n## PythonCodeExecutionTool  \n```python\nclass PythonCodeExecutionTool(\n    BaseTool[CodeExecutionInput, CodeExecutionResult], Component[PythonCodeExecutionToolConfig]\n):\n```\n- **Purpose**: This is the main class used to execute Python code. It combines functionalities from `BaseTool` and `Component`, allowing it to process inputs and outputs specific to code execution.\n\n### Initialization and Configuration  \n- **Constructor**: \n  ```python\n  def __init__(self, executor: CodeExecutor):\n      ...\n  ```\n  - The constructor takes an `executor`, which is an instance of `CodeExecutor`. This executor can be from various implementations, such as local command line executors or cloud-based environments.\n\n- **Configuration Methods**:\n  - `_to_config`: Converts the current instance into a configuration object that can be persisted or reloaded.\n  - `_from_config`: Class method that creates an instance of `PythonCodeExecutionTool` from a provided configuration object.\n\n## Execution Logic  \n```python\nasync def run(self, args: CodeExecutionInput, cancellation_token: CancellationToken) -> CodeExecutionResult:\n    code_blocks = [CodeBlock(code=args.code, language=\"python\")]\n    ...\n```\n- **Method Purpose**: This asynchronous method takes `CodeExecutionInput` as input and executes the specified Python code using the configured executor. It manages the execution flow and encapsulates the handling of cancellation requests.\n  \n- **Execution Process**:\n  1. The input code is wrapped in a `CodeBlock`.\n  2. The method invokes the executor\u2019s `execute_code_blocks` method, passing the code block and a cancellation token.\n  3. It evaluates the result upon execution to determine success and gather output.\n  4. Returns a `CodeExecutionResult` indicating success and the output of the executed code.\n\n## Usage Example  \nA typical usage of `PythonCodeExecutionTool` would involve setting up an environment with the necessary dependencies, such as code executors and libraries, to facilitate interaction with the tool. The example illustrates how to initialize the tool using a local command line executor and execute a task asynchronously, where the user requests to create a plot using the `yfinance` and `matplotlib` libraries.\n\n### Example Implementation  \n```python\nasync def main() -> None:\n    tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir=\"coding\"))\n    ...\n```\n- This demonstrates the initialization of the `PythonCodeExecutionTool` and its typical integration within an application context that may involve an AI assistant or interactive shell.\n\n## Conclusion  \nThe `PythonCodeExecutionTool` is a robust component tailored for executing Python code blocks safely and efficiently. It is configurable, allowing users to swap out execution environments, and presents a clear structure for both input and output via its use of data models. This makes it versatile for diverse applications, particularly in interactive environments requiring dynamic code execution.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/code_execution/_code_execution.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as an interface for importing various configuration and search-related tools. It organizes and exposes certain functionalities that facilitate global and local search capabilities in a structured manner. The imported components are likely designed to assist in data processing, context management, and search functionality, probably within a data-intensive application.\n\n## Imports\n\nThe module imports several classes and configurations from two submodules, namely:\n\n- **Submodule: `_config`**  \n  This submodule seems to define various configuration classes, likely related to both global and local contexts. The configurations imported include:\n  - `GlobalContextConfig`\n  - `GlobalDataConfig`\n  - `LocalContextConfig`\n  - `LocalDataConfig`\n  - `MapReduceConfig`\n  - `SearchConfig`\n\n- **Submodule: `_global_search`**  \n  This submodule appears to define tools specifically for global searches. The components imported include:\n  - `GlobalSearchTool`: A tool likely focused on performing searches across a global dataset.\n  - `GlobalSearchToolArgs`: Arguments that may be required for initializing or executing the global search.\n  - `GlobalSearchToolReturn`: Structure or class defining the return type from the global search tool.\n\n- **Submodule: `_local_search`**  \n  This submodule is tuned towards local searches, with components like:\n  - `LocalSearchTool`: A tool tailored for conducting searches within a more localized dataset.\n  - `LocalSearchToolArgs`: Arguments required for the local search operations.\n  - `LocalSearchToolReturn`: Structure or class defining the return type from the local search tool.\n\n## Exposed Components\n\nThe module specifies an `__all__` list, which controls what is publicly accessible when this module is imported. This is done to maintain a clean namespace and encapsulate internal implementations. The components included in `__all__` are:\n\n- `GlobalSearchTool`\n- `LocalSearchTool`\n- Configuration classes:\n  - `GlobalDataConfig`\n  - `LocalDataConfig`\n  - `GlobalContextConfig`\n  - `LocalContextConfig`\n  - `MapReduceConfig`\n  - `SearchConfig`\n- Argument structures and return types for search tools:\n  - `GlobalSearchToolArgs`\n  - `GlobalSearchToolReturn`\n  - `LocalSearchToolArgs`\n  - `LocalSearchToolReturn`\n\nThis selection of components indicates a robust framework for handling both global and local search functionalities with appropriate context and data configurations.\n\n## Purpose and Role of Classes\n\n### Search Tools\n\n- **GlobalSearchTool**  \n  This class likely implements functionality that enables searching within globally available datasets. Its presence suggests that the application addresses scenarios requiring wide-ranging data lookup and retrieval processes.\n\n- **LocalSearchTool**  \n  In contrast, this class focuses on searching within a local scope, which could be a subset of data or specific to a certain context. This differentiation allows users to target their search processes more efficiently based on data locality.\n\n### Configuration Classes\n\n- **GlobalContextConfig & LocalContextConfig**  \n  These classes probably define the contextual parameters that the search tools will utilize. The configurations may include settings or options that can be adjusted according to the user's needs.\n\n- **GlobalDataConfig & LocalDataConfig**  \n  These classes are likely responsible for specifying data-related configurations that inform the search tools on how to handle or treat the data during searches.\n\n- **MapReduceConfig**  \n  This class indicates support for distributed computing strategies, specifically designed to handle large datasets effectively. It probably configures parameters necessary for MapReduce operations within the context of search functionalities.\n\n- **SearchConfig**  \n  This class likely serves as a general configuration tool for managing search behavior and specifications.\n\n### Argument & Return Classes\n\n- **GlobalSearchToolArgs & LocalSearchToolArgs**  \n  These argument classes are essential for initializing the respective search tools. They define the parameters that users must provide when invoking these tools, ensuring proper usage.\n\n- **GlobalSearchToolReturn & LocalSearchToolReturn**  \n  These return classes encapsulate the output of the search operations performed by their respective tools, standardizing the results for easier handling and interpretation.\n\n## Summary\n\nIn summary, this module is focused on facilitating search functionality within an application that probably deals with large datasets. By structuring its components into clear categories \u2014 search tools, configuration classes, and argument/return specifications \u2014 it provides both modularity and ease of use. Users of this module can leverage global or local search tools, guided by well-defined configurations, to perform efficient data queries. The inclusion of MapReduce capabilities marks its readiness for handling extensive data processing requirements.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/graphrag/__init__.py"
  },
  {
    "code": false,
    "content": "# Configuration Management in Data Processing\n\nThis code defines a set of configuration classes that facilitate the setup of various parameters used in data processing applications. Built using the Pydantic library, these classes ensure that the configurations are validated against specific types, promoting robustness in configuration handling.\n\n## Base Configuration Classes\n\n### `DataConfig`\n\nThe `DataConfig` class serves as the foundational configuration data model. It encapsulates general data-related configurations necessary for initializing data processing tasks.\n\n- **Attributes**:\n  - `input_dir`: A string that specifies the directory from which data will be read.\n  - `entity_table`: A string that defaults to \"entities\", indicating the table name for storing entity data.\n  - `entity_embedding_table`: A string that defaults to \"entities\", pointing to the table responsible for storing entity embeddings.\n  - `community_table`: A string specifying the table name for community data, defaulting to \"communities\".\n  - `community_level`: An integer that sets the level of community hierarchy, defaulting to 2.\n\n## Extended Configuration Classes\n\n### `GlobalDataConfig`\n\n`GlobalDataConfig` inherits from `DataConfig` and is aimed at configurations specific to global data processing aspects.\n\n- **Attributes**:\n  - `community_report_table`: A string that defaults to \"community_reports\", designating the table used for storing community reports.\n\n### `LocalDataConfig`\n\nSimilarly, `LocalDataConfig` also extends `DataConfig`, focusing on configurations relevant to local data processing tasks.\n\n- **Attributes**:\n  - `relationship_table`: A string (defaulting to \"relationships\") that indicates the table for storing relationships.\n  - `text_unit_table`: A string (defaulting to \"text_units\") specifying the table for handling text unit data.\n\n## Context Configuration Classes\n\n### `ContextConfig`\n\n`ContextConfig` establishes a base class for managing general context-based configurations during data processing.\n\n- **Attributes**:\n  - `max_data_tokens`: An integer setting the maximum number of tokens for the data, defaulted to 8000.\n\n### `GlobalContextConfig`\n\nThis class extends `ContextConfig` to define settings for global context processing.\n\n- **Attributes**:\n  - `use_community_summary`: A boolean (defaulting to False) that controls whether to use a summary of communities.\n  - `shuffle_data`: A boolean (defaulting to True) indicating if the data should be shuffled.\n  - `include_community_rank`: A boolean (defaulting to True) for including community rank in the processing context.\n  - `min_community_rank`: An integer that defaults to 0, specifying the minimum rank of communities to consider.\n  - `community_rank_name`: A string (defaulting to \"rank\") that designates the name used for community rank.\n  - `include_community_weight`: A boolean (defaulting to True) that determines the inclusion of community weight.\n  - `community_weight_name`: A string (defaulting to \"occurrence weight\") for naming the community weight.\n  - `normalize_community_weight`: A boolean that indicates whether to normalize the community weight, defaulting to True.\n  - Furthermore, it overrides `max_data_tokens` to a higher limit of 12000.\n\n### `LocalContextConfig`\n\n`LocalContextConfig` focuses on local context configurations specifically.\n\n- **Attributes**:\n  - `text_unit_prop`: A float (defaulting to 0.5) representing the proportion of text units to consider.\n  - `community_prop`: A float (defaulting to 0.25) that signifies the proportion of communities to include.\n  - `include_entity_rank`: A boolean (defaulting to True) indicating whether to include entity rank in processing.\n  - `rank_description`: A string (defaulting to \"number of relationships\") used for describing the rank.\n  - `include_relationship_weight`: A boolean (defaulting to True) determining if the weight of relationships should be included.\n  - `relationship_ranking_attribute`: A string (defaulting to \"rank\") naming the attribute used for ranking relationships.\n\n## Map Reduce Configuration Class\n\n### `MapReduceConfig`\n\nThis class encapsulates configurations related to the map-reduce operations, typically used in data processing pipelines to handle large datasets.\n\n- **Attributes**:\n  - `map_max_tokens`: An integer (defaulting to 1000) that limits the maximum number of tokens for the mapping phase.\n  - `map_temperature`: A float to set the randomness during the mapping phase, defaulting to 0.0.\n  - `reduce_max_tokens`: An integer (defaulting to 2000) that confines the maximum token count during the reduce phase.\n  - `reduce_temperature`: A float (defaulting to 0.0) for the randomness in the reducing phase.\n  - `allow_general_knowledge`: A boolean (defaulting to False) indicating whether general knowledge is permissible during processing.\n  - `json_mode`: A boolean (defaulting to False) that signifies if the output should be in JSON format.\n  - `response_type`: A string (defaulting to \"multiple paragraphs\") describing the format of the response expected.\n\n## Search Configuration Class\n\n### `SearchConfig`\n\nLastly, `SearchConfig` is designed to handle specific search operational settings.\n\n- **Attributes**:\n  - `max_tokens`: An integer (defaulting to 1500) that limits the maximum number of tokens for search results.\n  - `temperature`: A float (defaulting to 0.0) indicating the randomness in search results.\n  - `response_type`: A string (defaulting to \"multiple paragraphs\") that specifies the expected format of the search results. \n\nThis structured hierarchy of configuration classes simplifies the management and validation of various parameters utilized across different contexts in data processing workflows, enhancing modularity and clarity in configuration handling.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/graphrag/_config.py"
  },
  {
    "code": false,
    "content": "# Global Search Tool Documentation\n\n## Overview\n\nThe `GlobalSearchTool` class is part of the GraphRAG (Graph-based Retrieval-Augmented Generation) framework. This tool enables performing semantic searches on a corpus of documents by leveraging graph relationships and semantic embeddings. It is particularly designed to help users retrieve relevant information based on specific queries within a dataset.\n\nBefore using the tool, essential setup steps must be completed, including project initialization, prompt configuration, and data indexing as detailed in the GraphRAG documentation. \n\n## Imports and Dependencies\n\nThe code imports several libraries and modules, crucial for its operation:\n\n- **Pathlib**: For handling file and directory paths in a platform-independent manner.\n- **Pandas**: For data manipulation and reading parquet files which store structured data.\n- **tiktoken**: A token encoder used to manage text inputs for the language model.\n- **Pydantic**: For defining data models with validation, ensuring that the inputs and outputs conform to specified types.\n- **GraphRAG modules**: These provide essential functionalities such as configuration loading, model management, and various indexing adapters used in forming the search context.\n\n## Data Models\n\n### GlobalSearchToolArgs\n\nThe `GlobalSearchToolArgs` is a data model defined using Pydantic. It contains:\n\n- `query`: A required string to hold the user's search query. It serves as the input for executing a global search.\n\n### GlobalSearchToolReturn\n\nThe `GlobalSearchToolReturn` is another Pydantic model representing the output of the tool. It contains:\n\n- `answer`: A string that holds the response derived from the search process.\n\n## GlobalSearchTool Class\n\nThe `GlobalSearchTool` class extends `BaseTool` and acts as a wrapper enabling the execution of global search queries. \n\n### Constructor\n\nThe constructor initializes several attributes and performs the following key tasks:\n\n- Takes several parameters such as token encoder, model, and configurations related to data, context, and map-reduce.\n- Reads parquet files to load community data, entity data, and community reports into Pandas dataframes.\n- Utilizes indexing functions to process these dataframes and create communities, reports, and entities for further use in searching.\n- Initializes a context builder `GlobalCommunityContext` using the processed data.\n- Configures parameters needed for the map-reduce model, including token limits and temperature settings for generating responses.\n\n### run Method\n\nThe `run` method is an asynchronous function that:\n\n- Takes the search parameters in the form of `GlobalSearchToolArgs`.\n- Invokes the search method on the search engine with the provided query.\n- Ensures the response is a string and returns it wrapped in a `GlobalSearchToolReturn` object.\n\n## Configuration and Initialization\n\n### from_settings Class Method\n\nThe `from_settings` method allows the creation of a `GlobalSearchTool` instance from a settings file. It performs the following tasks:\n\n- Accepts paths for the root directory and optional configuration file.\n- Loads the GraphRAG configuration settings.\n- Initializes the token encoder compatible with the specified model.\n- Creates a language model instance based on the configuration.\n- Constructs a `DataConfig` object to handle the paths for input data.\n\nThis method encapsulates the initialization process, streamlining the setup of the `GlobalSearchTool`.\n\n## Example Usage\n\nThe documentation includes an example illustrating how to utilize the `GlobalSearchTool` within an asynchronous context:\n\n1. **OpenAI Client Initialization**: Sets up a ChatGPT client using a specified model and API key.\n2. **Global Search Tool Configuration**: Instantiates the `GlobalSearchTool` using settings loaded from a YAML file.\n3. **Assistant Agent Creation**: Initializes an `AssistantAgent` equipped with the search tool to interpret user queries and formulate responses.\n4. **Running a Query**: Demonstrates handling a sample query regarding community reports and outputs the results using the console interface.\n\n## Prerequisite Steps and Documentation\n\nUsers are advised to follow specific setup steps before using the `GlobalSearchTool`:\n\n- Complete the GraphRAG project initialization.\n- Configure prompts tailored for specific use cases.\n- Execute the data indexing processes.\n- Ensure the presence of the settings YAML file.\n\nFor detailed steps, users are encouraged to consult the official [GraphRAG documentation](https://microsoft.github.io/graphrag/).\n\n## Conclusion\n\nThe `GlobalSearchTool` serves as a vital component in the GraphRAG ecosystem, bridging the gap between user queries and large volumes of document data. Through thoughtful architecture and integration with ML models, it enhances information retrieval in a semantically meaningful way.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/graphrag/_global_search.py"
  },
  {
    "code": false,
    "content": "# Documentation for `LocalSearchTool`\n\nThe `LocalSearchTool` module is designed to perform local semantic searches over a corpus of documents using the GraphRAG framework. It enables users to query data effectively by combining local document context with semantic embeddings to find relevant information. This documentation highlights the key components and functionalities of the code.\n\n## Imports and Setup\n\nThe script begins by importing necessary libraries and components:\n\n- **Path** from `pathlib` for file path manipulation.\n- **pandas** for data manipulation and analysis.\n- **tiktoken** for tokenization of text.\n- Various modules from **autogen_core** and **graphrag** that are essential for the tool's functionality.\n\nIt also defines default configurations for context and search parameters using `LocalContextConfig` and `SearchConfig`.\n\n## Data Models\n\n### LocalSearchToolArgs\n\nThe `LocalSearchToolArgs` class is a data model defined using `pydantic`, which includes:\n\n- A single string field, `query`, representing the user query to be executed during the local search. This makes the input structure defined and validated, ensuring that the query is provided correctly.\n\n### LocalSearchToolReturn\n\nSimilarly, the `LocalSearchToolReturn` class is another `pydantic` model that encapsulates the response of a search operation:\n\n- It contains an `answer` string which represents the resultant answer to the query.\n\n## LocalSearchTool Class\n\nThe centerpiece of the code is the `LocalSearchTool` class, which extends `BaseTool`. This class integrates various components to execute local searches based on the input arguments:\n\n### Constructor (`__init__`)\n\n- The constructor initializes with parameters for the token encoder, chat model, embedding model, data configuration, and optional context and search configurations. It validates and sets up the tool with necessary components.\n- It loads data from Parquet files using pandas, setting four DataFrames to hold relevant data for entities, relationships, text units, and community interactions.\n\n### Data Processing\n\n- Using the imported indexer adapters, the script processes the DataFrames (`entity_df`, `relationship_df`, and `text_unit_df`) to produce structured data for entities, relationships, and text units.\n- A `LanceDBVectorStore` is instantiated for managing vector embeddings of entity descriptions.\n\n### Context Builder Setup\n\n- A `LocalSearchMixedContext` instance is created, utilizing the previously processed data and parameters to build contextual information for the search. The class encapsulates the logic for how incoming queries should be interpreted in the context of the loaded documents.\n\n### Search Engine Initialization\n\n- The search engine is instantiated using the `LocalSearch` class, which takes the pre-configured models, context builder, and parameters for operating the search and returning results.\n\n## Method: run\n\nThe `run` method is an asynchronous function that executes the search operation:\n\n- It accepts `args` (the user input query) and `cancellation_token` (to cancel the operation if necessary).\n- It calls the `search` method of the `_search_engine` to retrieve results based on the provided query.\n- The method ensures that the response is of type string before returning it encapsulated in the `LocalSearchToolReturn` model.\n\n## Class Method: from_settings\n\nThe `from_settings` class method facilitates the creation of `LocalSearchTool` instances from an external configuration file:\n\n- It loads settings from a provided path and retrieves necessary configurations for the chat model and embedding model.\n- The method handles potential errors if required configurations are missing, ensuring robust initialization.\n- It attempts to create a token encoder based on the chat model being employed, with a fallback to a default encoder if the model isn't recognized.\n- Finally, it constructs the `LocalSearchTool` instance with all gathered configurations and models.\n\n## Example Usage\n\nThe provided code includes an example demonstrating how to use `LocalSearchTool` within an asynchronous context. The example outlines the steps to initialize an OpenAI client, prepare the local search tool, set up an assistant agent, and execute a sample query. The assistant uses the GraphRAG framework to operate intelligently based on user input.\n\n```python\nasync def main():\n    # Initialize the OpenAI client\n    openai_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o-mini\",\n        api_key=\"<api-key>\",\n    )\n\n    # Set up local search tool\n    local_tool = LocalSearchTool.from_settings(root_dir=Path(\"./\"), config_filepath=Path(\"./settings.yaml\"))\n\n    # Create assistant agent with the local search tool\n    assistant_agent = AssistantAgent(\n        name=\"search_assistant\",\n        tools=[local_tool],\n        model_client=openai_client,\n        system_message=(\n            \"You are a tool selector AI assistant using the GraphRAG framework. \"\n            \"Your primary task is to determine the appropriate search tool to call based on the user's query. \"\n            \"For specific, detailed information about particular entities or relationships, call the 'local_search' function.\"\n        ),\n    )\n\n    # Run a sample query\n    query = \"What does the station-master say about Dr. Becher?\"\n    await Console(assistant_agent.run_stream(task=query))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Conclusion\n\nThe `LocalSearchTool` module provides a robust mechanism for executing local searches within a document corpus using GraphRAG's capabilities. By leveraging semantic embeddings and contextual understanding, it enables more effective query processing, useful in various applications involving document retrieval and processing. The defined classes and methods ensure structured handling of inputs and outputs, promoting efficient usage within larger systems or applications. For further exploration, users are encouraged to refer to the GraphRAG documentation for detailed setup processes and configurations.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/graphrag/_local_search.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: `http_tool`\n\nThe `http_tool` module provides a structured way to work with HTTP-related functionalities through the `HttpTool` class. This documentation outlines the module's purpose and its components.\n\n## Overview\n\nThe primary purpose of the `http_tool` module is to encapsulate HTTP operations within the `HttpTool` class. It allows users to perform various HTTP interactions easily, promoting code reuse and modular design.\n\n## Imports\n\n```python\nfrom ._http_tool import HttpTool\n```\n\nThe module imports the `HttpTool` class from a sibling module named `_http_tool`. This indicates that `HttpTool` is likely defined in that module, and `_http_tool` may contain additional internal functionality that is not exposed directly to users.\n\n## Public Interface\n\n```python\n__all__ = [\"HttpTool\"]\n```\n\nThe `__all__` variable defines the public interface of the module. By including `HttpTool`, the module specifies that this is the only class to be exported when the module is imported using the `from module import *` syntax. This encapsulation protects other internal functions or classes that may be present in the `_http_tool` module.\n\n## Class: `HttpTool`\n\nAlthough the code itself does not provide details about the implementation of `HttpTool`, it can be inferred that this class serves as the main interface for HTTP operations within this module. Based on common practices, here are a few hypothetical functionalities that `HttpTool` might encompass:\n\n1. **Sending HTTP Requests**: This class may allow the user to send GET, POST, PUT, DELETE, and other HTTP requests.\n2. **Handling Responses**: It might offer features to manage responses, including status code checks, JSON parsing, and error handling.\n3. **Configuration Options**: `HttpTool` could include options for configuring headers, timeouts, and other parameters that are essential in HTTP requests.\n4. **Logging**: The class may provide mechanisms for logging requests and responses for debugging or auditing purposes.\n\n## Usage\n\nWhile specific usage examples are not provided in this snippet, typical utilization of the `HttpTool` class would likely involve:\n\n1. Importing the class:\n   ```python\n   from http_tool import HttpTool\n   ```\n\n2. Creating an instance of `HttpTool`:\n   ```python\n   http_tool = HttpTool()\n   ```\n\n3. Using its methods to perform HTTP operations, such as:\n   ```python\n   response = http_tool.get(\"https://api.example.com/resource\")\n   ```\n\n4. Processing the response returned by the HTTP requests.\n\n## Conclusion\n\nThe `http_tool` module serves as a wrapper around the `HttpTool` class, which is designed to manage HTTP interactions efficiently. By defining a clear public interface and encapsulating implementation details, this module enhances code maintainability and readability for those interacting with HTTP services.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/http/__init__.py"
  },
  {
    "code": false,
    "content": "# HTTP Tool Documentation\n\n## Overview\n\nThe provided code defines an HTTP tool integrated into the `autogen` framework. This tool enables asynchronous HTTP requests to be made using various HTTP methods (GET, POST, PUT, DELETE, PATCH) and allows for customizable parameters, headers, and response handling. It makes use of Pydantic for data validation and JSON schema handling.\n\n## Dependencies\n\nThe code imports several modules and classes:\n- `httpx`: Used for making HTTP requests.\n- `re`: For regular expressions used in templating path parameters.\n- `pydantic`: For defining data models and validation.\n- `autogen_core` components: Extends the framework to include tools for actions.\n- `json_schema_to_pydantic`: for creating Pydantic models from JSON Schema objects.\n\n## Configuration Model: `HttpToolConfig`\n\n### Class Definition\n\nThe `HttpToolConfig` class is a Pydantic model defining the configuration properties for the HTTP tool. It specifies:\n\n- `name`: The name of the tool.\n- `description`: An optional description of the tool's functionality.\n- `scheme`: HTTP or HTTPS protocol.\n- `host`: The URL of the server to send requests to.\n- `port`: The server port.\n- `path`: The endpoint path, supporting parameterization.\n- `method`: HTTP method used for requests.\n- `headers`: Optional HTTP headers as a dictionary.\n- `json_schema`: A JSON Schema defining expected request parameters.\n- `return_type`: Specifies if the response should be returned as text or JSON.\n- `timeout`: The duration in seconds to wait for the server's response.\n\n### Purpose\n\nThis model ensures that all necessary configurations are validated before being used by the HTTP tool, enabling clear documentation and enforceable structure for the tool's configuration.\n\n## HTTP Tool Class: `HttpTool`\n\n### Class Definition\n\nThe `HttpTool` class extends `BaseTool` and `Component`. It encapsulates the operations for making HTTP requests based on the configuration.\n\n### Constructor\n\nThe constructor initializes an instance of `HttpTool` using parameters provided by the user. It creates an instance of `HttpToolConfig` made from user-provided parameters. The path parameters are identified using regex, which will be used later to format the request path. Finally, it creates an input model from the provided JSON schema, thereby ensuring all input used in requests will adhere to the specified format.\n\n### Method `_to_config`\n\nThis method creates and returns a copy of the tool configuration, preserving the current state. This is useful for serializing the configuration or for cloning purposes.\n\n### Class Method `_from_config`\n\nThis class method allows creating an instance of the `HttpTool` from an existing configuration model, facilitating easier instantiation from saved or retrieved configurations.\n\n## Asynchronous Execution: `run` Method\n\n### Method Purpose\n\nThe `run` method is the core functionality of the `HttpTool`, responsible for executing the HTTP request based on validated arguments.\n\n### Parameters\n\n- `args`: A validated Pydantic model containing the parameters for the HTTP request.\n- `cancellation_token`: An instance of `CancellationToken` that can be used to cancel the HTTP operation if necessary.\n\n### Execution Steps\n\n1. **Parameter Handling**: The method extracts path parameters from the `args` model for path formatting while removing them from the parameter dictionary used in the request.\n2. **Path Construction**: The URL path is formatted by replacing placeholders in the `path` attribute with actual values from the provided arguments.\n3. **URL Construction**: Using the `httpx` library, a complete URL is constructed with the specified scheme, host, port, and path.\n4. **HTTP Client**: An asynchronous HTTP client is created using `httpx.AsyncClient`.\n5. **Request Execution**: Depending on the specified HTTP method, the appropriate request is sent to the server (GET, POST, PUT, DELETE, PATCH).\n6. **Response Handling**: \n   - If the `return_type` is set to \"text\", the response body is returned as a string.\n   - If it is \"json\", the JSON response is returned after parsing.\n   - An exception will be raised for invalid `return_type` values.\n\n## Example Usage\n\nThe documentation provides an example illustrating how to create an instance of `HttpTool` to decode base64 encoded strings using the `httpbin` API. It defines a JSON schema to validate the input and then demonstrates how to integrate this tool into an assistant using `OpenAIChatCompletionClient`. The assistant is capable of understanding user messages and making HTTP requests through the tool to produce responses.\n\n```python\n# Example of creating a base64 decode tool\nbase64_tool = HttpTool(\n    name=\"base64_decode\",\n    description=\"base64 decode a value\",\n    scheme=\"https\",\n    host=\"httpbin.org\",\n    port=443,\n    path=\"/base64/{value}\",\n    method=\"GET\",\n    json_schema=base64_schema,\n)\n```\n\nThis section showcases the intended integration with the `autogen` ecosystem and highlights the tool's versatility in handling distinct HTTP requests while conforming to specified schemas.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/http/_http_tool.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: `langchain_adapter`\n\nThis module is a part of a larger codebase that includes the `LangChainToolAdapter`. The purpose of this file is to import and expose the `LangChainToolAdapter` class or function from another module located within the same package.\n\n## Overview\n\nIn Python, the `__init__.py` file is commonly used to mark a directory as a Python package. It can also serve to define what is accessible when the package is imported. This particular module facilitates the import of the `LangChainToolAdapter`, making it available to other parts of the application or to users of the package.\n\n## Imports\n\nThe module begins by importing `LangChainToolAdapter` from the local submodule `._langchain_adapter`. This underscores that `LangChainToolAdapter` is likely defined in a file named `_langchain_adapter.py` which resides in the same package.\n\n```python\nfrom ._langchain_adapter import LangChainToolAdapter\n```\n\n## Public API\n\nThe `__all__` variable is defined to specify the public interface of this module. It allows the module to control which names are exported when `from module import *` is used. In this case, only `LangChainToolAdapter` is included in the public API.\n\n```python\n__all__ = [\"LangChainToolAdapter\"]\n```\n\n## Purpose of `LangChainToolAdapter`\n\nWhile the code does not provide details about the `LangChainToolAdapter`, we can infer from its name that it likely serves as an adapter for tools related to \"LangChain,\" which is a framework often used for natural language processing and working with language models. Adapters commonly facilitate interaction between different components or tools, allowing them to work together seamlessly.\n\n## Structural Considerations\n\nThis module follows a clean and structured design pattern typical in Python applications, especially those meant for reusability and organization. By defining `__all__`, it helps maintain a clear interface that prevents unintentional access to internal classes, functions, or variables, promoting encapsulation.\n\n## Code Organization\n\nThe import statement uses relative import syntax (`from ._langchain_adapter`), indicating that it's meant to work in a package context. This style of import is beneficial for code organization, allowing related functionality to be grouped, making maintenance easier.\n\n## Conclusion\n\nIn summary, the `langchain_adapter` module structures the accessibility of the `LangChainToolAdapter`, supporting a modular design that is prevalent in Python packages. By limiting the public interface and defining a clear purpose for its classes, it promotes organized development practices and enhances the maintainability of the overall codebase.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/langchain/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for `LangChainToolAdapter`\n\n## Overview\n\nThe `LangChainToolAdapter` class extends the functionality of the `BaseTool` class to allow integration of LangChain tools into the AutoGen framework. This adapter enables seamless interaction with various tools defined within the LangChain ecosystem, facilitating their use in automation and data processing tasks.\n\n### Purpose\n\nThe primary goal of `LangChainToolAdapter` is to wrap LangChain tools and expose them in a format compatible with AutoGen. This enables users to utilize sophisticated functionalities provided by LangChain tools easily, such as data manipulation or querying databases, by integrating them into asynchronous workflows.\n\n## Class Definition\n\n### `LangChainToolAdapter`\n\nThe `LangChainToolAdapter` class inherits from `BaseTool[BaseModel, Any]`, indicating that it is capable of processing a specific data model derived from Pydantic's `BaseModel`. This design enables type checking and validation of the parameters passed to the wrapped LangChain tool.\n\n#### Initialization\n\nThe constructor of the `LangChainToolAdapter` requires a `LangChainTool` as an argument. Inside the constructor, several essential operations are performed:\n\n1. **Tool Attributes**: The name and description of the LangChain tool are extracted for later use.\n2. **Callable Determination**: The constructor checks if the LangChain tool exposes a callable method named either `func` or `_run`. This is crucial as it determines how the tool's functionality can be executed.\n3. **Arguments Schema**: If the LangChain tool provides an arguments schema, it is used to define the argument types. If not, the argument types are inferred from the method's signature.\n4. **Model Creation**: A Pydantic model representing the expected arguments is created, ensuring it is a subclass of `BaseModel`. This is critical for type validation and ensures a consistent interface.\n\n### Errors and Validation\n\nThe constructor includes error handling to address potential issues, such as the absence of a callable method or failure in creating a valid Pydantic model for the arguments.\n\n## Asynchronous Execution\n\n### `run`\n\nThe `run` method is responsible for executing the wrapped LangChain tool with the provided arguments:\n\n- **Argument Preparation**: It converts the Pydantic model instance into a dictionary of keyword arguments.\n- **Callable Execution**: Depending on whether the wrapped function is asynchronous, the method either directly awaits the call or runs it in a separate thread to keep the event loop unblocked.\n\nThis design allows methods to maintain their required operational mode (synchronous or asynchronous) while being wrapped and executed through this adapter.\n\n### `_call_sync`\n\nThe `_call_sync` method handles the synchronous invocation of the wrapped function. This allows the adapter to call traditional synchronous functions without blocking the main event loop of the application.\n\n## Example Usage\n\nThe documentation includes examples that illustrate how to use the `LangChainToolAdapter` within an `async` main function:\n\n1. **DataFrame Interaction**:\n   - A `PythonAstREPLTool` is wrapped in the adapter to access a Pandas DataFrame.\n   - An `AssistantAgent` uses the adapter to process user queries regarding the DataFrame, demonstrating how LangChain tools can facilitate data analysis in conversational AI applications.\n\n2. **Database Querying**:\n   - An example shows how to integrate a SQL database via the `SQLDatabaseToolkit`.\n   - The agent iterates through database queries using the Round Robin strategy, showcasing the functionality of the adapter in processing multiple sequential tasks.\n\n## Integration Requirements\n\nTo utilize the `LangChainToolAdapter`, users must install the required dependencies by following the provided instruction:\n\n```bash\npip install -U \"autogen-ext[langchain]\"\n```\n\nThis ensures that all necessary components for LangChain interaction are present in the environment.\n\n## Final Notes\n\nThe `LangChainToolAdapter` serves as a bridge between the advanced capabilities of LangChain and the structured processing logic of AutoGen. By efficiently managing interaction and execution across different tool types, it enriches the potential for automation in data science and conversational applications. The thoughtful design around error handling and type safety contributes to a robust implementation suitable for production-grade systems.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/langchain/_langchain_adapter.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module imports various components essential for creating and managing a Multipurpose Communication Protocol (MCP) server. It facilitates communication, session management, configuration, and tool adaptations necessary for an MCP server environment.\n\n## Imports\n\nThe module imports several classes and functions from other modules within the same package. Each of these plays a distinct role in the overall functionality:\n\n1. **McpSessionActor**: This likely manages an MCP session's lifecycle, handling various session-related tasks.\n2. **McpServerParams, SseServerParams, StdioServerParams, StreamableHttpServerParams**: These classes hold configuration parameters for different types of MCP servers, defining how they should behave based on the provided parameters.\n3. **mcp_server_tools**: This might be a collection of helper functions or tools specifically designed to support the MCP server's operations.\n4. **create_mcp_server_session**: A function responsible for initializing an MCP server session, probably involving setting up the necessary environment and components for communication.\n5. **SseMcpToolAdapter, StdioMcpToolAdapter, StreamableHttpMcpToolAdapter**: These classes adapt different communication mechanisms (SSE, standard input/output, and streamable HTTP) for use with MCP, allowing flexible interaction methods with the server.\n6. **McpWorkbench**: This class seems to act as a workspace or an environment for developing and managing MCP-related tasks or sessions.\n\n## Purpose of Functions and Classes\n\n### `McpSessionActor`\n\n- Responsible for managing and orchestrating the sessions in the MCP protocol.\n- Facilitates communication between different components, ensuring that sessions are handled correctly.\n\n### Configuration Parameter Classes\n\n- **McpServerParams, SseServerParams, StdioServerParams, StreamableHttpServerParams**:\n  - Each class encapsulates specific server configuration settings required to customize the behavior and functionality of the respective server types.\n\n### `mcp_server_tools`\n\n- A utility module that likely contains functions to perform routine tasks associated with the MCP server, enhancing modularity and reusability.\n\n### `create_mcp_server_session`\n\n- Function designed to initiate an MCP server session.\n- Ensures that all necessary components are correctly configured and initialized, enabling the server's operation.\n\n### Tool Adapters\n\n- **SseMcpToolAdapter**: Adapts server-sent events for MCP communication, enabling real-time data delivery.\n- **StdioMcpToolAdapter**: Manages input/output interactions over standard console streams.\n- **StreamableHttpMcpToolAdapter**: Handles HTTP communications, potentially supporting streaming data formats for more efficient data transfer.\n\n### `McpWorkbench`\n\n- Serves as an integrated development environment or workspace for the MCP, facilitating session handling, configuration management, and tool integration.\n\n## Summary of Functionality\n\nThe script serves primarily as a connector and initializer for various components related to an MCP server. By aggregating classes and functions:\n\n1. It facilitates the creation and management of sessions for communication through the MCP protocol.\n2. It abstracts the configuration details necessary for various types of servers through parameter classes.\n3. It provides adaptable tools for different communication methods, allowing for versatile use cases with the MCP server.\n4. It supports a structured approach to tool integration and session management through the `McpWorkbench`, enhancing the developer experience.\n\nOverall, this module is aimed at establishing a robust framework for developing, configuring, and managing MCP servers and their sessions effectively.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/__init__.py"
  },
  {
    "code": false,
    "content": "# MCP Session Actor Documentation\n\n## Overview\n\nThe provided code defines a component called `McpSessionActor`, which integrates with a model client for performing various operations like handling user messages, processing commands, and managing resources in an asynchronous environment. This component is primarily designed to interact with a backend service, facilitating communication between user commands and the model's capabilities.\n\n## Imports and Dependencies\n\nThe code imports several modules necessary for its operation:\n\n- **asyncio**: For managing asynchronous programming.\n- **atexit**: To register cleanup actions upon program termination.\n- **base64 and io**: For encoding and manipulating binary data.\n- **logging**: For logging messages and errors.\n- **Pillow (PIL)**: For handling image processing.\n- **Pydantic**: For data validation and settings management through models.\n- **Custom types and classes**: Imported from `autogen_core` and `mcp`, which presumably hold the core functionalities and models for this component.\n\n## Content Parsing Functions\n\n### `_parse_sampling_content`\n\nThis function takes a content object from the MCP (Model Communication Protocol) and converts it into a compatible format for the `Autogen` framework. It distinguishes between different types of content:\n\n- **Text Content**: Directly returns the text.\n- **Image Content**: Decodes base64 image data into a PIL Image object. It raises an error if the model does not support image content.\n- **Audio Content**: Currently, it doesn't handle audio but is structured to support future extensions.\n\n### `_parse_sampling_message`\n\nThis function translates MCP sampling messages into `LLMMessage` objects used by the Autogen framework. It leverages `_parse_sampling_content` to extract the content and creates either a `UserMessage` or `AssistantMessage` based on the message's role.\n\n## McpSessionActor Class\n\nThe primary class in this module is `McpSessionActor`, which inherits from `ComponentBase` and implements the `Component` interface. This class provides various functionalities needed to manage an MCP session.\n\n### Class Attributes and Initialization\n\n- **`component_type`** and **`component_config_schema`**: Define metadata for the component type and its configuration schema.\n- **`server_params`**: Contains parameters needed to connect to the MCP server.\n- **`_model_client`**: An optional chat completion client interface for interacting with language models.\n\nUpon initialization, the class sets up internal parameters, creates an async command queue, and registers a shutdown handler to execute cleanup tasks upon interpreter exit.\n\n### Key Methods\n\n#### `initialize`\n\nActivates the actor. If it's not already running, this function starts the internal actor task that processes incoming commands.\n\n#### `call`\n\nThis method executes commands related to the MCP functionality, such as listing tools or prompts, calling tools, reading resources, and more. It raises an error if called before the actor has been initialized or if the command is not recognized.\n\n#### `close`\n\nSafely shuts down the actor, ensuring all pending commands are processed and resources are released. It handles shutdown signals properly to avoid leaving hanging tasks.\n\n### Internal Methods\n\n#### `_sampling_callback`\n\nHandles sampling requests. It converts incoming messages into the expected format, runs the model client to process these messages, and returns either the resulting content or an error. It effectively connects user queries to model outputs.\n\n#### `_run_actor`\n\nThis method manages the main event loop for the actor. It continuously listens for command events placed in the command queue and executes them. Each command is handled asynchronously, and exceptions are caught to prevent crashing.\n\n### Lifecycle Management\n\n#### `_sync_shutdown`\n\nThis method ensures a graceful shutdown by closing the actor's task and cleaning up resources when the program exits.\n\n#### `_to_config` and `_from_config`\n\nBoth methods handle converting the actor to and from its configuration representation. This is vital for maintaining consistent configurations across different instances of the actor.\n\n## Conclusion\n\nOverall, the MCP Session Actor is a robust component designed to interact asynchronously with an external model system and manage different types of user and system messages. It provides a structured way to handle commands, manage resources, and ensure reliable communication with backend services, making it suitable for applications requiring complex user interaction with AI models. Its design promotes modular, maintainable, and extensible integration points while adhering to asynchronous programming paradigms.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_actor.py"
  },
  {
    "code": false,
    "content": "# MCP Tool Adapter Documentation\n\n## Overview\n\nThe `McpToolAdapter` class is designed to provide an interface for integrating MCP (Multi-communication Protocol) tools within an AutoGen framework. This adapter is specifically designed to facilitate communication between client applications and MCP tool services, leveraging asynchronous programming capabilities through the `asyncio` library. The adapter converts tool input schemas to Pydantic models for data validation and serialization.\n\n## Class Definition: `McpToolAdapter`\n\n### Constructor: `__init__`\n\n- **Parameters:**\n  - `server_params` (`TServerParams`): Parameters that define the connection settings for the MCP server.\n  - `tool` (`Tool`): An instance representing the MCP tool to be wrapped.\n  - `session` (`ClientSession | None`): An optional client session for maintaining connection to the MCP server.\n\nThis constructor initializes the `McpToolAdapter` by extracting the name and description from the provided tool and creating a Pydantic model from the tool's input schema. The adapter defines a component type as \"tool,\" making it easier for other parts of the system to recognize its functionality.\n\n### Method: `run`\n\n- **Parameters:**\n  - `args` (`BaseModel`): Arguments in the form of a Pydantic model to execute the tool.\n  - `cancellation_token` (`CancellationToken`): Token to manage the cancellation of the operation.\n\nThis async method runs the MCP tool with the provided arguments. It prepares the input by serializing the model into a dictionary, excluding unset values to prevent transmission errors to the server. It supports both provided sessions and sessions created dynamically within the method, ensuring resource management through asynchronous context management.\n\n### Method: `_normalize_payload_to_content_list`\n\n- **Parameters:**\n  - `payload` (`Sequence[ContentBlock]`): The raw output from the MCP tool.\n\nThis method normalizes the tool's output by ensuring it is in the form of a list. It handles various input types, including sequences of content blocks, single content items, and raw strings, converting them into a standardized list of content items. This prepares the output for further processing or display.\n\n## Method: `_run`\n\n### Exception Handling\n\nThe `_run` method directly orchestrates the execution of the tool call. It employs exception handling for cancellation and tool execution failures. It produces a normalized content list based on the tool's output and raises errors where necessary, providing meaningful feedback.\n\n- **Key Features:**\n  - Utilizes futures to manage the asynchronous execution of tool calls.\n  - Normalizes and returns the result, while also serializing error messages when needed.\n\n## Class Method: `from_server_params`\n\n- **Parameters:**\n  - `server_params` (`TServerParams`): Connection parameters for the MCP server.\n  - `tool_name` (`str`): The name of the tool to be wrapped in the adapter.\n\nThis class method simplifies the process of creating an `McpToolAdapter` by connecting to the MCP server and searching for the specified tool. It raises an exception if no matching tool is found, ensuring that adapter instances are tightly coupled with valid tools.\n\n## Method: `return_value_as_string`\n\n- **Parameters:**\n  - `value` (`list[Any]`): A list of content items to be serialized into a string representation.\n\nThis method converts the tool's output items into a JSON format string. It handles various content types, ensuring that metadata fields are stripped if they are `None` and that URL fields are serialized correctly.\n\n## Summary\n\nThe `McpToolAdapter` serves as a crucial bridge between client applications and MCP tools by simplifying communication, managing asynchronous behavior, and ensuring robust error handling and data validation. This class encapsulates tools in a way that adheres to consistent input and output formats, making it easier to integrate with broader systems requiring dynamic tool interactions. By leveraging Pydantic for model validation and `asyncio` for asynchronous execution, it efficiently handles interactions with external services.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_base.py"
  },
  {
    "code": false,
    "content": "# Documentation for MCP Server Parameters Module\n\nThis module defines classes that encapsulate the parameters required for connecting to an MCP (Multi-Channel Protocol) server over different types of communication protocols. These include standard input/output (STDIO), server-sent events (SSE), and streamable HTTP. The use of Pydantic base models ensures that the data adheres to specified types and constraints.\n\n## Overview of Classes\n\n### StdioServerParams\n\n```python\nclass StdioServerParams(StdioServerParameters):\n    \"\"\"Parameters for connecting to an MCP server over STDIO.\"\"\"\n\n    type: Literal[\"StdioServerParams\"] = \"StdioServerParams\"\n\n    read_timeout_seconds: float = 5\n```\n\nThe `StdioServerParams` class models the parameters required to connect to an MCP server through STDIO. \n\n- **Attributes**:\n  - `type`: A constant string (literal) indicating the type of parameters. In this case, it is always `\"StdioServerParams\"`.\n  - `read_timeout_seconds`: Specifies the duration (in seconds) to wait for a read operation before timing out, with a default value set to `5`.\n\nThis class inherits from `StdioServerParameters`, suggesting there are additional functionalities or properties defined in the base class that may be relevant for STDIO connections.\n\n### SseServerParams\n\n```python\nclass SseServerParams(BaseModel):\n    \"\"\"Parameters for connecting to an MCP server over SSE.\"\"\"\n\n    type: Literal[\"SseServerParams\"] = \"SseServerParams\"\n\n    url: str  # The SSE endpoint URL.\n    headers: dict[str, Any] | None = None  # Optional headers to include in requests.\n    timeout: float = 5  # HTTP timeout for regular operations.\n    sse_read_timeout: float = 60 * 5  # Timeout for SSE read operations.\n```\n\nThe `SseServerParams` class is designed for configuring the connection to an MCP server using Server-Sent Events.\n\n- **Attributes**:\n  - `type`: A string literal indicating that this is an instance of `SseServerParams`.\n  - `url`: This string attribute stores the endpoint URL for the SSE connection.\n  - `headers`: This optional attribute allows for including any custom headers needed for the HTTP requests sent to the server.\n  - `timeout`: Sets the timeout duration for regular HTTP operations, defaulting to `5` seconds.\n  - `sse_read_timeout`: Configures the timeout specifically for SSE read operations, set at `300` seconds (5 minutes).\n\n### StreamableHttpServerParams\n\n```python\nclass StreamableHttpServerParams(BaseModel):\n    \"\"\"Parameters for connecting to an MCP server over Streamable HTTP.\"\"\"\n\n    type: Literal[\"StreamableHttpServerParams\"] = \"StreamableHttpServerParams\"\n\n    url: str  # The endpoint URL.\n    headers: dict[str, Any] | None = None  # Optional headers to include in requests.\n    timeout: float = 30.0  # HTTP timeout for regular operations in seconds.\n    sse_read_timeout: float = 300.0  # Timeout for SSE read operations in seconds.\n    terminate_on_close: bool = True\n```\n\nThe `StreamableHttpServerParams` class focuses on configuring parameters for connecting to an MCP server using streamable HTTP protocols.\n\n- **Attributes**:\n  - `type`: A literal indicating that the instance represents `StreamableHttpServerParams`.\n  - `url`: The connection endpoint's URL, specified as a string.\n  - `headers`: Similar to the `SseServerParams`, this optional attribute allows the inclusion of custom headers for the requests.\n  - `timeout`: Represents the timeout duration in seconds for typical HTTP requests, with a default set to `30.0`.\n  - `sse_read_timeout`: This attribute sets the timeout for read operations in SSE, defaulting to `300.0` seconds.\n  - `terminate_on_close`: A boolean attribute that indicates whether the connection should terminate automatically upon closure.\n\n## MCP Server Parameters Annotation\n\n```python\nMcpServerParams = Annotated[\n    StdioServerParams | SseServerParams | StreamableHttpServerParams, Field(discriminator=\"type\")\n]\n```\n\nThe `McpServerParams` annotation consolidates all three parameter types into a single type that can be used to handle any of the defined communication protocols. \n\n- **Field Discriminator**: The `Field` specifies that `type` is used as a discriminator to differentiate between the different types of parameter classes. This is particularly useful in scenarios where it becomes necessary to determine which protocol parameters are being utilized at runtime based on the value of the `type` attribute.\n\nThis design allows for enhanced flexibility and ensures that the system can accommodate different server connection configurations seamlessly. \n\n## Conclusions\n\nThe defined classes provide structured, validated configurations for various MCP server connections, ensuring developers can easily specify and manage the parameters for their desired communication protocols, such as STDIO, SSE, or streamable HTTP. By leveraging Pydantic's data model capabilities, these parameter classes make the handling of the connection configurations more robust and maintainable.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_config.py"
  },
  {
    "code": false,
    "content": "# MCP Server Tools Documentation\n\n## Overview\nThis piece of code defines an asynchronous function named `mcp_server_tools`, which is responsible for creating a list of MCP (Model Context Protocol) tool adapters for use with AutoGen agents. The function allows the user to connect to an MCP server using various parameters and adapters, particularly focusing on different types of server connections such as standard I/O, Server-Sent Events (SSE), and Streamable HTTP.\n\n## Imports\nThe code begins by importing necessary modules:\n- **ClientSession**: Represents a session for connecting to the MCP server.\n- **Server Parameter Classes**: `McpServerParams`, `SseServerParams`, `StdioServerParams`, and `StreamableHttpServerParams` are data classes used to define connection parameters for different types of MCP servers.\n- **Utility Functions**: `create_mcp_server_session` establishes a session to an MCP server, while adapter classes (`SseMcpToolAdapter`, `StdioMcpToolAdapter`, `StreamableHttpMcpToolAdapter`) are used to interact with the MCP tools.\n\n## Function: `mcp_server_tools`\n### Purpose\nThe `mcp_server_tools` function creates and returns a list of tool adapters corresponding to the tools available on an MCP server, allowing AutoGen agents to use them effectively.\n\n### Parameters\n- **server_params** (`McpServerParams`): This is required and specifies the connection parameters to the server, which can target local, SSE, or HTTP services.\n- **session** (`ClientSession | None`): This optional parameter allows the user to specify an existing session to reuse; if not provided, a new session is created.\n\n### Returns\n- The function returns a list of tool adapters, which can be one of the adapter types: `StdioMcpToolAdapter`, `SseMcpToolAdapter`, or `StreamableHttpMcpToolAdapter`.\n\n### Warning\nA warning is included that advises users to connect only to trusted MCP servers when using `StdioServerParams`, as it executes commands in the local environment.\n\n## Function Logic\nThe logic begins by checking if a session is provided. If no session is available, it:\n1. Creates a temporary session using `create_mcp_server_session` with the given `server_params`.\n2. Initializes the session to connect to the server.\n3. Retrieves a list of available tools from the server.\n\nIf a session is provided, it directly uses that session to list the tools.\n\nThe function evaluates the type of `server_params` and constructs a list of corresponding tool adapters for the available tools, throwing a `ValueError` for unsupported server parameter types.\n\n## Examples\nSeveral code snippets are provided to illustrate various use cases for the `mcp_server_tools` function:\n\n### 1. Local File System Example\nThis example demonstrates how to connect to a local filesystem MCP service using standard input/output:\n- It sets up the `StdioServerParams` with a command and arguments to run the MCP server.\n- Once tools are obtained via `mcp_server_tools`, an `AssistantAgent` is instantiated with these tools. The agent executes a task to create a file.\n\n### 2. Local Fetch Service Example\nIn this scenario, the code illustrates connecting to an MCP service that provides a fetch tool:\n- The fetch tool is obtained similarly through `mcp_server_tools`.\n- An agent is created to use this tool, illustrating the functionality of summarizing webpage content.\n\n### 3. Sharing an MCP Session\nThis example emphasizes session sharing:\n- A single MCP client session is created and shared among multiple tools to preserve state.\n- An agent runs a task that requires maintaining a session across possibly multiple requests.\n\n### 4. Remote MCP Service via SSE\nAnother example focuses on connecting to a remote MCP service using server-sent events:\n- The `SseServerParams` is set up with a URL and authorization headers.\n- Tools are retrieved and an agent is instantiated to interact with those tools.\n\n## Conclusion\nThe `mcp_server_tools` function is pivotal in facilitating communication between AutoGen agents and MCP servers. Through various server types and session management, it helps in establishing a robust framework for utilizing different tools effectively. The documented examples provide practical insights into its use, enhancing the overall understanding for potential users or developers working with the model context protocol and associated functionalities. For in-depth exploration, users are encouraged to refer to additional resources and samples provided in the package repository.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_factory.py"
  },
  {
    "code": false,
    "content": "# MCP Server Session Management\n\nThis document provides an overview of a piece of code that handles the creation of client sessions for various types of MCP (Multi-Channel Protocol) servers. The code employs asynchronous programming paradigms and utilizes context management to ensure proper handling of the server connections.\n\n## Overview\n\nThe main component of the script is the `create_mcp_server_session` function, defined as an asynchronous context manager. It facilitates the establishment of client sessions based on the specified server parameters\u2014different configurations require different types of session handling.\n\nThe `ClientSession` class is pivotal in this system, allowing for communication with the server once the session is properly established. The function adapts to different server types as determined by the `server_params` argument, which can be an instance of `StdioServerParams`, `SseServerParams`, or `StreamableHttpServerParams`.\n\n## Function Explanation\n\n### `create_mcp_server_session`\n\n```python\n@asynccontextmanager\nasync def create_mcp_server_session(\n    server_params: McpServerParams, sampling_callback: SamplingFnT | None = None\n) -> AsyncGenerator[ClientSession, None]:\n```\n\nThis function is the core async context manager for creating MCP client sessions. It takes two parameters:\n\n- **`server_params`**: An instance of `McpServerParams` (or its derived classes) that dictates how the session will be established.\n- **`sampling_callback`**: An optional callback function that can be used to sample data during the session.\n\n### Handling Different Server Types\n\nWithin the function, different server types are identified and handled using conditional statements. Each type corresponds to a specific way of initiating a client session.\n\n#### Stdio Server\n\n```python\nif isinstance(server_params, StdioServerParams):\n    async with stdio_client(server_params) as (read, write):\n```\n\nFor `StdioServerParams`, the function uses `stdio_client` to create a read and write stream. These streams are then fed into the `ClientSession`, where the read timeout is determined by the `read_timeout_seconds` property of the `server_params`.\n\n#### SSE Server\n\n```python\nelif isinstance(server_params, SseServerParams):\n    async with sse_client(**server_params.model_dump(exclude={\"type\"})) as (read, write):\n```\n\nFor `SseServerParams`, a similar structure is followed. The `sse_client` is employed, leveraging the model-dumped parameters from `server_params` to establish communication. Again, a `ClientSession` is created with the appropriate read timeout.\n\n#### Streamable HTTP Server\n\n```python\nelif isinstance(server_params, StreamableHttpServerParams):\n    # Convert float seconds to timedelta for the streamablehttp_client\n    params_dict = server_params.model_dump(exclude={\"type\"})\n    params_dict[\"timeout\"] = timedelta(seconds=server_params.timeout)\n```\n\nFor `StreamableHttpServerParams`, the function first constructs a parameters dictionary. It converts timeout values from float seconds into `timedelta` objects. The `streamablehttp_client` is then utilized to generate the read and write streams for the `ClientSession`.\n\n### Yielding the Session\n\n```python\n            async with ClientSession(\n                read_stream=read,\n                write_stream=write,\n                read_timeout_seconds=timedelta(seconds=server_params.sse_read_timeout),\n                sampling_callback=sampling_callback,\n            ) as session:\n                yield session\n```\n\nIn all branches, after successfully establishing the communication streams, a `ClientSession` instance is created and then yielded. This allows for the session to be used in a `with` statement context, ensuring proper resource management. When the context exits, the session is cleaned up.\n\n## Error Handling and Future Developments\n\n### Handling `session_id_callback`\n\n```python\n# TODO: Handle session_id_callback if needed\n```\n\nA placeholder comment is present in the Streamable HTTP server handling, indicating a future need to consider the `session_id_callback` that may provide important session identifiers. This implies that further error handling or session tracking might be required as the code evolves.\n\n## Conclusion\n\nThe `create_mcp_server_session` function is a flexible and robust way to manage various types of MCP server connections within an asynchronous framework. It enables developers to seamlessly switch between different server types by abstracting the specifics of connection handling. With room for enhancement, particularly in the handling of session identifiers, the function lays a solid foundation for MCP server interaction.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_session.py"
  },
  {
    "code": false,
    "content": "# SseMcpToolAdapter Documentation\n\n## Overview\n\nThe `SseMcpToolAdapter` is a specialized component designed to interface with tools that operate over Server-Sent Events (SSE) within the Model Context Protocol (MCP). This adapter facilitates the integration of MCP-compatible tools that communicate via HTTP, enabling seamless interaction with remote services, cloud-based applications, and web APIs using AutoGen agents.\n\n## Purpose\n\nThis adapter primarily serves as a bridge, allowing AutoGen agents to utilize tools that implement the MCP protocol over SSE. It supports use cases such as connecting to remote services that provide translation, data processing, or other functionalities suitable for seamless integration into automated workflows.\n\n## Key Components\n\n### Configuration Class\n\n#### `SseMcpToolAdapterConfig`\n\nThis class defines the configuration schema for the `SseMcpToolAdapter`. It inherits from `BaseModel` (part of the Pydantic library), ensuring strong data validation and type enforcement. The key attributes of this class include:\n\n- `server_params`: An instance of `SseServerParams`, which contains the necessary details for connecting to the MCP server (e.g., URL, headers, timeouts).\n- `tool`: An instance of `Tool`, representing the specific MCP tool being wrapped by the adapter.\n\nThe class serves to encapsulate and validate configuration details required for initializing the adapter.\n\n### Adapter Class\n\n#### `SseMcpToolAdapter`\n\nThis class extends `McpToolAdapter` and `Component`, allowing it to implement the features necessary for dealing with MCP tools over SSE. The constructor and primary methods provide essential functionality for users:\n\n- **Constructor (`__init__`)**: Initializes the adapter with the server parameters and tool. It can also accept an optional `session` parameter, providing flexibility for managing the lifecycle of the client session. The constructor calls its superclass's constructor to complete the initialization process.\n\n- **Method `_to_config()`**: Converts the current instance of the adapter back to its configuration representation. It returns an instance of `SseMcpToolAdapterConfig`, encapsulating the server parameters and tool information.\n\n- **Class Method `_from_config()`**: A factory method that creates an instance of `SseMcpToolAdapter` from a given configuration. This method accepts a `SseMcpToolAdapterConfig` instance and returns a new adapter instance initialized with the specified configuration.\n\n## Usage Example\n\nThe following example demonstrates how to use the `SseMcpToolAdapter` to integrate a remote translation service that implements MCP over SSE:\n\n```python\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\n\n\nasync def main() -> None:\n    # Create server params for the remote MCP service\n    server_params = SseServerParams(\n        url=\"https://api.example.com/mcp\",\n        headers={\"Authorization\": \"Bearer your-api-key\", \"Content-Type\": \"application/json\"},\n        timeout=30,  # Connection timeout in seconds\n    )\n\n    # Get the translation tool from the server\n    adapter = await SseMcpToolAdapter.from_server_params(server_params, \"translate\")\n\n    # Create an agent that can use the translation tool\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4\")\n    agent = AssistantAgent(\n        name=\"translator\",\n        model_client=model_client,\n        tools=[adapter],\n        system_message=\"You are a helpful translation assistant.\",\n    )\n\n    # Let the agent translate some text\n    await Console(\n        agent.run_stream(task=\"Translate 'Hello, how are you?' to Spanish\", cancellation_token=CancellationToken())\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Explanation of the Example\n\n1. **Server Parameters Creation**: In the `main()` function, an instance of `SseServerParams` is created, specifying the service URL, headers for authentication, and connection timeout.\n\n2. **Adapter Initialization**: The `SseMcpToolAdapter` is instantiated using the provided server parameters and the desired tool (`translate`). This allows the adapter to handle communication with the translation service.\n\n3. **Agent Configuration**: An `AssistantAgent` is set up with the necessary configurations, including model parameters and the tools it can access (in this case, just the translation adapter). The system message informs the agent of its role as a translation assistant.\n\n4. **Task Execution**: Finally, the agent is tasked with translating a specific string. The result is outputted to the console, providing a real-time interaction with the translation service.\n\n## Installation Requirements\n\nTo utilize the `SseMcpToolAdapter`, ensure that the `mcp` extra for the `autogen-ext` package is installed. This can be done with the following command:\n\n```bash\npip install -U \"autogen-ext[mcp]\"\n```\n\n## Summary\n\nThe `SseMcpToolAdapter` is a powerful abstraction that simplifies the integration of MCP tools over SSE within AutoGen environments. Its clear structure, validation, and the combination of flexible session management make it an essential tool for developers looking to enhance their applications with automated capabilities leveraging remote services.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_sse.py"
  },
  {
    "code": false,
    "content": "# Documentation for `StdioMcpToolAdapter`\n\n## Overview\n\nThe `StdioMcpToolAdapter` is designed to integrate Model Context Protocol (MCP) tools that communicate over standard input and output with AutoGen agents. This adapter is essential for wrapping command-line tools or local services that follow the MCP specification. It enables a seamless connection between AutoGen and the MCP-compatible tools, allowing developers to leverage existing command-line utilities within an automated environment.\n\n## Core Classes and Functions\n\n### StdioMcpToolAdapterConfig\n\nThe `StdioMcpToolAdapterConfig` class is a configuration model for the `StdioMcpToolAdapter`.\n\n#### Purpose:\n- This class inherits from `BaseModel` provided by Pydantic, ensuring robust data validation and serialization.\n- It encapsulates all configuration settings necessary to instantiate an `StdioMcpToolAdapter`.\n\n#### Attributes:\n- **server_params (StdioServerParams)**: This attribute contains parameters required for establishing a connection with the MCP server, including the command to run and its arguments.\n- **tool (Tool)**: Represents the specific MCP tool being wrapped, facilitating its integration into the AutoGen environment.\n\n### StdioMcpToolAdapter\n\nThe `StdioMcpToolAdapter` class extends both `McpToolAdapter` and `Component`, allowing it to serve as a bridge between AutoGen agents and MCP tools.\n\n#### Purpose:\n- This class enables automation systems to utilize MCP-compatible tools while maintaining ease of use and consistency in how these tools are accessed and interacted with.\n- It provides an interface for configuration, instantiation, and interaction with the underlying MCP tool.\n\n#### Constructor:\n```python\ndef __init__(self, server_params: StdioServerParams, tool: Tool, session: ClientSession | None = None) -> None:\n```\n- **Parameters**:\n  - `server_params`: The parameters required for the MCP server connection.\n  - `tool`: The specific MCP tool being wrapped.\n  - `session`: An optional `ClientSession` for managing the MCP session lifecycle. If not provided, a new session is created.\n\n#### Key Methods:\n\n##### `_to_config`\n\n```python\ndef _to_config(self) -> StdioMcpToolAdapterConfig:\n```\n- **Description**: Converts the current instance of the adapter into its configuration representation.\n- **Returns**: An instance of `StdioMcpToolAdapterConfig` containing the server parameters and tool information.\n\n##### `_from_config`\n\n```python\n@classmethod\ndef _from_config(cls, config: StdioMcpToolAdapterConfig) -> Self:\n```\n- **Description**: Creates a new instance of `StdioMcpToolAdapter` from the given configuration object.\n- **Parameters**:\n  - `config`: An instance of `StdioMcpToolAdapterConfig`.\n- **Returns**: A new `StdioMcpToolAdapter` instance, initialized with the parameters provided in the configuration.\n\n## Usage Notes\n\n- To properly utilize `StdioMcpToolAdapter`, it is mandatory to have the `mcp` extra for the `autogen-ext` package installed. This can be done by running:\n\n```bash\npip install -U \"autogen-ext[mcp]\"\n```\n\n## Conclusion\n\nThe `StdioMcpToolAdapter` serves as a critical component for integrating MCP-compatible tools into the AutoGen framework. By allowing developers to leverage existing command-line utilities, it enhances automation capabilities while keeping setup and usage straightforward. The combination of configuration validation via Pydantic and the integration with AutoGen agents positions this class as a versatile tool for automation development.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_stdio.py"
  },
  {
    "code": false,
    "content": "# Documentation for StreamableHttpMcpToolAdapter\n\n## Overview\n\nThe `StreamableHttpMcpToolAdapter` is a class that acts as a bridge between AutoGen agents and MCP-compatible tools that operate over Streamable HTTP. This setup allows developers to integrate various remote services and APIs that comply with the Model Context Protocol (MCP), enabling seamless communication and functional operations within AutoGen's ecosystem. Key uses include leveraging cloud-based services for tasks like translation and processing requests directed to MCP endpoints.\n\n## Dependencies\n\n### External Dependencies\n\n1. **autogen_core**: This module provides foundational components necessary for constructing AutoGen agents and tools.\n2. **pydantic**: Utilized for creating data validation and settings management via `BaseModel`.\n3. **mcp**: A specialized dependency enabling connection to services and tools that adhere to the MCP, which must be installed via the `autogen-ext` package.\n\n### Installation Instructions\n\nTo successfully utilize the `StreamableHttpMcpToolAdapter`, ensure that the `mcp` extras for the `autogen-ext` package are installed using the command:\n```bash\npip install -U \"autogen-ext[mcp]\"\n```\n\n## Configuration Model\n\n### StreamableHttpMcpToolAdapterConfig\n\nThis is a configuration class derived from `BaseModel`, which encapsulates the parameters required for initializing the `StreamableHttpMcpToolAdapter`. It has the following attributes:\n\n- `server_params` (StreamableHttpServerParams): This holds the connection parameters for the MCP server. This includes the URL to access the service, HTTP headers, and timeout values.\n- `tool` (Tool): The specific MCP tool that we intend to wrap and use within the application.\n\n## Class: StreamableHttpMcpToolAdapter\n\n### Inheritance\n\n`StreamableHttpMcpToolAdapter` inherits from both:\n- `McpToolAdapter`: A generic adapter class specifically designed for connecting MCP tools.\n- `Component`: A general foundational class that defines essential attributes and methods for AutoGen components.\n\n### Constructor\n\nThe constructor (`__init__`) takes the following parameters:\n\n- `server_params`: An instance of `StreamableHttpServerParams`, providing the necessary connection settings to communicate with the MCP tool.\n- `tool`: An instance of `Tool`, representing the MCP tool that will be utilized.\n- `session` (Optional): A `ClientSession` object that manages the HTTP session. If this is not provided, a new session is created internally, offering flexibility for session management.\n\n### Methods\n\n#### _to_config\n\n```python\ndef _to_config(self) -> StreamableHttpMcpToolAdapterConfig:\n```\n\n**Purpose**: Converts the current instance into its configuration representation.\n\n**Returns**: An object of type `StreamableHttpMcpToolAdapterConfig` populated with the adapter's server parameters and tool.\n\n#### _from_config\n\n```python\n@classmethod\ndef _from_config(cls, config: StreamableHttpMcpToolAdapterConfig) -> Self:\n```\n\n**Purpose**: This class method allows the creation of a `StreamableHttpMcpToolAdapter` instance from a previously constructed configuration object.\n\n**Parameters**:\n- `config`: An instance of `StreamableHttpMcpToolAdapterConfig`, containing the necessary configuration for initializing the adapter.\n\n**Returns**: A new instance of `StreamableHttpMcpToolAdapter` configured according to the provided settings.\n\n## Example Usage\n\nAn illustrative example is provided within the docstring that demonstrates how to instantiate and utilize the `StreamableHttpMcpToolAdapter` to create a translation service. The following key steps are outlined in the example:\n\n1. **Setting Up Server Parameters**: Create a `StreamableHttpServerParams` object with the necessary details, such as URL and authentication headers.\n  \n2. **Instantiating the Adapter**: Asynchronously create the `StreamableHttpMcpToolAdapter` using the server parameters and the target tool (\"translate\").\n\n3. **Creating the Agent**: An instance of `AssistantAgent` is constructed to perform the translation tasks, receiving the adapter, a model client, and a system message to guide its behavior.\n\n4. **Executing the Translation Task**: The agent runs a task to translate a specified phrase, demonstrating the adapter's utility in facilitating communication with the remote MCP service.\n\n### Main Application Execution\n\nThe `asyncio.run(main())` construct in the script serves as the entry point when the module is executed directly, initializing the whole process asynchronously.\n\n## Conclusion\n\nThe `StreamableHttpMcpToolAdapter` is a powerful tool for integrating MCP-compatible services into AutoGen agents, making it easier to utilize cloud functionality and remote tools in applications. With the ability to handle configuration and session management effectively, this adapter promotes clean and efficient interactions with various web APIs adhering to the Model Context Protocol.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_streamable_http.py"
  },
  {
    "code": false,
    "content": "# MCP Workbench Documentation\n\nThe `McpWorkbench` class is designed to interface with MCP (Multi-Component Processing) servers, allowing users to list and call tools, manage resources, and handle prompts. This high-level overview clarifies the functionality provided by the workbench, its associated config and state classes, and its primary methods.\n\n## Overview of MCP Workbench\n\nThe `McpWorkbench` serves as a wrapper around an MCP server. It lets the client execute various operations such as listing tools, calling tools with specific parameters, managing resources, and retrieving prompts. The class is implemented as an asynchronous context manager, ensuring that the MCP session is properly initialized and cleaned up when no longer needed. \n\nThe workbench supports tool name overrides, allowing users to customize tool appearance without affecting the underlying functionality. It's crucial to ensure connections are made to trusted MCP servers, especially when executing commands that could affect the local environment.\n\n### Dependencies and Imports\n\nThe `McpWorkbench` relies on several imported modules, which include:\n- **`asyncio`** for asynchronous programming.\n- **`builtins`** for working with Python's built-in functions.\n- **`pydantic`** for data validation and settings management through models.\n- **Various classes and functions from `autogen_core`**, `mcp.types`, and  other local modules that define server parameters and tool operations.\n\n## MCP Workbench Configuration\n\n### `McpWorkbenchConfig`\n\nThe `McpWorkbenchConfig` class is a configuration model that expects:\n- **`server_params`**: Defines the parameters to connect to an MCP server (e.g., `StdioServerParams` or `SseServerParams`).\n- **`tool_overrides`**: An optional dictionary allowing the mapping of tool names to their overridden configurations.\n- **`model_client`**: An optional client for managing sampling requests from supported servers.\n\nThis configuration is essential for instantiating the workbench with the necessary context for operation.\n\n### `McpWorkbenchState`\n\nThe `McpWorkbenchState` serves as a lightweight class to represent the current state of the workbench. It is mainly used when saving and loading the state of the workbench for persistence or session management.\n\n## MCP Workbench Functionality\n\n### Major Methods\n\n#### `__init__`\n\nThe constructor initializes the workbench with server parameters, tool overrides, and an optional model client. It ensures that tool override names are unique to avoid conflicts during tool calls.\n\n#### `start`\n\nThis method initializes the MCP server connection. It raises a warning if called when the workbench is already started. The method ensures that the connection is established before any operations are performed.\n\n#### `stop`\n\nThis method safely stops the MCP session actor, if it has been started. An error is raised if the workbench isn't running.\n\n#### `list_tools`\n\nThis method retrieves a list of available tools from the server. It performs startup checks to ensure the actor is initialized and constructs a schema of tools based on any applied overrides.\n\n#### `call_tool`\n\nThis is a critical method that executes a specified tool with the given arguments. It translates any overridden tool names back to their original counterparts before sending requests to the server.\n\n### Resource and Prompt Management\n\nThe workbench also supports methods for managing resources and prompts, including:\n- **`list_resources`**: Lists all available resources from the MCP server.\n- **`list_prompts`**: Retrieves available prompts for use in processing requests.\n- **`read_resource`**: Reads a specific resource by its URI.\n- **`get_prompt`**: Fetches a single prompt by its name, optionally supplying arguments.\n\n### Error Handling\n\nThe `_format_errors` utility method captures and formats exceptions, particularly useful when debugging tool calls. It specifically recognizes groups of exceptions in Python 3.11+.\n\n## Context Management and Lifecycle\n\n### Using as a Context Manager\n\nThe `McpWorkbench` is intended to be used within an asynchronous context (using `async with`) to ensure proper initialization and cleanup of resources. When used in this manner, it guarantees that the session is started upon entry and safely stopped upon exit.\n\n### State Management\n\nThe workbench includes methods for saving and loading its state, `save_state` and `load_state`, which utilize the `McpWorkbenchState` class for serialized representation. \n\n## Examples and Usage\n\nSeveral examples demonstrate how to utilize the `McpWorkbench` effectively:\n\n1. **Basic tool interaction**:\n   ```python\n   async with McpWorkbench(server_params=params) as workbench:\n       tools = await workbench.list_tools()\n       result = await workbench.call_tool(tools[0][\"name\"], {\"url\": \"https://example.com\"})\n   ```\n\n2. **Using tool overrides**:\n   ```python\n   overrides = {\"fetch\": ToolOverride(name=\"web_fetch\", description=\"Enhanced fetching tool\")}\n   async with McpWorkbench(server_params=params, tool_overrides=overrides) as workbench:\n       tools = await workbench.list_tools()\n       result = await workbench.call_tool(\"web_fetch\", {\"url\": \"https://example.com\"})\n   ```\n\n3. **Advanced integration with an agent**:\n   Here, the workbench is used in conjunction with an AI agent to manage specific tasks.\n\n## Conclusion\n\nThe `McpWorkbench` establishes a comprehensive framework for interacting with MCP servers, offering flexibility through tool overrides and robust management of tools, prompts, and resources. By utilizing `asyncio`, it ensures efficient, non-blocking operations, making it a powerful utility in Python environments that require dynamic tool utilization.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/mcp/_workbench.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: Kernel Function Import\n\nThis module is responsible for importing specific components related to kernel functions from a submodule. The imported elements are part of a larger package designed to facilitate the understanding and application of kernel functions, which are fundamental in various machine learning contexts, particularly in methods such as Support Vector Machines (SVM) and Gaussian Processes.\n\n## Imported Components\n\n### KernelFunctionFromTool\n\n`KernelFunctionFromTool` is likely a class (or function) that encapsulates logic related to a specific kernel function used within the system. This component may provide the necessary methods for defining, evaluating, or manipulating kernel functions, possibly allowing for interfacing with external tools or libraries that contribute to kernel computation. Its design is expected to streamline the workflow involved in using kernel functions.\n\n### KernelFunctionFromToolSchema\n\n`KernelFunctionFromToolSchema` appears to be a schema definition, possibly using a validation library like Pydantic or Marshmallow. Its purpose is likely to standardize the structure of inputs or outputs related to the `KernelFunctionFromTool`. This schema can enforce validation rules to ensure that the data being used in conjunction with `KernelFunctionFromTool` adheres to expected formats and data types. This is essential for maintaining robustness in data handling throughout the application.\n\n## Exported Symbols\n\nThe `__all__` variable is defined to control the public interface of the module. It specifies which components should be accessible when the module is imported. The presence of both `KernelFunctionFromTool` and `KernelFunctionFromToolSchema` in the `__all__` array indicates an intentional design to expose these two components while potentially hiding internal details or additional components not listed.\n\nThis encapsulation allows developers to manage their imports more effectively, ensuring that only the necessary components are exposed to users of the module, which in turn promotes a cleaner and more manageable codebase.\n\n## Summary of Functionality\n\nIn summary, this module serves as an intermediary that imports and organizes a specific set of functions and structures\u2014specifically `KernelFunctionFromTool` and `KernelFunctionFromToolSchema`\u2014to facilitate the use of kernel functions within a broader application. The explicit declaration of exported symbols ensures a clear interface for developers utilizing this module, preventing unexpected behaviors associated with unintended imports. \n\nThis design structure is commonly employed in Python packages to promote better organization, clarity, and maintainability of the code, especially when working within larger software frameworks. By adhering to this modular pattern, improvements and refinements can be localized to specific components without disrupting the overall functionality of related systems.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/semantic_kernel/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Kernel Function Integrations\n\nThis documentation provides an overview of the classes defined in the given code snippet, which are part of a structure designed to create specific kernel functions from tools or tool schemas. These classes primarily interact with the types defined in the Pydantic library and integrate with a semantic kernel. \n\n## Overview of Classes\n\n### `KernelFunctionFromTool`\n\n#### Purpose\nThe `KernelFunctionFromTool` class is designed to create a kernel function based on a specified tool that extends `BaseTool`. This class enables asynchronous interaction with the tool, providing metadata about input parameters required for the tool function as well as the expected output.\n\n#### Constructor\n\n```python\ndef __init__(self, tool: BaseTool[InputT, OutputT], plugin_name: str | None = None):\n```\n\n- **Parameters:**\n  - `tool`: An instance of a `BaseTool` that contains the input and output specifications through Pydantic models.\n  - `plugin_name`: An optional string representing the name of the plugin using the tool.\n  \nThe constructor performs the following actions:\n\n1. **Tool Type Extraction**: It retrieves the argument and return types from the tool using the `args_type()` and `return_type()` methods.\n  \n2. **Async Function Definition**: A nested asynchronous function called `tool_method` is defined, which will call the tool's `run_json` method to execute the operation.\n\n3. **Parameter Metadata Creation**: The constructor extracts parameter schemas, validating each property's type and metadata. It builds a list of `KernelParameterMetadata`, which details each input parameter's characteristics, including its name, description, and whether it's required.\n\n4. **Return Parameter Metadata**: It constructs metadata for the return value expected from the tool, specifying its type based on whether it extends from `BaseModel`.\n\n5. **Superclass Initialization**: It calls the parent class's constructor (`super().__init__`) to finalize the initialization with the created function, parameter metadata, and other details.\n\n6. **Store Tool Reference**: The `tool` instance is stored for later use.\n\n### `KernelFunctionFromToolSchema`\n\n#### Purpose\nThe `KernelFunctionFromToolSchema` class similarly allows for the creation of a kernel function but focuses on extracting information directly from a tool schema (`ToolSchema`). This class is especially useful for functions that are defined declaratively using schemas.\n\n#### Constructor\n\n```python\ndef __init__(self, tool_schema: ToolSchema, plugin_name: str | None = None):\n```\n\n- **Parameters:**\n  - `tool_schema`: An instance of `ToolSchema`, which contains metadata about the tool's parameters.\n  - `plugin_name`: An optional string for identifying the plugin.\n\nThe constructor performs the following actions:\n\n1. **Property Extraction**: It retrieves properties and required parameters from the tool schema.\n\n2. **Prompt Template Configuration**: A `PromptTemplateConfig` instance is created using properties from the tool schema. This includes detailed information like the schema's name, description, and input variables derived from the parameter properties.\n\n3. **Superclass Initialization**: Finally, the superclass (`KernelFunctionFromPrompt`) is initialized with the function name, plugin name, description, and prompt configuration.\n\n## Summary of Functionality\n\nThe described classes enable a type-safe and structured way to connect semantic functions to operational tools. They provide the infrastructure for defining both asynchronous functions that call tools and functions driven by schema definitions.\n\n1. **Tool Integration**: Both classes facilitate the integration of tools into a semantic kernel environment, promoting modularity and reusability.\n\n2. **Parameter Management**: They emphasize the management of function parameters, capturing rich metadata that can be used when these functions are invoked, thus minimizing runtime errors due to type mismatches.\n\n3. **Asynchronous Execution**: With the use of `async` functions, the code is designed to handle I/O operations efficiently, which is crucial for applications interacting with external services or databases.\n\n4. **Support for Custom Metadata**: The ability to include custom metadata not only makes the function definitions clearer but also enriches the operational context with descriptions and default values.\n\n5. **Error Handling**: The use of assertions to check property consistency and schema compliance adds a layer of safety, ensuring that any issues with tool definitions can be caught early in the execution process.\n\nOverall, this structure promotes clarity in defining complex interactions between tools and semantic functions, aligning with the principles of modular design and type safety in modern Python applications.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/tools/semantic_kernel/_kernel_function_from_tool.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: Agent Message Formatting Utilities\n\n## Overview\n\nThis Python module provides utility classes specifically designed for formatting and printing messages related to agents. The core functionality revolves around enhancing the visual representation of agent messages, likely for improved readability and user experience.\n\n## Imports\n\nThe module imports the `RichConsole` class from a sibling module named `_rich_console`. This import suggests that the `RichConsole` is the primary class of interest, responsible for the formatted output of messages.\n\n```python\nfrom ._rich_console import RichConsole\n```\n\n## Public Interface\n\nThe module defines the `__all__` list, which explicitly declares the public interface of the module. This list indicates which attributes or classes should be accessible when the module is imported elsewhere. Here, it includes only the `RichConsole` class.\n\n```python\n__all__ = [\"RichConsole\"]\n```\n\n## Class: `RichConsole`\n\n### Purpose\n\nThe `RichConsole` class serves as an interface for enhanced message formatting and printing. While the specific functionalities of `RichConsole` are not detailed within the provided code, it is understood to be a utility that may leverage advanced features\u2014such as colored text, styled output, or structured display formats\u2014commonly used in console applications or interactive environments.\n\n### Role\n\nAs the primary export of the module, `RichConsole` is likely designed to be instantiated and used in different contexts where agent messages need to be displayed in a user-friendly manner. This might include logging activity, providing feedback in real-time applications, or presenting information that requires clear emphasis.\n\n### Potential Features\n\nAlthough the implementation details are not provided in this snippet, one can infer that `RichConsole` may include features such as:\n\n- **Color Formatting:** Allow messages to be styled with various colors to indicate severity or type (e.g., error messages in red and success messages in green).\n- **Text Styling:** Options for bold, italic, or underlined text for emphasis on specific parts of the output.\n- **Layout Management:** Methods to better structure how messages are printed, potentially supporting multiple lines, bullet points, or aligned formats.\n\n## Conclusion\n\nIn summary, this module serves as a foundation for formatting and displaying agent messages through the `RichConsole` class. The use of the `__all__` directive clarifies the public API, guiding users towards the intended use of the module. For detailed functionality, users will need to refer to the `_rich_console` module where the specific implementation of `RichConsole` is defined.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/ui/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for Console Output Management in Async Messaging\n\n## Overview\n\nThis script defines functionality for rendering messages in a rich console output, specifically tailored for an asynchronous messaging system. It leverages the `asyncio` library to define asynchronous functions and handle streams of messages in a non-blocking manner. The main goal of the script is to consume messages from an event stream and display them in a formatted console output, with the ability to handle text and image content.\n\n### Key Imports\n\n- **asyncio**: For asynchronous programming, enabling non-blocking operations.\n- **os, sys, time**: Standard libraries for system operations, environment variables, and timing.\n- **autogen_agentchat modules**: These likely define the data structures for messages, responses, and events relevant to the chat system.\n- **rich**: A library for rendering beautiful terminal output which helps display content with styles and panels.\n\n## Global Constants\n\n### Agent Colors and Alignments\n\nThe script defines constants for `AGENT_COLORS` and `AGENT_ALIGNMENTS`, establishing styles for various agents in the messaging system. This allows for colored outputs based on the agent type, enhancing the readability of the console output.\n\n### Default Values\n\nConstants such as `DEFAULT_AGENT_COLOR` and `DEFAULT_AGENT_ALIGNMENT` set fallback styles if an agent type is not explicitly defined.\n\n## Helper Functions\n\n### Environment Check Functions\n\n- **_is_running_in_iterm()**: Checks if the terminal is iTerm, enabling image rendering capabilities based on certain conditions.\n- **_is_output_a_tty()**: Verifies if the standard output is connected to an interactive terminal, affecting how output is rendered.\n\n### Async Print Function\n\n- **aprint()**: A utility function designed to print output in a non-blocking way using a separate thread, which enhances the responsiveness of the application.\n\n### Message Content Extraction\n\n- **_extract_message_content()**: This function takes a message object and extracts text and image content from it. It supports multimodal messages, separating different content types for rendering.\n\n## Async Message Rendering Functions\n\n### Rich Console Output Function\n\n- **RichConsole()**: This is the main function of the script. It consumes a stream of messages asynchronously and formats them for output in the console. It handles different message types, including `TaskResult`, `Response`, `UserInputRequestedEvent`, and others, while rendering them appropriately.\n\n#### Parameters:\n- **stream**: An asynchronous generator of message events, crucial for processing incoming messages.\n- **no_inline_images**: A boolean flag to disable inline rendering of images for terminals that support it.\n- **output_stats**: An experimental option for outputting additional stats regarding messages and token usage.\n- **user_input_manager**: An optional manager to handle user input events in the output process.\n\n#### Functionality:\n1. Initializes console output settings.\n2. Iterates through the incoming message stream, categorizing messages:\n   - For `TaskResult`, it outputs statistics along with messaging summary.\n   - For `Response`, it extracts content and handles associated token usage.\n   - Handles user input events.\n   - Optionally processes other types of messages, rendering them appropriately.\n3. Implements error handling by raising a `ValueError` if no valid message was processed before completion.\n\n## Image Rendering Functionality\n\n### iTerm2 Image Protocol\n\n- **_image_to_iterm()**: Converts image data to a format compatible with iTerm2, allowing inline image display within the terminal. This supports the ability to harness multimedia alongside textual responses.\n\n## Conclusion\n\nThe script effectively manages asynchronous message output to a terminal interface, leveraging the Rich library for enhanced visuals and bolstering user interaction with clear, color-coded agent outputs. It primarily serves within a context of chat and task processing, integrating statistical outputs and accommodating multimedia displays based on terminal capabilities. The great utility of this script lies in its ability to present dynamic content in an engaging way, making it suitable for chat applications interacting with various agents and responses.",
    "filename": "python/packages/autogen-ext/src/autogen_ext/ui/_rich_console.py"
  },
  {
    "code": false,
    "content": "It appears that there is no source code provided in your request. To assist you effectively, please share the code you would like me to analyze and document.",
    "filename": "python/packages/autogen-ext/tests/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for OpenAI Agent Built-in Tool Validation Tests\n\nThis document outlines the structure and purpose of the test suite designed to validate the built-in tool functionalities of an OpenAI agent. The tests leverage the `pytest` framework and involve asynchronous interactions with OpenAI's API.\n\n## Overview\n\nThe test suite focuses on various built-in tools available in the `OpenAIAgent`, ensuring that the basic functionalities and configurations operate as expected. Each test employs a consistent pattern of setup, execution, and validation for seamless integration with the OpenAI API.\n\n### Libraries and Dependencies\n\n- **Standard Library**: The code utilizes the `os` library to manage environment variables.\n- **Third-party Libraries**: The `pytest` and `pytest.asyncio` libraries facilitate testing frameworks for synchronous and asynchronous operations.\n- **Local Imports**: Custom classes like `TextMessage`, `CancellationToken`, and `OpenAIAgent` from `autogen_agentchat` and `autogen_ext` modules form the core functionalities tested.\n\n## Test Fixtures\n\n### Dummy OpenAI API Key\n\n```python\n@pytest.fixture(autouse=True)\ndef set_dummy_openai_key(monkeypatch: MonkeyPatch) -> None:\n```\n\nThis fixture ensures that a dummy API key is applied for tests when a real key is not present, preventing accidental failures due to missing environment variables.\n\n### OpenAI Client Fixture\n\n```python\n@pytest.fixture\ndef openai_client() -> AsyncOpenAI:\n```\n\nProvides a configured instance of an `AsyncOpenAI` client, crucial for tests requiring interaction with the OpenAI API.\n\n### Cancellation Token Fixture\n\n```python\n@pytest.fixture\ndef cancel_token() -> CancellationToken:\n```\n\nThis fixture generates unique cancel tokens for each test, allowing for the specification of cancellation policies during asynchronous executions.\n\n## Built-in Tool Validation Tests\n\n### String-Based Tool Validation\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"tool_name,model,should_raise\", [ ... ])\nasync def test_builtin_tool_string_validation( ... ) -> None:\n```\n\nThis test checks the validation of string-based built-in tools against various models, asserting that the agent either raises a `ValueError` or confirms successful tool initialization based on conditions.\n\n### Mixed Tool Validation\n\n```python\n@pytest.mark.asyncio\nasync def test_builtin_tool_validation_with_custom_and_builtin(openai_client: AsyncOpenAI) -> None:\n```\n\nTests the functionality of an agent utilizing both string and dictionary-based tools by ensuring both types are included in the agent's tools attribute.\n\n## Integration Tests\n\n### Basic Integration with OpenAI API\n\n```python\n@pytest.mark.asyncio\n@skip_if_no_real_openai_key\nasync def test_integration_with_openai_api() -> None:\n```\n\nThis integration test validates that a basic interaction with the OpenAI API functions correctly, checking for valid responses when sending user messages.\n\n### Web Search Preview Tool\n\n```python\n@pytest.mark.asyncio\n@skip_if_no_real_openai_key\nasync def test_integration_web_search_preview_tool() -> None:\n```\n\nHere, the test assesses the working of the `web_search_preview` tool, executing an actual API call to retrieve real-world data based on a user query.\n\n### Image Generation Tool\n\n```python\n@pytest.mark.asyncio\n@skip_if_no_real_openai_key\nasync def test_integration_image_generation_tool() -> None:\n```\n\nTests the image generation functionality of the agent, asserting that valid images are generated in response to user requests.\n\n## Configured Tool Validation Tests\n\n### Testing Configured Tools with Varying Settings\n\nSeveral tests assess the configuration of tools with specific options, such as geographical location and user contexts. For example:\n\n```python\n@pytest.mark.asyncio\nasync def test_integration_configured_web_search_tool() -> None:\n```\n\nThis test ensures that the web search tool responds correctly when configured to utilize specific settings like user location and search context size.\n\n### Config Serialization and Deserialization\n\nVarious tests examine the serialization of tool configurations to and from JSON formats, ensuring that agent settings are preserved throughout the lifecycle of the agent creation, configuration, and serialization.\n\n## Conclusion\n\nThese structured tests provide a comprehensive framework for validating the built-in tools available in the `OpenAIAgent`. By leveraging `pytest` with various fixtures for setup, these tests guarantee that tools not only perform correctly under expected usage conditions but also adhere to defined invariant behaviors throughout their configurations and integrations.",
    "filename": "python/packages/autogen-ext/tests/agents/test_openai_agent_builtin_tool_validation.py"
  },
  {
    "code": false,
    "content": "# DiskCacheStore Test Suite Documentation\n\nThis document provides a high-level overview of the test suite designed to validate the functionality of the `DiskCacheStore` class from the `autogen_ext.cache_store.diskcache` module using the `diskcache` library. The tests assess various functionalities such as basic storage, retrieval, and handling of different instances of the cache.\n\n## Overview\n\nThe test suite consists of two main functions, each testing different aspects of the `DiskCacheStore` class. Both tests make use of the `tempfile` module to create temporary directories and the `pytest` framework for running test cases. The `diskcache` library is conditionally imported, ensuring that the tests will only run if the library is available.\n\n### Dependencies\n\nThe suite relies on the following external libraries:\n- `pytest`: A testing framework for Python.\n- `diskcache`: A disk-and-memory cache library, which must be installed for the tests to run.\n\n## Test: `test_diskcache_store_basic`\n\n### Purpose\n\nThe `test_diskcache_store_basic` function tests the fundamental operations of storing and retrieving items in the `DiskCacheStore`. It verifies that:\n- Data can be stored and retrieved correctly.\n- Updating a value for a given key behaves as expected.\n- Non-existent keys return a specified default value.\n\n### Operations\n\n1. **Setup**: A temporary directory is created to serve as the cache storage location.\n2. **Cache Creation**: A cache instance is created that points to the temporary directory.\n3. **Store Initialization**: An instance of `DiskCacheStore` is created, parameterized with an integer type, using the cache created in the previous step.\n4. **Storing Data**:\n   - A key-value pair (`test_key`: `42`) is stored in the cache.\n   - The stored value is retrieved, and its correctness is asserted.\n5. **Updating Value**: \n   - The value for `test_key` is updated to `2`.\n   - The new value is retrieved and asserted for correctness.\n6. **Handling Non-Existent Keys**:\n   - An attempt to retrieve a value for a `non_existent_key` is made with a default value of `99`.\n   - The assertion checks if the returned value matches the default.\n\n## Test: `test_diskcache_with_different_instances`\n\n### Purpose\n\nThe `test_diskcache_with_different_instances` function tests the behavior of `DiskCacheStore` when multiple instances are used with different cache directories. This test examines:\n- The ability to maintain data integrity across separate instances.\n- The functionality of serialization and deserialization of the cache component.\n\n### Operations\n\n1. **Setup**: Two temporary directories are created for two cache instances.\n2. **Cache Creation**: Two separate cache instances are created, each pointing to its respective temporary directory.\n3. **Store Initialization**: Two instances of `DiskCacheStore` are created, each parameterized with an integer type, using the respective caches.\n4. **Storing Data**:\n   - A key-value pair (`test_key`: `5`) is stored in the first cache instance and verified.\n   - A different value (`test_key`: `6`) is stored in the second cache instance and verified.\n5. **Testing Serialization**:\n   - The configuration of the first store is dumped, allowing for serialization of its state.\n   - A new instance of `DiskCacheStore` is created using the serialized configuration, which checks if the state can be successfully restored.\n   - The value for `test_key` is retrieved from the new instance, and its correctness is asserted.\n6. **Cleanup**: The cache used in the restored store is closed to free resources.\n\n## Conclusion\n\nThe test suite for `DiskCacheStore` provides a robust framework for ensuring that the essential functionalities of storing and retrieving data are working as expected. The tests also cover more advanced scenarios, such as handling multiple cache instances and validating serialization capabilities. By using temporary directories for storage, the tests ensure that they do not interfere with the actual file system, maintaining a clean environment during execution.",
    "filename": "python/packages/autogen-ext/tests/cache_store/test_diskcache_store.py"
  },
  {
    "code": false,
    "content": "# Redis Store Unit Tests Documentation\n\nThis document provides a comprehensive overview of a series of unit tests designed for the `RedisStore` class, which is part of a caching layer that interacts with a Redis database using Python. The tests utilize the `pytest` testing framework and mock Redis interactions to verify the system's expected behavior without relying on a real Redis server. The tests encompass various scenarios including basic caching, serialization of complex models, error handling, and stress-tested edge cases.\n\n## Test Setup\n\n### Imports\nThe necessary libraries and modules are imported at the beginning, including:\n- `json`: For handling JSON serialization and deserialization.\n- `typing`: For type annotations and hints.\n- `unittest.mock.MagicMock`: For mocking the Redis instance during testing.\n- `pytest`: The testing framework used to write and run the unit tests.\n- `autogen_core.models`: Contains model definitions like `CreateResult` and `RequestUsage`.\n- `pydantic.BaseModel`: For defining data models with validation.\n\n### Redis Dependency Handling\nThe `redis` library is conditionally imported using `pytest.importorskip`, which ensures that if the Redis library is not available, the tests will be skipped rather than failing outright.\n\n## Basic Functionality Tests\n\n### Test 1: `test_redis_store_basic`\nThis test validates basic operations of the `RedisStore` class by:\n- Initializing a mock Redis instance and the `RedisStore`.\n- Setting and getting a basic integer value to and from the store.\n- Validating that the expected methods on the mock Redis instance are called correctly.\n- Checking behavior when non-existent keys are accessed, using a default value.\n\n### Test 2: `test_redis_with_different_instances`\nThis test verifies that multiple instances of `RedisStore` interacting with separate mock Redis instances work correctly. It checks:\n- Independent storage and retrieval of the same key in different store instances.\n- Correct handling of internal serialization process through invocation of `dump_component`.\n\n## Advanced Serialization Tests\n\n### Test 3: `test_redis_store_serialization`\nThis test focuses on the serialization of Pydantic models, specifically `SampleModel`, into the Redis store:\n- It tests setting a model instance, ensuring it\u2019s serialized as bytes.\n- It also checks deserialization when retrieving the model from Redis.\n- Handling of edge cases like non-existent keys and malformed data is tested, ensuring robustness in error situations.\n\n### Test 4: `test_redis_store_nested_model_serialization`\nThis test extends the serialization functionality to nested models:\n- It evaluates setting a complex model that contains nested structures.\n- The test verifies that the retrieved data matches the original nested model structure.\n- Similar checks for error handling and retrieval are performed as in previous tests.\n\n## List Serialization Tests\n\n### Test 5: `test_redis_store_list_with_strings_only`\nThis test ensures that lists containing only strings can be serialized and deserialized correctly:\n- It checks the internal representation of the list in Redis and validates the retrieval process returns the correct data structure.\n\n### Test 6: `test_redis_store_list_with_create_results_only`\nThis test examines lists containing `CreateResult` objects:\n- It performs similar checks to ensure the objects are correctly serialized and deserialized.\n- The test validates individual properties of the results after retrieval.\n\n### Test 7: `test_redis_store_mixed_list_streaming_scenario`\nThis test checks for mixed content in a list combining strings and `CreateResult` objects:\n- It ensures that the store can handle serialization of diverse types and retrieves them appropriately.\n- Assertions confirm that data integrity is maintained throughout the process.\n\n### Test 8: `test_redis_store_empty_list`\nIn this test, an empty list is tested:\n- The test confirms that the system can handle empty collections properly without errors.\n\n## Error Handling Tests\n\n### Test 9: `test_redis_store_list_serialization_error_handling`\nThis test is designed to verify the resilience of the `RedisStore` when handling errors during serialization:\n- It simulates Redis errors during both `set` and `get`.\n- The test ensures that exceptions do not cause test failures, validating the robustness of error handling in the store operations. \n\n## Conclusion\n\nThis suite of tests for the `RedisStore` class comprehensively addresses various scenarios that the caching system may encounter, ensuring that it performs correctly under typical conditions and gracefully handles errors. These tests serve as an important safety net for future code changes and enhancements, fostering confidence in the reliability of the caching layer integrated with Redis.",
    "filename": "python/packages/autogen-ext/tests/cache_store/test_redis_store.py"
  },
  {
    "code": false,
    "content": "# Test Command Line Code Executor Documentation\n\nThis document serves as an overview of the test file for the `ACADynamicSessionsCodeExecutor` from the `autogen_ext` package. It utilizes pytest for testing asynchronous code execution in Azure environments. The primary focus is on ensuring that the code executor can handle various tasks, including executing code blocks, uploading, and downloading files, while effectively managing session IDs, timeouts, and cancellation.\n\n## Imports and Constants\n\nThe script starts by importing necessary libraries and modules:\n\n```python\nimport asyncio\nimport os\nimport sys\nimport tempfile\nimport pytest\nfrom anyio import open_file\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import CodeBlock\nfrom autogen_ext.code_executors.azure import ACADynamicSessionsCodeExecutor\nfrom azure.identity import DefaultAzureCredential\n```\n\n### Libraries\n- **asyncio**: Used for asynchronous programming.\n- **os, sys**: Standard libraries for operating system interactions.\n- **tempfile**: Provides functions for creating temporary files and directories.\n- **pytest**: A testing framework to facilitate test case creation and execution.\n- **anyio**: An asynchronous file handling library.\n- **autogen_core and autogen_ext**: Custom modules that encapsulate code execution logic and Azure integrations.\n\n### Shell and Environment Variables\nThe script defines specific shell types and retrieves the Azure pool endpoint from the environment variables.\n\n```python\nUNIX_SHELLS = [\"bash\", \"sh\", \"shell\"]\nWINDOWS_SHELLS = [\"ps1\", \"pwsh\", \"powershell\"]\nPYTHON_VARIANTS = [\"python\", \"Python\", \"py\"]\n\nENVIRON_KEY_AZURE_POOL_ENDPOINT = \"AZURE_POOL_ENDPOINT\"\n\nPOOL_ENDPOINT = os.getenv(ENVIRON_KEY_AZURE_POOL_ENDPOINT)\n```\n\n## Session ID Tests\n\n### `test_session_id_preserved_if_passed`\nThis test checks if a predetermined session ID remains unchanged when explicitly set in the executor.\n\n```python\ndef test_session_id_preserved_if_passed() -> None:\n    executor = ACADynamicSessionsCodeExecutor(...)\n    session_id = \"test_session_id\"\n    executor._session_id = session_id\n    assert executor._session_id == session_id\n```\n\n### `test_session_id_generated_if_not_passed`\nHere, the test verifies that if no session ID is provided, the executor will automatically generate one.\n\n```python\ndef test_session_id_generated_if_not_passed() -> None:\n    executor = ACADynamicSessionsCodeExecutor(...)\n    assert executor._session_id is not None\n    assert len(executor._session_id) > 0\n```\n\n## Code Execution Tests\n\n### `test_execute_code`\nThis asynchronous test executes multiple code blocks, ensuring that they run correctly and return expected output.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_execute_code() -> None:\n    ...\n    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)\n    assert code_result.exit_code == 0 and \"hello world!\" in code_result.output\n    ...\n    await executor.stop()\n```\n\nThe test covers:\n- Execution of a single block of Python code.\n- Multiple blocks of Python code with combined outputs.\n- Validation that unknown language execution results in an error.\n\n### `test_execute_code_create_image`\nThis test ensures that code that generates an image behaves correctly while suppressing result outputs.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_execute_code_create_image() -> None:\n    ...\n    assert code_result.exit_code == 0 and \"base64_data\" not in code_result.output\n```\n\n## Timeout and Cancellation Tests\n\n### `test_azure_container_code_executor_timeout`\nThis test validates that if a code block exceeds a specified timeout, an `asyncio.TimeoutError` is raised.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_azure_container_code_executor_timeout() -> None:\n    ...\n    with pytest.raises(asyncio.TimeoutError):\n        await executor.execute_code_blocks(code_blocks, cancellation_token)\n```\n\n### `test_azure_container_code_executor_cancellation`\nThis function tests if an ongoing execution can be canceled, checking for the correct handling of the cancellation token.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_azure_container_code_executor_cancellation() -> None:\n    ...\n    with pytest.raises(asyncio.CancelledError):\n        await coro\n```\n\n## File Upload and Download Tests\n\n### `test_upload_files`\nThis test checks the executor's ability to upload files to the Azure environment and verifies their availability post-upload.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_upload_files() -> None:\n    ...\n    assert test_file_1 in file_list\n    assert test_file_2 in file_list\n```\n\n### `test_download_files`\nSimilar to the upload test, this verifies the ability to download files from Azure back to the local environment.\n\n```python\n@pytest.mark.skipif(not POOL_ENDPOINT, reason=\"do not run if pool endpoint is not defined\")\n@pytest.mark.asyncio\nasync def test_download_files() -> None:\n    ...\n    assert os.path.isfile(os.path.join(temp_dir, test_file_1))\n```\n\n## Conclusion\n\nThis script is a set of comprehensive tests designed to validate the functionality of the `ACADynamicSessionsCodeExecutor`. It ensures that session management works, code execution is successful under various conditions, and file operations are handled correctly. The use of pytest for structuring tests allows for extending functionality and maintaining code quality within the Azure-based execution context.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_aca_dynamic_sessions.py"
  },
  {
    "code": false,
    "content": "# Documentation for `test_user_defined_functions.py`\n\nThis Python script contains a series of unit tests designed to validate user-defined functions within an Azure dynamic execution environment. It utilizes the `pytest` framework to organize tests and integrates with Azure services to validate functionality under various conditions.\n\n## Dependencies and Imports\n\nThe script begins by importing necessary libraries and modules:\n\n```python\nimport os\nimport polars\nimport pytest\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import (\n    CodeBlock,\n    FunctionWithRequirements,\n    with_requirements,\n)\nfrom autogen_ext.code_executors.azure import ACADynamicSessionsCodeExecutor\nfrom azure.identity import DefaultAzureCredential\n```\n\nKey dependencies include:\n- `os`: To access environment variables.\n- `polars`: A DataFrame library used for data manipulation.\n- `pytest`: The testing framework utilized for running the tests.\n- `autogen_core` and `autogen_ext`: Custom modules that provide the necessary tools for code execution and function management in Azure.\n\nThe script also retrieves the Azure pool endpoint from the environment variable `AZURE_POOL_ENDPOINT`.\n\n## Helper Functions\n\n### add_two_numbers\n\n```python\ndef add_two_numbers(a: int, b: int) -> int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n```\nThis simple function accepts two integers and returns their sum. It serves as a utility function to verify that user-defined functions can be executed correctly within the Azure execution context.\n\n### load_data\n\n```python\n@with_requirements(python_packages=[\"polars\"], global_imports=[\"polars\"])\ndef load_data() -> polars.DataFrame:\n    \"\"\"Load some sample data.\"\"\"\n    data = {...}\n    return polars.DataFrame(data)\n```\nThis function loads a sample dataset containing names, locations, and ages into a `polars.DataFrame`. It is annotated with `@with_requirements`, indicating that it has dependencies on the `polars` library.\n\n### Incorrect Import Functions\n\nTwo functions, `function_incorrect_import` and `function_incorrect_dep`, are defined to illustrate how the Azure executor handles import errors:\n\n```python\n@with_requirements(global_imports=[\"NOT_A_REAL_PACKAGE\"])\ndef function_incorrect_import() -> \"polars.DataFrame\":\n    return polars.DataFrame()\n```\n\n```python\n@with_requirements(python_packages=[\"NOT_A_REAL_PACKAGE\"])\ndef function_incorrect_dep() -> \"polars.DataFrame\":\n    return polars.DataFrame()\n```\n\nBoth functions attempt to use non-existent packages and serve to test failure cases in the subsequent unit tests.\n\n## Unit Tests\n\nThe script contains multiple tests that validate functionality when using the Azure dynamic execution environment.\n\n### test_azure_can_load_function_with_reqs\n\n```python\n@pytest.mark.skipif(\n    not POOL_ENDPOINT,\n    reason=\"do not run if pool endpoint is not defined\",\n)\n@pytest.mark.asyncio\nasync def test_azure_can_load_function_with_reqs() -> None:\n    ...\n```\nThis test checks whether the `load_data` function can be executed correctly in the Azure environment. After starting the executor, it executes a code block that retrieves the name of the first entry in the `DataFrame` and asserts the expected output.\n\n### test_azure_can_load_function\n\n```python\n@pytest.mark.skipif(\n    not POOL_ENDPOINT,\n    reason=\"do not run if pool endpoint is not defined\",\n)\n@pytest.mark.asyncio\nasync def test_azure_can_load_function() -> None:\n    ...\n```\nThis test validates the execution of the `add_two_numbers` function in Azure. It confirms the correct output when adding two integers.\n\n### test_azure_fails_for_function_incorrect_import\n\n```python\n@pytest.mark.skipif(\n    not POOL_ENDPOINT,\n    reason=\"do not run if pool endpoint is not defined\",\n)\n@pytest.mark.asyncio\nasync def test_azure_fails_for_function_incorrect_import() -> None:\n    ...\n```\nThis test ensures that an attempt to execute `function_incorrect_import` raises a `ValueError`, verifying that incorrect imports are properly handled by the executor.\n\n### test_azure_fails_for_function_incorrect_dep\n\n```python\n@pytest.mark.skipif(\n    not POOL_ENDPOINT,\n    reason=\"do not run if pool endpoint is not defined\",\n)\n@pytest.mark.asyncio\nasync def test_azure_fails_for_function_incorrect_dep() -> None:\n    ...\n```\nSimilar to the previous test, this checks that a failure due to incorrect dependencies results in a `ValueError`.\n\n## Function Formatting Tests\n\n### test_azure_formatted_prompt\n\n```python\ndef test_azure_formatted_prompt() -> None:\n    ...\n```\nThis test verifies that the Azure executor can correctly format the function definitions for prompts. It checks if the formatted string for `add_two_numbers` is as expected.\n\n### test_azure_formatted_prompt_str_func\n\n```python\ndef test_azure_formatted_prompt_str_func() -> None:\n    ...\n```\nThis test checks the ability to parse a string representation of a function into a `FunctionWithRequirements` object and then validates the formatted output.\n\n## Error Handling in Broken Functions\n\n### test_azure_cant_run_broken_str_function_with_reqs\n\n```python\n@pytest.mark.skipif(\n    not POOL_ENDPOINT,\n    reason=\"do not run if pool endpoint is not defined\",\n)\n@pytest.mark.asyncio\nasync def test_azure_cant_run_broken_str_function_with_reqs() -> None:\n    ...\n```\nThis test ensures that if an invalid operation is performed (e.g., adding incompatible types), the executor captures the error and provides appropriate output.\n\n## Conclusion\n\nOverall, the script is a robust testing suite that creates a thorough environment for validating user-defined functions within Azure. It effectively tests both successful and erroneous conditions, ensuring reliable behavior when integrating with Azure's dynamic execution features.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_aca_user_defined_functions.py"
  },
  {
    "code": false,
    "content": "# High-Level Documentation of `test_commandline_code_executor.py`\n\nThis document provides an overview and explanation of the key components and functionality of the `test_commandline_code_executor.py` file, which is designed to test the execution of code blocks in a local command line environment using the `LocalCommandLineCodeExecutor` from the `autogen` library.\n\n## Environment Setup and Utilities\n\nThe script begins by importing necessary modules, including standard libraries and third-party packages, which are essential for its operation. This includes modules for handling asynchronous programming (`asyncio`), system operations (`os`, `platform`), file handling (`shutil`, `tempfile`), and subprocess management (`subprocess`). The script also utilizes `pytest` for testing and includes utility methods for managing virtual environments.\n\n### Platform-Specific Checks\n\nSeveral boolean variables are defined to check for platform capabilities:\n- **`HAS_POWERSHELL`**: Determines if PowerShell is available on Windows.\n- **`IS_MACOS`**: Checks if the current platform is macOS.\n- **`IS_UV_VENV`**: Validates if the current environment is a specific virtual environment, by checking for the presence of a configuration file.\n\n### Creating Virtual Environments\n\nThe `create_venv_with_uv` function is responsible for creating a virtual environment using the `uv` tool. It constructs and returns a context holding essential information about the virtual environment, such as paths for executables and libraries. It utilizes `subprocess.run` to execute the command for creating the virtual environment and includes error handling to capture and report failures.\n\n## Testing Fixtures and Async Test Functions\n\n### Executor Fixture\n\nThe `executor_and_temp_dir` function is a fixture that sets up a temporary directory and an instance of `LocalCommandLineCodeExecutor` for testing. The temporary directory will be cleaned up automatically after the tests, ensuring there are no persistent side effects.\n\n### Test Functions\n\nThe main body of the script contains several test functions. Each function is decorated with `pytest.mark.asyncio`, allowing them to run asynchronously. The tests generally verify that code blocks execute correctly in the command line environment.\n\n#### Test Execution of Code Blocks\n\n- **`test_execute_code`**: Tests the execution of single and multiple Python code blocks, checks outputs, and verifies that the correct results are returned and saved. It also tests executing shell commands on non-Windows platforms.\n\n#### Timeout and Cancellation Tests\n\n- **`test_commandline_code_executor_timeout`**: Assesses how the executor handles timeouts, ensuring that a long-running command is terminated correctly and that the appropriate output is logged.\n  \n- **`test_commandline_code_executor_cancellation`**: Tests the cancellation of a long-running operation and checks that the expected output confirms cancellation. Additionally, it asserts that expected files are not created if execution is canceled.\n\n### Path Validity Tests\n\n- **`test_invalid_relative_path`**: Validates that relative paths outside the workspace are handled appropriately, confirming that the executor rejects the input and logs an appropriate message.\n\n- **`test_valid_relative_path`**: Tests the executor's ability to correctly process a valid relative path and checks that the output is logged and the expected file is created.\n\n## Virtual Environment Functionality\n\nSeveral tests evaluate the ability of the `LocalCommandLineCodeExecutor` to work with different types of virtual environments, particularly those created using `venv` or `uv`, including:\n- **`test_local_executor_with_custom_venv`**: Creates a custom virtual environment and checks whether the executor runs Python code within that environment.\n- **`test_local_executor_with_custom_uv_venv`**: Similar to the previous test but specifically for environments created with `uv`. It verifies execution within that environment.\n  \nBoth tests ensure that the expected environment is utilized by comparing the output of the executed Python code against the expected virtual environment paths.\n\n## Cleanup and Error Handling\n\nThe script contains specific tests to assess the cleanup behavior of temporary files generated during code execution, verifying both scenarios where temp files are deleted on completion and those where they are retained.\n\n### Cleanup Behavior Tests\n\n- **`test_cleanup_temp_files_behavior`**: Tests both behaviors of the cleanup option \u2014 confirming that files are deleted or retained based on the configuration.\n\n- **`test_cleanup_temp_files_oserror`**: Uses mocking to simulate an error during the cleanup process and checks that proper error messages are logged.\n\n## PowerShell Script Testing\n\nLastly, there is a dedicated test to ensure that scripts can be executed in PowerShell:\n- **`test_ps1_script`**: Confirms that PowerShell scripts run correctly when PowerShell is available, asserting that the expected output is produced.\n\n## Conclusion\n\nOverall, `test_commandline_code_executor.py` serves as a robust suite for testing the execution capabilities of code in various formats and environments, particularly focusing on the `LocalCommandLineCodeExecutor`'s function within different virtual environments and platforms, ensuring reliability and error handling in various scenarios.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_commandline_code_executor.py"
  },
  {
    "code": false,
    "content": "# Documentation of Docker Command Line Code Executor Tests\n\nThis document provides a high-level overview of a test suite designed for validating the functionality of a Docker-based command line code executor. The tests employ an asynchronous framework utilizing the `pytest` and `pytest-asyncio` libraries to run a collection of tests aimed at executing code blocks within a Docker environment. \n\n## Code Structure\n\nThe code begins with necessary imports that include standard libraries, Docker-related libraries, and testing utilities. The key imports are `asyncio`, `pytest`, and the `DockerCommandLineCodeExecutor`, which is a crucial component for executing code in Docker containers.\n\n### Docker Tests Enabling Function\n\n```python\ndef docker_tests_enabled() -> bool:\n    ...\n```\n\nThis function checks whether Docker tests should be executed based on:\n\n1. An environment variable (`SKIP_DOCKER`) that, if set to \"true\", disables Docker tests.\n2. The availability of the Docker Python client and whether it can successfully ping the Docker daemon.\n\nIt returns a boolean value indicating the status of Docker testing capability.\n\n## Fixture Definitions\n\n### Executor and Temporary Directory Fixture\n\n```python\n@pytest_asyncio.fixture(scope=\"module\")  \nasync def executor_and_temp_dir(...):\n    ...\n```\n\nThis asynchronous fixture creates a temporary directory and starts the `DockerCommandLineCodeExecutor`. The executor will be used throughout the test suite. If Docker tests are disabled, it skips the tests accordingly. The temporary directory is important for isolating each test's filesystem interactions.\n\n### Cleanup Temporary Directory Fixture\n\n```python\n@pytest_asyncio.fixture(scope=\"function\")  \nasync def cleanup_temp_dir(executor_and_temp_dir: ExecutorFixture) -> AsyncGenerator[None, None]:\n    ...\n```\n\nThis fixture is designed to remove any files or directories created during the execution of tests. It operates on the temporary directory defined in the `executor_and_temp_dir` fixture, ensuring a clean environment for each test.\n\n## Test Cases\n\nThe test cases validate various functionalities of the `DockerCommandLineCodeExecutor` and ensure it behaves as expected when executing different code blocks.\n\n### Executing Code Blocks\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"executor_and_temp_dir\", [\"docker\"], indirect=True)\nasync def test_execute_code(executor_and_temp_dir: ExecutorFixture, cleanup_temp_dir: None):\n    ...\n```\n\nThis test checks the executor's ability to run both single and multiple Python code blocks, as well as a Bash script while validating their outputs and exit codes.\n\n### Command-line Code Executor Timeout\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"executor_and_temp_dir\", [\"docker\"], indirect=True)\nasync def test_commandline_code_executor_timeout(...):\n    ...\n```\n\nIn this test, a code block that includes a delayed operation is presented. The executor is configured with a timeout, ensuring that code execution abides by the specified limit and fails gracefully when the limit is exceeded.\n\n### Command-line Code Executor Cancellation\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"executor_and_temp_dir\", [\"docker\"], indirect=True)\nasync def test_commandline_code_executor_cancellation(...):\n    ...\n```\n\nThis test validates the cancellation mechanism of the executor. A long-running code block is executed and is expected to be canceled after a specific period. Assertions confirm that the correct cancellation message is returned, and no artifacts are left on the filesystem.\n\n### Validating Relative Paths\n\n#### Invalid Relative Path Test\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"executor_and_temp_dir\", [\"docker\"], indirect=True)\nasync def test_invalid_relative_path(...):\n    ...\n```\n\nWhen a code block expects a filename that lies outside the permissible workspace, the test checks for the correct error handling from the executor.\n\n#### Valid Relative Path Test\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"executor_and_temp_dir\", [\"docker\"], indirect=True)\nasync def test_valid_relative_path(...):\n    ...\n```\n\nConversely, this test verifies the successful execution of a code block defined with a valid filename, asserting that the expected outputs are returned and that the correct file is created in the expected location.\n\n## Additional Executor Functionality Tests\n\n### Start and Stop Executor\n\n```python\n@pytest.mark.asyncio\nasync def test_docker_commandline_code_executor_start_stop() -> None:\n    ...\n```\n\nThese tests confirm that the executor can correctly initialize and terminate its operations, ensuring that resources are properly managed throughout.\n\n### Checking for Errors with Wrong Paths\n\n```python\n@pytest.mark.asyncio\nasync def test_error_wrong_path(...) -> None:\n    ...\n```\n\nWhen erroneous paths are used within an executable code block, the executor needs to signal failure correctly, with appropriate messaging regarding the issue encountered.\n\n### Deprecation Warning Handling\n\n```python\n@pytest.mark.asyncio\nasync def test_deprecated_warning() -> None:\n    ...\n```\n\nThis test checks if appropriate warnings are raised upon using deprecated features of the executor, ensuring the library maintains compatibility and communicates changes effectively to the users.\n\n## Cleanup After Test Execution\n\n### Directory Creation and Cleanup\n\n```python\n@pytest.mark.asyncio\nasync def test_directory_creation_cleanup() -> None:\n    ...\n```\n\nThis test is aimed at ensuring the temporary directories created for executor operation are removed after their use, confirming that file system pollution does not occur.\n\n### Deletion of Temporary Files\n\n```python\n@pytest.mark.asyncio\nasync def test_delete_tmp_files() -> None:\n    ...\n```\n\nVerifies whether files created during code execution are deleted based on the configuration settings provided to the executor, thus ensuring the expected behavior aligns with the provided parameters.\n\n## Conclusion\n\nThis comprehensive suite of tests assesses various functionalities of the `DockerCommandLineCodeExecutor`, ensuring robust error handling, cancellation, timeout mechanisms, and valid path management, all while maintaining a clean testing environment. The modular design of the test cases allows for scalability and potential extension as new features are added to the executor or as additional requirements arise.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_docker_commandline_code_executor.py"
  },
  {
    "code": false,
    "content": "# Documentation for Docker Jupyter Code Executor Tests\n\nThis document provides an overview of the testing suite for a Docker-based Jupyter Code Executor, primarily utilizing `pytest` and `pytest-asyncio` for asynchronous testing. The tests are structured to ensure that the code executor behaves as expected in various scenarios using Docker containers.\n\n## Overview\n\nThe code focuses on testing the capabilities of the `DockerJupyterCodeExecutor` class, which is responsible for executing code blocks in a Jupyter environment hosted within Docker containers. The test cases cover various aspects such as code execution, variable persistence, cancellation, timeouts, and error handling related to the executor.\n\nKey imports in the code include:\n- `inspect`, `os`, `tempfile`, and `pathlib` for environment and temporary file management.\n- `pytest` and `pytest_asyncio` for the test framework and async capabilities.\n- Classes from the `autogen_core` module that handle code execution and cancellation.\n\n## Function: `docker_tests_enabled`\n\n### Purpose\n\nThis function checks if Docker testing is enabled by verifying the environment variable `SKIP_DOCKER` and determining the availability of the Docker client. \n\n### Role\n\n1. **Environment Check**: It first checks if the `SKIP_DOCKER` variable is set to \"true\".\n2. **Docker Client Access**: It tries to import the Docker library and ping the Docker service to confirm the client can communicate with the Docker daemon.\n3. **Return Value**: If Docker is available and the environment variable permits, it returns `True`. Otherwise, it returns `False`.\n\n## Fixture: `executor_and_temp_dir`\n\n### Purpose\n\nThis asynchronous fixture provides an instance of the `DockerJupyterCodeExecutor` and a temporary directory for tests that need to execute code blocks in a Docker container. \n\n### Role\n\n1. **Conditional Execution**: If Docker tests are not enabled, the test is skipped.\n2. **Temporary Directory Management**: It creates a temporary directory using `tempfile.TemporaryDirectory`.\n3. **Jupyter Server Setup**: Sets up the `DockerJupyterServer` and the `DockerJupyterCodeExecutor`.\n4. **Yielding**: It yields the executor instance along with the temporary directory path, which can be used in the tests.\n\n## Test Case: `test_execute_code`\n\n### Purpose\n\nThis test checks the ability of the executor to execute both single and multiple code blocks, ensuring the output is as expected.\n\n### Role\n\n1. **Single Block Execution**: Tests a single code block that prints \"hello world!\".\n2. **Multiple Block Execution**: Tests multiple code blocks that print the same string and a calculated value.\n3. **Assertions**: Validates the exit code and checks the output for expected results.\n\n## Test Case: `test_execute_code_and_persist_variable`\n\n### Purpose\n\nThis test verifies that variables defined in one code block can be accessed and used in subsequent blocks.\n\n### Role\n\n1. **Initial Variable Declaration**: Executes a block that defines variable 'a'.\n2. **Variable Persistence**: Executes a second block that uses 'a' to calculate 'b' and assert its value.\n3. **Assertions**: Confirms successful execution and correct variable handling across blocks.\n\n## Test Case: `test_timeout`\n\n### Purpose\n\nThis test examines the executor's behavior when a code block exceeds a specified execution time limit.\n\n### Role\n\n1. **Timeout Setup**: Executes a block that intentionally sleeps for 10 seconds with a timeout set to 1 second.\n2. **Assertions**: Validates that the exit code reflects an error due to timeout and checks for a corresponding message in the output.\n\n## Test Case: `test_cancellation`\n\n### Purpose\n\nThis test ensures the executor can handle cancellation gracefully during code execution.\n\n### Role\n\n1. **Cancellation Token Usage**: The test runs a code block that sleeps for 10 seconds; it is not explicitly canceled in the current implementation but validates file creation.\n2. **File Existence Check**: Checks if the specified file was created during execution, verifying the successful execution of the block.\n\n## Test Case: `test_start_stop`\n\n### Purpose\n\nTests the instantiation and shutdown process of the `DockerJupyterServer` and `DockerJupyterCodeExecutor`.\n\n### Role\n\n1. **Executor Lifecycle Management**: The test starts and stops the executor and verifies that no exceptions are raised during this process.\n\n## Test Case: `test_invalid_timeout`\n\n### Purpose\n\nConfirms that providing an invalid timeout raises the expected exception.\n\n### Role\n\n1. **Exception Handling**: Attempts to create a `DockerJupyterCodeExecutor` with a timeout of zero and expects a `ValueError` exception.\n\n## Test Case: `test_execute_code_with_image_output`\n\n### Purpose\n\nTests the executor's capability to handle code that produces graphical output.\n\n### Role\n\n1. **Graphics Code Execution**: Executes code that uses the `PIL` library to create an image and display it.\n2. **Output Verification**: Checks that the executor's output contains the expected results, including validating the existence of generated files.\n\n## Conclusion\n\nThe provided test suite ensures the reliable functioning of the `DockerJupyterCodeExecutor` in various scenarios, including basic execution, variable persistence, timeout handling, and graphical output management. Through these tests, the functionality and robustness of the executor can be verified before deployment in production environments.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_docker_jupyter_code_executor.py"
  },
  {
    "code": false,
    "content": "# Jupyter Code Executor Tests Documentation\n\nThis document provides a detailed description of a set of test cases designed to verify the functionality of the `JupyterCodeExecutor` class within the `autogen_ext.code_executors.jupyter` module. The tests are implemented using the `pytest` framework with asyncio support. Each test case validates different scenarios of executing Python code from Jupyter-like code blocks and ensuring expected outcomes.\n\n## Overview of the JupyterCodeExecutor\n\nThe `JupyterCodeExecutor` is designed to execute code blocks in a Jupyter-like environment. It can handle multiple code blocks, manage execution environments, and capture outputs, including standard prints, HTML, and images. Cancellation and timeout features are also covered to ensure robust execution handling.\n\n### Key Components\n\n- **CodeBlock**: Represents a block of code to execute. It includes the code string and its respective language.\n- **JupyterCodeResult**: A data structure that stores the results of executing code blocks, including exit codes, output strings, and files generated during execution.\n\n### Test Suite Structure\n\nThe test suite is structured into several asynchronous tests, each focusing on different execution scenarios:\n\n## Test Execution of Code Blocks\n\n### 1. Basic Execution Test\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_code(tmp_path: Path) -> None:\n    ...\n```\n\nThis test checks the basic functionality of executing a simple Python print statement. It initializes a `JupyterCodeExecutor`, executes a code block that prints \"hello world!\", and then asserts that the output matches the expected output, confirming that the code executed without errors (exit code 0).\n\n### 2. Error Handling Test\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_code_error(tmp_path: Path) -> None:\n    ...\n```\n\nThis test case verifies how the executor handles a runtime error caused by trying to use an undefined variable. The output is examined to ensure it correctly captures the error message, and the exit code reflects failure (exit code 1).\n\n### 3. Multiple Code Blocks Execution\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_multiple_code_blocks(tmp_path: Path) -> None:\n    ...\n```\n\nIn this scenario, two code blocks are executed sequentially. The first block prints a greeting, and the second computes a sum. The expected output should include results from both executions, and the exit code should indicate success.\n\n## Test Dependency Management\n\n### 4. Dependent Executions Test\n\n```python\n@pytest.mark.asyncio\nasync def test_depedent_executions(tmp_path: Path) -> None:\n    ...\n```\n\nThis test checks whether the executor can handle dependent code blocks. The first block defines a variable, and the second block utilizes this variable. Success is expected if the variable access is correct, confirming that the state is persisted between executions.\n\n### 5. Multiple Code Blocks with Errors\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_multiple_code_blocks_error(tmp_path: Path) -> None:\n    ...\n```\n\nHere, the test anticipates a mixed outcome where one code block runs successfully while another causes an error. The executor should capture outputs and errors effectively, with the final output reflecting both successes and failures.\n\n## Edge Case Testing \n\n### 6. Restart Logic Test\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_code_after_restart(tmp_path: Path) -> None:\n    ...\n```\n\nThis test examines the ability of the executor to maintain functionality after restarting. It evaluates executing a code block post-restart to ensure that the executor can recover and still execute code correctly.\n\n### 7. Timeout Handling Test\n\n```python\n@pytest.mark.asyncio\nasync def test_commandline_code_executor_timeout(tmp_path: Path) -> None:\n    ...\n```\n\nIn this test, a code block that exceeds a set timeout is executed. It verifies that the executor correctly raises a `TimeoutError`, confirming the implementation of timeout handling procedures.\n\n### 8. Cancellation of Execution Test\n\n```python\n@pytest.mark.asyncio\nasync def test_commandline_code_executor_cancellation(tmp_path: Path) -> None:\n    ...\n```\n\nThis test assesses the cancellation feature of the executor. By invoking cancellation during code execution, it ensures the executor responds appropriately and raises a `CancelledError`.\n\n## Output Handling Tests\n\n### 9. Image Output Test\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_code_with_image_output(tmp_path: Path) -> None:\n    ...\n```\n\nThis test checks the executor's capability to handle outputs that involve images. The result is validated to ensure a generated image is present in the expected directory and that the output information is accurate.\n\n### 10. HTML Output Test\n\n```python\n@pytest.mark.asyncio\nasync def test_execute_code_with_html_output(tmp_path: Path) -> None:\n    ...\n```\n\nSimilar to the image output test, this scenario evaluates if HTML output can be produced and captured correctly, verifying the executor's versatility in handling different output types.\n\n## Executor Functionality and Serialization\n\n### 11. Serialization Test\n\n```python\n@pytest.mark.asyncio\nasync def test_jupyter_code_executor_serialization(tmp_path: Path) -> None:\n    ...\n```\n\nThis test explores the serialization capabilities of the `JupyterCodeExecutor`. The executor is serialized and then deserialized to check if it retains its state, ensuring that the component's data integrity remains intact across serialization events.\n\n## Exception Testing\n\n### 12. Invalid Timeout Configuration Test\n\n```python\ndef test_invalid_timeout() -> None:\n    ...\n```\n\nHere, the test ensures configuration validation by submitting an invalid timeout parameter to the `JupyterCodeExecutor`. It checks that a `ValueError` is raised, safeguarding against incorrect usage scenarios.\n\n### 13. Deprecation Warning Test\n\n```python\n@pytest.mark.asyncio\nasync def test_deprecation_output_dir() -> None:\n    ...\n```\n\nThis test confirms the executor issues a deprecation warning when using the current directory as the output directory. It ensures users are informed about best practice changes within the API.\n\n### 14. Runtime Error When Not Started\n\n```python\n@pytest.mark.asyncio\nasync def test_runtime_error_not_started() -> None:\n    ...\n```\n\nFinally, this test checks for appropriate error handling when an attempt is made to execute code without the executor being started. It verifies that a `RuntimeError` is raised to prevent usage without proper initialization.\n\n## Conclusion\n\nThe test suite collectively covers the functionality, edge cases, and error handling provisions of the `JupyterCodeExecutor`, ensuring it behaves as expected under various conditions. Each test case plays a crucial role in establishing confidence in the executor's reliability and robustness within an asynchronous Python environment.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_jupyter_code_executor.py"
  },
  {
    "code": false,
    "content": "# Documentation for test_user_defined_functions.py\n\nThis module contains a series of unit tests for user-defined functions, validating their import, execution, and handling of requirements using the `LocalCommandLineCodeExecutor` from the `autogen_core` library. The tests are conducted using the `pytest` framework, with asynchronous test cases facilitated through the use of `pytest.mark.asyncio`. \n\n## Imports and Setup\n\nThe code begins by importing necessary libraries and modules:\n\n```python\nimport os\nimport tempfile\nfrom pathlib import Path\n\nimport polars\nimport pytest\nfrom autogen_core import CancellationToken\nfrom autogen_core.code_executor import (\n    CodeBlock,\n    FunctionWithRequirements,\n    with_requirements,\n)\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n\nENVIRON_KEY_AZURE_POOL_ENDPOINT = \"AZURE_POOL_ENDPOINT\"\nDUMMY_POOL_ENDPOINT = \"DUMMY_POOL_ENDPOINT\"\nPOOL_ENDPOINT = os.getenv(ENVIRON_KEY_AZURE_POOL_ENDPOINT)\n```\n\n- **os** and **tempfile** are standard Python libraries used for file management.\n- **polars** is a dataframe library utilized for data handling.\n- **pytest** is the testing framework that facilitates the execution of tests.\n- The `autogen_core` modules provide the necessary classes for handling code execution and defining function requirements.\n\n## Function Definitions\n\n### Basic Operations\n\n```python\ndef add_two_numbers(a: int, b: int) -> int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n```\nThis function takes two integers, adds them, and returns the result. It serves as a simple test example for the executor.\n\n### Data Loading with Dependencies\n\n```python\n@with_requirements(python_packages=[\"polars\"], global_imports=[\"polars\"])\ndef load_data() -> polars.DataFrame:\n    \"\"\"Load some sample data.\n    \n    Returns:\n        polars.DataFrame: A DataFrame with the following columns: name(str), location(str), age(int)\n    \"\"\"\n    data = {\n        \"name\": [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n        \"location\": [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n        \"age\": [24, 13, 53, 33],\n    }\n    return polars.DataFrame(data)\n```\nThis function loads sample data into a `polars.DataFrame`. The decorator `@with_requirements()` ensures that the function specifies its dependencies (in this case, the `polars` package).\n\n### Error Handling for Invalid Imports\n\n```python\n@with_requirements(global_imports=[\"NOT_A_REAL_PACKAGE\"])\ndef function_incorrect_import() -> \"polars.DataFrame\":\n    return polars.DataFrame()\n```\nThe presence of an invalid import will be caught during execution, simulating a scenario where the required packages are not found.\n\n```python\n@with_requirements(python_packages=[\"NOT_A_REAL_PACKAGE\"])\ndef function_incorrect_dep() -> \"polars.DataFrame\":\n    return polars.DataFrame()\n\ndef function_missing_reqs() -> \"polars.DataFrame\":\n    return polars.DataFrame()\n```\nThese two functions also serve as examples of handling incorrect dependencies and missing requirements, set for later testing.\n\n## Testing Functions\n\nThe tests are wrapped with the `pytest.mark.asyncio` decorator to indicate they are asynchronous.\n\n### Loading Functions with Requirements\n\n```python\n@pytest.mark.asyncio\nasync def test_can_load_function_with_reqs() -> None:\n    # Test the loading and execution of the load_data function\n```\nThis test verifies that the `load_data` function correctly loads data and returns the expected output when called.\n\n### Testing Function Formatting\n\n```python\nasync def test_local_formatted_prompt() -> None:\n    # Checks if the function formatting is as expected\n```\nThis test focuses on formatting the user-defined functions into a prompt suitable for further processing.\n\n### Validation of Error Cases\n\nSeveral tests aim to ensure that the code handles invalid scenarios correctly:\n\n```python\n@pytest.mark.asyncio\nasync def test_fails_for_function_incorrect_import() -> None:\n    # Test for failure in importing a function with incorrect requirements\n```\n\n### Testing for File Operations and Working Directory\n\n```python\nasync def test_error_wrong_path() -> None:\n    # Factors like invalid paths are handled correctly\n```\nThese tests ensure that file I/O operations handle errors gracefully and that the working directory is appropriately managed.\n\n## Various Edge Cases\n\nAdditional tests check for proper handling of deprecated features, timeout values, and ensuring that valid Python identifiers are used for module names.\n\n```python\ndef test_invalid_timeout() -> None:\n    # Ensure proper error handling for invalid timeout values\n```\n\n### Temporary Directory Creation\n\n```python\n@pytest.mark.asyncio\nasync def test_create_temp_dir() -> None:\n    executor = LocalCommandLineCodeExecutor()\n    assert executor.work_dir.is_dir()\n```\nThis test confirms that a temporary directory is created correctly for the executor\u2019s work.\n\n## Conclusion\n\nThe `test_user_defined_functions.py` module thoroughly evaluates the behavior of user-defined functions within a controlled environment, ensuring that they can be executed correctly, handle dependencies, and respond well to errors. This structure and variety in testing cases allow for robust unit tests, which can be crucial for maintaining code quality in larger applications.",
    "filename": "python/packages/autogen-ext/tests/code_executors/test_user_defined_functions.py"
  },
  {
    "code": false,
    "content": "# Pytest Configuration for Windows Testing\n\nThis code snippet is a configuration module intended for use with the **pytest** testing framework. It facilitates running tests specifically targeting the Windows operating system. The module leverages pytest's capabilities to enhance the testing environment by adding command-line options and markers.\n\n## Overview\n\nThe code includes two primary functions:\n\n1. **`pytest_addoption`**: This function allows for the addition of custom command-line options to pytest.\n2. **`pytest_configure`**: This function configures various aspects of pytest, such as adding custom markers to identify specific test requirements.\n\nBoth functions utilize pytest's built-in functionality, enhancing the testing framework for targeted use cases, particularly for developers who need to run tests in a Windows environment.\n\n## `pytest_addoption` Function\n\n### Purpose\n\nThe `pytest_addoption` function is responsible for adding a custom command-line option to pytest. This customization allows individuals running tests to specify whether they want to include tests that are designed to run on Windows.\n\n### Parameters\n\n- `parser`: An instance of `pytest.Parser`, which is used to parse command-line options provided by the user.\n\n### Functionality\n\nInside the function:\n\n- The `addoption` method is called on the `parser` object.\n- A new option `--windows` is defined, which:\n  - Is a boolean flag (`action=\"store_true\"`).\n  - Has a default value of `False`.\n  - Provides help text that describes its purpose.\n\nWhen the `--windows` flag is passed during the execution of pytest, it indicates that tests requiring a Windows environment should be executed.\n\n## `pytest_configure` Function\n\n### Purpose\n\nThe `pytest_configure` function serves to configure the pytest runtime environment by establishing key markers for test functions. It enables the identification of tests that specifically require Windows for execution.\n\n### Parameters\n\n- `config`: An instance of `pytest.Config`, which carries the configuration details for pytest.\n\n### Functionality\n\nWithin this function:\n\n- The `addinivalue_line` method is invoked on the `config` object.\n- A new marker named \"windows\" is added. This marker can be used in test functions to indicate that they should only be executed in a Windows environment.\n\n### Use Case\n\nWhen a test is marked with `@pytest.mark.windows`, it serves as a clear indication to both the test runner and anyone reading the code that this particular test has dependencies or aspects that require the Windows OS. This is particularly useful for cross-platform applications where certain functionalities may only be supported on Windows.\n\n## Summary\n\nIn conclusion, this code snippet provides a straightforward way to enhance the pytest testing framework for Windows-specific tests. By implementing command-line options and custom markers, it allows developers to easily manage the execution of tests tailored for the Windows environment. This setup ensures that any tests that cannot run on non-Windows systems are appropriately flagged and thus are only executed when the correct environment is present. The approach taken here exemplifies good practice in creating modular and maintainable testing configurations within the pytest ecosystem.",
    "filename": "python/packages/autogen-ext/tests/conftest.py"
  },
  {
    "code": false,
    "content": "# Simple MCP Server Documentation\n\nThis document outlines the functionality and structure of the Simple MCP Server implemented in Python. The server is designed to handle prompts, resources, and tools, utilizing asynchronous programming for efficient processing.\n\n## Overview\n\nThe Simple MCP Server serves as a basic implementation of a server using the MCP (Multi-Channel Protocol) framework. It is capable of listing prompts, providing resources, and executing tools with predefined functionality. The server utilizes asynchronous I/O to manage requests and responses effectively.\n\n## Imports and Logger Configuration\n\nThe server's code begins with several imports:\n- `asyncio`: For asynchronous programming, enabling concurrent I/O operations.\n- `json`: To handle JSON data serialization and deserialization.\n- `logging`: For logging events and messages throughout the server\u2019s operation.\n- `datetime`: To manage timestamps.\n- Several classes and functions from the MCP library, including those needed to define prompts, resources, and tools.\n\nA logger is configured at the INFO level to log server activities.\n\n```python\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n```\n\n## Sample Data Definition\n\nThe server includes sample data that mimics user and project details. This data is intended for demonstration purposes, providing structure and content for the resources available through the server.\n\n```python\nSAMPLE_DATA = {\n    \"users\": [...],\n    \"projects\": [...],\n}\n```\n\n## SimpleMcpServer Class\n\n### Class Initialization\n\nThe `SimpleMcpServer` class encapsulates the functionalities of the server. It initializes an instance of `Server`, registers handlers for prompts, resources, and tools, and sets the groundwork for incoming client connections.\n\n```python\nclass SimpleMcpServer:\n    def __init__(self) -> None:\n        self.server: Server[object] = Server(\"simple-mcp-server\")\n        self.register_handlers()  # Registers the various handlers for server functionality\n```\n\n### Register Handlers Method\n\nThe `register_handlers` method ties together the various aspects of the server:\n\n1. **Prompts:** It defines two primary prompts (`code_review` and `documentation`) for generating code reviews and documentation from provided content.\n2. **Resources:** It sets up resources such as user and project information that can be accessed via specific URIs.\n3. **Tools:** It creates tools like `echo` and `get_time`, which perform specific functions and return the output.\n\n```python\ndef register_handlers(self) -> None:\n    # Defines various handlers for prompts, resources, and tools\n```\n\n## Prompts Functionality\n\n### Listing Prompts\n\nThe server has a handler to list the available prompts. Each prompt consists of a name, description, and required arguments.\n\n```python\n@self.server.list_prompts()\nasync def list_prompts() -> list[Prompt]:\n    # Returns a list of defined prompts\n```\n\n### Retrieving Specific Prompts\n\nAnother handler allows users to get details for aspecific prompt, including associated messages based on provided arguments.\n\n```python\n@self.server.get_prompt()\nasync def get_prompt(name: str, arguments: Optional[Dict[str, str]] = None) -> GetPromptResult:\n    # Returns prompt details based on the request\n```\n\n## Resource Functionality\n\n### Listing Resources\n\nSimilar to prompts, the server can provide a list of resources, which in this case includes JSON files containing sample user and project data.\n\n```python\n@self.server.list_resources()\nasync def list_resources() -> list[Resource]:\n    # Returns available resources with metadata\n```\n\n### Reading Resources\n\nThis handler enables the retrieval of specific resource data based on a given URI, providing either the user list or project list in JSON format.\n\n```python\n@self.server.read_resource()\nasync def read_resource(uri: AnyUrl) -> str:\n    # Returns JSON data for the requested resource\n```\n\n## Tool Functionality\n\n### Listing Tools\n\nThe server defines a handler to enumerate available tools and their descriptions, which include schemas for input parameters.\n\n```python\n@self.server.list_tools()\nasync def list_tools() -> list[Tool]:\n    # Lists available tools with their capabilities\n```\n\n### Invoking Tools\n\nA handler facilitates calling specific tools. Users can invoke the `echo` tool to repeat input or use the `get_time` tool to retrieve the current timestamp.\n\n```python\n@self.server.call_tool()\nasync def call_tool(name: str, arguments: Optional[Dict[str, Any]] = None) -> list[TextContent]:\n    # Executes the requested tool based on the input arguments\n```\n\n## Running the Server\n\nThe `run` method orchestrates the execution of the server by initiating an asynchronous session that listens for incoming requests and handles them according to registered prompts, resources, and tools. It initializes server capabilities, which include prompts, resources, and tools configuration.\n\n```python\nasync def run(self) -> None:\n    # Starts the server with specified initialization options\n```\n\n## Main Entry Point\n\nThe script's entry point is the `main` function which creates an instance of the `SimpleMcpServer` and runs it. This ensures the server starts when the script is executed.\n\n```python\nasync def main() -> None:\n    server: SimpleMcpServer = SimpleMcpServer()\n    await server.run()  # Calls the server to start processing requests\n```\n\n### Script Execution\n\nFinally, if the script is run as the main module, it uses `asyncio.run()` to initiate the `main` function, establishing the server lifecycle.\n\n```python\nif __name__ == \"__main__\":\n    asyncio.run(main())  # Executes the main function\n```\n\n## Conclusion\n\nThe Simple MCP Server serves as a basic yet comprehensive example of using asynchronous programming to handle requests for prompts, resources, and tools. It provides a structured method for creating, retrieving, and utilizing these components, all while logging key activities for debugging and monitoring purposes.",
    "filename": "python/packages/autogen-ext/tests/mcp_server_comprehensive.py"
  },
  {
    "code": false,
    "content": "# ChromaDB Vector Memory Module Tests Documentation\n\nThis document provides an overview of the tests defined for the ChromaDB Vector Memory system implemented with `pytest`. The tests ascertain the functionality of various configurations, memory operations, error handling, and integration with embeddings functionality. \n\n## Overview\n\nThe tests are structured using the `pytest` framework, ensuring that functionality is verifiable and robust against various conditions. The tests rely on the `ChromaDBVectorMemory` and associated configurations to store and retrieve memory content. The system supports various content types, score thresholds, and embedding functions, making it highly adaptable for different use cases.\n\n### Code Imports and Basic Configuration\n\n```python\nfrom pathlib import Path\nimport pytest\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.memory.chromadb import (\n    ChromaDBVectorMemory,\n    PersistentChromaDBVectorMemoryConfig,\n)\n```\n\nThe code imports necessary modules, including `pytest` for testing and specific classes from `autogen_core` and `autogen_ext.memory.chromadb` related to memory handling and configurations. The `MemoryContent` and `MemoryMimeType` classes define how contents are structured and processed.\n\n### ChromaDB Availability Check\n\n```python\ntry:\n    import chromadb\nexcept ImportError:\n    pytest.skip(\"ChromaDB not available\", allow_module_level=True)\n```\n\nHere, an attempt is made to import `chromadb`. If it's not available, the tests are skipped, ensuring that tests only run in environments where all required dependencies are met.\n\n## Fixture Definitions\n\nThree fixtures are defined that set up different configurations for `PersistentChromaDBVectorMemoryConfig`:\n\n### Base Configuration\n\n```python\n@pytest.fixture\ndef base_config(tmp_path: Path) -> PersistentChromaDBVectorMemoryConfig:\n    \"\"\"Create base configuration without score threshold.\"\"\"\n    return PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"test_collection\", allow_reset=True, k=3, persistence_path=str(tmp_path / \"chroma_db\")\n    )\n```\n\nThe `base_config` fixture creates a configuration meant for general tests, allowing resets and configuring path storage.\n\n### Strict Configuration\n\n```python\n@pytest.fixture\ndef strict_config(tmp_path: Path) -> PersistentChromaDBVectorMemoryConfig:\n    \"\"\"Create configuration with strict score threshold.\"\"\"\n    return PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"test_collection\",\n        allow_reset=True,\n        k=3,\n        score_threshold=0.8,\n        persistence_path=str(tmp_path / \"chroma_db_strict\"),\n    )\n```\n\nThis fixture sets a rigorous score threshold, ensuring only highly relevant results are returned.\n\n### Lenient Configuration\n\n```python\n@pytest.fixture\ndef lenient_config(tmp_path: Path) -> PersistentChromaDBVectorMemoryConfig:\n    \"\"\"Create configuration with lenient score threshold.\"\"\"\n    return PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"test_collection\",\n        allow_reset=True,\n        k=3,\n        score_threshold=0.0,\n        persistence_path=str(tmp_path / \"chroma_db_lenient\"),\n    )\n```\n\nConversely, the `lenient_config` allows for broader query matches by setting the score threshold to zero.\n\n## Test Cases\n\n### Basic Workflow Tests\n\n```python\n@pytest.mark.asyncio\nasync def test_basic_workflow(base_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test basic memory operations with default threshold.\"\"\"\n    memory = ChromaDBVectorMemory(config=base_config)\n    await memory.clear()\n    await memory.add(MemoryContent(content=\"Paris is known for the Eiffel Tower and amazing cuisine.\", mime_type=MemoryMimeType.TEXT, metadata={\"category\": \"city\", \"country\": \"France\"}))\n    results = await memory.query(\"Tell me about Paris\")\n    assert len(results.results) > 0\n    assert any(\"Paris\" in str(r.content) for r in results.results)\n    await memory.close()\n```\n\nThis test validates the fundamental operations of adding a memory, clearing, and querying. The expected results are checked to confirm functionality.\n\n### Content Types Handling\n\n```python\n@pytest.mark.asyncio\nasync def test_content_types(lenient_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test different content types with lenient matching.\"\"\"\n    memory = ChromaDBVectorMemory(config=lenient_config)\n    await memory.clear()\n    text_content = MemoryContent(content=\"Simple text content for testing\", mime_type=MemoryMimeType.TEXT)\n    await memory.add(text_content)\n    json_content = MemoryContent(content={\"key\": \"value\", \"number\": 42}, mime_type=MemoryMimeType.JSON)\n    await memory.add(json_content)\n    results = await memory.query(\"simple text content\")\n    assert len(results.results) > 0\n    await memory.close()\n```\n\nThe test assesses the ability to handle different content types (text and JSON). It verifies correct processing and querying for both.\n\n### Strict Matching Tests\n\n```python\n@pytest.mark.asyncio\nasync def test_strict_matching(strict_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test matching behavior with high score threshold.\"\"\"\n    memory = ChromaDBVectorMemory(config=strict_config)\n    await memory.clear()\n    await memory.add(MemoryContent(content=\"Specific technical details about quantum computing\", mime_type=MemoryMimeType.TEXT))\n    exact_results = await memory.query(\"quantum computing details\")\n    assert len(exact_results.results) > 0\n    await memory.close()\n```\n\nThis test confirms that the implementation respects the defined score threshold during querying, producing strict results in line with user queries.\n\n### Metadata Handling Tests\n\n```python\n@pytest.mark.asyncio\nasync def test_metadata_handling(base_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test metadata handling with default threshold.\"\"\"\n    memory = ChromaDBVectorMemory(config=base_config)\n    await memory.clear()\n    test_content = \"Test content with specific metadata\"\n    content = MemoryContent(content=test_content, mime_type=MemoryMimeType.TEXT, metadata={\"test_category\": \"test\", \"test_priority\": 1})\n    await memory.add(content)\n    results = await memory.query(test_content)\n    assert len(results.results) > 0\n    await memory.close()\n```\n\nThe metadata handling test ensures that relevant metadata can be added alongside content and retrieved effectively.\n\n### Error Handling Tests\n\n```python\n@pytest.mark.asyncio\nasync def test_error_handling(base_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test error cases with default threshold.\"\"\"\n    memory = ChromaDBVectorMemory(config=base_config)\n    await memory.clear()\n    with pytest.raises(ValueError, match=\"Unsupported content type\"):\n        await memory.add(MemoryContent(content=b\"binary data\", mime_type=MemoryMimeType.BINARY))\n    await memory.close()\n```\n\nThis test checks for correct error handling by adding unsupported content types and verifying that appropriate exceptions are raised.\n\n### Initialization Tests\n\n```python\n@pytest.mark.asyncio\nasync def test_initialization(base_config: PersistentChromaDBVectorMemoryConfig) -> None:\n    \"\"\"Test initialization with default threshold.\"\"\"\n    memory = ChromaDBVectorMemory(config=base_config)\n    assert memory.collection_name == \"test_collection\"\n    await memory.close()\n```\n\nThis test validates that the `ChromaDBVectorMemory` class initializes correctly, confirming the configuration passed is appropriately set during instantiation.\n\n## Embedding Function Configurations\n\nA suite of tests further explores operations involving different embedding function configurations such as default embedding, SentenceTransformer, custom functions, and OpenAI.\n\n### Testing Different Embedding Functions\n\nEach function will validate that its specific configuration is correct and operational. The tests include verification of serialization and deserialization, as well as error handling for invalid function setups. \n\n```python\n@pytest.mark.asyncio\nasync def test_default_embedding_function(tmp_path: Path) -> None:\n    \"\"\"Test ChromaDB memory with default embedding function.\"\"\"\n    config = PersistentChromaDBVectorMemoryConfig(\n        ...\n    )\n    # Add memory content and query\n```\n\n### Functionality Validation\n\nEach embedding type\u2019s validity is confirmed with assertions for expected properties ensuring that the right embeddings get used and operations execute without runtime errors.\n\n## Conclusion\n\nThis set of tests ensures that the ChromaDB Vector Memory implementation functions as intended across various scenarios and input types. This provides strong coverage for basic operations, advanced content handling, error scenarios, and configurations, promoting reliable performance in production.",
    "filename": "python/packages/autogen-ext/tests/memory/test_chroma_memory.py"
  },
  {
    "code": false,
    "content": "# Documentation for Memory Management Tests\n\nThis document describes the functionality and purpose of the provided Python code, which constitutes a suite of unit tests aimed at verifying memory operations within an automated system using the `Mem0Memory` class from the `autogen_ext.memory.mem0` module. The tests utilize the `pytest` framework for structuring and executing the test cases.\n\n## Introduction\n\nThe code initializes environments, configurations, and mock setups to facilitate the testing process. It ensures that methods for adding, querying, and managing memory contents are functioning correctly, both in local and cloud configurations. All interactions with the memory class are mocked to avoid dependencies on an actual database or external API, making tests fast and reliable.\n\n## Environment Configuration\n\n### Loading Environment Variables\n\nThe code begins by loading environment variables from a `.env` file using the `load_dotenv()` function. It retrieves the `MEM0_API_KEY` to determine if tests that require cloud capabilities can be executed. If the key is not set, those tests are skipped to avoid execution failures.\n\n```python\nload_dotenv()\n```\n\n### Conditional Test Fixtures\n\nThe code defines several fixtures that provide configurations based on whether the execution is set up for local or cloud environments. It also uses `pytest.mark.skipif` to skip tests if the necessary API keys or libraries are not available.\n\nFor instance:\n\n```python\n@pytest.fixture\ndef full_local_config() -> Dict[str, Any]:\n    \"\"\"Return the local configuration for testing.\"\"\"\n    return FULL_LOCAL_CONFIG\n```\n\n## Unit Tests Overview\n\nThe test suite employs several unit tests to verify the functionality of memory management methods:\n\n### Basic Memory Operations\n\nThe `test_basic_workflow` function tests fundamental memory operations, such as adding content and querying it. Mock responses are set up to simulate expected behaviors, ensuring that both adding and searching functionalities work as intended.\n\n```python\nasync def test_basic_workflow(mock_mem0_class: MagicMock, local_config: Mem0MemoryConfig) -> None:\n    ...\n```\n\n### Cloud Memory Operations\n\nSimilarly, `test_basic_workflow_with_cloud` validates the same operations but simulates interactions with a cloud configuration. This ensures that the cloud setup and its API interactions function correctly without requiring actual API calls.\n\n```python\n@requires_mem0_api\n@pytest.mark.asyncio\nasync def test_basic_workflow_with_cloud(mock_memory_client_class: MagicMock, cloud_config: Mem0MemoryConfig) -> None:\n    ...\n```\n\n### Metadata Handling\n\nThe `test_metadata_handling` function focuses on ensuring that metadata associated with memories is stored and retrieved accurately. The test checks that added metadata remains intact during adding and querying operations.\n\n```python\nasync def test_metadata_handling(mock_mem0_class: MagicMock, local_config: Mem0MemoryConfig) -> None:\n    ...\n```\n\n### Context Management\n\nIn the `test_update_context` function, the handling of system context updates is verified. This involves checking whether adding memories enriches the model context correctly, reflecting the expected changes in the context.\n\n```python\nasync def test_update_context(mock_mem0_class: MagicMock, local_config: Mem0MemoryConfig) -> None:\n    ...\n```\n\n## Advanced Testing Scenarios\n\n### Component Serialization\n\nThe `test_component_serialization` function checks the serialization and deserialization process of memory configurations. It confirms that the configuration structure is preserved when dumped and loaded back into a new memory instance.\n\n```python\nasync def test_component_serialization(mock_memory_client_class: MagicMock) -> None:\n    ...\n```\n\n### Result Format Handling\n\nTests like `test_result_format_handling` evaluate the ability of the system to handle different response formats (dictionary vs. list) correctly. The tests verify that results maintain their integrity and can be accessed in various structured formats.\n\n```python\nasync def test_result_format_handling(mock_mem0_class: MagicMock, local_config: Mem0MemoryConfig) -> None:\n    ...\n```\n\n### Initialization with Local Config\n\nThe `test_init_with_local_config` function confirms that the `Mem0Memory` class can be correctly instantiated with local configuration parameters, ensuring proper logging and setup.\n\n```python\nasync def test_init_with_local_config(mock_mem0_class: MagicMock, full_local_config: Dict[str, Any]) -> None:\n    ...\n```\n\n### Local Configuration Memory Operations\n\nThe last test, `test_local_config_with_memory_operations`, validates the complete workflow involving adding, querying, and deleting memory contents while utilizing the full local configuration. It assures that all operations function seamlessly and integrate well with the mock setup.\n\n```python\nasync def test_local_config_with_memory_operations(\n     mock_mem0_class: MagicMock,\n     full_local_config: Dict[str, Any],\n) -> None:\n    ...\n```\n\n## Conclusion\n\nThis suite of tests thoroughly examines the core functionalities of the `Mem0Memory` class, ensuring that it can handle various scenarios related to memory operations effectively. By leveraging mocking and environment configurations, the tests provide a reliable framework for validating the logic without external dependencies. Each test scenario is carefully constructed to allow for isolated verification of specific functionalities, maintaining the robustness of the memory management system. Enhanced error handling and deeper integration tests could be considered for further test suites as the project evolves.",
    "filename": "python/packages/autogen-ext/tests/memory/test_mem0.py"
  },
  {
    "code": false,
    "content": "# Redis Memory Module Documentation\n\nThis document provides a high-level overview and detailed explanation of the Redis Memory module's functionality, including its tests and interactions. This particular module is designed to utilize a Redis database for storing and retrieving memory content in an AI-driven environment.\n\n## Overview\n\nThe Redis Memory module allows the management of memory content using a Redis data structure. The content represents user interactions, and the module provides functionality for adding, querying, clearing, and closing memory operations. The memory content can include text, JSON, or markdown formats, enabling the system to handle diverse data types.\n\nBelow are the primary classes and functions that the module incorporates, along with their usages in regards to memory management.\n\n## Memory Content and Configuration\n\nThe module includes the following core components:\n\n- **MemoryContent**: Represents the content to be stored in memory. It includes attributes such as `content`, `mime_type`, and `metadata`. The `mime_type` helps distinguish different types of content (e.g., text, JSON, or markdown).\n\n- **MemoryMimeType**: An enumeration that defines various types of content, facilitating the identification of content formats when adding or querying memories.\n\n- **RedisMemoryConfig**: Configures the Redis memory instance with mutable attributes such as `redis_url`, `index_name`, `prefix`, and parameters related to memory retrieval (e.g., `top_k` for specifying how many results to return).\n\n## Asynchronous Testing Functions\n\nThe module employs `pytest` and `pytest_asyncio` for conducting asynchronous testing on memory functionalities. Key tests include:\n\n- **Adding Memory Content**: The `test_redis_memory_add_with_mock` function checks if memory content can be successfully added and confirms that the underlying history mechanism is called once when adding content.\n\n- **Querying Memory Content**: The `test_redis_memory_query_with_mock` function verifies that the memory can be queried correctly and that the expected content is returned along with accurate metadata.\n\n- **Clearing Memory**: The `test_redis_memory_clear_with_mock` function checks if the memory content can be cleared successfully, validating that the clearing mechanism is called.\n\n- **Closing Memory**: The `test_redis_memory_close_with_mock` function ensures that the memory can be closed properly without leaving open connections.\n\n## Redis Availability Check\n\nA helper function, `redis_available`, checks if a Redis server is accessible at the specified URI. If it\u2019s not accessible, certain tests are skipped to prevent false negatives during the test suite execution.\n\n## Fixture Definitions for Testing\n\nSeveral fixtures are defined for different configurations of memory, including:\n\n- **semantic_config**: Returns a basic configuration for semantic memory.\n  \n- **sequential_config**: Returns a configuration tailored for sequential operations.\n  \n- **semantic_memory**: An asynchronous fixture for creating and yielding a `RedisMemory` instance based on the `semantic_config`.\n\n- **sequential_memory**: An asynchronous fixture for providing a `RedisMemory` instance based on the `sequential_config`.\n\nThese fixtures help in setting up controlled environments for testing memory functionality.\n\n## Unit Tests for Configuration Validations\n\nThe `test_memory_config` function validates the default and custom configurations of `RedisMemoryConfig`. It asserts that the default values align with expectations and raises `ValidationError` for incorrect parameters, ensuring that the configuration adheres to predefined constraints.\n\n## Integration Tests\n\nIntegration tests, such as `test_basic_workflow`, validate memory operations, confirming that:\n\n- Specific content can be added to the memory, ensuring the addition reflects in query results.\n- Queries are executed correctly against added content, capturing expected results.\n\nAdditional tests further explore content in various formats (JSON, markdown) through structured queries, confirming that the Redis memory operates correctly across varied data types and conditions.\n\n## Semantic and Sequential Memory Tests\n\nNotably, there are tests specifically assessing the functionality of:\n\n- **Sequential Memory**: The model\u2019s behavior with sequential configurations is validated through tests that ensure the last N memories are returned, independent of semantic similarity.\n\n- **Semantic Memory**: This portion ensures that memory queries accurately assess content based on context similarity, thereby allowing for more nuanced memory retrievals.\n\nOverall, the Redis Memory module is effectively designed to enable dynamic and flexible memory management through the integration of Redis as a backend store. Testing comprehensively covers expected operations, effectively ensuring the robustness of operations within the system.",
    "filename": "python/packages/autogen-ext/tests/memory/test_redis_memory.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis document outlines the functionality and testing of a Python module that employs asynchronous programming to manipulate and manage text content through a memory canvas. The module utilizes the `pytest` framework for testing, along with specific components from the `autogen_core` and `autogen_ext` libraries.\n\n## Overview\n\nThe code defines a series of tests for managing text content versioning, specifically in the context of a narrative. It allows updating and patching text stories stored in a memory-like structure, verifying the state of the canvas before and after operations.\n\nThe primary elements of the code include:\n- Creation and manipulation of text stories.\n- The ability to create, update, and patch text files programmatically.\n- Testing these functionalities using pytest fixtures and asynchronous test cases.\n\n## Fixtures\n\n### Story Fixtures\n\n1. **`story_v1`**: \n   - This fixture returns an initial version of a story titled \"The Bunny and the Sunflower.\" It is a string that serves as the base content for further manipulations and tests.\n\n2. **`story_v2`**: \n   - This fixture modifies `story_v1` by renaming the sunflower to \"Sunny.\" This small change demonstrates an update to content, mimicking a patch operation.\n\n### Memory Fixture\n\n3. **`memory`**: \n   - Returns an instance of `TextCanvasMemory`, which acts as the storage medium for the text content. This object is crucial for performing updates and patches on the content.\n\n## Test Cases\n\n### Test Case: Initial Memory State\n\n**`test_canvas_initial_state`**:\n- This test ensures that the initial state of the memory canvas does not contain any files and returns a proper snapshot of the canvas's contents. It checks that the list of files in the canvas is empty and that the contents start with the expected header.\n\n### Test Case: Creating a File\n\n**`test_update_file_tool_creates_file`**:\n- In this test, the `UpdateFileArgs` is used to create a file named \"story.md\" with the content of `story_v1`. \n- It verifies that after execution, the content in the canvas matches `story_v1` and that the file count for \"story.md\" is correct.\n\n### Test Case: Applying a Patch\n\n**`test_apply_patch_increments_revision`**:\n- This test first updates \"story.md\" with the content of `story_v1`, then generates a unified diff between `story_v1` and `story_v2`.\n- The generated diff is applied to \"story.md,\" which is expected to update the content to `story_v2` and increment the revision count of the file.\n- The test ensures that after applying the patch, the content matches `story_v2`, the revision count is accurately increased, and the patch history correctly records the operation.\n\n### Test Case: Context Update with Snapshot\n\n**`test_update_context_injects_snapshot`**:\n- This test checks if updating the context with the latest revision of \"story.md\" effectively includes a snapshot of the respective contents in a new message.\n- It verifies the message count in the `chat_ctx` and asserts that the injected text contains specific headers and the updated story content. Additionally, it ensures that the result from the memory update reflects the correct content.\n\n## Conclusion\n\nThis module effectively tests the functionalities required for managing and updating textual content in a structured format. Through the use of fixtures and asynchronous test cases, it evaluates the initial memory state, file creation, content patching, and context updates, encapsulating a robust workflow for storyline manipulations. This system could be efficiently leveraged in applications requiring version control for narrative content, showcasing rigorous testing practices in Python.",
    "filename": "python/packages/autogen-ext/tests/memory/test_text_canvas_memory.py"
  },
  {
    "code": false,
    "content": "# Documentation for Async Tests with Anthropic Client\n\nThis documentation provides an overview and explanation of the provided Python code, which consists of asynchronous test cases using the `pytest` framework. The primary focus of the tests is on the `AnthropicChatCompletionClient` from the `autogen_ext` module, where functionalities pertaining to tool invocation, streaming outputs, and working with various configurations are tested.\n\n## Overview of the Code Structure\n\nThe code includes several import statements to bring in required modules and libraries, including `pytest` for testing, `asyncio` for asynchronous functionality, and models from the `autogen_core` and `autogen_ext` libraries. \n\nThe tests defined in the script cover a variety of scenarios related to the client functionality, such as:\n- Invoking tools with specific choices.\n- Testing stream outputs and tool usage.\n- Checking cancellation capabilities and environment variable dependencies.\n\n## Test Functions\n\n### 1. `_pass_function` and `_add_numbers`\n\nThese are utility functions defined to perform basic tasks:\n- `_pass_function`: It takes a string input and returns a processed string.\n- `_add_numbers`: It accepts two integers and returns their sum.\n\nThey serve as examples of tools that can be invoked by the client.\n\n### 2. `test_mock_tool_choice_specific_tool`\n\nThis test checks that when a specific tool is chosen through the `tool_choice` parameter, the correct tool is invoked. \n\n- **Mock Clients**: It sets up a mock client to simulate API behavior.\n- **Assertions**: After calling the client to perform an operation, it asserts that the `tool_choice` used was indeed the desired tool.\n\n### 3. `test_mock_tool_choice_auto`\n\nThis function tests the automatic selection of tools by the model when `tool_choice` is set to \"auto.\" Similar structure and mock utility are utilized to verify API interactions.\n\n### 4. `test_mock_tool_choice_none`\n\nThis test verifies behavior when no tools are provided. When the `create` method is called without any tools, the expectations are set that `tool_choice` would not be included in the API call.\n\n### 5. `test_mock_tool_choice_validation_error`\n\nThis specific test validates that if a non-existing tool is referenced, it raises a `ValueError`. It checks the robustness of the client to handle inappropriate tool choices.\n\n### 6. `test_mock_serialization_api_key`\n\nIn this test, the serialization functionality of the client is checked to ensure sensitive information like API keys do not get serialized in logs or outputs.\n\n### 7. Tests for Completion Functions\n\nSeveral tests such as `test_anthropic_basic_completion`, `test_anthropic_streaming`, and `test_anthropic_tool_calling` check the basic completion responses from the Anthropic model using both direct queries and tool invocations.\n\n- **Streaming**: The `test_anthropic_streaming` function checks if the client successfully handles and returns multiple chunks of data in streaming mode.\n- **Tool Invocation**: Both tools defined earlier are invoked, and the correct responses from the tool calls are verified.\n\n## Advanced Functionality\n\n### 8. Cancellation and Multimodal Testing\n\nTests like `test_anthropic_cancellation` and `test_anthropic_multimodal` check for the cancellation of requests and multimodal capabilities, respectively. The cancellation tests ensure that requests can be cancelled gracefully without unhandled exceptions.\n\n### 9. Token Counting and Management\n\nThe `test_anthropic_token_counting` validates that the counting of tokens in messages works as expected, and it tests integration with tools to ensure correct token management.\n\n### 10. Merging System Messages\n\nSeveral tests validate the internal logic for merging multiple system messages into a cohesive output. This helps ensure that the client correctly interprets and processes combined instructions.\n\n### 11. Thinking Mode Functionality\n\nTests like `test_anthropic_thinking_mode_basic` check the new \"thinking\" mode where the model is expected to reason through queries. Further tests validate its integration with streaming and tool calling.\n\n## Conclusion \n\nThe provided code is a comprehensive set of test cases that utilize Python's `pytest` for testing functionality of the `AnthropicChatCompletionClient` from the `autogen_ext` library. With a focus on rigorous validation of API interactions, tool invocation, client behavior under various configurations, and response handling in both synchronous and asynchronous modes, this code is well-structured to ensure the reliability of the Anthropic model. \n\nThe tests serve as a guideline for developers to use the client effectively, while also showcasing potential pitfalls and areas to watch for bugs or misunderstandings in functionality.",
    "filename": "python/packages/autogen-ext/tests/models/test_anthropic_model_client.py"
  },
  {
    "code": false,
    "content": "# Documentation for Azure AI Chat Completion Client Tests\n\nThis documentation outlines the functionality and testing framework of a Python codebase designed to test the Azure AI Chat Completion Client. The tests are implemented using the `pytest` framework, and they simulate interactions with Azure's chat model, including both streaming and batch completion requests.\n\n## Overview\n\nThe primary purpose of the code is to define and validate the functionality of an Azure AI Chat Completion client, which interacts with various AI models to generate responses based on user input. The tests cover various scenarios, such as standard completions, streaming responses, function calling, and handling multiple modalities (text and images).\n\n## Mock Functions\n\n### `_mock_create_stream`\n\nThis asynchronous generator simulates the streaming response of the completion model. It yields a sequence of \"mock chunks\" of responses featuring content such as \"Hello,\" followed by similar phrases. This function helps fake delays that would occur in a real API call, making testing more realistic.\n\n```python\nasync def _mock_create_stream(*args: Any, **kwargs: Any) -> AsyncGenerator[StreamingChatCompletionsUpdate, None]\n```\n\n### `_mock_create`\n\nThis function simulates the standard model completion request. Depending on whether streaming is enabled, it either returns a mocked `ChatCompletions` object or calls the `_mock_create_stream` to return streamed responses.\n\n```python\nasync def _mock_create(*args: Any, **kwargs: Any) -> ChatCompletions | AsyncGenerator[StreamingChatCompletionsUpdate, None]\n```\n\n## Test Fixtures\n\n### `azure_client`\n\nThis pytest fixture creates an instance of `AzureAIChatCompletionClient` for tests. If the required environment variables for the Azure endpoint and API key are set, it initializes the client with those values. Otherwise, it resorts to using mocked methods defined earlier.\n\n```python\n@pytest.fixture\ndef azure_client(monkeypatch: pytest.MonkeyPatch) -> AzureAIChatCompletionClient\n```\n\n## Basic Tests\n\n### `test_azure_ai_chat_completion_client_validation`\n\nThis test ensures that the `AzureAIChatCompletionClient` raises appropriate errors when required parameters (like `endpoint`, `credential`, `model`, or `model_info`) are missing.\n\n```python\n@pytest.mark.asyncio\nasync def test_azure_ai_chat_completion_client_validation() -> None\n```\n\n### `test_azure_ai_chat_completion_client_create`\n\nThis test checks that the client can create a successful response when given valid input. It verifies that the correct logs are produced and that the output matches expectations.\n\n```python\n@pytest.mark.asyncio\nasync def test_azure_ai_chat_completion_client_create(azure_client: AzureAIChatCompletionClient, caplog: pytest.LogCaptureFixture) -> None\n```\n  \n### `test_azure_ai_chat_completion_client_create_stream`\n\nSimilar to the previous test, but specifically for validating the streaming functionality. It gathers chunks of results from the `create_stream` method and asserts the integrity of the final result.\n\n```python\n@pytest.mark.asyncio\nasync def test_azure_ai_chat_completion_client_create_stream(azure_client: AzureAIChatCompletionClient, caplog: pytest.LogCaptureFixture) -> None\n```\n\n## Advanced Tests\n\n### Function Calling and Multimodal Support\n\nSeveral tests are designed to validate the function calling functionality and multimodal support:\n\n- `test_function_calling_success`: Ensures function calling works and returns the correct function call content.\n  \n- `test_multimodal_unsupported_raises_error`: Confirms that the system raises an error when an image is provided to a model that does not support vision.\n\n- `test_multimodal_supported`: Validates that when the model does support vision, providing an image will not raise any errors.\n\n```python\n@pytest.mark.asyncio\nasync def test_function_calling_success(function_calling_client: AzureAIChatCompletionClient) -> None\n```\n\n### Tool Choice Tests\n\nVarious tests are designed to check the behavior when specific tools are chosen for function invocation:\n- `test_azure_ai_tool_choice_specific_tool`: Ensures that if a specific tool is specified, it is invoked correctly.\n\n- `test_azure_ai_tool_choice_auto`: Validates the auto selection of tools by the model.\n\n- `test_azure_ai_tool_choice_none`: Confirms that when the choice is set to 'none', the model will simply return a response without invoking any tool.\n\n- `test_azure_ai_tool_choice_required`: Ensures that if 'required' is set, the model must invoke a tool.\n\n```python\n@pytest.mark.asyncio\nasync def test_azure_ai_tool_choice_specific_tool(tool_choice_client: AzureAIChatCompletionClient) -> None\n```\n\n## Conclusion\n\nThe provided code is a comprehensive testing suite for an Azure AI Chat Completion client. It effectively uses mocking to simulate various scenarios, ensuring that the client behaves as expected under different conditions. Various features such as function calling, multimodal support, and tool selection are thoroughly validated, ensuring robustness for production use.",
    "filename": "python/packages/autogen-ext/tests/models/test_azure_ai_model_client.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code is a set of unit tests written in Python, utilizing the `pytest` framework, for testing an implementation of a chat completion caching mechanism. It primarily focuses on caching functionalities, including basic cache operations, structured output caching, token usage tracking, and more. The code involves classes and methods from an external library (specified as `autogen_core` and `autogen_ext`) that presumably define the behavior of the components being tested.\n\n## Key Components\n\n1. **Imports and Type Annotations**\n   - Several libraries such as `copy`, `json`, and `pydantic` are imported for various functionalities, including object copying, JSON manipulation, and data validation.\n   - Type annotations are provided for better code clarity and help with IDE support.\n\n2. **Functions and Test Cases**\n   - A series of functions decorated with `@pytest.mark.asyncio` indicate that they contain asynchronous test cases, essential for testing asynchronous calls in the caching behavior.\n\n### `get_test_data(num_messages: int = 3) -> Tuple[...]`\n\n- **Purpose:** This utility function generates test data for the test cases.\n- **Parameters:** \n  - `num_messages`: An integer defining how many messages and prompts to create.\n- **Returns:** A tuple containing:\n  - A list of dummy responses.\n  - A list of dummy prompts.\n  - A `SystemMessage` instance.\n  - An instance of `ReplayChatCompletionClient`, mimicking a chat completion client with predefined responses.\n  - An instance of `ChatCompletionCache`, which caches the chat completion responses.\n\n### Test Cases\n\n#### `test_cache_basic_with_args()`\n\n- **Overview:** Tests the basic caching functionality with different messages and prompts.\n- **Assertions:**\n  - Checks that the response is not cached on the first call.\n  - Validates that the correct response is returned.\n  - Ensures that the cached response is returned on a second call with the same arguments.\n  - Tests cache misses when arguments change.\n\n#### `test_cache_structured_output_with_args()`\n\n- **Overview:** Tests caching mechanisms when structured output calls are involved.\n- **Assertions:**\n  - Similar checks as the previous test, including validating whether the correct output is returned as structured data.\n  - Asserts cached responses and checks behavior when the output type changes.\n\n#### `test_cache_model_and_count_api()`\n\n- **Overview:** Validates the caching behavior for model and token counting.\n- **Assertions:**\n  - Ensures that both the replay client and cached client yield the same model info and token counts.\n\n#### `test_cache_token_usage()`\n\n- **Overview:** Examines the token usage tracking in the caching mechanism.\n- **Assertions:**\n  - Checks that token usage is being tracked correctly for requests, both cache hits and misses.\n\n#### `test_cache_create_stream()`\n\n- **Overview:** Tests the streaming functionality of the cache.\n- **Assertions:**\n  - Ensures that streaming results are consistent between calls, validating cached vs. original stream outputs.\n\n### Mock Classes\n\n#### `MockCacheStore`\n\n- **Purpose:** A mock implementation of the `CacheStore` for testing purposes.\n- **Methods:** \n  - `get`: Returns a predefined value for cache retrieval.\n  - `set`: Stores key-value pairs for testing.\n  - `_to_config` & `_from_config`: Dummy methods to comply with an interface, likely for converting the cache state to/from a configuration object.\n\n## Execution Flow and Cache Handling\n\nThe code simulates the behavior of a caching mechanism by testing multiple sequences of cached and non-cached calls. It focuses on:\n\n- **Caching Logic:** Ensuring that results from previous API calls are cached correctly, allowing for faster subsequent calls with the same arguments.\n- **Error Handling in Caching:** Testing graceful degradation when encountering errors in cache retrieval, such as deserialization issues or invalid data formats.\n- **Interoperability:** Verifying that cached results between different methods (like `create` and `create_stream`) can work interchangeably, ensuring robust cache management.\n  \n## Conclusion\n\nThis code serves as a comprehensive suite of tests for a caching system within a chat application. It utilizes mocking, asynchronous testing, and structured data validation to cover various scenarios that might arise in real-world usage. This helps ensure the proper performance of the caching logic under different conditions, maintaining responsiveness and reliability in the chat interactions.",
    "filename": "python/packages/autogen-ext/tests/models/test_chat_completion_cache.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code is a test suite designed to verify the functionality of a simulated chat completion client using the `llama_cpp` library. The tests utilize the `pytest` framework for unit testing. The main focus is on creating chat completions, validating outputs, counting tokens, and managing message types, while providing a structured framework for extending the functionality in the future.\n\n## Imports and Dependencies\n\nThe script begins with several imports which set up necessary modules and classes:\n- **Context Management and Type Hinting**: Modules like `contextlib`, `sys`, and `typing` are imported for typing hints and context management functionalities.\n- **Testing Framework**: `pytest` is included for handling unit tests.\n- **Machine Learning Framework**: `torch` is imported, presumably for handling GPU tasks.\n- **Data Models**: The script imports specific data models from `autogen_core.models` and defines a `BaseModel`-derived class, `AgentResponse`, to structure responses from agents.\n\nThe script includes a conditional import for `ChatCompletionMessageToolCalls` from `llama_cpp`, which is only attempted if the module is available, otherwise, the tests that depend on it are skipped.\n\n## AgentResponse Class\n\n### Purpose\nThe `AgentResponse` class is designed to encapsulate the response from a chat agent, consisting of `thoughts` and `content`. This class extends `BaseModel` from Pydantic, providing validation and serialization capabilities.\n\n### Role\n- **Thoughts**: This field captures the internal reflections of the agent.\n- **Content**: Contains the main response text provided by the agent.\n\n## FakeLlama Class\n\n### Purpose\n`FakeLlama` is a placeholder implementation of a language model to simulate responses for testing purposes without needing access to the actual `llama_cpp` provider.\n\n### Key Methods\n- **`__init__`**: Initializes the class with a model path and defines a default context size.\n- **`tokenize`**: A simple method that converts bytes to a list of integers, helping to mock tokenization.\n- **`create_chat_completion`**: Simulates the generation of a chat completion based on input messages while providing controlled outputs, either in a simple fake format or a structured response based on `AgentResponse`.\n- **`__call__`**: Simulates streaming outputs asynchronously, yielding a predefined greeting.\n\n## Fixture: get_completion_client\n\n### Purpose\nThe `get_completion_client` fixture uses `pytest`'s fixture functionality to setup and teardown a context manager that simulates the Llama client for testing.\n\n### Functionality\n- **Monkey Patching**: It temporarily replaces the `Llama` class with the `FakeLlama` class for the duration of the tests.\n- **Setup and Teardown**: Ensures that the environment is reset by removing the monkey-patched modules after the tests complete.\n\n## Tests Overview\n\nThe script contains several asynchronous test functions to validate the behavior of the chat completion client.\n\n### Test Function Descriptions\n1. **test_llama_cpp_create**: Tests the creation of chat completions from the client, asserting the expected fake response and token usage.\n  \n2. **test_llama_cpp_create_structured_output**: Similar to the previous test but validates the structured response format using the `AgentResponse` class.\n\n3. **test_create_invalid_message**: Checks that a `ValueError` is raised when an unsupported message type (like an integer) is passed to the client.\n\n4. **test_count_and_remaining_tokens**: Validates the functionality of counting tokens and calculating the remaining tokens based on a given message.\n\n5. **test_llama_cpp_integration_non_streaming**: Tests the integration of the chat completion client with actual model parameters, ensuring it can create responses from user messages.\n\n6. **test_llama_cpp_integration_non_streaming_structured_output**: Similar to the previous integration test but checks for structured output using `AgentResponse`.\n\n### Commented Out Tests\nThe script includes several commented-out test functions that indicate planned future functionality, such as testing streaming responses and tool uses (e.g., for mathematical operations). These functions hint at the extensibility of the codebase.\n\n## Conclusion\n\nThis code serves as a comprehensive test suite for a chat completion client, allowing validation of both simulated and real interactions with a language model. The use of mock objects, structured response validation, and targeted error handling illustrates a robust testing strategy that ensures reliability and ease of extension for future capabilities.",
    "filename": "python/packages/autogen-ext/tests/models/test_llama_cpp_model_client.py"
  },
  {
    "code": false,
    "content": "# OllamaChatCompletionClient Tests Documentation\n\nThis document provides a high-level description of the functionality of the code, focusing on testing the `OllamaChatCompletionClient` class from the `autogen_ext.models.ollama` module and its interactions with the underlying `AsyncClient` for chat operations.\n\n## Overview\n\nThe code defines a series of test cases that validate the functionality and robustness of the `OllamaChatCompletionClient`, which is designed to handle interactions with a language model using an HTTP client. Various tests ensure that the client can successfully create chat interactions, handle streaming responses, and manage tools that augment its capabilities. The tests take advantage of the `pytest` framework, especially `pytest-asyncio`, to handle asynchronous code effectively.\n\n## Mocking HTTP Requests\n\nThe test cases utilize the `monkeypatch` fixture from `pytest` to mock the behavior of the `AsyncClient`. This is done to avoid making actual HTTP calls during testing, which significantly improves test performance and reliability. A helper function `_mock_request` simulates an HTTP response, returning a successful status with a predefined result.\n\n### Mock Request Function\n```python\ndef _mock_request(*args: Any, **kwargs: Any) -> Response:\n    return Response(status_code=200, content=\"{'response': 'Hello world!'}\")\n```\n\n## Test Cases\n\n### 1. Client Configuration and Argument Handling\n**Test: `test_ollama_chat_completion_client_doesnt_error_with_host_kwarg`**\n\nThis test checks whether the `OllamaChatCompletionClient` correctly processes the `host` keyword argument and ensures that unexpected keyword arguments do not lead to errors during the `create` method invocation. The client is instantiated with a model and a host URL, and the test confirms that the `create` method can be called without raising a `TypeError`.\n\n### 2. Argument Validation from Config\n**Test: `test_create_args_from_config_drops_unexpected_kwargs`**\n\nIn this test, a configuration dictionary with several valid and invalid arguments is passed to the client. The test asserts that the resulting arguments do not contain any unexpected keys, ensuring the client can effectively filter configurations.\n\n### 3. Successful Chat Completion\n**Test: `test_create`**\n\nThis test verifies that the `create` method of the client can generate a proper response from the model. The `chat` method of the mocked `AsyncClient` simulates returning a specific message content. The test checks logs to ensure the expected logging behavior occurs, along with validating the structure of the returned result, focusing on the content and usage metrics.\n\n### 4. Streaming Responses\n**Test: `test_create_stream`**\n\nThe test ensures that the client can handle streaming responses properly by checking if messages are returned in chunks from the model. The mock simulates a streaming API return, and the test validates the entire result structure across received chunks, ensuring the final content matches expectations.\n\n### 5. Tool Handling in Chat Messages\n**Test: `test_create_tools`**\n\nThis test assesses the client's capability to integrate functional tools. A simple 'add' function is defined and registered as a tool. The test simulates a chat that invokes this tool, checking if the content returned from the model indicates a function call and contains the expected arguments.\n\n### 6. Handling of Structured Outputs\n**Test: `test_create_structured_output`**\n\nThis test evaluates the `create` method with a structured output specified by the user. It ensures that the model can correctly output JSON-formatted responses that conform to the defined Pydantic model.\n\n## Fixture Management\n\n### Ollama Client Fixture\n**Fixture: `ollama_client`**\n\nThe provided fixture sets up an instance of `OllamaChatCompletionClient` that can be reused across tests. It ensures the model is available locally before conducting tests. If the model is not available, the tests using this fixture will be skipped.\n\n## Parameterized Tests\n\nSeveral tests, such as `test_ollama_create` and `test_ollama_create_structured_output`, utilize `pytest.mark.parametrize` to execute the same test logic across different model configurations. This approach aids in validating the robustness of the client with various model settings.\n\n## Conclusion\n\nThis test suite comprehensively checks the `OllamaChatCompletionClient` class's capabilities, ensuring it operates correctly under different scenarios. Tests cover configuration validation, response generation, and tool integration, all essential for maintaining high-quality AI interactions. The mocking of HTTP requests allows for quick testing and isolation of behaviors without relying on external services.",
    "filename": "python/packages/autogen-ext/tests/models/test_ollama_chat_completion_client.py"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis code provides a complex asynchronous implementation for an AI chat completion client, often interfacing with models from OpenAI and other AI systems. It utilizes various features such as streaming responses, tool calling, and handling multimodal messages (images, text, etc.). The code heavily employs asynchronous programming and incorporates robust testing strategies using pytest.\n\n## Key Components\n\n### Imports\n\nThe code begins with multiple import statements that include:\n- **Standard Libraries**: `asyncio`, `json`, `logging`, `os`.\n- **Type Hinting**: Various typing utilities like `Annotated`, `Any`, `Dict`, `List`, `Literal`, `Optional`, and custom `TypeVar`.\n- **Testing Libraries**: `pytest` and `unittest.mock` for testing purposes.\n- **AI Models and Functions**: Classes and functions from `autogen` and `openai`, facilitating interaction with AI models and completion methods.\n\n### Function Definitions\n\n1. **Utility Functions**:\n   - `_pass_function`, `_fail_function`, and `_echo_function` are simple pass-through or dummy functions for testing purposes.\n\n2. **Mock Functions**:\n   - Areas such as `_mock_create_stream` mimics streaming responses where it produces chunks that simulate the output of an AI model.\n   - `_mock_create` configured to produce mock completions with or without streaming based on the parameters passed.\n\n3. **Mock Data Classes**:\n   - Classes such as `MockChunkDefinition` and `MockChunkEvent` are defined to encapsulate structures that are employed for mocking.\n\n### Classes\n\n1. **BaseModel Subclasses**:\n   - For example, `MyResult` and `MyArgs`, these are built using Pydantic to create structured schemas for argument and result validation used within the system.\n\n### Asynchronous Client\n\n- The main functionality centers around the `OpenAIChatCompletionClient` class, designed to interact with models via chat completion methods. The corresponding test cases ensure that the client handles various situations like invalid input, multi-modal messages, and tool calling correctly.\n\n## Testing Suite\n\n### Test Functions\n\nThe code contains numerous pytest test functions that validate different scopes of functionality:\n\n1. **Client Creation Tests**:\n   - Various tests confirm that the client instantiates correctly with valid models and raises exceptions for invalid setups. This includes model-specific configurations and capabilities.\n\n2. **Stream and Asynchronous Functionality**:\n   - Tests confirm how well the client handles both synchronous and asynchronous responses, including mocked streams to ensure expected behavior across various scenarios, such as including usage metrics and handling cancellation tokens.\n\n3. **Tool Calling Tests**:\n   - Validate the calling of additional functionalities as tools, verifying proper operations when model responses dictate output completion based on predetermined tools.\n\n4. **Structural Verification**:\n   - Tests ensure that structured data from models is validated and that any intended outputs retain their integrity. This includes handling structured outputs based on input formats.\n\n5. **Parameterized Tests**:\n   - Several functionalities are tested with different model configurations to ensure consistent behavior and responses from the client.\n\n## Mocking and Integration\n\n### Mocking Tools\n\n- The tests extensively utilize mocks to simulate responses for external dependencies without attempting actual network calls.\n\n### Integration Tests\n\n- Some tests check the full integration with the OpenAI API, including real requests to validate tool choices, responses, and error handling.\n\n## Error Handling\n\n- Multiple test functions ensure that the client gracefully handles unexpected scenarios, such as missing API keys or receiving malformed responses.\n\n### Specific Cases\n\n- The implementation is robust enough to cover a multitude of identity sources for each message (e.g., user or assistant), capturing and asserting the intention behind user interactions with the AI model. \n\n## Conclusion\n\nThis code is a comprehensive framework for building, testing, and executing responses from AI chat models, supporting advanced features like streaming responses, error handling, and integration with various toolsets. The extensive test coverage demonstrates a solid commitment to quality assurance within the application.",
    "filename": "python/packages/autogen-ext/tests/models/test_openai_model_client.py"
  },
  {
    "code": false,
    "content": "# LLM Agent Code Overview\n\nThis document provides a detailed overview of the source code implementing a simple LLM (Large Language Model) agent using asynchronous messaging. The code utilizes dependencies from the `autogen_core` and `autogen_ext` libraries, specifically for agent-based message handling and client interactions.\n\n## Dependencies\n\n### Import Statements\n\nThe code begins by importing various modules and classes, such as:\n\n- **Standard Libraries**: `copy` for object copying, `dataclass` for structured data storage, and `List` for type hinting.\n- **Testing Framework**: `pytest` is imported to facilitate the testing of the code's functionality.\n- **Agent Framework**: Several entities from `autogen_core` (like `AgentId`, `MessageContext`, `RoutedAgent`) and models from `autogen_core.models` and `autogen_ext.models.replay`, which facilitate chat completion handling and response replay capabilities.\n\n## Data Class and LLM Agent Definition\n\n### ContentMessage Data Class\n\n```python\n@dataclass\nclass ContentMessage:\n    content: str\n```\n\nThis simple data class structures the messages that the LLM Agent will process, encapsulating a single string field, `content`.\n\n### LLMAgent Class\n\n```python\nclass LLMAgent(RoutedAgent):\n    ...\n```\n\nThe `LLMAgent` class extends `RoutedAgent`, allowing it to route messages within an agent-based architecture. Key attributes and methods of this class include:\n\n#### Constructor\n\n- Initializes the agent with a given `model_client`, which will handle the interaction with a large language model.\n- Maintains a chat history using a list of `ContentMessage`.\n- Tracks the number of calls made (`num_calls`) to the model client.\n\n#### Message Handling\n\n```python\n@message_handler\nasync def on_new_message(self, message: ContentMessage, ctx: MessageContext) -> None:\n    ...\n```\n\n- The `on_new_message` method is decorated with `@message_handler`, registering it to handle new messages. Upon receiving a `ContentMessage`, it appends it to the chat history and increments the call count. \n- The method then calls the `model_client` to generate a response based on the message history. If the response is valid (a string), it publishes this message to a default topic; otherwise, it raises a `TypeError`.\n\n#### Fixed Message History Property\n\n```python\n@property\ndef _fixed_message_history_type(self) -> List[SystemMessage]:\n    ...\n```\n\nThis property returns the chat history in a format suitable for the model client, converting `ContentMessage` instances into `SystemMessage` instances.\n\n## Default Subscription Agent\n\n### LLMAgentWithDefaultSubscription Class\n\n```python\n@default_subscription\nclass LLMAgentWithDefaultSubscription(LLMAgent): ...\n```\n\nThis class extends `LLMAgent` and is decorated with `@default_subscription`, enabling automatic subscription capabilities for the agent. It does not introduce any new functionality but instead leverages the base class capabilities.\n\n## Test Suite for the LLM Agent\n\n### Test Cases\n\nMultiple asynchronous test cases are defined using the `pytest` framework to validate the behavior of the messaging system and the LLM agent.\n\n1. **Testing Replay Chat Completion Client**:\n\n```python\n@pytest.mark.asyncio\nasync def test_replay_chat_completion_client() -> None:\n    ...\n```\n\n   - This test ensures that the `ReplayChatCompletionClient` delivers responses in the expected order. Once all responses are exhausted, an error is appropriately raised.\n\n2. **Testing Streaming Completion**:\n\n```python\n@pytest.mark.asyncio\nasync def test_replay_chat_completion_client_create_stream() -> None:\n    ...\n```\n\n   - It verifies that responses from the streaming API correctly match the anticipated messages while preserving order and integrity.\n\n3. **Testing Message Registration**:\n\n```python\n@pytest.mark.asyncio\nasync def test_register_receives_publish_llm() -> None:\n    ...\n```\n\n   - This test evaluates the registration process for different agents and confirms they correctly receive and process published messages.\n\n4. **Token Count Logic**:\n\n```python\n@pytest.mark.asyncio\nasync def test_token_count_logics() -> None:\n    ...\n```\n\n   - Checks the token counting mechanism and the remaining token logic through the replay model client. It guarantees tokens utilized during message creation are tracked correctly.\n\n5. **Testing Reset Functionality**:\n\n```python\n@pytest.mark.asyncio\nasync def test_replay_chat_completion_client_reset() -> None:\n    ...\n```\n\n   - Confirms that resetting the `ReplayChatCompletionClient` allows the user to retrieve responses anew after exhausting the initial queue.\n\n## Conclusion\n\nThis code provides a foundational structure for an LLM agent equipped with message handling, chat history management, and a testing suite to ensure robust functionality. By leveraging asynchronous programming and established design patterns, the code establishes a framework capable of handling interactive messaging scenarios effectively. Further enhancements may be built upon this foundation by expanding client capabilities or refining agent interactions.",
    "filename": "python/packages/autogen-ext/tests/models/test_reply_chat_completion_client.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis code is structured as a set of tests utilizing the `pytest` framework to validate various features of a chat completion adapter that integrates with the Azure OpenAI service. The main functionality revolves around creating a mock chat assistant that can handle both simple textual prompts and function calls, particularly a calculator operation. This is achieved by defining tools and methods to simulate interactions with the chat completion model, including both synchronous and asynchronous operations. \n\n## Imports and Dependencies\n\nThe code starts with importing several libraries and modules essential for its operations. Key imports include:\n\n- **Logging**: Used for capturing logs.\n- **OS**: Provides functionalities to interact with the operating system.\n- **Typing**: Useful for type hinting in function definitions.\n- **Pytest**: A testing framework for Python to facilitate the creation of unit tests and fixtures.\n- **Pydantic**: Used for data validation with models defined using Python type annotations.\n- **Custom Modules**: Multiple modules from `autogen_core`, `autogen_ext`, and `semantic_kernel` are imported. These provide essential classes and functions for chat functionalities, models, tools, and memory management.\n\n## Calculator Tool Definition\n\nThe `CalculatorTool` class is defined, inheriting from a base class `BaseTool`. This tool is responsible for performing addition operations. The main components include:\n\n- **CalculatorArgs**: A Pydantic model that defines the input arguments expected by the tool (two float numbers).\n- **CalculatorResult**: Another Pydantic model that defines the output format, which is a single float number as the result of the addition.\n- **run Method**: The core method of the class that performs the addition of two numbers passed as arguments and returns a `CalculatorResult` instance.\n\n### Purpose\n\nThe purpose of the `CalculatorTool` is to provide a straightforward example of a function that the chat assistant can call based on user queries. It allows the application to execute arithmetic operations within the chat conversation flow.\n\n## Fixture for Mocking Azure Client\n\nThe `sk_client` function is defined as a pytest fixture to provide a mock Azure Chat Completion client. The mock client simulates responses based on specific user prompts:\n\n- **Azure Configuration**: The fixture retrieves Azure OpenAI service configurations from environment variables (deployment name, endpoint, and API key).\n- **Mock Functions**: Two mocked functions, `mock_get_chat_message_contents` and `mock_get_streaming_chat_message_contents`, simulate how the Azure chat service would respond when queried with specific messages. Depending on the input, either a calculator function call or a simple greeting will be returned.\n\n### Purpose\n\nThis fixture ensures that the tests can run independently of actual external services, maintaining a predictable and controlled environment for testing different functionalities.\n\n## Testing Chat Completion with Tools\n\nSeveral test functions are defined using the `pytest.mark.asyncio` decorator to enable asynchronous tests. Each function tests a different aspect of the chat completion features.\n\n### Test Function Descriptions\n\n1. **test_sk_chat_completion_with_tools**:\n   - Tests whether the adapter can respond correctly when the calculator tool is invoked with a valid query (e.g., \"What is 2 + 2?\").\n\n2. **test_sk_chat_completion_with_prompt_tools**:\n   - Similar to the previous test but validating using a different tool schema instead. It ensures that the tool can also handle parameters defined through a Pydantic schema.\n\n3. **test_sk_chat_completion_without_tools**:\n   - Verifies that the chat assistant can handle plain text input without invoking any function tools and produces the expected greeting.\n\n4. **test_sk_chat_completion_stream_with_tools**:\n   - Tests the streaming capability of chat responses when using tools, ensuring multiple pieces of output can be managed.\n\n5. **test_sk_chat_completion_stream_without_tools**:\n   - Similar to the previous test but without invoking tools, confirming that the output is still valid under streaming conditions.\n\n6. **test_sk_chat_completion_default_model_info** and **test_sk_chat_completion_custom_model_info**:\n   - Validate the retrieval of default and custom model information from the adapter, making sure settings are correctly applied.\n\n### Purpose\n\nThese tests collectively ensure that the chat assistant can manage both querying and functional execution paths, providing confidence in the behavior of the system under various scenarios.\n\n## Handling Function Calls and Results\n\nTwo additional test functions focus on validating the behavior of the system when function calls and execution results are involved:\n\n1. **test_sk_chat_completion_with_function_call_and_execution_result_messages**:\n   - Validates how the assistant manages a conversation that includes both function calls (to the calculator tool) and their results, ensuring it correctly constructs and submits the intended chat history for processing.\n\n2. **test_sk_chat_completion_stream_with_multiple_function_calls**:\n   - Tests streaming functionality while managing multiple distinct function calls. It checks if function call information (like arguments and identifiers) is correctly merged in the response.\n\n### Purpose\n\nThese tests check that function call messages and execution results are processed correctly, confirming that the assistant can handle complex interactions involving multiple tools and responses concisely.\n\n## Conclusion\n\nThe overall structure of the code represents a comprehensive testing suite for a chat assistant that leverages Azure's capabilities combined with custom tools, particularly arithmetic functions. It emphasizes modularity through the use of classes for tools, interaction patterns, and thorough test coverage for various use cases, ensuring robustness and reliability in the production environment.",
    "filename": "python/packages/autogen-ext/tests/models/test_sk_chat_completion_adapter.py"
  },
  {
    "code": false,
    "content": "# Documentation for `test_parse_r1_content`\n\n## Overview\n\nThe provided code snippet is a set of unit tests written using the `pytest` framework, designed to validate the functionality of the `parse_r1_content` function from the `autogen_ext.models._utils.parse_r1_content` module. The purpose of these tests is to ensure that the `parse_r1_content` function accurately extracts a \"thought\" enclosed in `<think>` tags from a string while also handling various edge cases, such as malformed content and missing tags.\n\n## Test Function\n\n### `test_parse_r1_content()`\n\nThis is the primary test function defined in the code. It conducts several assertions to verify the behavior of the `parse_r1_content(content)` function. Below is a detailed breakdown of what this function does:\n\n#### Step 1: Valid Input Test\n\n```python\ncontent = \"Hello, <think>world</think> How are you?\"\nthought, content = parse_r1_content(content)\nassert thought == \"world\"\nassert content == \"How are you?\"\n```\n\n- **Purpose**: This initial test provides a correctly formatted string that contains a `<think>` tag.\n- **Expected Outcome**: It asserts that `thought` should be \"world\" (the content within `<think>` tags) and that `content` should be the remaining string \"How are you?\" after parsing.\n\n#### Step 2: Missing Opening Tag\n\n```python\nwith pytest.warns(\n    UserWarning,\n    match=\"Could not find <think>..</think> field in model response content. No thought was extracted.\",\n):\n    content = \"Hello, world How are you?\"\n    thought, content = parse_r1_content(content)\n    assert thought is None\n    assert content == \"Hello, world How are you?\"\n```\n\n- **Purpose**: This test checks how the function handles a string that does not contain an opening `<think>` tag.\n- **Expected Outcome**: A warning is triggered indicating no thought was extracted, and `thought` should be `None` while `content` remains unchanged.\n\n#### Step 3: Malformed Opening Tag\n\n```python\nwith pytest.warns(\n    UserWarning,\n    match=\"Could not find <think>..</think> field in model response content. No thought was extracted.\",\n):\n    content = \"Hello, <think>world How are you?\"\n    thought, content = parse_r1_content(content)\n    assert thought is None\n    assert content == \"Hello, <think>world How are you?\"\n```\n\n- **Purpose**: This test evaluates how the parser deals with a string that has an opening `<think>` tag but is missing a closing tag.\n- **Expected Outcome**: Similar to the previous test, it expects a warning, `thought` to be `None`, and the unchanged original `content`.\n\n#### Step 4: Closing Tag Before Opening Tag\n\n```python\nwith pytest.warns(\n    UserWarning, match=\"Found </think> before <think> in model response content. No thought was extracted.\"\n):\n    content = \"</think>Hello, <think>world</think>\"\n    thought, content = parse_r1_content(content)\n    assert thought is None\n    assert content == \"</think>Hello, <think>world</think>\"\n```\n\n- **Purpose**: This test examines a scenario where a closing `</think>` tag appears before an opening `<think>` tag.\n- **Expected Outcome**: It expects a warning and that `thought` is `None`. The `content` remains the same as the input.\n\n#### Step 5: Malformed Closing Tag\n\n```python\nwith pytest.warns(\n    UserWarning, match=\"Found </think> before <think> in model response content. No thought was extracted.\"\n):\n    content = \"</think>Hello, <think>world\"\n    thought, content = parse_r1_content(content)\n    assert thought is None\n    assert content == \"</think>Hello, <think>world\"\n```\n\n- **Purpose**: This test checks the function's behavior when a closing tag appears before the corresponding opening tag, but also when the string contains additional text afterwards.\n- **Expected Outcome**: Again, it expects a warning signifying that thought extraction failed, with `thought` as `None` and the original `content` unchanged.\n\n## Conclusion\n\nThe `test_parse_r1_content()` function is meticulously designed to test the resilience of the `parse_r1_content` function against various input scenarios. Each segment of the function addresses specific edge cases, ensuring not only that valid inputs return expected results but also that invalid inputs are gracefully handled by issuing warnings while maintaining the integrity of untouched parts of the original string. This contributes to the robustness of the underlying `parse_r1_content` function in handling user input effectively.",
    "filename": "python/packages/autogen-ext/tests/models/test_utils.py"
  },
  {
    "code": false,
    "content": "# Protobuf Serialization Test Documentation\n\nThis document provides an overview of the generated Python code from the given Protocol Buffers (protobuf) definition file, `serialization_test.proto`. The code defines data structures that can be serialized and deserialized using the Protocol Buffers mechanism. \n\n## Overview\n\nThe code is auto-generated by the Protocol Buffers compiler, and thus it should not be edited manually. It provides a Python module that defines two main message types: `ProtoMessage` and `NestingProtoMessage`. These messages can be used for serialization and deserialization of structured data, compatible with the protobuf framework.\n\n## Import Statements\n\nThe beginning of the code contains import statements necessary for the protobuf functionality:\n- **descriptor**: Used for defining the structure of protobuf messages.\n- **descriptor_pool**: Allows access to a pool of descriptors.\n- **symbol_database**: Manages access to the mapping of symbols in protobuf, ensuring that the generated code integrates smoothly with dynamic typing.\n- **builder**: Helps in constructing message and enumeration descriptors.\n\nThese imports facilitate the creation and management of protobuf data structures.\n\n## Runtime Version Check\n\nThe code includes a check to validate the version of the protobuf runtime being used. This is crucial to ensure compatibility and correct functionality of the generated code. The check verifies that the runtime version matches the expected version (5.29.0 in this case) before proceeding.\n\n```python\n_runtime_version.ValidateProtobufRuntimeVersion(\n    _runtime_version.Domain.PUBLIC,\n    5,\n    29,\n    0,\n    '',\n    'serialization_test.proto'\n)\n```\n\n## Descriptor Initialization\n\nThe code initializes a descriptor, which is a foundational construct in Protocol Buffers that defines a specific message format. This involves adding a serialized representation of the `serialization_test.proto` file to a descriptor pool, making it available for further processing and constructing message objects.\n\n```python\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x18serialization_test.proto\\x12\\x06\\x61gents...')\n```\n\nThe messages defined in the protobuf file are expressed in binary form, which is handled when instantiated.\n\n## Message Definitions\n\n### ProtoMessage\n\nThe `ProtoMessage` is a simple message type that contains a single field:\n\n```protobuf\nmessage ProtoMessage {\n  string message = 1;\n}\n```\n\n- **Field `message`**: This is a string field designated to hold textual data. The field is assigned the tag number 1, which is used for serialization.\n\n### NestingProtoMessage\n\nThe `NestingProtoMessage` is a more complex message that includes both a string field and a nested message:\n\n```protobuf\nmessage NestingProtoMessage {\n  string message = 1;\n  ProtoMessage nested = 2;\n}\n```\n\n- **Field `message`**: Similar to `ProtoMessage`, this string field holds text.\n- **Field `nested`**: This field is of type `ProtoMessage`, allowing you to nest messages. It enables creating hierarchical structures in your data models.\n\n## Globals and Descriptor Building\n\nThe code utilizes the `_globals` dictionary to maintain references to the generated message classes and enumerations. Following the definition of descriptors, the `BuildMessageAndEnumDescriptors` function is called to construct the actual message classes based on the specified descriptors.\n\n```python\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n```\n\n## Protobuf Code Generation Structure\n\nThe generated code provides data types that are compatible with the Protocol Buffers format. It constructs adequate serialized message representations, which can easily be converted to and from binary form for efficient data transfer and storage. This structure allows developers to interact with defined messages while controlling serialization mechanics, providing better performance and consistency.\n\n## Conclusion\n\nThis documentation outlines the purpose and structure of the generated protobuf module derived from `serialization_test.proto`. The module defines two key message types \u2014 `ProtoMessage` and `NestingProtoMessage` \u2014 which facilitate serialization of structured data. This design is crucial for developers working with protobuf, as it ensures that complex data can be efficiently transmitted and stored while maintaining strict typing and structure.",
    "filename": "python/packages/autogen-ext/tests/protos/serialization_test_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Generated Code Check\n\nThis Python script serves as a safety check to ensure compatibility between the installed `grpc` version and the version specified for the generated gRPC classes. The script is auto-generated by the gRPC Python protocol compiler, and it is imperative that it remains unaltered.\n\n## Purpose of the Script\n\nThe primary purpose of this script is to validate the version of the `grpc` library currently installed against the version required by the auto-generated gRPC classes. If there is a discrepancy, it raises an error, providing guidance on how to resolve the issue. This helps to prevent runtime errors and ensures that any changes in the library version do not lead to unexpected behavior in applications using these generated classes.\n\n## Import Statements\n\n```python\nimport grpc\nimport warnings\n```\n\n### Description\nThe script begins by importing the necessary modules: `grpc` and `warnings`. The `grpc` module is crucial for the functionality of gRPC services, while `warnings` could potentially be used to manage warning outputs; however, in this script, it is not utilized.\n\n## Version Variables\n\n```python\nGRPC_GENERATED_VERSION = '1.70.0'\nGRPC_VERSION = grpc.__version__\n```\n\n### Description\nHere, the script establishes two version variables:\n- `GRPC_GENERATED_VERSION`: This holds the version of gRPC that the generated code is compatible with.\n- `GRPC_VERSION`: This retrieves the version of the gRPC library currently installed.\n\n## Version Check Logic\n\n```python\n_version_not_supported = False\ntry:\n    from grpc._utilities import first_version_is_lower\n    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n    _version_not_supported = True\n```\n\n### Description\nIn this section, the script attempts to import a utility function called `first_version_is_lower` from the `grpc._utilities` module. This function checks if the installed version of gRPC is lower than the required version.\n\n- If the import is successful, it conducts a check for compatibility, setting `_version_not_supported` accordingly.\n- If the import fails (if the utilities are not available), it assumes that the version is not supported by setting `_version_not_supported` to `True`.\n\n## Error Handling\n\n```python\nif _version_not_supported:\n    raise RuntimeError(\n        f'The grpc package installed is at version {GRPC_VERSION},'\n        + f' but the generated code in serialization_test_pb2_grpc.py depends on'\n        + f' grpcio>={GRPC_GENERATED_VERSION}.'\n        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n    )\n```\n\n### Description\nThe final section of the script performs the critical action of raising a `RuntimeError` if the version check indicates that the installed gRPC version does not meet the requirements. The error message provides:\n- The current installed version.\n- The version requirement.\n- Suggestions for resolution, either by upgrading or downgrading relevant packages.\n\nThis section helps ensure that users are informed of any actions necessary to rectify compatibility issues, facilitating a smoother development experience.\n\n## Summary\n\nIn conclusion, this script is a crucial component of any project utilizing gRPC in Python. By verifying the version compatibility of the gRPC package, it effectively prevents runtime errors that could arise from using incompatible versions. The automatic checks and informative error messages enhance the robustness and usability of generated gRPC services. Developers should ensure that this file remains unchanged to maintain its integrity and functionality.",
    "filename": "python/packages/autogen-ext/tests/protos/serialization_test_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "# Overview of Async Test Code for Chat Completion Client Recorder\n\nThis code defines a set of asynchronous tests using the `pytest` framework to validate the functionality of a chat completion client recorder. The primary focus is on two modes of operation: **record** and **replay**. The tests utilize mock clients and a logging utility, ensuring that interactions with a chat service are properly documented and can be replayed or validated against expected outcomes.\n\n## Imports\n\nThe code imports necessary libraries and components:\n\n- **asyncio**: To run asynchronous operations.\n- **pytest**: For testing framework functionalities and async tests.\n- Various classes from `autogen_core` and `autogen_ext` modules:\n  - `CreateResult`: Likely an object capturing the result of a chat message creation.\n  - `UserMessage`: Represents a message sent by the user.\n  - `PageLogger`: A utility for logging.\n  - `ChatCompletionClientRecorder`: Handles the recording of interactions with a chat completion client.\n  - `ReplayChatCompletionClient`: A mock client used for replaying recorded responses.\n\n## Test Function: `test_record`\n\n### Description\nThe `test_record` function verifies that when operating in **record mode**, the `create()` method of the `ChatCompletionClientRecorder` correctly records user interactions and saves these to disk when finalized.\n\n### Step-by-step Breakdown\n\n1. **Logger Creation**: Initializes a `PageLogger` instance for logging the function's execution at a DEBUG level. The log output is directed to a specified log file.\n\n2. **Mock Client Setup**: A mock chat completion client is created using `ReplayChatCompletionClient` with a predefined response to simulate chat interactions.\n\n3. **Recorder Initialization**: A `ChatCompletionClientRecorder` is instantiated in `record` mode, taking the mock client, session file path, and logger as arguments.\n\n4. **Message Creation**: A user message is created using the `UserMessage` class.\n\n5. **Response Generation**: The recorder's `create()` method is called with the user message. The function should return a `CreateResult` containing the expected response.\n\n6. **Assertions**: The type of response is checked to ensure it is `CreateResult`, and the content matches the expected response.\n\n7. **Finalization**: The `finalize()` method is called on the recorder, which presumably commits the recorded data to disk.\n\n8. **Logging**: Logs the exit from the function.\n\n## Test Function: `test_replay`\n\n### Description\nThe `test_replay` function verifies that the `create()` method can successfully replay recorded interactions when in **replay mode**. It checks that matching messages return the recorded response and raises an error upon mismatch or depletion of records.\n\n### Step-by-step Breakdown\n\n1. **Logger Creation**: Similar to `test_record`, initializes a `PageLogger` for DEBUG-level logging to a log file.\n\n2. **Mock Client Setup**: A `ReplayChatCompletionClient` is created with a response that is expected to not be returned based on the incoming messages.\n\n3. **Recorder Initialization**: A `ChatCompletionClientRecorder` is created in `replay` mode with the mock client, session file path, and logger.\n\n4. **Message Creation**: A user message identical to that in `test_record` is created.\n\n5. **Response Generation**: The recorder's `create()` method is invoked with the user message. It attempts to return the corresponding response from the recorded session based on incoming messages.\n\n6. **Assertions**: The response type is validated as being `CreateResult`, and the content should match the expected recorded response.\n\n7. **Finalization**: Calls the `finalize()` method on the recorder to complete the interaction.\n\n8. **Logging**: Logs the exit from the function.\n\n## Main Execution\n\nThe code execution begins in the `if __name__ == \"__main__\"` block:\n\n- The `asyncio.run(test_record())` statement triggers the `test_record` function to run, ensuring that it executes in an event loop appropriate for asynchronous operations.\n- The `asyncio.run(test_replay())` next runs the `test_replay` function within the same event loop context.\n\nThis setup allows for both tests to execute sequentially within their asynchronous environment, both validating critical functionalities of the chat completion recording and replaying mechanisms. Each test ensures the integrity of interactions with the system through clear expectations and assertions on the results.",
    "filename": "python/packages/autogen-ext/tests/task_centric_memory/test_chat_completion_client_recorder.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Evaluation Script for Learning from Demonstrations\n\nThis script evaluates the capability of an agent to learn from demonstrations within a task-centric memory framework. It connects various components including an agent (Apprentice), a client for chat-based completions, and logging utilities, enabling a seamless interaction without modifying the core agent's logic. Below is a structured breakdown of the functionality and flow of the code.\n\n## Overview\n\nThe script is designed to run an evaluation that involves:\n1. Performing a series of reasoning tasks where the agent typically struggles.\n2. Providing a demonstration of a similar task to enhance the agent's understanding.\n3. Testing the agent again to see if it can utilize the gained knowledge from the demonstration.\n\n### Key Imports\n- Various modules including `asyncio`, `pytest`, and model components from `autogen_core` and `autogen_ext` are imported for the implementation.\n- Utility functions like `create_oai_client` and `load_yaml_file` facilitate the creation and loading of configuration details and client setups.\n\n## Main Evaluation Function\n\n### `eval_learning_from_demonstration`\n\n#### Purpose\nThis core asynchronous function evaluates the learning ability of the agent (Apprentice) by executing a set of tasks before and after providing a demonstration.\n\n#### Parameters\n- `apprentice`: An instance of the `Apprentice` class responsible for performing tasks.\n- `client`: An instance of the `ChatCompletionClient` to facilitate chat interactions.\n- `logger`: A utility for logging information and results during the function execution.\n- `config`: A dictionary containing configuration settings such as file paths and parameters.\n\n#### Functionality\n1. **Initialize Logging**: Logs the entry into the function using the `logger`.\n2. **Load Configuration Data**: Retrieves the main task, expected answers, demonstration tasks, and solutions from YAML configuration files for easy modification.\n3. **Baselining**: Clears the agent's memory and tests its initial performance on the main task with logging the success rate.\n4. **Demonstration**: Adds a new task solution to the agent's memory, representing a relevant demonstration.\n5. **Post-Demonstration Testing**: Conducts another series of tests to check if the agent can utilize the memory from the demonstration, logging the results.\n6. **Finalize and Return**: Exits the function and returns a formatted string summarizing the results before and after the demonstration.\n\n## Test Function\n\n### `test_memory`\n\n#### Purpose\nDefines a test function that initializes the necessary components based on a specified mode (`record` or `replay`), and invokes the evaluation to test the learning capability of the agent.\n\n#### Parameters\n- `mode`: A string indicating whether to record a new session or replay an existing one (default is `replay`).\n\n#### Functionality\n1. **Configuration Loading**: Loads test configuration from a specified YAML file that outlines parameters for logging and client creation.\n2. **Logger Initialization**: Instantiates a `PageLogger` for tracking progress and information.\n3. **Client Setup**: Depending on the mode, configures either a real client (for recording) or a mocked client (for replaying).\n4. **Agent Initialization**: An `Apprentice` is instantiated with the configured client and logger.\n5. **Evaluation Call**: Invokes the `eval_learning_from_demonstration` function to assess the agent's learning ability.\n6. **Cleanup**: Finalizes the client session to release resources and potentially save data.\n\n## Main Execution Block\n\n### `if __name__ == \"__main__\":`\n\n#### Purpose\nThis section allows the script to be executed directly from the command line.\n\n#### Functionality\n1. **Argument Parsing**: Checks command-line arguments to determine if the user wishes to record a new session or replay an existing one.\n2. **Event Loop Execution**: Uses `asyncio.run` to execute the test_memory function asynchronously based on the determined mode.\n\n## Conclusion\n\nThis script serves as a robust framework for evaluating how well an agent can learn from demonstrations within a controlled setting, allowing for easy adjustments through configuration files. It effectively separates concerns by modularizing functionality into evaluation and testing operations, and leverages asynchronous programming for efficient execution.",
    "filename": "python/packages/autogen-ext/tests/task_centric_memory/test_learning_from_demonstration.py"
  },
  {
    "code": false,
    "content": "# Overview of the Self-Teaching Evaluation Code\n\nThis Python script is designed to evaluate the self-teaching capabilities of an agent within a task-centric memory framework. The agent learns from its own experiences and iteratively improves its performance on specified tasks without user input. The script executes a predefined set of tasks, assesses the agent's performance, and logs results, thereby enabling both training and testing phases of agent development. \n\n## Key Imports and Dependencies\n\nThe script imports several modules necessary for its functionality:\n\n- **asyncio**: Facilitates asynchronous programming.\n- **sys**: Interacts with the Python runtime environment.\n- **pytest**: Provides a framework for writing and running automated tests.\n- **autogen_core.models**: Contains the `ChatCompletionClient`, a core model component.\n- **autogen_ext.experimental.task_centric_memory.utils**: Includes classes like `Apprentice`, `ChatCompletionClientRecorder`, `Grader`, and `PageLogger` that are crucial for agent training, grading, and logging.\n- **autogen_ext.models.replay**: Provides `ReplayChatCompletionClient` for session replay functionality.\n- **utils**: Contains utility functions for creating the OpenAI client and loading configuration files.\n\n## Main Function: `eval_self_teaching`\n\nThe primary function of the script, `eval_self_teaching`, runs the evaluation process for the agent\u2019s learning ability.\n\n### Parameters\n- **apprentice**: An instance of the `Apprentice` class that handles the learning of the agent.\n- **client**: An instance of `ChatCompletionClient` used for interactions.\n- **logger**: An instance of `PageLogger` to record performance metrics and logs.\n- **config**: A dictionary containing configuration options like the number of training loops and task details.\n\n### Functionality\n1. **Logging Initialization**: The function starts by entering a logging context to keep track of its execution.\n2. **Reading Configuration**: It reads task data from two separate YAML files specified in the configuration, extracting the task descriptions and expected answers.\n3. **Memory Reset**: The agent's memory is cleared to ensure it starts with a blank slate.\n4. **Training Loop**: A loop runs a set number of iterations (as specified in the configuration), training and testing the agent on both tasks.\n5. **Training on Tasks**: For each iteration, the agent is trained on the first task and subsequently tested on both the first and second tasks.\n6. **Performance Grading**: After each training phase, the `Grader` class evaluates the agent\u2019s success rate against the expected answer and logs the performance.\n7. **Overall Success Calculation**: After all iterations, the function calculates and logs the overall success rates for both tasks before exiting the logging context.\n\n## Testing Function: `test_memory`\n\nThe function `test_memory` uses pytest's async capabilities to test the memory of the agent as defined in the configuration file.\n\n### Parameters\n- **mode**: Determines whether the test runs in \"record\" mode (to create a new session) or \"replay\" mode (to use a pre-recorded session).\n\n### Functionality\n1. **Setup**: Loads the configuration specifics for the test and initializes the `PageLogger`.\n2. **Client Creation**: Depending on the mode, it either creates a client that records a session or uses the `ReplayChatCompletionClient` to replay historical data.\n3. **Apprentice Initialization**: Initializes an `Apprentice` object with the setup client and logging configurations.\n4. **Evaluation Invocation**: Calls the `eval_self_teaching` function with the prepared components.\n5. **Cleanup**: Finalizes the client to ensure all resources are properly closed or released.\n\n## Main Execution Block\n\nThe script has an entry point that allows it to be executed directly from the command line.\n\n### Functionality\n- It parses command-line arguments to check if the mode should be set to \"record\" or \"replay\" (default mode).\n- It then initiates the `test_memory` function wrapped in an `asyncio.run` call to handle the asynchronous execution.\n\n## Summary\n\nIn essence, this script is built to evaluate and enhance the learning capabilities of an agent through iterative trials on specific tasks. By incorporating both training and evaluation functionalities, it provides a comprehensive framework for assessing the agent\u2019s ability to learn from its experiences and to generalize its knowledge across similar tasks. This method enables continual improvement of the agent's performance in dynamic environments.",
    "filename": "python/packages/autogen-ext/tests/task_centric_memory/test_self_teaching.py"
  },
  {
    "code": false,
    "content": "# Documentation for `eval_teachability.py`\n\nThis script aims to evaluate the ability of an agent, dubbed as a \u201cteachable agent,\u201d to learn from user-provided insights over multiple interactions. The agent is built with a memory system that intends to enhance its performance by recalling information from past conversations. The process follows a defined workflow, where the agent initially fails to answer a question, receives teaching or hints, and is then reevaluated.\n\n## Overview\nThis script is designed to connect task-centric memory to a selectable agent without modifying the agent's internal code structure. The user can execute the script via a command line, specifying a configuration file that outlines the task description and expected answers. \n\nThe agent operates in three phases:\n1. Initial question posing without any memory.\n2. Providing the agent with hints or insights.\n3. Second question posing to see if the memory retrieves the insight effectively, improving the agent's response.\n\nThe interactions with the agent are logged, and the outcomes are evaluated based on expected results.\n\n## Function: `eval_teachability`\n```python\nasync def eval_teachability(\n    apprentice: Apprentice, client: ChatCompletionClient, logger: PageLogger, config: Dict[str, Any]\n) -> str:\n```\n### Purpose\nThe `eval_teachability` function's role is to orchestrate the learning evaluation process for the agent.\n\n### Parameters\n- `apprentice (Apprentice)`: An instance of the `Apprentice` class, representing the teachable agent.\n- `client (ChatCompletionClient)`: The chat client used for interactions with the agent.\n- `logger (PageLogger)`: A logger instance that tracks the sequence of operations and outputs information.\n- `config (Dict[str, Any])`: A dictionary containing configuration details such as task files.\n\n### Workflow\n1. **Initialization**: The function logs its entry and loads task and insight data from specified YAML configuration files.\n   \n2. **Initial Response Evaluation**: \n   - The agent\u2019s memory is reset to simulate a lack of prior knowledge.\n   - The agent is prompted with a question and generates a response.\n   - The response is checked against the expected outcome using the `Grader` class, logging whether it was correct or incorrect.\n\n3. **Providing Insight**: The function logs the provision of advice to the agent, which should enhance its knowledge for task completion.\n\n4. **Second Response Evaluation**: \n   - The agent is asked the same question again to see if it can correctly answer with the newly provided insight.\n   - The response is re-evaluated, and the outcome is logged.\n\n5. **Finalization**: The function logs its exit and returns the results of both evaluations for summary.\n\n## Function: `test_memory`\n```python\n@pytest.mark.asyncio\nasync def test_memory(mode: Literal[\"record\", \"replay\"] = \"replay\") -> None:\n```\n### Purpose\nThe `test_memory` function is a test case designed to verify the memory capabilities of the agent as specified in the configuration file.\n\n### Parameters\n- `mode (Literal[\"record\", \"replay\"])`: A string that determines the operational mode\u2014either recording a new session or replaying a previously recorded one (default is `replay`).\n\n### Workflow\n1. **Configuration Loading**: The test-specific YAML file is loaded to configure the experiment\u2019s parameters.\n \n2. **Component Initialization**:\n   - A logger instance is created.\n   - Depending on the mode, either a real or mock chat client is instantiated. \n     - If recording, a real chat client is generated to capture responses live.\n     - If replaying, a mock client simulates past interactions without actual conversation.\n\n3. **Agent Setup**: An instance of `Apprentice` is initialized to serve as the teachable agent.\n\n4. **Execution of Learn Evaluation**: The `eval_teachability` function is called to start the evaluation, using the initialized components.\n\n5. **Cleanup**: The client is finalized, ensuring resources are released after the test ends.\n\n## Main Execution\n```python\nif __name__ == \"__main__\":\n    args = sys.argv[1:]\n    mode: Literal[\"record\", \"replay\"] = \"replay\"\n    ...\n    asyncio.run(test_memory(mode=mode))\n```\n### Purpose\nThe script's entry point determines whether to run in recording or replay mode based on command-line arguments. By default, it initializes the script in replay mode to evaluate memory without modifying existing records.\n\n### Workflow\n1. **Argument Parsing**: Command-line arguments are parsed to check for a \u201crecord\u201d command, setting the mode accordingly.\n  \n2. **Async Execution**: The `test_memory` function is called within an asyncio environment to ensure asynchronous operations are handled correctly.\n\nThis modular approach allows for flexible testing of the agent\u2019s learning abilities while maintaining clear boundaries between different functionalities and logging for future improvements and reviews.",
    "filename": "python/packages/autogen-ext/tests/task_centric_memory/test_teachability.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis Python script provides functionality to create an OpenAI chat completion client based on a configuration and to load YAML files that contain settings or parameters. The code utilizes the `yaml` library for parsing YAML files and relies on specific classes from the `autogen_core` and `autogen_ext` modules for client creation. Two main functions are defined: `create_oai_client` and `load_yaml_file`.\n\n## Imports\n\nThe script starts with several import statements:\n\n```python\nfrom typing import Any, Dict\nimport yaml\nfrom autogen_core.models import ChatCompletionClient\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n```\n\n- **typing**: The `Any` and `Dict` types are imported from the `typing` module to enable type hinting in function definitions.\n- **yaml**: The `yaml` library is used for reading and parsing YAML files.\n- **ChatCompletionClient**: This is likely a base or interface class for chat clients imported from `autogen_core.models`.\n- **OpenAIChatCompletionClient**: A specific implementation of a chat client designed to interface with OpenAI's chat models, imported from `autogen_ext.models.openai`.\n\n## Function: `create_oai_client`\n\n```python\ndef create_oai_client(config: Dict[str, Any]) -> ChatCompletionClient:\n    \"\"\"\n    Creates a chat completion client from OpenAI.\n    \"\"\"\n```\n\nThis function is responsible for instantiating a chat completion client using configuration details passed as a dictionary. \n\n### Parameters\n- **config**: A dictionary that contains key-value pairs for configuring the OpenAI client.\n\n### Purpose\nThe function utilizes various parameters from the passed `config` dictionary to initialize an instance of `OpenAIChatCompletionClient`. The parameters include:\n- `model`: Specifies the AI model to be used.\n- `max_tokens`: Sets the maximum number of tokens for the generated completion.\n- `max_retries`: Determines how many times the client will attempt to get a response.\n- `temperature`: Controls the randomness of the output (higher values mean more randomness).\n- `presence_penalty`: Modifies how much the model should avoid repeating the same topics.\n- `frequency_penalty`: Affects how often the model should repeat specific tokens.\n- `top_p`: This parameter influences the diversity of responses by limiting the choices of tokens.\n\n### Return Value\nThe function returns an instance of `ChatCompletionClient` configured according to the provided settings.\n\n## Function: `load_yaml_file`\n\n```python\ndef load_yaml_file(file_path: str) -> Any:\n    \"\"\"\n    Opens a file and returns its contents.\n    \"\"\"\n```\n\nThis function is designed to read a YAML file and return its parsed contents.\n\n### Parameters\n- **file_path**: A string representing the path to the YAML file that needs to be loaded.\n\n### Purpose\nThe `load_yaml_file` function opens the specified YAML file in read mode and uses `yaml.safe_load()` to parse its contents safely without executing any arbitrary code that could be included within the YAML.\n\n### Return Value\nIt returns the parsed content of the YAML file, which can be of any type, such as a dictionary or list, depending on the structure of the YAML file.\n\n## Summary\n\nIn summary, this script provides essential functions for working with OpenAI's chat model and managing configuration settings through YAML files. The `create_oai_client` function allows for the flexible creation of a client tailored to varying operational parameters, which can enable enhanced interaction with the OpenAI API. The `load_yaml_file` function facilitates the easy and safe retrieval of configuration data stored in an organized YAML format, setting the stage for dynamic client instantiation. Together, these functions streamline the process of engaging with AI-driven chat systems, making it easier for developers to configure and deploy such capabilities.",
    "filename": "python/packages/autogen-ext/tests/task_centric_memory/utils.py"
  },
  {
    "code": false,
    "content": "# Teams Tests Initialization\n\n## Overview\n\nThis is an initialization file, commonly referred to as a `__init__.py` file in Python. Its presence indicates that the directory it resides in should be treated as a Python package. This particular file serves a special purpose for organizing tests related to a component or module named \"teams.\"\n\n## Purpose of `__init__.py`\n\n- **Package Recognition**: The primary function of this file is to signal to Python that the directory it is contained within should be treated as a package. This allows for the module imports and code reusability throughout various tests defined in this package.\n- **Test Organization**: By having an `__init__.py` file, it allows for better structuring of test cases specifically written for the \"teams\" module or functionality. This structure can aid in maintaining clarity and organization within the testing framework.\n\n## Contents\n\nWhile the file itself doesn\u2019t contain any executable code or defined functions, its very existence plays a crucial role in the overall Python ecosystem for tests associated with \u201cteams.\u201d The content within the file is summarized as follows:\n\n- **Empty Initialization**: The file does not contain any code, indicating that it is solely serving the purpose of package initialization.\n- **Importing Functionalities**: In a broader context, if this file were to be developed further, it could be used to import and set up specific test utilities or fixtures necessary for testing.\n\n## Future Considerations\n\n- **Expansion**: Although currently empty, there is potential for future enhancement by including imports of test configurations, setup routines, or fixtures that may be used across multiple test files in the same package.\n- **Modular Testing**: To adhere to good testing practices, any functionalities specifically related to teams can be modularized within this package alongside other relevant test modules.\n\n## Conclusion\n\nIn summary, this `__init__.py` file serves as a crucial component for defining the testing landscape for the \"teams\" module within a Python project. It allows for the organization and recognition of the directory as a package that can encapsulate various test cases related to that specific functionality. Although it is currently empty, its existence lays the groundwork for structured and maintainable testing practices in the future.",
    "filename": "python/packages/autogen-ext/tests/teams/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for MagneticOne Tests\n\nThis document describes the tests for the `MagenticOne` team regarding their integration and functionality when interacting with Docker and other code executors, using the `pytest` framework and mock objects.\n\n## Overview\n\nThe tests are designed to verify the behavior of the `MagenticOne` class, particularly focusing on how it interacts with different code executors (Docker and Local) based on the environment configuration. There are multiple functionalities tested, including approval functions for code execution, appropriate warnings upon fallback scenarios, and explicit code executor functionalities.\n\n## Dependency Check\n\n### `docker_tests_enabled`\n\nThis function checks whether Docker tests should be enabled by returning a boolean value. It evaluates the environment variable `SKIP_DOCKER`. If set to \"true,\" the function will return `False`, preventing Docker-related tests from running. It also attempts to import the `docker` package and pings the Docker client to confirm that Docker is operational.\n\n### `_is_docker_available`\n\nA utility function that relies on `docker_tests_enabled` to confirm Docker's availability in the current environment. It serves as a foundational check that other tests depend on, ensuring that relevant tests can be skipped gracefully when Docker is unavailable.\n\n## Mocking\n\n### `mock_chat_client`\n\nThis fixture creates a mock instance of `ChatCompletionClient`, simulating interactions with a chat client during tests. It predefines model information for function calling and JSON output, which the tests can leverage, ensuring that they can focus on behavior rather than implementation details.\n\n## Approval Functions\n\nTwo functions are defined for testing approval logic:\n\n### `approval_function_allow_all`\n\nAn implementation of an approval function that always approves code execution. It simply returns an `ApprovalResponse` indicating success.\n\n### `approval_function_deny_all`\n\nConversely, this function always denies code execution. It returns an `ApprovalResponse` indicating failure.\n\n## Tests Covering Docker Integration\n\n### `test_magentic_one_uses_docker_by_default`\n\nThis test verifies that when Docker is available, the `MagenticOne` instance uses `DockerCommandLineCodeExecutor` by default. It ensures that no approval function is set unless explicitly provided. If Docker is unavailable, it is skipped automatically based on the previous checks.\n\n### `test_magentic_one_uses_docker_with_approval_function`\n\nSimilar to the previous test, but this time it checks for the correct setting of an approval function, ensuring that it correctly utilizes `approval_function_allow_all`.\n\n## Docker Availability Check\n\n### `test_docker_availability_check`\n\nA straightforward test to ensure that the `_is_docker_available()` function returns a boolean value, confirming its functionality regardless of Docker's actual availability.\n\n## Fallback to Local Executor\n\nSeveral tests verify that `MagenticOne` correctly falls back to `LocalCommandLineCodeExecutor` when Docker is not available:\n\n### `test_magentic_one_falls_back_to_local_when_docker_unavailable`\n\nThis test sets the mocked Docker availability to `False`, ensuring that a local executor is instantiated. It checks for appropriate warning messages indicating that Docker is unavailable.\n\n### `test_magentic_one_falls_back_to_local_with_approval_function`\n\nThis variant of the previous test incorporates the `approval_function_deny_all`, asserting that the correct approval function is set when falling back to the local executor. Relevant warnings are also validated.\n\n## Explicit Code Executor Functionality\n\n### `test_magentic_one_with_explicit_code_executor`\n\nThis test evaluates functionality when an explicit `LocalCommandLineCodeExecutor` is provided at instantiation. It verifies that the provided executor is used and that no implicit approval function is set, avoiding deprecation warnings.\n\n### `test_magentic_one_with_explicit_code_executor_and_approval_function`\n\nFinally, this test extends the previous functionality by also providing an approval function. It confirms that both the executor and approval function are properly set and that there are no deprecation warnings triggered.\n\n## Conclusion\n\nThese tests collectively validate the `MagenticOne` class's behavior regarding code execution through Docker and local executors, ensuring robust functionality concerning approval mechanisms and fallback logic. They leverage mocking to isolate behavior, promoting reliability in unit testing practices.",
    "filename": "python/packages/autogen-ext/tests/teams/test_magentic_one.py"
  },
  {
    "code": false,
    "content": "# Documentation for Azure AI Agent Test Suite\n\nThis document describes the functionalities and purpose of the codebase which implements tests for an Azure AI agent system. The code is primarily focused on testing various aspects of the `AzureAIAgent` class and its components, employing mocks for external dependencies. The tests utilize the `pytest` framework alongside asynchronous testing techniques.\n\n## 1. Import Statements and External Dependencies\n\nThe code begins with the import statements that load various modules and classes needed for the implementation:\n\n```python\nimport json\nfrom asyncio import CancelledError\nfrom typing import Any, AsyncGenerator, List, Optional, Union\nfrom unittest.mock import AsyncMock, MagicMock\n\nimport pytest\nfrom autogen_agentchat.base._chat_agent import Response\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core._cancellation_token import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\n```\n\nKey imports include `pytest` for testing functionalities, and various classes and definitions from the `autogen` package that facilitate chat message handling, cancellation management, and Azure agent functionalities.\n\n## 2. Fake Classes for Messaging Simulation\n\nA series of fake classes such as `FakeText`, `FakeMessage`, and related styles are defined to simulate real message content and structure. The classes provide a way to create mock messages with properties that mimic actual messages the Azure AI agent would handle:\n\n```python\nclass FakeText:\n    # Initializes with a string value\n    def __init__(self, value: str) -> None:\n        self.value = value\n\nclass FakeMessage:\n    def __init__(self, id: str, text: str) -> None:\n        self.id = id\n        self.content = [FakeTextContent(text)]\n        self.role = \"user\"\n\nclass FakeMessageWithUrlCitationAnnotation:\n    # Similar to FakeMessage; adds URL citation features\n```\n\nThese classes serve to encapsulate the message attributes with mock data to facilitate isolated testing of the `AzureAIAgent` functionalities without requiring real HTTP calls.\n\n## 3. Mocked Asynchronous Functions\n\nThe code defines several asynchronous generator functions such as `mock_messages_list`, `mock_messages_list_empty`, and `mock_messages_list_multiple` designed to yield predetermined mock messages:\n\n```python\nasync def mock_messages_list(**kwargs: Any) -> AsyncGenerator[FakeMessage, None]:\n    messages = [FakeMessage(\"msg-mock\", \"response\")]\n    for message in messages:\n        yield message\n\nasync def mock_messages_list_multiple(**kwargs: Any) -> AsyncGenerator[FakeMessage, None]:\n    messages = [\n        FakeMessage(\"msg-mock-1\", \"response-1\"),\n        FakeMessage(\"msg-mock-2\", \"response-2\"),\n    ]\n    for message in messages:\n        yield message\n```\n\nThese functions allow tests to simulate various message retrieval scenarios, whether it's a single message, multiple messages, or a case with no messages.\n\n## 4. Agent Creation and Mock Project Client Setup\n\nThe `create_agent` function is defined to initialize instances of the `AzureAIAgent` class. It accepts various parameters to customize the agent\u2019s configuration and return a ready-to-use agent instance:\n\n```python\ndef create_agent(\n    mock_project_client: MagicMock,\n    tools: Optional[ListToolType] = None,\n    ...\n) -> AzureAIAgent:\n    return AzureAIAgent(\n        name=agent_name,\n        description=description,\n        ...\n    )\n```\n\nIn addition, the `mock_project_client` fixture creates a MagicMock instance mimicking the `AIProjectClient` used by the agent, enabling controlled testing of agent methods without dependencies on actual Azure services.\n\n## 5. Testing the Azure AI Agent Initialization\n\nA series of tests are then defined to validate the initialization of an `AzureAIAgent`. These tests ensure that the agent correctly sets its attributes based on the parameters passed:\n\n```python\n@pytest.mark.asyncio\nasync def test_azure_ai_agent_initialization(mock_project_client: MagicMock) -> None:\n    agent = create_agent(mock_project_client, [\"file_search\"])\n    assert agent.name == \"test_agent\"\n    ...\n```\n\nThese tests check if the agent\u2019s properties such as `name`, `description`, and `tools` are properly assigned.\n\n## 6. Handling Messages and State\n\nTests are further defined to check the agent\u2019s behavior when processing messages:\n\n```python\n@pytest.mark.asyncio\nasync def test_on_messages(mock_project_client: MagicMock) -> None:\n    agent = create_agent(mock_project_client)\n    messages = [TextMessage(content=\"Hello\", source=\"user\")]\n    response = await agent.on_messages(messages)\n    assert response is not None\n```\n\nThis section investigates how the agent responds to incoming messages, whether it can process them correctly, and if it can return a valid response.\n\n## 7. File Upload and Tool Handling\n\nMultiple tests validate the agent's ability to handle file uploads through its functionality, simulating conditions for both success and failure cases:\n\n```python\n@pytest.mark.asyncio\nasync def test_upload_files(mock_project_client: MagicMock) -> None:\n    await agent.on_upload_for_file_search([\"test_file.txt\"], cancellation_token=CancellationToken())\n    mock_project_client.agents.files.upload_and_poll.assert_any_await(...)\n```\n\nThis part of the test suite evaluates the interaction between the agent and the mocked file handling methods, ensuring they are called correctly.\n\n## 8. Parameterized Tests and Validation\n\nThe code also employs parameterized tests to validate the behavior of the agent under various configurations:\n\n```python\n@pytest.mark.parametrize(\n    \"tool_name, should_raise_error\",\n    [\n        (\"file_search\", False),\n        (\"unknown_tool\", True),\n    ],\n)\nasync def test_adding_tools_as_literals(mock_project_client: MagicMock, tool_name: Any, should_raise_error: bool) -> None:\n    ...\n```\n\nBy using parameterized tests effectively, various scenarios such as valid and invalid tool associations are confirmed without duplicating code.\n\n## Conclusion\n\nThe code represents a comprehensive test suite for the Azure AI agent system, primarily focused on verifying the proper initialization and behavior of the `AzureAIAgent`. Through the use of mocks, asynchronous functions, and structured tests, reliable and maintainable tests are established to safeguard against regressions in the future. The structure and clarity of the code lay a strong foundation for effective unit testing in complex systems that interact with external resources and services.",
    "filename": "python/packages/autogen-ext/tests/test_azure_ai_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis documentation provides a high-level overview of the provided Python script. The script is designed for testing the functionalities of a `FileSurfer` agent that interacts with files and directories, using OpenAI's chat capabilities. It employs asynchronous programming and utilizes testing tools from the `pytest` framework.\n\n## Import Statements\n\nThe script begins with importing various libraries and modules essential for its operation:\n\n- **Standard Libraries**: `asyncio`, `json`, `logging`, `os`, and `datetime` handle asynchronous tasks, JSON processing, logging, file operations, and date handling.\n- **Type Hinting**: `Any`, `AsyncGenerator`, and `List` from the `typing` module provide type annotations for better code clarity and structure.\n- **Third-Party Libraries**: \n  - `aiofiles` for asynchronous file handling.\n  - `pytest` for testing functionalities.\n  - `autogen_agentchat` and `autogen_ext` modules for agent and chat-related functionalities.\n  - `openai` library for accessing OpenAI API features.\n\n## Custom Logging Handler: `FileLogHandler`\n\nThe script defines a logging handler class:\n```python\nclass FileLogHandler(logging.Handler):\n```\n- **Purpose**: The `FileLogHandler` class extends `logging.Handler` to create a custom file logger that formats log messages, including timestamps and structured message content using JSON.\n- **Initialization**: It initializes with a filename and configures a file handler for logging.\n- **Emit Method**: The `emit()` method formats the log records before writing them to a file, converting `BaseModel` messages to JSON format.\n\n## Mock Class: `_MockChatCompletion`\n\nA mock class is defined to simulate OpenAI chat completions:\n```python\nclass _MockChatCompletion:\n```\n- **Purpose**: This class is designed to provide pre-defined responses for tests, allowing for controlled testing without making actual API calls.\n- **Initialization**: It takes a list of `ChatCompletion` objects and initializes the index for tracking which response to return.\n- **Mock Create Method**: The `mock_create()` method simulates asynchronous API calls, returning predefined completions with a slight delay to mimic actual processing time.\n\n## Logger Configuration\n\nA logger is set up to capture events during the execution:\n```python\nlogger = logging.getLogger(EVENT_LOGGER_NAME)\n```\n- **Logger Name**: The logger is configured to use a specific event logger name.\n- **Logging Level**: The level is set to `DEBUG` to capture detailed logs.\n- **Adding the File Handler**: The custom `FileLogHandler` is added, directing logs to `test_filesurfer_agent.log`.\n\n## Testing FileSurfer Functionality: `test_run_filesurfer`\n\nThe script includes an asynchronous test function:\n```python\n@pytest.mark.asyncio\nasync def test_run_filesurfer(monkeypatch: pytest.MonkeyPatch) -> None:\n```\n- **Setup**: It creates a sample HTML file to be used by the `FileSurfer` agent, simulating a common file type.\n- **Mocking API Calls**: The OpenAI API responses are mocked with predefined completions. The `AsyncCompletions.create` method is patched to replace it with the mock class, ensuring the tests run without external API dependency.\n- **Agent Initialization**: A `FileSurfer` instance is created using the mocked chat completion client.\n- **Functionality Testing**: The test calls the `agent.run()` method with tasks to read the contents of the test file and the directory, asserting expected outcomes in the results.\n\n## Testing Serialization: `test_file_surfer_serialization`\n\nAnother test function is provided:\n```python\n@pytest.mark.asyncio\nasync def test_file_surfer_serialization() -> None:\n```\n- **Purpose**: This test verifies the serialization and deserialization processes of the `FileSurfer` agent.\n- **Serialization**: The agent is serialized into a suitable format using the `dump_component()` method.\n- **Deserialization**: The serialized data is then used to recreate the agent instance using `load_component()`.\n- **Assertions**: The test checks if the deserialized agent retains the same attributes as the original, confirming the integrity of the serialization process.\n\n## Summary\n\nThe provided code defines a testing suite for a `FileSurfer` agent utilizing OpenAI's chat capabilities. It comprises a custom logging mechanism, a mock API response generator for test stability, and structured tests to validate the agent's functionality and serialization. The use of asynchronous programming and the `pytest` framework facilitates effective testing practices, ensuring that the agent operates as intended without relying on external API calls.",
    "filename": "python/packages/autogen-ext/tests/test_filesurfer_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Test Suite of OpenAIAgent\n\nThis document provides a high-level overview of the Python test suite designed for the `OpenAIAgent`, which utilizes the OpenAI API for chat-based interactions. The tests aim to validate various functionalities of the agent, including response handling, error management, state management, and message conversion.\n\n## Overview of the Code\n\nThe code leverages the `pytest` framework and employs asynchronous testing techniques to ensure smooth operations with the OpenAI API. It relies heavily on mock objects, particularly from the `unittest.mock` library, to simulate API responses without making actual calls. The following sections detail the structure and functionality of the codebase.\n\n## Mock Client Creation\n\n### `create_mock_openai_client`\n\nThis function creates a mock instance of the `AsyncOpenAI` client. It simulates responses from the OpenAI API through a nested class called `MockResponse`, which generates expected output based on query parameters. \n\nWhen the `kwargs` contain \"tools\", the mock response returns weather data in JSON format; otherwise, it returns a simple greeting. This mock client is utilized in the tests to avoid API rate limits and connection issues.\n\n### `mock_openai_client` and `mock_error_client`\n\nThese are pytest fixtures that provide different instances of the mock OpenAI client. \n\n- **`mock_openai_client`** returns a properly configured mock client using the `create_mock_openai_client` function.\n- **`mock_error_client`** creates a client to simulate operational failures, raising exceptions when interactions occur, thereby allowing tests to validate error-handling capabilities of the agent.\n\n## Fixtures for Agent Configuration\n\n### Agent Instances\n\nSeveral fixtures are defined to create instances of the `OpenAIAgent` with specific configurations:\n\n- **`agent`**: A basic assistant configured to respond using the mocked client.\n- **`json_mode_agent`**: An assistant set to return JSON formatted responses.\n- **`error_agent`**: An agent designated to simulate failure scenarios.\n\nThese fixtures allow for easy construction of agent instances with varying behaviors, facilitating comprehensive testing of different functionalities.\n\n## Test Cases\n\n### Basic Response Testing\n\n#### `test_basic_response`\n\nThis async test checks if the `OpenAIAgent` can return a basic text response when provided with a simple user message. Assertions validate that the response is present and matches the expected output from the mocked client.\n\n### Tool Calling Behavior\n\n#### `test_tool_calling`\n\nThis test ensures that when a tool is invoked through the agent (e.g., for weather queries), it returns a properly formatted JSON response. The agent's message stream is tested to validate the correct response.\n\n### Error Handling\n\n#### `test_error_handling`\n\nThis test assesses the agent\u2019s ability to manage errors. It checks if an appropriate error message is generated when the mocked error client raises exceptions, confirming that the agent can gracefully handle API failures.\n\n## State Management\n\n### State Preservation and Recovery\n\n#### `test_state_management`\n\nThis test validates the agent's capability to save and restore its internal state. It checks if the last response ID and message history are preserved correctly and that the agent resets state as expected after a reset command is issued.\n\n## Message Conversion Functions\n\n### Conversion Validation\n\n#### `test_convert_message_functions`\n\nThis test examines the functionality of converting different message types into a format that is compatible with the OpenAI API. It ensures that user, system, and assistant messages are correctly converted, solidifying the integration between the agent and the API.\n\n## Handling Nested Messages\n\n### Inner Messages Testing\n\n#### `test_on_messages_inner_messages`\n\nIn this test, inner messages are checked for proper handling when multi-layered responses are generated. Using a dummy message class, it confirms that both the outer response and inner messages are returned correctly.\n\n## API Parameter Construction\n\n### Parameter Building\n\n#### `test_build_api_params`\n\nThis async test focuses on the construction of API parameters within the agent, ensuring that optional parameters such as `previous_response_id` are included only when relevant and that expected default values are utilized.\n\n## Serializing and Configuring Agents\n\n### Serialization Testing\n\n#### `test_component_serialization`\n\nThis test evaluates if the agent\u2019s configuration can be serialized correctly, asserting that essential attributes like name, model, and instructions are accurately stored in a config dictionary.\n\n### Loading from Configuration\n\n#### `test_from_config`\n\nThis test checks that an agent can be reconstructed from a saved configuration, validating that all relevant parameters are retained, ensuring consistency across agent instances.\n\n## MultiModal Message Response\n\n### MultiModal Integration Testing\n\n#### `test_multimodal_message_response`\n\nThe final test in the suite examines the agent's ability to handle multimodal messages, specifically images. A mock function simulates the processing of an image, checking if the response includes a description that aligns with the input.\n\n---\n\nOverall, this test suite systematically validates the different aspects of the `OpenAIAgent` class, ensuring robustness, reliability, and correct integration with the OpenAI API. Each test is designed to cover distinct functionalities, providing comprehensive coverage of the agent's anticipated use cases.",
    "filename": "python/packages/autogen-ext/tests/test_openai_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Quiz and OpenAI Agent Integration Code\n\n## Overview\n\nThis Python script integrates an OpenAI Assistant with a quiz functionality through an asynchronous `DisplayQuizTool`. It utilizes various libraries for mocking, asynchronous file I/O, and creating a testable agent for handling user queries related to quizzes. The code also includes test cases to verify the behavior of the OpenAI Assistant Agent with both mock and real OpenAI/Azure services.\n\n## Dependencies and Imports\n\nThe script employs several dependencies: \n\n- **asyncio** for managing asynchronous operations.\n- **aiofiles** for asynchronous file handling.\n- **pytest** for testing functionalities.\n- **pydantic** for data validation and settings management.\n- **unittest.mock** to create mock objects for testing without dependencies on a real OpenAI service.\n- **azure.identity** and **openai** to interact with Azure OpenAI services.\n\nEach imported module plays a critical role in building, altering, and testing the core functionalities within the code.\n\n## Class Definitions\n\n### QuestionType Enum\n\nThe `QuestionType` class defines the types of questions that can be included in a quiz. It includes:\n- `MULTIPLE_CHOICE`: Represents multiple-choice questions.\n- `FREE_RESPONSE`: Represents open-ended questions.\n\n### Question Class\n\nThe `Question` class is a Pydantic model representing a singular quiz question with attributes:\n- `question_text`: A string describing the question.\n- `question_type`: An instance of `QuestionType` indicating how the question should be answered.\n- `choices`: An optional list of string choices for multiple-choice questions.\n\n### DisplayQuizArgs Class\n\nThe `DisplayQuizArgs` class models the arguments required to display the quiz:\n- `title`: A string representing the title of the quiz.\n- `questions`: A list of `Question` instances that constitute the quiz.\n\n### QuizResponses Class\n\nThe `QuizResponses` class models the responses collected from the quiz:\n- `responses`: A list of strings containing user answers to the quiz questions.\n\n### DisplayQuizTool Class\n\nThe `DisplayQuizTool` class, which extends `BaseTool`, is designed to handle displaying quizzes. Its functionality includes:\n- The constructor, which initializes the tool's metadata for use in the OpenAI ecosystem.\n- The `run` method, which asynchronously processes quiz questions and collects responses. This method returns an instance of `QuizResponses` based on the type and number of questions presented.\n\n## Mocking and Client Setup\n\n### create_mock_openai_client Function\n\nThis function sets up a mock OpenAI client using `AsyncMock`. Various attributes (like `beta`, `threads`, `files`, etc.) are mocked to simulate interactions with the OpenAI API:\n- It includes methods for creating, retrieving, and deleting resources such as threads, messages, and files.\n- The function is critical for ensuring that the tests can run without needing to connect to the actual OpenAI service.\n\n### pytest Fixtures\n\nThe script defines multiple pytest fixtures that encapsulate setup for testing:\n- `mock_openai_client`: Returns a mock OpenAI client for testing purposes.\n- `client`: It provides three types of clients (mock, OpenAI, Azure) based on environment configuration.\n- `agent`: Creates an instance of the `OpenAIAssistantAgent` populated with tools and a client.\n- `cancellation_token`: Provides a token for managing asynchronous cancellation.\n\n## Test Cases\n\n### `test_file_retrieval`\n\nThis test case simulates retrieving a file within the context of an OpenAI Assistant conversation. It mocks file handling and alters messages to return specific responses simulating real interactions. The test checks that the assistant returns a valid response when prompted for content.\n\n### `test_code_interpreter`\n\nThis test verifies the assistant's functionality when tasked with solving a mathematical equation. It ensures the assistant provides a valid output based on predefined messages, thus validating the interpolation between the simulated environment and the expected assistant behavior.\n\n### `test_quiz_creation`\n\nThis test checks the workflow of creating a quiz through the `DisplayQuizTool`. It simulates the function call, all the way to retrieving the associated responses. The test ensures that the quiz is correctly generated and verifies that inner messages are returned as expected.\n\n### `test_on_reset_behavior`\n\nThis scenario tests how the assistant behaves when it is reset. It checks if the assistant retains context even after the reset while assessing its responses against user queries before and after the reset event.\n\n### `test_save_and_load_state`\n\nA more advanced test covering state management, this case examines the ability of the assistant to save its state and then later load that state into a new instance. This functionality is essential for persistent user data and continuing conversations over multiple sessions.\n\n## Conclusion\n\nThe code effectively integrates OpenAI's async capabilities in a testing scenario while simulating a quiz environment. The well-defined structures make it modular and extensible, offering a robust foundation for implementing further features. The use of mocking is vital in this context to facilitate testing without reliance on external services, ensuring easy and reliable unit tests for the OpenAI Assistant's functionality.",
    "filename": "python/packages/autogen-ext/tests/test_openai_assistant_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Playwright Controller Tests\n\nThis document provides a high-level description of a Python script that utilizes the Playwright testing framework with pytest to test a `PlaywrightController` class. The tests focus on verifying the functionality of the controller in an automated web surfing scenario.\n\n## Overview\n\nThe code begins by importing necessary libraries, including `pytest` and `autogen_ext.agents.web_surfer.playwright_controller`, which contains the `PlaywrightController` class. Additionally, the Playwright framework's asynchronous API is imported for browser automation testing. A sample HTML structure (`FAKE_HTML`) is defined, which will serve as the basis for the tests.\n\n## Test Suite\n\n### 1. `test_playwright_controller_initialization`\n\nThis asynchronous test function verifies that the `PlaywrightController` initializes correctly. During the test:\n- An instance of the `PlaywrightController` is created.\n- The viewport dimensions are checked to confirm they are set to the expected values: width of 1440 pixels and height of 900 pixels.\n- The `animate_actions` attribute is validated to ensure it defaults to `False`.\n\n### 2. `test_playwright_controller_visit_page`\n\nThe second test evaluates the `visit_page` method of the `PlaywrightController`. Here\u2019s the step-by-step breakdown:\n- An asynchronous context manager is used to launch the Chromium browser in headless mode.\n- A new browser context and a page instance are created, and the fake HTML content is set on the page.\n- The controller processes the `visit_page` method to navigate to a data URL containing the fake HTML content.\n- Finally, the test checks that the page URL starts with \"data:text/html\", confirming access to the page content.\n\n### 3. `test_playwright_controller_click_id`\n\nThis test focuses on the `click_id` method, which simulates clicking an HTML element by ID. The steps include:\n- An asynchronous context manager sets up a headless Chromium browser with a new page displaying the fake HTML.\n- The `get_interactive_rects` method is called to retrieve interactive elements on the page.\n- The test iterates over the retrieved rectangles to locate the \"Click Me\" button's ID.\n- The `click_id` method is invoked to click the button using its ID.\n- An assertion checks that the active element on the page after the click is indeed the button with ID \"click-me\".\n\n### 4. `test_playwright_controller_fill_id`\n\nThe final test checks the `fill_id` method, which is designed to input text into an HTML input field. The process is similar to the previous tests:\n- A new headless browser instance and page context are established with the fake HTML content.\n- The `get_interactive_rects` method is again used to fetch interactive elements, specifically looking for an input box.\n- The ID of the input box is determined by inspecting the interactive elements.\n- The `fill_id` method is invoked to fill the input box with the string \"test input\".\n- An assertion checks that the value of the input box is updated correctly, confirming that the method works as intended.\n\n## Conclusion\n\nThe provided test script effectively demonstrates the use of the `PlaywrightController` for basic web page interaction tasks. Each test validates specific functionalities such as initialization, page visits, element clicks, and filling input fields. This structured testing approach allows for robust verification of the controller's behavior when engaging with a simulated web page, ensuring reliability in automated UI tasks.",
    "filename": "python/packages/autogen-ext/tests/test_playwright_controller.py"
  },
  {
    "code": false,
    "content": "# Documentation for WebSurfer Agent Test Suite\n\nThis documentation provides a high-level overview of a test suite designed for a multimodal web-surfing agent implemented using asynchronous programming and OpenAI technologies. The primary focus of this code is to test the functionality of the `MultimodalWebSurfer` agent by simulating chat interactions and validating various behaviors.\n\n## Overview\n\nThe test suite utilizes several libraries, including asyncio, pytest, and logging, to coordinate asynchronous tasks, run unit tests, and log events. The suite includes custom logging and mock classes to facilitate testing without making actual API calls. It specifically tests the behavior of the `MultimodalWebSurfer` class using predefined chat responses.\n\n## Logging Setup\n\nThe `FileLogHandler` class extends Python's `logging.Handler` to customize log message formatting. \n\n```python\nclass FileLogHandler(logging.Handler):\n    def __init__(self, filename: str) -> None:\n        ...\n```\n\n- **Initialization**: Takes a filename to log messages to a specified file.\n- **Emit Method**: Customizes how log messages are formatted, converting instances of `BaseModel` messages to JSON format with a timestamp.\n\nAn instance of this handler is added to a logger named `EVENT_LOGGER_NAME`, where the logging level is set to DEBUG.\n\n## Mocking Chat Completions\n\nThe `_MockChatCompletion` class simulates the OpenAI chat completions by storing a list of `ChatCompletion` objects.\n\n```python\nclass _MockChatCompletion:\n    def __init__(self, chat_completions: List[ChatCompletion]) -> None:\n        ...\n```\n\n- **Initialization**: Accepts a list of `ChatCompletion` objects and keeps track of the current index for successive calls.\n- **Mock Create Method**: Asynchronously simulates the completion creation process, returning predefined chat completions with a delay.\n\nThis mocking strategy allows for testing without hitting actual OpenAI APIs, making the tests faster and more reliable.\n\n## Test: `test_run_websurfer`\n\nThe `test_run_websurfer` function tests the core functionality of the `MultimodalWebSurfer`.\n\n```python\n@pytest.mark.asyncio\nasync def test_run_websurfer(monkeypatch: pytest.MonkeyPatch) -> None:\n    ...\n```\n\n### Steps:\n\n1. **Model Definition**: Sets up a model name and a list of sample chat completions.\n2. **Monkeypatching**: Replaces the actual `AsyncCompletions.create` method with the mock method for controlled testing.\n3. **Agent Initialization**: An instance of `MultimodalWebSurfer` is created.\n4. **Initial State Assertions**: Validates the initial state of the agent before any tasks are run.\n5. **Run Task**: Executes the `run` method with a specified task and captures the result.\n6. **Result Assertions**: Validates the response message types, content, and internal states. Also checks the chat history for expected values.\n7. **Second Run**: Repeats the task and validates that the responses are appropriate based on internal state changes.\n\n## Test: `test_run_websurfer_declarative`\n\nThe `test_run_websurfer_declarative` function is another test case focusing on the declarative loading of agent configurations.\n\n```python\n@pytest.mark.asyncio\nasync def test_run_websurfer_declarative(monkeypatch: pytest.MonkeyPatch) -> None:\n    ...\n```\n\n### Steps:\n\n1. **Model and Mock Setup**: Similar to the previous test case, sets up a model and chat completions.\n2. **Monkeypatching**: Similar approach to replace the actual API method with a mock.\n3. **Component Dumping**: Uses the `dump_component` method of the `MultimodalWebSurfer` to retrieve its configuration for validation.\n4. **Configuration Assertions**: Validates that the dumped configuration matches expected properties.\n5. **Loading Component**: Tests the loading capability by reconstructing the agent from its configuration.\n6. **Final Validations**: Ensures that the loaded agent operates as expected.\n\n## Conclusion\n\nThe suite effectively validates the core functionalities of the `MultimodalWebSurfer` agent, ensuring it can handle tasks, maintain internal state, and respond appropriately. The combination of logging, mocking, and structured testing helps ensure robustness and reliability in agent behavior. The use of asynchronous programming and Pydantic data models adds scalability and ease of use to the agent's design, while Python's unittesting capabilities provided by pytest help streamline the testing process.",
    "filename": "python/packages/autogen-ext/tests/test_websurfer_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for gRPC-Based Agent Framework Tests\n\n## Overview\nThis code forms a suite of tests for a gRPC-based framework that manages agents. Agents are registered in a server environment where they can send and receive messages asynchronously. The tests are primarily focused on the registration of agents, message publishing and receiving, and the verification of agent behavior in various scenarios of subscriptions and runtime interactions.\n\n## Dependencies\nThe code utilizes several libraries:\n- **`pytest`**: For structuring tests and assertions.\n- **`asyncio`**: For asynchronous programming with coroutines.\n- **Custom classes and types** from the `autogen_core` and `autogen_ext.runtimes.grpc` modules, which likely define agent behaviors, message types, and gRPC-related functionalities.\n\n## Worker Agent Runtime Setup\n### GrpcWorkerAgentRuntimeHost and GrpcWorkerAgentRuntime\n- An instance of `GrpcWorkerAgentRuntimeHost` is created to manage connections and runtime for agents. It listens on specified host addresses.\n- Each `GrpcWorkerAgentRuntime` represents an individual worker (i.e., agent instance) that can register agent types, publish messages, and manage subscriptions. \n\n## Test Functions\n\n### Test Unique Agent Registrations\n1. **`test_agent_types_must_be_unique_single_worker`**: \n   - Checks that agent types must be unique in a single worker instance. Attempts to register the same agent type twice result in a `ValueError`.\n\n2. **`test_agent_types_must_be_unique_multiple_workers`**: \n   - Similar to the previous test but ensures uniqueness across multiple worker instances, verifying that if one worker registers an agent type, another cannot register the same type.\n\n### Messaging and Subscription Tests\n3. **`test_register_receives_publish`**: \n   - Tests message publishing. Agents subscribe to message types, and the test ensures that they receive messages properly while others do not.\n   - Utilizes the `LoopbackAgent`, which is likely designed to handle messages and perform specific actions in response.\n\n4. **`test_register_doesnt_receive_after_removing_subscription`**: \n   - Validates that once a subscription is removed from an agent, it should not receive messages from the publisher.\n\n5. **`test_register_receives_publish_cascade_single_worker`**: \n   - Examines cascading message receiving where a series of agents respond to a published message, verifying that each agent receives the expected number of messages.\n\n6. **`test_register_receives_publish_cascade_multiple_workers`**: \n   - Tests the same cascading behavior across multiple worker instances, ensuring synchronized message handling.\n\n### Default Subscriptions\n7. **`test_default_subscription`** and **`test_default_subscription_other_source`**: \n   - Verify that agents registered with a default subscription receive messages published to the corresponding topic while agents in other namespaces do not receive those messages.\n\n8. **`test_type_subscription`**: \n   - Tests type-specific message subscriptions to ensure that messages of certain types are received only by the relevant subscribers.\n\n### Agent Registration and Instance Management\n9. **`test_duplicate_subscription`**: \n   - Ensures that an agent can only be registered once across worker instances. If it is registered and one instance disconnects, a new instance can register it only after the previous instance stops.\n\n10. **`test_disconnected_agent`**: \n   - Deals with agent disconnection, ensuring that upon disconnection, the agent's subscriptions are cleared and any duplicate registrations are properly handled.\n\n11. **Agent Instance Tests**: \n   - The tests `test_agent_type_register_instance`, `test_agent_type_register_instance_different_types`, and `test_register_instance_factory` focus on agent instance registration. They ensure that agents of different types cannot occupy the same identifier and validate behavior when sending messages.\n\n### Protocol Buffers Handling\n12. **`test_proto_payloads`**: \n   - Examines how agents handle messages serialized with Protocol Buffers, ensuring proper transmission and receipt of such messages, including message content verification.\n\n### gRPC Configuration\n13. **`test_grpc_max_message_size`**: \n   - Tests the ability of the system to handle messages that exceed a certain predefined size. It adjusts the gRPC server configuration to permit larger messages and verifies the successful delivery and reception of small and large messages under different worker instances.\n\n## Conclusion\nThis test suite comprehensively covers the agent framework's functionality, including registration, messaging, and subscription mechanisms. Using pytest's framework helps maintain clarity and provides structured assertion capabilities. The tests validate both normal operation and edge cases, ensuring robustness within the asynchronous environment of the agent system.",
    "filename": "python/packages/autogen-ext/tests/test_worker_runtime.py"
  },
  {
    "code": false,
    "content": "# Code Overview Documentation\n\nThis document provides a high-level analysis of the provided source code, outlining its primary function, structure, and operational flow.\n\n## Purpose of the Code\n\nThe code primarily serves to accomplish a specific task, which can be understood through the functions and logic that have been implemented. Although the exact nature of the task is not described here, the code likely processes data, performs calculations, or interacts with a user interface or external system.\n\n## Structure of the Code\n\nThe code is divided into several key components that contribute to its overall functionality. These may include variable definitions, control structures (such as loops and conditionals), and function definitions. Each of these components plays an essential role in executing the code's intended task.\n\n### Variable Definitions\n\nAt the start of the code or within specific functions, several variables are declared. These variables store initial values or configurations that will likely be manipulated throughout the script. Their names typically provide insight into their purpose, such as holding user input, configuration settings, or intermediate calculation results.\n\n### Functions and Their Roles\n\nThe code includes function definitions, which encapsulate specific tasks or computations. Each function has a designated purpose, returning values or altering global state based on its logic. The naming convention employed generally signifies the function's intent, enhancing readability and maintainability:\n\n- **[Function Name 1]**: This function is responsible for [function purpose here]. It takes inputs like [input parameters], processes them, and returns [output].\n- **[Function Name 2]**: The primary role of this function is [function purpose here], which may involve [details on processing logic or calculations].\n  \nThese functions may be called sequentially or conditionally, depending on the flow of the program and the user\u2019s input.\n\n### Control Structures\n\nThe code employs various control structures such as loops and conditionals to dictate the flow of execution. The presence of loops, such as `for` or `while`, indicates repeated execution of certain blocks of code, which is essential for tasks that require iteration, such as processing arrays or collections. Conversely, conditional statements (like `if` or `switch`) guide the logic based on user input or variable states, allowing for more dynamic behavior.\n\n### Input Handling\n\nAn essential aspect of the code is how it handles input. This may involve gathering input from users via a command line, GUI, or reading from files. The handling ensures that the data received is valid, and if necessary, pre-processing, such as sanitization or conversion, is performed prior to further computation.\n\n### Output Generation\n\nFollowing processing, the code typically generates output. This could be in the form of returned values, printed messages to the console, or updates to the user interface. The output is organized to be comprehensible and relevant to the user\u2019s input or the purpose of the computation.\n\n### Error Handling\n\nGood practices in coding include error handling mechanisms to ensure robustness. The code likely includes try-catch blocks or similar constructs to manage exceptions gracefully. This allows for informing users when something goes wrong and possibly recovering from errors to ensure the program remains functional.\n\n## Conclusion\n\nOverall, the source code showcases a structured approach to achieving its goals through clear variable definitions, organized function designs, proper control structures, and comprehensive input and output handling. By following this logical framework, the code aims to fulfill its intended functionality effectively. Further exploration of specific areas can yield deeper insights, especially regarding complex operations or algorithms employed within the functions.",
    "filename": "python/packages/autogen-ext/tests/tools/__init__.py"
  },
  {
    "code": false,
    "content": "# Azure AI Search Tool Test Fixtures\n\nThis document provides a high-level overview of the test fixtures designed for testing Azure AI Search components. The code utilizes the `pytest` framework and includes mock implementations to facilitate testing without requiring actual Azure resources.\n\n## Imports and Dependencies\n\nThe script begins by importing various modules essential for the testing framework and mock implementations. Key components include:\n\n- `pytest`: The testing framework that provides fixtures and testing utilities.\n- `unittest.mock`: Utilities for creating mock objects (both synchronous and asynchronous).\n- `ComponentModel` from `autogen_core`: This is likely a model for configurations used in the Azure AI Search tool.\n\nThe code also checks for the availability of Azure SDK components, primarily `azure-search-documents` and `azure-identity`. A marker `skip_if_no_azure_sdk` is defined to skip certain tests if these components are unavailable.\n\n## Protocol and Mock Classes\n\n### `AccessTokenProtocol`\n\nA protocol named `AccessTokenProtocol` is defined, which outlines the structure required for an Azure Access Token. It expects two properties: `token` (a string) and `expires_on` (an integer timestamp).\n\n### `MockAccessToken`\n\n`MockAccessToken` is a class that implements a mock version of an access token. It has an initializer that accepts `token` and `expires_on`, mimicking the actual Azure access token structure. \n\n### `MockAzureKeyCredential` and `MockTokenCredential`\n\nBoth classes serve as mock implementations for Azure credentials:\n\n- `MockAzureKeyCredential`: This class mimics the AzureKeyCredential and holds a key.\n- `MockTokenCredential`: This class implements the `get_token` method, returning an instance of `MockAccessToken` when called. This allows tests to use mocked token retrieval without needing a real Azure authentication flow.\n\n## Test Fixtures\n\nSeveral `pytest` fixtures are defined to create various testing configurations and mocks.\n\n### `mock_vectorized_query`\n\nThis fixture creates a mock object for `VectorizedQuery`, which is part of the Azure SDK. If the Azure SDK is not available, it simply returns a generic `MagicMock` object.\n\n### `test_config`, `keyword_config`, `vector_config`, and `hybrid_config`\n\nEach of these fixtures returns a `ComponentModel` instance with specific configurations for different types of search:\n\n- **`test_config`**: Configuration for a standard Azure AI Search tool.\n- **`keyword_config`**: Configuration specifically for keyword searches.\n- **`vector_config`**: Configuration for vector searches, including specific fields for querying.\n- **`hybrid_config`**: Combines both keyword and vector search capabilities.\n\nAll these configurations specify required fields such as `endpoint`, `index_name`, and `credential`, ensuring they remain valid for tests.\n\n### `mock_search_response`\n\nThis fixture generates a mock response for search operations. It returns a list of dictionaries, each representing a document with fields like score, id, content, title, and source. This is useful for simulating responses from search queries.\n\n### `AsyncIterator`\n\nThe `AsyncIterator` class provides a mock asynchronous iterator for testing. It allows for easy iteration over the list of items (i.e., search results) while also providing an `async` method to return the count of items. This simulates the behavior of actual asynchronous APIs, enhancing the test's realism.\n\n### `mock_search_client`\n\nFinally, the `mock_search_client` fixture creates a mocked search client for testing purposes. It uses `AsyncMock` to mock the asynchronous context manager behavior (`__aenter__` and `__aexit__`) and generates a search method that yields mocked search results through the `AsyncIterator`.\n\nThe mock client is patched into the appropriate target path, allowing the tests to operate as if they were using a real Azure Search client, but without needing network access or authentication.\n\n## Summary\n\nThe provided test fixtures cover various scenarios for testing Azure AI Search components. By using mock implementations, the tests can validate functionality without relying on the actual Azure SDK. This ensures that developers can run tests in isolation, improving reliability and speed while developing and validating their Azure-integrated applications.",
    "filename": "python/packages/autogen-ext/tests/tools/azure/conftest.py"
  },
  {
    "code": false,
    "content": "# Azure AI Search Configuration Tests Documentation\n\nThis document provides a high-level overview and detailed descriptions of the tests defined for the Azure AI Search Configuration. The tests validate various aspects of the configuration object, ensuring correctness when creating and using instances of `AzureAISearchConfig`.\n\n## Overview\n\nThe code utilizes the `pytest` framework to define a suite of unit tests focused on verifying the functionality of the `AzureAISearchConfig` class. This class represents the configuration required to interact with Azure AI search services. The tests cover configuration creation, endpoint validation, query type normalization, and embedding configurations, among others.\n\n## Test Setup and Dependencies\n\nBefore executing the tests, the code checks if the Azure SDK components required for the tests are available using the marker `skip_if_no_azure_sdk`. This ensures that the tests won't run unless the necessary Azure modules are installed, preventing potential failures in environments without the SDK.\n\n```python\nskip_if_no_azure_sdk = pytest.mark.skipif(\n    not azure_sdk_available, reason=\"Azure SDK components (azure-search-documents, azure-identity) not available\"\n)\n```\n\n## Basic Configuration Tests\n\n### `test_basic_config_creation`\n\nThis test checks if a basic configuration can be successfully created using valid parameters. It verifies the correct initialization of the configuration's properties, including the default query type.\n\n```python\ndef test_basic_config_creation() -> None:\n    ...\n```\n\n### `test_endpoint_validation`\n\nHere, the test validates the functionality that checks if the endpoint is a valid URL. It performs checks against several valid and invalid endpoint strings to ensure that appropriate `ValidationError` exceptions are raised when the inputs fail to meet the expected URL format.\n\n```python\ndef test_endpoint_validation() -> None:\n    ...\n```\n\n### `test_top_validation`\n\nThis test verifies the `top` parameter, used to limit the number of results returned. It ensures that only valid positive integers are accepted and that invalid values raise the correct exceptions.\n\n```python\ndef test_top_validation() -> None:\n    ...\n```\n\n## Query Type Tests\n\n### `test_query_type_normalization`\n\nThis function tests the normalization of various query types. It ensures that input query types are mapped correctly to their expected standardized form. It also checks that invalid query types raise the necessary validation errors.\n\n```python\ndef test_query_type_normalization() -> None:\n    ...\n```\n\n### `test_semantic_config_validation`\n\nThis test checks the validation of the semantic configuration. It ensures that when a semantic query type is used, the corresponding `semantic_config_name` must be provided. It verifies that missing values trigger validation exceptions.\n\n```python\ndef test_semantic_config_validation() -> None:\n    ...\n```\n\n### `test_vector_fields_validation`\n\nThis test validates the vector fields parameter for vector searches. It verifies that when the vector query type is specified, an appropriate list of vector field names is accepted.\n\n```python\ndef test_vector_fields_validation() -> None:\n    ...\n```\n\n## Embedding Configuration Tests\n\n### `test_azure_openai_endpoint_validation`\n\nThis test checks whether the Azure OpenAI endpoint is correctly validated when using it for client-side embeddings. It ensures that the corresponding `openai_endpoint` is required if the embedding provider is set to Azure OpenAI, raising validation errors when the configuration is incorrect.\n\n```python\ndef test_azure_openai_endpoint_validation() -> None:\n    ...\n```\n\n## Credential and Serialization Tests\n\n### `test_credential_validation`\n\nThis function tests various scenarios associated with credential validation. It ensures that appropriate credential types are accepted and that custom credential implementations can successfully integrate with the `AzureAISearchConfig`.\n\n```python\ndef test_credential_validation() -> None:\n    ...\n```\n\n### `test_model_dump_scenarios`\n\nThis function tests the serialization of the configuration model through the `model_dump` method. It verifies that the credential attribute is correctly serialized and checks its integrity against both `AzureKeyCredential` and custom token credential classes.\n\n```python\ndef test_model_dump_scenarios() -> None:\n    ...\n```\n\n## Summary\n\nThe provided test suite serves as a comprehensive unit testing framework for the `AzureAISearchConfig` class. The tests effectively validate configuration creation, URL and parameter validation, and embedding configurations. Adherence to validation rules is rigorously enforced, ensuring the integrity and robustness of the configuration setup for Azure AI search operations.",
    "filename": "python/packages/autogen-ext/tests/tools/azure/test_ai_search_config.py"
  },
  {
    "code": false,
    "content": "# Documentation for Azure AI Search Tool Tests\n\n## Overview\n\nThis document provides a detailed examination of a test suite designed for the Azure AI Search tool. The tests utilize the `pytest` framework and are primarily asynchronous, testing the functionalities of various components associated with Azure AI Search. The suite includes extensive coverage of search functionality, error handling, and configuration management, ensuring robust verification of the underlying implementation.\n\n## Imports and Mock Configurations\n\nThe test suite starts with a series of imports, including necessary modules from `unittest`, `pytest`, and Azure SDKs required for asynchronous operations. \n\n```python\nimport asyncio\nfrom collections.abc import Generator\nfrom typing import Any, Dict, List\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nimport pytest\nfrom autogen_core import CancellationToken\nfrom autogen_ext.tools.azure import (\n    AzureAISearchConfig,\n    AzureAISearchTool,\n    SearchResult,\n    SearchResults,\n)\nfrom autogen_ext.tools.azure._ai_search import BaseAzureAISearchTool\nfrom autogen_ext.tools.azure._config import DEFAULT_API_VERSION\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.credentials_async import AsyncTokenCredential\nfrom azure.core.exceptions import HttpResponseError, ResourceNotFoundError\nfrom pydantic import BaseModel, Field, ValidationError\n```\n\n### Mock Credentials and Configuration\n\nA mock endpoint, index, and API key are defined to simulate interactions with the Azure search service. The `MockAsyncTokenCredential` class mimics an asynchronous credential provider returning a mocked token for testing purposes.\n\n```python\nMOCK_ENDPOINT = \"https://test-search.search.windows.net\"\nMOCK_INDEX = \"test-index\"\nMOCK_API_KEY = \"test-key\"\nMOCK_CREDENTIAL = AzureKeyCredential(MOCK_API_KEY)\n\nclass MockAsyncTokenCredential(AsyncTokenCredential):\n    \"\"\"Mock async token credential for testing.\"\"\"\n    ...\n```\n\n## Fixtures for Setup\n\nThe test suite includes several `pytest` fixtures that set up common testing contexts. Fixtures like `search_config`, `mock_search_client`, and `mock_search_results` encapsulate the configuration needed for the tests, allowing for efficient reuse across multiple test cases.\n\n### Search Configuration Fixture\n\nThe `search_config` fixture sets up the necessary base configuration for the search tool.\n\n```python\n@pytest.fixture\ndef search_config() -> AzureAISearchConfig:\n    return AzureAISearchConfig(\n        name=\"test-search\",\n        endpoint=MOCK_ENDPOINT,\n        index_name=MOCK_INDEX,\n        credential=MOCK_CREDENTIAL,\n        description=\"Test search tool\",\n    )\n```\n\n### Mock Search Client Fixture\n\nThe `mock_search_client` fixture uses `unittest.mock` to provide a mocked search client, enabling isolation from the actual Azure service during tests.\n\n```python\n@pytest.fixture\ndef mock_search_client() -> Generator[AsyncMock, None, None]:\n    with patch(\"azure.search.documents.aio.SearchClient\", autospec=True) as mock:\n        ...\n```\n\n## Test Cases\n\nThe following sections summarize the various tests implemented in this suite, categorized by their functionality.\n\n### Validation of Input Models\n\nSeveral tests validate the input models used in search queries and search results. The `TestSearchQuery` model ensures that any queries passed must adhere to specified constraints.\n\n```python\n@pytest.mark.asyncio\nasync def test_search_query_model() -> None:\n    \"\"\"Test SearchQuery model validation.\"\"\"\n    ...\n```\n\n### Search Tool Creation and Configuration\n\nThe tests verify the ability to create various search tools\u2014full-text, vector, and hybrid searches\u2014ensuring that configurations like search fields and query types are validated accordingly.\n\n#### Full-Text Search Tool\n\n```python\n@pytest.mark.asyncio\nasync def test_create_full_text_search() -> None:\n    \"\"\"Test creation of full text search tool.\"\"\"\n    ...\n```\n\n#### Vector Search Tool\n\n```python\n@pytest.mark.asyncio\nasync def test_create_vector_search() -> None:\n    \"\"\"Test creation of vector search tool.\"\"\"\n    ...\n```\n\n### Execution of Search Operations\n\nThese tests ensure that the search operations function as intended using mocked client responses. \n\n#### Search Execution Tests\n\nThe tests can assert the returned results from the Azure search tool against expected outcomes, confirming proper functionality.\n\n```python\n@pytest.mark.asyncio\nasync def test_search_execution(mock_search_client: AsyncMock, mock_search_results: List[Dict[str, Any]]) -> None:\n    \"\"\"Test search execution with mocked client.\"\"\"\n    ...\n```\n\n### Caching and Performance\n\nThere are dedicated tests that assess the caching functionalities built into the search tool, verifying the logic of cache hits and expiration.\n\n```python\n@pytest.mark.asyncio\nasync def test_search_with_caching(mock_search_client: AsyncMock, mock_search_results: List[Dict[str, Any]]) -> None:\n    \"\"\"Test search caching functionality.\"\"\"\n    ...\n```\n\n### Error and Exception Handling\n\nThe suite includes comprehensive error handling tests to ensure robustness against various error conditions such as missing indices, unauthorized access, and service unavailability.\n\n```python\n@pytest.mark.asyncio\nasync def test_error_handling(mock_search_client: AsyncMock) -> None:\n    \"\"\"Test error handling in search execution.\"\"\"\n    ...\n```\n\n### Embedding Provider Functionality\n\nTests are present for the embedding provider functionality, checking that embeddings can be retrieved and validate correctly from different sources.\n\n```python\n@pytest.mark.asyncio\nasync def test_embedding_provider_mixin() -> None:\n    \"\"\"Test the embedding provider functionality.\"\"\"\n    ...\n```\n\n## Conclusion\n\nThe Azure AI Search Tool Tests are designed to thoroughly verify the expected functionalities and edge cases of the Azure AI Search tool implementations. By leveraging mocking, input validation, and exception handling, these tests provide a solid foundation for ensuring the correctness and reliability of the search tool operations. Each test case is focused on specific functionalities, contributing to an overall holistic testing strategy for the Azure service interactions.",
    "filename": "python/packages/autogen-ext/tests/tools/azure/test_ai_search_tool.py"
  },
  {
    "code": false,
    "content": "# Code Analysis and Documentation\n\n## Overview\nThe provided source code is a Python script that likely implements a particular functionality, possibly related to data processing or manipulation. In analyzing the code, the goal is to outline the purpose, main components, and overall flow, detailing the roles of any defined functions, classes, or relevant constructs.\n\n## Main Functionality\nAt a high level, the script appears to perform a series of computations, condition checks, and data handling processes. The exact purpose can vary\u2014common use cases include data analysis, web scraping, or automated task execution. The flow of execution is segmented into clear, logical steps, which will be outlined in the following sections.\n\n## Initialization\nThe script begins by importing necessary libraries or modules. This is standard practice in Python, allowing access to additional functions and classes that simplify the code\u2019s tasks. These imports may involve standard libraries for data manipulation (like `pandas` or `numpy`) or even specialized libraries specific to the domain of the code (like `requests` for web applications).\n\n## Configuration and Setup\nFollowing the import statements, the script typically defines configurations, such as constants or settings needed for execution. This section may include variable initializations, such as file paths, thresholds, or API keys. Properly setting up these configurations is crucial as they determine how the script will behave during execution.\n\n## Data Loading\nMost scripts that involve computation deal with data\u2014either importing it from external sources or generating it within the script. This part of the code likely contains logic to read data files (CSV, JSON, etc.) into structured formats. For instance, using libraries like `pandas`, the script could load a DataFrame for easier manipulation.\n\n## Data Processing\nOnce the data is loaded, the script would process it according to the specified logic. This section may define functions or methods that implement algorithms for cleaning, filtering, or transforming the data. Common operations include handling missing values, aggregating data, or converting data types. This is a critical part as it prepares data for any subsequent analysis or presentations.\n\n## Conditional Logic\nConditional statements are prevalent in scripts to handle different cases depending on data characteristics or user inputs. This section involves checking certain conditions and executing specific code blocks based on these checks. For example, if the script processes datasets of varying formats, it may include conditionals to switch processing methods based on the input format.\n\n## Output Generation\nAfter processing the data, the script usually generates output, which could be visualizations, reports, or modified data files. This section is responsible for presenting the results of the computations, allowing users to derive insights or save the results for future use. Note that efficient output handling is essential for scalability and usability of the script.\n\n## Error Handling\nRobust scripts include error handling mechanisms, which ensure the script can gracefully handle unexpected issues, such as file access errors or data type mismatches. This section might use try-except blocks to catch exceptions and implement fallback procedures or informative logging. This improves the reliability of the script and provides a better user experience.\n\n## Conclusion\nIn summary, this Python script is organized to perform a series of structured tasks that include importing libraries, setting configurations, loading data, processing it according to defined functions, and generating outputs. Each logical segment contributes to the overall functionality, demonstrating a systematic approach typical in script development. The presence of error handling further indicates a consideration for robustness and usability in real-world applications.",
    "filename": "python/packages/autogen-ext/tests/tools/graphrag/__init__.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code defines a series of test fixtures for use in unit tests, primarily using the `pytest` testing framework. Each fixture creates a pandas DataFrame consisting of structured data relevant to different entities such as communities, entities, reports, and relationships connected to works of literature, specifically relating to the famous detective Sherlock Holmes.\n\n## Overview of Imports\n\nThe script imports the necessary libraries:\n- `pandas`: for data manipulation and analysis, especially to create and handle DataFrames.\n- `pytest`: for the testing framework that allows for the creation of fixtures and assertions.\n\n## Fixture: Community DataFrame\n\n### `community_df_fixture`\nThis fixture encapsulates data related to different communities. The DataFrame consists of the following columns:\n- **id**: Unique identifiers for the communities.\n- **human_readable_id**: A simplified integer identifier.\n- **community**: An integer indicating community categorization.\n- **parent**: Indicates the parent community, with `-1` suggesting a top-level community.\n- **level**: Represents the hierarchy level of the community.\n- **title**: The name given to each community.\n- **entity_ids**: Lists of UUIDs representing entities connected to each community.\n- **relationship_ids**: Lists of UUIDs representing relationships associated with each community.\n- **text_unit_ids**: Identifiers for text units relevant to the community.\n- **period**: A date string indicating the relevant time period.\n- **size**: The number of entities associated with the community.\n\n### Purpose\nThis fixture is crucial for testing any functionality that interacts with community data such as retrieval, modification, or aggregation of community-related info.\n\n## Fixture: Entity DataFrame\n\n### `entity_df_fixture`\nThis fixture defines entities and their relationships to the communities. The DataFrame includes:\n- **id**: Unique identifiers for the entities.\n- **human_readable_id**: A simplified form for easier reference.\n- **title**: The name or title of the entity.\n- **community**: An integer indicating to which community the entity belongs.\n- **level**: Represents the hierarchy level of the entity.\n- **degree**: Could signify relevance or involvement in community activities.\n- **x** and **y**: Coordinates, possibly for graphical representation or positioning.\n\n### Purpose\nThis fixture is essential for testing features related to entity data, ensuring proper entity creation, editing, and integration with communities.\n\n## Fixture: Report DataFrame\n\n### `report_df_fixture`\nThis fixture represents community-related reports, encapsulating:\n- **id**: Unique identifiers for the reports.\n- **human_readable_id**: A simplified integer identifier.\n- **community**: Community id associated with the report.\n- **parent**: A reference to a parent report.\n- **level**: Represents the hierarchy level of the report.\n- **title**: Title of the report.\n- **summary**: Short description of the report's content or focus.\n- **full_content**: Detailed descriptions or content related to the report.\n- **rank**: A numerical value showing the report's relevance or importance.\n- **rank_explanation**: An explanation of how the rank is derived.\n- **findings**: Detailed findings from the report, including explanations and summaries.\n- **full_content_json**: JSON representation of the report for easier serialization.\n- **period**: A string indicating the timeframe relevant to the report.\n- **size**: The number of findings associated with the report.\n\n### Purpose\nThis fixture enables testing functionalities related to report generation, dissemination, and analysis within the community context.\n\n## Fixture: Entity Embedding DataFrame\n\n### `entity_embedding_fixture`\nThis fixture defines the structure for entity embedding data, including:\n- **id**: Unique identifiers for each entity.\n- **human_readable_id**: A simplified reference.\n- **title**: Name of the entity.\n- **type**: The classification (e.g., organization, person).\n- **description**: A textual description offering insights about the entity.\n- **text_unit_ids**: Lists of UUIDs pointing to related text units.\n\n### Purpose\nIt serves to test advanced features such as embedding operations, relevance scoring, or connections within the community's entities.\n\n## Fixture: Relationship DataFrame\n\n### `relationship_df_fixture`\nThis fixture specifies how entities relate to each other, with data fields such as:\n- **id**: Unique identifiers for the relationships.\n- **human_readable_id**: A simpler version for easier reference.\n- **source**: The originating entity for the relationship.\n- **target**: The entity that the source is related to.\n- **description**: Detailed information explaining the nature of the relationship.\n- **weight**: A numerical value that may indicate the strength or relevance of the relationship.\n- **combined_degree**: Potentially a measure of connectedness or influence.\n- **text_unit_ids**: Identifiers for related text units.\n\n### Purpose\nEnables the testing of relational integrity, functionality involving linkages among entities, and traversing relationships within the community framework.\n\n## Fixture: Text Unit DataFrame\n\n### `text_unit_df_fixture`\nThis fixture provides data concerning text units, and it includes:\n- **id**: Unique identifiers for each text unit.\n- **human_readable_id**: Simplified reference.\n- **text**: The actual text or content of the unit.\n- **n_tokens**: Number of tokens, likely for natural language processing applications.\n- **document_ids**: IDs of associated documents.\n- **entity_ids**: Lists UUIDs of entities related to the text unit.\n- **relationship_ids**: IDs of relationships relevant to the text unit.\n\n### Purpose\nThis fixture is essential for testing operations on text data, including natural language processing tasks, and how text interrelates within the system.\n\n## Conclusion\n\nEach fixture in this script serves as a foundational component for unit tests, facilitating the assessment of different functionalities related to community structures, entities, reports, relationships, and text units. By providing structured data, these fixtures enable robust testing of system behaviors and reliability within the context of data manipulation and analysis pertaining to literary works, especially those revolving around Sherlock Holmes.",
    "filename": "python/packages/autogen-ext/tests/tools/graphrag/conftest.py"
  },
  {
    "code": false,
    "content": "# Documentation for Mock Implementations and Test Cases\n\n## Overview\n\nThis source code provides mock implementations of various models and search tools that are used for testing purposes. It features mocking of a chat model, embedding model, and a vector store to facilitate unit tests for the `GlobalSearchTool` and `LocalSearchTool`. The tests utilize Pytest to validate the functionality of these tools by feeding in predefined data, ensuring that the behavior of the tools can be observed without relying on actual AI models or data sources.\n\n## Mock Classes\n\n### MockModelOutput\n\nThe `MockModelOutput` class simulates an output from a model. It has:\n\n- **Constructor**: Initializes with a `content` parameter which defaults to \"Mock response\".\n- **Properties**:\n  - `content`: Returns the stored content.\n  - `full_response`: Returns a dictionary containing the content.\n\n### MockModelResponse\n\nThe `MockModelResponse` class simulates a response from a model API. It contains:\n\n- **Constructor**: Initializes a `MockModelOutput` instance and an empty history list.\n- **Properties**:\n  - `output`: Returns the mock model output.\n  - `parsed_response`: Currently returns `None`.\n  - `history`: Returns the list that stores the conversation history.\n\n### MockChatModel\n\nThe `MockChatModel` is designed to mimic a chat-based language model. Its functionality includes:\n\n- **Constructor**: Initializes a `LanguageModelConfig` instance with placeholder values.\n- **Methods**:\n  - `achat`, `achat_stream`, `chat`, `chat_stream`: Each method returns a `MockModelResponse` with a predefined content. The methods vary slightly in their signatures to simulate asynchronous and synchronous behavior.\n\n### MockEmbeddingModel\n\nThe `MockEmbeddingModel` simulates an embedding model. It has:\n\n- **Constructor**: Initializes a `LanguageModelConfig` similarly to `MockChatModel`.\n- **Methods**:\n  - `aembed_batch`, `aembed`, `embed_batch`, `embed`: Each method returns a list of ten identical float values, simulating embeddings for the given text.\n\n### MockVectorStore\n\nThe `MockVectorStore` class is an emulation of a vector store that handles documents and similarity searches. Key features include:\n\n- **Constructor**: Initializes with an empty `documents` dictionary.\n- **Methods**:\n  - `connect`: A placeholder method with no implementation.\n  - `load_documents`: Loads documents into the store, with an option to overwrite existing entries.\n  - `filter_by_id`: Returns `None`, presumably for future functionality.\n  - `similarity_search_by_vector`, `similarity_search_by_text`: Both methods return a list of `VectorStoreSearchResult` containing mock results.\n  - `search_by_id`: Returns a document by its ID or a default empty document.\n\n## Test Cases\n\n### test_global_search_tool\n\nThis test validates the behavior of the `GlobalSearchTool`. It performs the following steps:\n\n1. **Setup Temporary Directory**: Creates a temporary directory to store parquet files containing mock data.\n2. **Save Fixtures**: Saves provided data frames to parquet files representing community reports and entities.\n3. **Initialize Configuration**: Sets up a `GlobalDataConfig` instance that points to the temporary directory.\n4. **Initialize Global Search Tool**: Creates an instance of `GlobalSearchTool` using `MockChatModel` and `tiktoken`.\n5. **Execution**: Runs a mock query about community reports.\n6. **Assertions**: Checks that the result is of the proper type and includes a response, and verifies logging of the output.\n\n### test_local_search_tool\n\nThis test focuses on the `LocalSearchTool`. The procedure is similar to the previous test:\n\n1. **Setup Temporary Directory**: Creates a temporary directory for storing mock parquet files.\n2. **Save Fixtures**: Saves data frames to the temporary directory.\n3. **Initialize Configuration**: Sets up a `LocalDataConfig` instance.\n4. **Mock Vector Store**: Defines a factory function that returns a `MockVectorStore` and patches the relevant method.\n5. **Initialize Local Search Tool**: Creates an instance of `LocalSearchTool` using mock models and the mock vector store.\n6. **Execution**: Runs a mock query regarding relationships between entities.\n7. **Assertions**: Verifies that the result is correct and logs are updated with the expected output.\n\n## Conclusion\n\nThis code provides foundational mock implementations and a testing framework for validating the `GlobalSearchTool` and `LocalSearchTool`. By simulating necessary components, it allows for testing functionality in isolation without requiring actual data access or dependencies on external API services. This setup is essential for ensuring that the search tools operate as designed under controlled conditions.",
    "filename": "python/packages/autogen-ext/tests/tools/graphrag/test_graphrag_tools.py"
  },
  {
    "code": false,
    "content": "It appears there was no source code included in your request. Please provide the code you'd like analyzed, and I'll be happy to help with a high-level description and documentation!",
    "filename": "python/packages/autogen-ext/tests/tools/http/__init__.py"
  },
  {
    "code": false,
    "content": "# FastAPI Test Application Documentation\n\nThis document describes the structure and functionality of a FastAPI test application developed using Python. The application exposes several endpoints that allow for various types of HTTP requests (GET, POST, PUT, DELETE, PATCH) to handle test data. In addition, the application integrates with pytest for unit testing purposes.\n\n## Overview of Classes\n\n### TestArgs\n\n```python\nclass TestArgs(BaseModel):\n    query: str = Field(description=\"The test query\")\n    value: int = Field(description=\"A test value\")\n```\n\n- **Purpose**: `TestArgs` is a data model that defines the expected input for certain endpoints. It uses Pydantic's BaseModel to validate that incoming request bodies contain a string `query` and an integer `value`.\n  \n### TestResponse\n\n```python\nclass TestResponse(BaseModel):\n    result: str = Field(description=\"The test result\")\n```\n\n- **Purpose**: `TestResponse` is a Pydantic model that represents the response format for the endpoints. Each response will include a `result` string that conveys the outcome of the request made to the endpoint.\n\n## FastAPI Application Structure\n\n### Application Initialization\n\n```python\napp = FastAPI()\n```\n\n- **Purpose**: This line initializes a FastAPI application instance called `app`, which serves as the main entry point for defining API endpoints.\n\n### Endpoint Definitions\n\nThe application defines several API endpoints that interact with the `TestArgs` and `TestResponse` models. Each endpoint performs a unique operation based on the HTTP method and the route it corresponds to.\n\n#### 1. POST Endpoint: `/test`\n\n```python\n@app.post(\"/test\")\nasync def test_endpoint(body: TestArgs) -> TestResponse:\n    return TestResponse(result=f\"Received: {body.query} with value {body.value}\")\n```\n\n- **Purpose**: Accepts a JSON body with fields defined by `TestArgs`. Returns a response that confirms the receipt of the `query` and `value`.\n\n#### 2. POST Endpoint with Path Parameters: `/test/{query}/{value}`\n\n```python\n@app.post(\"/test/{query}/{value}\")\nasync def test_path_params_endpoint(query: str, value: int) -> TestResponse:\n    return TestResponse(result=f\"Received: {query} with value {value}\")\n```\n\n- **Purpose**: This endpoint takes `query` and `value` as path parameters. It returns a message confirming the received parameters.\n\n#### 3. PUT Endpoint with Path Parameters and Body: `/test/{query}/{value}`\n\n```python\n@app.put(\"/test/{query}/{value}\")\nasync def test_path_params_and_body_endpoint(query: str, value: int, body: Dict[str, Any]) -> TestResponse:\n    return TestResponse(result=f\"Received: {query} with value {value} and extra {body.get('extra')}\")\n```\n\n- **Purpose**: Accepts both path parameters and a JSON body. It returns a response that includes an additional value from the body.\n\n#### 4. GET Endpoint: `/test`\n\n```python\n@app.get(\"/test\")\nasync def test_get_endpoint(query: str, value: int) -> TestResponse:\n    return TestResponse(result=f\"Received: {query} with value {value}\")\n```\n\n- **Purpose**: Similar to the POST endpoint but uses GET. It expects `query` and `value` as query parameters.\n\n#### 5. PUT Endpoint: `/test`\n\n```python\n@app.put(\"/test\")\nasync def test_put_endpoint(body: TestArgs) -> TestResponse:\n    return TestResponse(result=f\"Received: {body.query} with value {body.value}\")\n```\n\n- **Purpose**: Accepts a JSON body similar to the POST `/test` endpoint but uses a PUT request method.\n\n#### 6. DELETE Endpoint: `/test`\n\n```python\n@app.delete(\"/test\")\nasync def test_delete_endpoint(query: str, value: int) -> TestResponse:\n    return TestResponse(result=f\"Received: {query} with value {value}\")\n```\n\n- **Purpose**: Expects `query` and `value` as query parameters in a DELETE request, returning a confirmation message.\n\n#### 7. PATCH Endpoint: `/test`\n\n```python\n@app.patch(\"/test\")\nasync def test_patch_endpoint(body: TestArgs) -> TestResponse:\n    return TestResponse(result=f\"Received: {body.query} with value {body.value}\")\n```\n\n- **Purpose**: Similar to the PUT endpoints, but for partial updates, this endpoint responds to PATCH requests.\n\n## Testing Configuration\n\n### Test Fixture: `test_config`\n\n```python\n@pytest.fixture\ndef test_config() -> ComponentModel:\n    ...\n```\n\n- **Purpose**: Configures a component model used for testing the HTTP tool with predefined settings\u2014name, description, host, port, and method with corresponding JSON schema definitions.\n\n### Async Test Fixture: `test_server`\n\n```python\n@pytest_asyncio.fixture(scope=\"function\")\nasync def test_server() -> AsyncGenerator[None, None]:\n    ...\n```\n\n- **Purpose**: This asynchronous fixture manages a FastAPI server instance for testing. It starts the server before the tests run and ensures proper cleanup afterwards. The fixture waits for the server to start by sleeping before yielding control back to the tests.\n\n## Conclusion\n\nThis FastAPI application provides a suite of endpoints designed for testing various HTTP methods with request payloads configured via Pydantic models. It serves as a scaffold for further development and testing, with integration capabilities for automated testing methodologies using pytest and pytest-asyncio. The setup demonstrates effective handling of asynchronous requests, path parameters, and dynamic responses suited for an array of client interactions.",
    "filename": "python/packages/autogen-ext/tests/tools/http/conftest.py"
  },
  {
    "code": false,
    "content": "# Documentation for HTTP Tool Test Suite\n\nThis document serves as a high-level overview of the provided test suite, which is designed to validate the functionality of an HTTP Tool (`HttpTool`). The tests focus on the proper generation of schemas, HTTP request handling, response validation, and error handling related to component configurations. \n\n## Overview\n\nThe test suite consists of a series of unit tests written using the `pytest` framework, which verify various aspects of the `HttpTool` component. Each test ensures that the expected behavior aligns with the actual behavior of the tool when interacting with JSON data over HTTP. The tests validate functionalities including schema generation, request execution, error handling, and serialization/deserialization of the configuration.\n\n## Dependencies\n\nThe test suite utilizes several libraries:\n- **httpx**: For making asynchronous HTTP requests.\n- **pytest**: For creating and running the tests.\n- **autogen_core**: Provides fundamental structures such as `Component` and `ComponentModel`.\n- **autogen_ext.tools.http.HttpTool**: The main class being tested.\n- **pydantic**: For model validation and error handling.\n\n## Test Functions\n\n### 1. Schema Generation Tests\n\n- **`test_tool_schema_generation`**: \n  - Asserts that the tool schema is generated correctly. It checks the presence of required fields such as `name`, `description`, and `parameters`, along with their specific metadata (like types and descriptions).\n\n### 2. Tool Properties Verification\n\n- **`test_tool_properties`**:\n  - Validates the essential properties of the `HttpTool`, such as its name, description, and server parameters (host, port, path, scheme, method) to ensure they've been set correctly.\n\n### 3. Component Base Class Tests\n\n- **`test_component_base_class`**:\n  - Confirms that the `HttpTool` adheres to the `Component` base class structure and can serialize and deserialize properly.\n\n## Asynchronous HTTP Request Tests\n\nThe following tests utilize asynchronous execution to validate different types of HTTP requests:\n\n### 4. POST Request Handling\n\n- **`test_post_request`**:\n  - Tests the execution of a POST request, confirming that the tool correctly handles input and generates an appropriate response while also verifying logging output.\n\n- **`test_post_request_json_return`**:\n  - Similar to the previous test but modifies the configuration to expect a JSON response, checking that the result is returned as a dictionary.\n\n### 5. GET, PUT, DELETE, and PATCH Requests\n\n- **`test_get_request`, `test_put_request`, `test_delete_request`, `test_patch_request`**:\n  - Each of these tests verifies the behavior of the HTTP Tool for various request methods, confirming that the output is as expected and follows the configured method type.\n\n### 6. Path Parameters Handling\n\n- **`test_path_params`**:\n  - Tests that path parameters are correctly utilized within the request path.\n\n- **`test_path_params_and_body`**:\n  - Expands path parameter testing by including body parameters in the HTTP request to validate combined usage.\n\n## Error Handling Tests\n\n### 7. Invalid Schema and Request Handling\n\n- **`test_invalid_schema`**:\n  - Confirms that invalid configurations (missing required parameters) correctly raise validation errors.\n\n- **`test_invalid_request`**:\n  - Tests the error handling mechanism for an invalid URL. The test expects exceptions related to connection errors when executing a request with a misconfigured host.\n\n## Configuration Serialization Tests\n\n### 8. Configuration Submission and Retrieval\n\n- **`test_config_serialization`**:\n  - Asserts that the configuration of the `HttpTool` can be serialized correctly, validating that all relevant fields are preserved.\n\n- **`test_config_deserialization`**:\n  - Similar to serialization, this test ensures that a serialized configuration can be deserialized back into the expected structure, confirming that all properties match the original configuration.\n\n## Conclusion\n\nThe provided test suite effectively covers various critical functional areas of the `HttpTool`, ensuring that it not only behaves as expected in ideal circumstances but also handles potential errors gracefully. This thorough approach to testing aids in maintaining the reliability and robustness of the tool in production scenarios.",
    "filename": "python/packages/autogen-ext/tests/tools/http/test_http_tool.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code implements a simple calculator tool using the LangChain framework and includes unit tests to verify its functionality. The primary features include defining a basic addition operation as a tool and creating a custom calculator that can perform multiplication. Additionally, the code utilizes pytest for testing the correctness of the tool's implementation and schema generation.\n\n## Functions\n\n### Tool Definition\n\n#### `add(a: int, b: int) -> int`\n\nThis function is defined as a LangChain tool using the `@tool` decorator. Its purpose is to add two integers:\n\n- **Parameters**:\n  - `a`: An integer representing the first number.\n  - `b`: An integer representing the second number.\n  \n- **Returns**: The sum of `a` and `b`.\n\n### Class Definitions\n\n#### `CalculatorInput`\n\nThis class is a Pydantic model used to define the input schema for the calculator tools. It includes:\n\n- **Attributes**:\n  - `a`: An integer representing the first number with a descriptive field.\n  - `b`: An integer representing the second number with a descriptive field.\n\nThe use of Pydantic enables validation and serialization of the input data.\n\n#### `CustomCalculatorTool`\n\nThis class extends `LangChainTool`, providing a custom tool that can be used for mathematical operations:\n\n- **Attributes**:\n  - `name`: A string indicating the tool's name (\"Calculator\").\n  - `description`: A string describing the tool's purpose (used for math questions).\n  - `args_schema`: The schema for the input arguments, defined from `CalculatorInput`.\n  - `return_direct`: A boolean indicating whether the return value is directly passed back (set to True).\n\n- **Methods**:\n  - `_run(self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun]) -> int`\n    - Synchronously performs multiplication of the two integers `a` and `b`.\n  \n  - `_arun(self, a: int, b: int, run_manager: Optional[AsyncCallbackManagerForToolRun]) -> int`\n    - Asynchronously calls the `_run` method to perform multiplication.\n\n## Testing\n\n### Functionality Testing with Pytest\n\n#### `test_langchain_tool_adapter(caplog: pytest.LogCaptureFixture) -> None`\n\nThis is an asynchronous test function that validates several aspects of the calculator tool and its adapter:\n\n- **Setup**: \n  - The `add` tool is assigned to `langchain_tool`.\n  - A LangChain tool adapter is created for the `add` tool and assigned to `adapter`.\n\n- **Schema Verification**:\n  - Asserts that the schema generated by the adapter is correct, including checks for name, description, and parameter structure (types, properties).\n\n- **Logging Check**:\n  - Captures logs while testing the `run_json` method on the adapter, ensuring that the addition operation produces the expected results and correctly logs these results.\n\n- **Multiple Calls**: \n  - Calls the `run_json` method multiple times with different inputs to validate consistent functionality.\n\n- **Custom Calculator Tool Testing**:\n  - Instantiates a `CustomCalculatorTool` and creates an adapter for it.\n  - Validates the schema generation for this custom tool similarly as above.\n  - Tests the `run_json` method for the custom tool to confirm it produces the expected multiplication results.\n\n## Conclusion\n\nThe code effectively demonstrates the creation of tools in the LangChain framework, including a built-in addition operation and a custom multiplication tool. It also illustrates how to integrate Pydantic for input validation and how to use pytest for comprehensive testing of functionality and schema generation. The modular design allows for easy extension and maintenance, aligning with modern practices in Python development.",
    "filename": "python/packages/autogen-ext/tests/tools/test_langchain_tools.py"
  },
  {
    "code": false,
    "content": "# Documentation for McpSessionActor Tests\n\nThis documentation provides a comprehensive overview of the tests implemented for the `McpSessionActor` component in the provided Python script. The tests are primarily designed to verify the functionality and error handling of various methods associated with the `McpSessionActor`. The tests make extensive use of the `pytest` framework and its related functionalities.\n\n## Overview\n\nThe script imports necessary libraries and modules, including `pytest` for testing, and various classes from `autogen_core` and `mcp`. It prepares a set of fixtures and tests to ensure that `McpSessionActor` behaves correctly under various circumstances. These tests primarily focus on validating edge cases, ensuring robust error handling, and verifying the correct integration with model clients and message parsing functionality.\n\n### Key Imports\n\n- **asyncio**: For handling asynchronous operations.\n- **pytest**: The testing framework used for writing and executing the tests.\n- **MagicMock** and **AsyncMock**: These are used to create mock objects for testing without depending on external systems.\n- **pathlib.Path**: For handling file paths in a platform-independent way.\n\n## Test Fixtures\n\n### `mcp_server_params`\n\nThis fixture sets up parameters required to launch the actual MCP server as a subprocess. It points to a server script (`mcp_server_comprehensive.py`) and defines a command with an associated read timeout.\n\n### `mock_model_client` and `mock_model_client_with_vision`\n\nThese fixtures generate mock models, one without vision support and another with it. Each mock model simulates a response for the `create` method, which is key to testing the handling of various content types.\n\n## Test Cases\n\n### Parsing Content\n\n#### `test_parse_sampling_content_unsupported_image_without_vision`\n\nThis test verifies that `_parse_sampling_content` raises a `ValueError` when attempting to parse image content with a model that doesn't support vision. An instance of `mcp_types.ImageContent` is created and passed along with model information, leading to the expected error.\n\n#### `test_parse_sampling_content_with_vision_support`\n\nHere, the test validates that the `McpSessionActor` correctly processes image content when the underlying model supports vision capabilities.\n\n#### `test_parse_sampling_content_text`\n\nThis checks that the `text` content type is processed correctly by `_parse_sampling_content`, validating that the output matches the input text.\n\n#### `test_parse_sampling_content_unknown_type`\n\nThis test confirms that the parser raises a `ValueError` when provided with unknown content types, ensuring that invalid input handling works as expected.\n\n### Parsing Messages\n\n#### `test_parse_sampling_message_unrecognized_role`\n\nThis test checks for an appropriate error response when an unrecognized role is encountered in the message being parsed, ensuring robust error handling.\n\n#### `test_parse_sampling_message_assistant_with_non_string_content`\n\nIt verifies that attempting to use non-string content for an assistant message raises an `AssertionError`, as assistant messages are expected to solely contain string content.\n\n### Actor State Tests\n\n#### `test_call_when_not_active`\n\nThis test ensures that the `call` method raises a `RuntimeError` if the actor is not currently active. The test confirms that without initialization, no calls can be processed.\n\n#### `test_call_when_actor_task_crashed`\n\nVerifying the handling of crashed actor tasks, this test ensures that a `RuntimeError` is thrown when attempting to call a method after the actor task experiences a failure.\n\n#### `test_close_when_not_active`\n\nChecking for early termination in the close sequence, this test verifies that the actor can exit smoothly when it is not active.\n\n## Error Handling\n\n### General Exception Management\n\nSeveral tests focus on how the `McpSessionActor` handles various exceptions, such as:\n\n- **Session Errors**: Testing how the actor responds when session initialization fails.\n- **Command Errors**: Verifying that the actor properly reports issues when executing commands that cause errors, such as invoking unknown tools or commands with invalid parameters.\n\n### Integration with Real MCP Server\n\nThe comprehensive tests such as `test_actor_basic_functionality` and `test_actor_prompt_operations` confirm that the `McpSessionActor` can communicate correctly with a real MCP server, processing complex commands and validating responses.\n\n### Sampling Callbacks\n\nTests related to `_sampling_callback` evaluate the effective handling of messages and responses, ensuring that both valid and error-generating cases are thoroughly examined, such as checking for valid responses and managing unexpected non-string returns.\n\n## Conclusion\n\nOverall, the test suite for `McpSessionActor` is rigorous, ensuring that various aspects of the actor's functionality are thoroughly examined. By using mocks and integration tests, the suite balances unit and system-level testing, reinforcing reliability and robustness in handling asynchronous operations and various message types. This comprehensive testing approach aids in maintaining high code quality and functionality stability as the associated libraries evolve.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_actor.py"
  },
  {
    "code": false,
    "content": "# Documentation for MCP Tool Adapter Tests\n\n## Overview\n\nThis Python code provides a comprehensive testing suite for various MC Protocol (MCP) tools, including features for tool execution, configuration serialization, and error handling. The tests utilize the `pytest` framework and leverage asynchronous programming features provided by Python's `asyncio`. The suite is designed to validate that the tool adapters function correctly within a simulated server environment.\n\n## Test Fixtures\n\n### Sample Tool Fixtures\n\nSeveral fixtures are defined to create sample `Tool` instances required for tests:\n\n- **`sample_tool`**: Creates a basic tool with a simple input schema.\n- **`sample_server_params`**: Sets parameters for standard input/output server parameters.\n- **`sample_sse_tool`**: Creates a sample tool specifically for Server-Sent Events (SSE).\n- **`sample_streamable_http_tool`**: Creates a sample tool for HTTP streaming.\n\n### Session Mocks\n\nMocks of the `ClientSession` are created to simulate interactions with an MCP server:\n\n- **`mock_sse_session`**: Mock session for letting the SSE tool operate as expected.\n- **`mock_streamable_http_session`**: Mock session for HTTP streaming tool interactions.\n- **`mock_session`**: A generic mock session to handle standard tool operations.\n\n### Tool Response Mocks\n\nSeveral mocked responses are created to simulate expected results from calling various tools:\n\n- **`mock_tool_response`**: Represents a successful response from a tool.\n- **`mock_error_tool_response`**: Represents an error response from a tool.\n\n### Cancellation Token Fixture\n\nThe **`cancellation_token`** fixture provides a cancellation token which can be used to manage task cancellations in asynchronous operations, enhancing the responsiveness of tests involving tool executions.\n\n## Test Functions\n\n### Adapter Configuration Serialization\n\nThe tests ensure that the adapters can be saved and loaded correctly:\n\n- **`test_adapter_config_serialization`**: Verifies that the `StdioMcpToolAdapter` retains its properties and schema upon serialization and deserialization.\n- **`test_sse_adapter_config_serialization`**: Checks the same functionality for the SSE adapter.\n- **`test_streamable_http_adapter_config_serialization`**: Validates the serialization methods for the Streamable HTTP adapter.\n\n### Tool Execution\n\nVarious tests validate that tools can properly execute through different adapters and sessions:\n\n- **`test_mcp_tool_execution`**: Tests the execution of a tool through `StdioMcpToolAdapter`, checking the result against a mocked response.\n- **`test_sse_tool_execution`**: Validates execution through the SSE adapter, ensuring proper handling of server responses.\n- **`test_streamable_http_tool_execution`**: Confirms successful tool execution through the Streamable HTTP adapter.\n\n### Adapter Creation\n\nTests to verify that adapters can be created from server parameters and existing sessions are present:\n\n- **`test_adapter_from_server_params`**: Confirms an adapter can be created from parameters and retains expected properties.\n- **`test_adapter_from_factory`**: Checks if the factory function returns the expected tool list.\n- **`test_adapter_from_factory_existing_session`**: Ensures that existing sessions can successfully retrieve tool lists.\n\n### Error Handling\n\nMultiple tests assess the behavior when errors occur or operations are cancelled:\n\n- **`test_mcp_tool_adapter_run_error`**: Tests how the adapter handles tool responses that indicate errors.\n- **`test_mcp_tool_adapter_run_cancelled_before_call`**: Checks cancellation behavior before an adapter attempts to call a tool.\n- **`test_mcp_tool_adapter_run_cancelled_during_call`**: Validates cancellation during an ongoing tool call.\n\n### Normalization and Utility Methods\n\nAdditional tests verify the functionality of utility methods:\n\n- **`test_mcp_tool_adapter_normalize_payload`**: Ensures the `_normalize_payload_to_content_list` correctly normalizes various types of payloads.\n- **`test_return_value_as_string_with_resource_link`**: Checks that the `return_value_as_string` correctly handles `ResourceLink` objects.\n\n## Workbench Tests\n\nA separate section of tests focuses on the `McpWorkbench`, validating management of tool execution and server interactions:\n\n- **`test_mcp_workbench_start_stop`**: Validates that the workbench can start and stop properly.\n- **`test_mcp_workbench_server_fetch`**: Ensures that the workbench can successfully fetch tools from a server and call them.\n- **`test_mcp_workbench_server_filesystem`**: Validates functionality with filesystem tools through the workbench.\n\n## Conclusion\n\nThe provided test suite is well-structured to ensure the reliability and functionality of MCP tool adapters and their respective implementations within a workbench context. The combination of mock objects, comprehensive fixture setups, and focused test cases contribute to robust software validation and enhanced confidence in the codebase's integrity.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_tools.py"
  },
  {
    "code": false,
    "content": "# Documentation for MCP Workbench Tests\n\nThis document describes a set of unit tests for the `McpWorkbench` class and associated functionalities. The tests cover various scenarios in which the MCP (Model Communication Protocol) server can list, get, and read prompts and resources, ensuring the expected behavior of the class under different conditions.\n\n## Test Fixtures\n\n### Sample Server Parameters\n```python\n@pytest.fixture\ndef sample_server_params() -> StdioServerParams:\n    \"\"\"Sample server parameters for testing.\"\"\"\n    return StdioServerParams(command=\"echo\", args=[\"test\"])\n```\nThe `sample_server_params` fixture creates sample server parameters using the `StdioServerParams` class, which simulates underlying server behavior for testing.\n\n### Mock MCP Actor\n```python\n@pytest.fixture\ndef mock_mcp_actor() -> AsyncMock:\n    \"\"\"Mock MCP session actor.\"\"\"\n    actor = AsyncMock()\n    return actor\n```\nThe `mock_mcp_actor` fixture provides an asynchronous mock actor. This mock simulates the functionality of the actual MCP actor used in communication with the server.\n\n### Sample Prompts\n```python\n@pytest.fixture\ndef sample_prompts() -> list[Prompt]:\n    \"\"\"Create sample MCP prompts for testing.\"\"\"\n    ...\n```\nThis fixture generates sample prompts consisting of various parameters, which are utilized in the tests that require prompt interaction.\n\n### Sample Resources\n```python\n@pytest.fixture\ndef sample_resources() -> list[Resource]:\n    \"\"\"Create sample MCP resources for testing.\"\"\"\n    ...\n```\nSimilar to the prompts fixture, `sample_resources` provides a list of sample resources with attributes necessary for evaluating MCP resource queries.\n\n### Sample Resource Templates\n```python\n@pytest.fixture\ndef sample_resource_templates() -> list[ResourceTemplate]:\n    \"\"\"Create sample MCP resource templates for testing.\"\"\"\n    ...\n```\nThis fixture sets up resource templates for testing resource management in the MCP.\n\n## Test Cases\n\n### Listing Prompts\n```python\n@pytest.mark.asyncio\nasync def test_list_prompts(...)\n```\nThis test verifies that the `McpWorkbench` can successfully list prompts when called. It simulates a scenario where the MCP actor returns a predefined list of `Prompt` objects to the workbench's request.\n\n### Getting a Prompt Without Arguments\n```python\n@pytest.mark.asyncio\nasync def test_get_prompt_without_arguments(...)\n```\nThe test ensures that when the `get_prompt` method is called without any arguments, it retrieves the expected prompt details and asserts the correct interaction with the mock actor.\n\n### Getting a Prompt With Arguments\n```python\n@pytest.mark.asyncio\nasync def test_get_prompt_with_arguments(...)\n```\nThis case checks if the workbench can fetch a prompt correctly when specific arguments are passed, verifying the integrity of the parameters when interacting with the actor.\n\n### Listing Resources\n```python\n@pytest.mark.asyncio\nasync def test_list_resources(...)\n```\nThe `test_list_resources` checks if the `list_resources` method fetches resource data as expected. It evaluates that the result matches predefined sample resources.\n\n### Reading a Resource\n```python\n@pytest.mark.asyncio\nasync def test_read_resource(...)\n```\nThis test ensures the `read_resource` method is functional by calling it with a resource URI and confirming that it retrieves the expected contents.\n\n### Listing Resource Templates\n```python\n@pytest.mark.asyncio\nasync def test_list_resource_templates(...)\n```\nThe purpose of this test is to confirm that the `list_resource_templates` method retrieves the defined resource templates accurately.\n\n## Auto Start Functionality Tests\n\n### Auto Start on Listing Prompts\n```python\n@pytest.mark.asyncio\nasync def test_workbench_auto_start_on_list_prompts(...)\n```\nThis test observes the auto-start behavior of the workbench when `list_prompts` is called without prior initialization. It ensures the workbench starts up as expected.\n\n### Auto Start on Getting Prompt\n```python\n@pytest.mark.asyncio\nasync def test_workbench_auto_start_on_get_prompt(...)\n```\nSimilar to the previous test, this case verifies that invoking `get_prompt` triggers the workbench to start if it hasn't already.\n\n### Auto Start on Listing Resources\n```python\n@pytest.mark.asyncio\nasync def test_workbench_auto_start_on_list_resources(...)\n```\nThis test checks whether the `list_resources` method can prompt the workbench to start automatically, thus asserting the functionality of the actor initialization.\n\n## Error Handling\n\n### Actor Initialization Failures\n```python\n@pytest.mark.asyncio\nasync def test_workbench_methods_raise_error_if_actor_fails_to_initialize(...)\n```\nFinally, this test evaluates the workbench's error handling mechanism when the actor cannot be initialized. It asserts that appropriate exceptions (specifically `RuntimeError`) are raised when methods are called without a valid actor.\n\n## Conclusion\nThe described tests provide thorough coverage of the MCP workbench functionality, simulating interactions with the actor and ensuring that expected behaviors occur in both standard and exceptional circumstances. Each test aids in maintaining the integrity of the functionality as development continues, facilitating robust error handling and efficiency in communication with the MCP server.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_workbench_features.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Source Code\n\nThis code implements a testing framework for an asynchronous chat completion service. It utilizes mocking techniques to simulate the behavior of various components, such as model clients and server parameters related to message processing and tool management. The testing framework is built using the `pytest` library, especially focused on asynchronous testing via the `pytest.mark.asyncio` decorator.\n\n## Components Overview\n\n### MockChatCompletionClient Class\n\nThe `MockChatCompletionClient` class inherits from `ChatCompletionClient` and implements various functions tailored for testing purposes. Its role is to simulate the behavior of a chat completion service without connecting to actual backend services. The class contains:\n\n- **Model Info**: This property returns model capabilities, initialized with default settings.\n- **Methods**:\n  - `create()`: Returns a mocked response to represent a completed message generation.\n  - `create_stream()`: Simulates streaming responses to the caller.\n  - `close()`: No operations are defined as there's nothing to close for a mock.\n  - `actual_usage()` and `total_usage()`: Return zero usage metrics.\n  - `count_tokens()` and `remaining_tokens()`: Provide fixed numbers for token accounting.\n  - `_to_config()` and `_from_config()`: Enable the client to be serialized and deserialized efficiently.\n\n### Fixtures\n\nSeveral test fixtures are created using `pytest` to mock various necessary components, making the tests more cohesive and focused:\n\n- **sample_mcp_tools**: Generates sample tools for testing with predefined schemas.\n- **mock_mcp_actor**: Creates a mock actor for handling communication within the MCP framework.\n- **sample_server_params**: Provides sample server parameters necessary for initiating test conditions.\n- **mock_model_client** and **mock_model_client_with_vision**: Represents different configurations of the `MockChatCompletionClient`\u2014with and without vision support, respectively.\n\n## Test Cases\n\n### Workbench Initialization Tests\n\nSeveral tests validate the initialization of the `McpWorkbench` class, which is critical for checking if it integrates appropriately with the provided model client and server parameters:\n\n- **Model Client Initialization Test**: Confirms that initializing the workbench with a model client sets it correctly within the workbench instance.\n- **Without Model Client Initialization**: Verifies that the workbench can be initialized even when no model client is provided, ensuring default behavior.\n\n### Serialization and Deserialization\n\nTests validate that the `McpWorkbench` can be serialized and deserialized correctly. This is essential for maintaining state, configurations, and properties across sessions.\n\n### Actor Creation and Integration\n\nThe tests assert that the actor within the `McpWorkbench` correctly initializes itself with the model client. The functions check:\n\n- **Proper Actor Creation**: Ensures that when calling the `list_tools()` function, the proper actor gets initialized.\n- **Automatic Actor Start**: Validates that operations like `call_tool()` still function correctly even when called without manually starting the actor.\n\n### State Management\n\nTests check that the workbench can save and load state while maintaining the integrity of the model client. It ensures that the proper state representation is sustained even after state operations.\n\n### Model Capabilities Testing\n\nTests ensure that the `McpWorkbench` recognizes and maintains specific features, such as vision capabilities. It checks that the properties remain consistent during serialization/deserialization.\n\n### Callbacks and Error Handling\n\nSeveral tests target the functionality of the `sampling_callback`, verifying:\n\n- **Correct Invocation**: Ensuring the sampling callback is called correctly under various conditions.\n- **Error Handling**: The system's behavior is scrutinized when facing issues such as the model client failing or when integrating with the session.\n\n### Integration Tests\n\nA more extensive integration test simulates a real-life scenario where the sampling callback operates within a session-managed environment. It checks the end-to-end workflow and ensures that components communicate effectively in their expected asynchronous context.\n\n## Conclusion\n\nOverall, the source code defines a structured and isolate-testing framework for a chat completion service, focusing on reliability, component interactions, and mock implementations. The testing leverages `pytest` to validate various functionalities, ensuring that changes in code do not inadvertently break expected behaviors. The presence of fixtures enhances the modular testing capability, allowing for easy adjustments and expansions in future tests.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_workbench_model_client.py"
  },
  {
    "code": false,
    "content": "# MCP Workbench Tests Documentation\n\nThis documentation provides a high-level overview of the test suite focused on the MCP (Multi-Component Protocol) Workbench. The suite includes various tests that validate functionality related to tool overrides, communication with an asynchronous actor, serialization, and conflict detection.\n\n## Overview\n\nThe MCP Workbench is designed to facilitate interaction with a set of tools for different functionalities, such as fetching and searching data. The tests utilize the `pytest` framework and include asynchronous call handling with the `AsyncMock` functionality. Each test case uses fixtures to set up sample tools, mocked actors, and server parameters, which help isolate and verify specific functionalities of the `McpWorkbench` class.\n\n### Import Statements\n\nThe code begins with various import statements:\n- `asyncio` for asynchronous programming.\n- `Any`, `Dict` from `typing` for type annotations.\n- `AsyncMock` from `unittest.mock` for mocking asynchronous calls.\n- `pytest` for testing.\n\nThen, it imports relevant classes and types from the `autogen_core` and `mcp` modules.\n\n## Fixtures\n\n### Sample MCP Tools\n\nThe `sample_mcp_tools` fixture creates a list of sample tools. Each tool is defined with:\n- `name`: The tool's name (e.g., \"fetch\", \"search\").\n- `description`: A brief overview of what the tool does.\n- `inputSchema`: Specifies the parameters required by the tool, ensuring they are validated before being processed.\n\n### Mock MCP Actor\n\nThe `mock_mcp_actor` fixture provides an asynchronous mock actor. This is essential for testing how the `McpWorkbench` communicates with the actor, without requiring a real implementation.\n\n### Sample Server Parameters\n\nThe `sample_server_params` fixture delivers sample parameters for a standard input/output server used in the tests. This is necessary to configure the workbench correctly during testing.\n\n## Test Case: Tool Overrides\n\n### Listing Tools with Overrides\n\nThe `test_mcp_workbench_with_tool_overrides` function tests the `McpWorkbench` for its ability to apply name and description overrides for tools.\n- It defines a dictionary of tool overrides that specify modified names or descriptions.\n- It then asserts that when tools are listed, the correct overridden attributes are returned.\n- Additionally, it confirms that the actor was called correctly to fetch the tool list.\n\n### Calling Tools with Overrides\n\nThe `test_mcp_workbench_call_tool_with_overrides` function checks that the call to a tool using its overridden name successfully maps back to the original tool name.\n- A specific tool\u2019s name is overridden, and assertions validate the returned response matches expectations.\n- It ensures that the original tool name, rather than the overridden name, is used in the actual call.\n\n## Test Case: Without Overrides\n\n### Default Behavior\n\nThe `test_mcp_workbench_without_overrides` validates the `McpWorkbench` functionality without any overrides in place. \n- It checks that the original tool names and descriptions are preserved when listing tools.\n- This establishes a baseline behavior of the workbench.\n\n## Test Case: Serialization\n\n### Serializing and Deserializing Overrides\n\nThe `test_mcp_workbench_serialization_with_overrides` function ensures that the `McpWorkbench` can be serialized and reconstituted correctly with tool overrides.\n- It verifies that the configuration retains correct override information and maintains the same internal state after being loaded.\n\n## Test Case: Partial Overrides\n\n### Handling Partial Overrides\n\nThe `test_mcp_workbench_partial_overrides` function evaluates how `McpWorkbench` manages partial overrides where only specific attributes (name or description) are changed.\n- This test ensures that adjustments can be made to tool attributes independently, confirming the robustness of the overriding functionality.\n\n## Test Case: ToolOverride Model\n\n### Functionality of ToolOverride\n\nThe `test_mcp_tool_override_model` function tests various instantiations of the `ToolOverride` model.\n- It checks that all combinations of provided fields (`name`, `description`) behave as expected, ensuring the model correctly reflects the intended overrides.\n\n## Test Case: Mapping Overrides to Original Names\n\n### Reverse Mapping Test\n\nThe `test_mcp_workbench_override_name_to_original_mapping` function verifies that the reverse mapping from override names back to original names is set up correctly.\n- This is essential for ensuring that the system accurately tracks which overrides correspond to which original tools for seamless interactions.\n\n## Test Case: Conflict Detection\n\n### Detecting Override Conflicts\n\nThe `test_mcp_workbench_conflict_detection` function tests the `McpWorkbench` for its ability to detect conflicts when two or more tools are given the same override name.\n- It validates that the system raises appropriate exceptions for conflicting names, confirming integrity in the override setup.\n\nThis detailed structure of tests ensures that the `McpWorkbench` class is functioning correctly, allowing for flexibility with tool overrides while maintaining reliable core functionalities.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_workbench_overrides.py"
  },
  {
    "code": false,
    "content": "# MCP Workbench Testing Documentation\n\nThis document describes a set of unit tests for the `McpWorkbench` class from the `autogen_ext.tools.mcp` module. The tests utilize the `pytest` framework and are asynchronous in nature, leveraging Python's `asyncio` capabilities. The main focus is on testing for expected errors and warnings during different interactions with the `McpWorkbench`.\n\n## Overview\n\nThe `McpWorkbench` class appears to serve as a tool for interacting with various resources\u2014a key aspect being its ability to handle tools and responses effectively. The tests cover several functionalities, including error handling, response content management, and initialization properties of the workbench. The server parameters for the tests are based on the `StdioServerParams`, which is a fixture that specifies the command and arguments for the server.\n\n## Fixtures\n\n### `sample_server_params`\n\n```python\n@pytest.fixture\ndef sample_server_params() -> StdioServerParams:\n    \"\"\"Sample server parameters for testing.\"\"\"\n    return StdioServerParams(command=\"echo\", args=[\"test\"])\n```\n\nThis fixture returns a sample instance of `StdioServerParams`, providing default parameters to initialize a server for the tests.\n\n### `mock_actor`\n\n```python\n@pytest.fixture\ndef mock_actor() -> AsyncMock:\n    \"\"\"Mock actor for testing.\"\"\"\n    return AsyncMock()\n```\n\nThis fixture creates a mock asynchronous actor, mimicking the behavior expected from the actual actor in the `McpWorkbench`.\n\n## Test Cases\n\n### Actor Initialization Tests\n\nSeveral test cases are dedicated to checking the behavior when the actor is not initialized or set to `None` after an attempt to start. These tests validate that attempts to list tools, call tools, or list prompts correctly raise a `RuntimeError` when the actor is uninitialized.\n\n#### `test_list_tools_actor_none_after_start`\n\n```python\n@pytest.mark.asyncio\nasync def test_list_tools_actor_none_after_start(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test list_tools when actor is None after start attempt.\"\"\"\n```\n\nThis test ensures that an appropriate error message is raised when trying to list tools after the actor is found to be `None`.\n\n#### `test_call_tool_actor_none_after_start`\n\n```python\n@pytest.mark.asyncio\nasync def test_call_tool_actor_none_after_start(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test call_tool when actor is None after start attempt.\"\"\"\n```\n\nSimilar to the previous test, this checks that calling a tool raises a `RuntimeError` when the actor is uninitialized.\n\n#### `test_list_prompts_actor_none_after_start`\n\n```python\n@pytest.mark.asyncio\nasync def test_list_prompts_actor_none_after_start(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test list_prompts when actor is None after start attempt.\"\"\"\n```\n\nThis test attempts to list prompts and verifies that the expected error is raised if the actor is not available.\n\n### Content Handling Tests\n\nThese tests focus on the handling and processing of different types of content returned as tool call results, specifically `ImageContent` and `EmbeddedResource`.\n\n#### `test_call_tool_image_content_handling`\n\n```python\n@pytest.mark.asyncio\nasync def test_call_tool_image_content_handling(sample_server_params: StdioServerParams, mock_actor: AsyncMock) -> None:\n    \"\"\"Test call_tool with ImageContent.\"\"\"\n```\n\nThis test checks if the `McpWorkbench.call_tool` method processes an image content response correctly. It asserts that the content's type is recognized as `ImageResultContent`.\n\n#### `test_call_tool_embedded_resource_handling`\n\n```python\n@pytest.mark.asyncio\nasync def test_call_tool_embedded_resource_handling(sample_server_params: StdioServerParams, mock_actor: AsyncMock) -> None:\n    \"\"\"Test call_tool with EmbeddedResource.\"\"\"\n```\n\nThis test ensures that when an embedded resource is returned from a tool call, it is processed correctly and can generate a valid `TextResultContent`.\n\n### Exception Handling Tests\n\nThese tests validate the exception management capabilities of the `McpWorkbench`.\n\n#### `test_call_tool_exception_handling`\n\n```python\n@pytest.mark.asyncio\nasync def test_call_tool_exception_handling(sample_server_params: StdioServerParams, mock_actor: AsyncMock) -> None:\n    \"\"\"Test call_tool exception handling.\"\"\"\n```\n\nThis case ensures that when the actor raises an error, the `McpWorkbench` handles the error gracefully, returning appropriate error content.\n\n### Property Access Tests\n\nSeveral tests check the behavior of various properties within the `McpWorkbench`, ensuring they return expected values under different conditions.\n\n#### `test_initialize_result_property_with_actor`\n\n```python\n@pytest.mark.asyncio\nasync def test_initialize_result_property_with_actor(sample_server_params: StdioServerParams, mock_actor: AsyncMock) -> None:\n    \"\"\"Test initialize_result property when actor exists.\"\"\"\n```\n\nThis test validates that the `initialize_result` property correctly reflects the state when the actor exists.\n\n#### `test_initialize_result_property_without_actor`\n\n```python\n@pytest.mark.asyncio\nasync def test_initialize_result_property_without_actor(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test initialize_result property when actor is None.\"\"\"\n```\n\nThis validates that the `initialize_result` property returns `None` when there is no actor.\n\n### Configuration Method Tests\n\nTests are included to validate the configuration methods of the `McpWorkbench`, ensuring they correctly handle initializations from configurations.\n\n#### `test_to_config_method`\n\n```python\n@pytest.mark.asyncio\nasync def test_to_config_method(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test _to_config method.\"\"\"\n```\n\nThis test checks that the `_to_config` method produces a configuration object with the proper server parameters.\n\n#### `test_from_config_method`\n\n```python\n@pytest.mark.asyncio\nasync def test_from_config_method(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test _from_config method.\"\"\"\n```\n\nThis case ensures that a `McpWorkbench` instance can be correctly instantiated from a configuration object.\n\n## Asynchronous Context Management Test\n\n### `test_async_context_manager`\n\n```python\n@pytest.mark.asyncio\nasync def test_async_context_manager(sample_server_params: StdioServerParams) -> None:\n    \"\"\"Test async context manager functionality.\"\"\"\n```\n\nThis test checks that the `McpWorkbench` can be used as an asynchronous context manager, ensuring that the `start` and `stop` methods are called correctly during the context's lifecycle.\n\n## Miscellaneous Tests\n\n### `test_misc_lambda_types`\n\n```python\ndef test_misc_lambda_types() -> None:\n    \"\"\"Test miscellaneous lambda types for coverage.\"\"\"\n```\n\nThis is a general test, verifying the functionality of lambda functions with unknown parameters and return types, ensuring type checks work as expected.\n\n## Conclusion\n\nThe unit tests implemented for the `McpWorkbench` class ensure that various functionalities are thoroughly validated, including error handling, response content management, and property access. This comprehensive testing framework helps ensure the reliability and robustness of the `McpWorkbench`, allowing for future enhancements and integrations without compromising its existing capabilities.",
    "filename": "python/packages/autogen-ext/tests/tools/test_mcp_workbench_warnings_and_errors.py"
  },
  {
    "code": false,
    "content": "# Documentation for Python Code Execution Tool Tests\n\nThis document provides a high-level overview of the test module for the `PythonCodeExecutionTool`, which is part of a larger system for executing Python code. The tests validate the functionality and behavior of the tool, ensuring it performs as expected under various scenarios.\n\n## Overview\n\nThe code is structured as a testing module using the `pytest` framework. It contains two test functions designed to test the usability and reliability of the `PythonCodeExecutionTool` class alongside the `LocalCommandLineCodeExecutor`. The primary objective is to verify that the tool can execute code snippets correctly, handle errors gracefully, and perform serialization and deserialization.\n\nThe key components involved in these tests include:\n- **LocalCommandLineCodeExecutor**: An executor that runs code in a local command line environment.\n- **PythonCodeExecutionTool**: The main tool under test, which uses the executor to execute Python code.\n- **CancellationToken**: A utility that allows for cancellation of code execution.\n- **Logging**: The module captures logs to verify output during tests.\n\n## Tests for Code Execution\n\n### Test Function: `test_python_code_execution_tool`\n\nThis asynchronous test function is responsible for verifying the basic functionality of the `PythonCodeExecutionTool`.\n\n1. **Setup Temporary Directory**: \n   A temporary directory is created to serve as the working directory for the code executor. Using `tempfile.TemporaryDirectory()` ensures that resources are cleaned up after the tests run.\n\n2. **Executor and Tool Initialization**: \n   An instance of `LocalCommandLineCodeExecutor` is created with the temporary directory, and then a `PythonCodeExecutionTool` is initialized using this executor.\n\n3. **Logging Setup**: \n   The test captures log outputs at the INFO level using `caplog`, allowing verification of log messages generated during code execution.\n\n4. **Simple Code Execution**: \n   The first test executes a basic print statement (`print('hello world!')`). The result is checked to ensure it executed successfully and that the expected output appears in the logs.\n\n5. **Computation Test**: \n   A second block of code performs a simple arithmetic operation. The result of this operation is validated to ensure it outputs the expected result (`Result: 300`).\n\n6. **Error Handling Test**: \n   The final block of code is designed to intentionally fail due to a reference to an undefined variable. The success status of the execution is checked to verify that the tool properly handles errors, specifically looking for a `NameError` in the output.\n\n### Assertions\n\nThroughout the testing process, several assertions are made:\n- It verifies the output of executed code snippets using conditions like `assert \"hello world!\" in caplog.text`.\n- It checks the success status of code execution results with flags such as `result.success`.\n- The presence of error messages is also asserted for failure scenarios, ensuring the tool behaves correctly under erroneous conditions.\n\n## Test for Serialization\n\n### Test Function: `test_python_code_execution_tool_serialization`\n\nThis test focuses on the serialization and deserialization capabilities of the `PythonCodeExecutionTool`.\n\n1. **Setup Temporary Directory**: \n   Similar to the previous test, a temporary directory is created for the code executor.\n\n2. **Original Tool Creation**: \n   An original instance of `PythonCodeExecutionTool` is created using the `LocalCommandLineCodeExecutor`. This is used to perform serialization operations.\n\n3. **Serialization**: \n   The tool is serialized using `dump_component()`, which is expected to return a configuration object. The test checks that the executor was correctly included in the serialized data.\n\n4. **Deserialization**: \n   The test then deserializes the configuration back into a new `PythonCodeExecutionTool` instance using `load_component(config)`.\n\n5. **Configuration Verification**: \n   Final assertions verify that the loaded tool is of the correct type and that its configuration matches the original tool.\n\n## Summary\n\nThis testing module ensures that the `PythonCodeExecutionTool` can execute code snippets accurately, handle errors, and persist its configuration through serialization. By employing Pytest functionalities and structured assertions, the tests contribute to maintaining the reliability and robustness of the tool as part of a larger code execution framework.",
    "filename": "python/packages/autogen-ext/tests/tools/test_python_code_executor_tool.py"
  },
  {
    "content": "# Magentic-One\n\n> Magentic-One is now available as part of the `autogen-agentchat` library.\n> Please see the [user guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html) for information.\n\n> Looking for the original implementation of Magentic-One? It is available [here](https://github.com/microsoft/autogen/tree/v0.4.4/python/packages/autogen-magentic-one).\n\n[Magentic-One](https://aka.ms/magentic-one-blog) is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step forward for multi-agent systems, achieving competitive performance on a number of agentic benchmarks (see the [technical report](https://arxiv.org/abs/2411.04468) for full details).\n\nWhen originally released in [November 2024](https://aka.ms/magentic-one-blog) Magentic-One was [implemented directly on the `autogen-core` library](https://github.com/microsoft/autogen/tree/v0.4.4/python/packages/autogen-magentic-one). We have now ported Magentic-One to use `autogen-agentchat`, providing a more modular and easier to use interface. To this end, the older implementation is deprecated, but can be accessed at [https://github.com/microsoft/autogen/tree/v0.4.4/python/packages/autogen-magentic-one](https://github.com/microsoft/autogen/tree/v0.4.4/python/packages/autogen-magentic-one).\n\nMoving forward, the Magentic-One orchestrator [MagenticOneGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.MagenticOneGroupChat) is now simply an AgentChat team, supporting all standard AgentChat agents and features. Likewise, Magentic-One's [MultimodalWebSurfer](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html#autogen_ext.agents.web_surfer.MultimodalWebSurfer), [FileSurfer](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html#autogen_ext.agents.file_surfer.FileSurfer), and [MagenticOneCoderAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html) agents are now broadly available as AgentChat agents, to be used in any AgentChat workflows.\n\nLastly, there is a helper class, [MagenticOne](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html#autogen_ext.teams.magentic_one.MagenticOne), which bundles all of this together as it was in the paper with minimal configuration\n\n## Citation\n\n```\n@misc{fourney2024magenticonegeneralistmultiagentsolving,\n      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},\n      author={Adam Fourney and Gagan Bansal and Hussein Mozannar and Cheng Tan and Eduardo Salinas and Erkang and Zhu and Friederike Niedtner and Grace Proebsting and Griffin Bassman and Jack Gerrits and Jacob Alber and Peter Chang and Ricky Loynd and Robert West and Victor Dibia and Ahmed Awadallah and Ece Kamar and Rafah Hosn and Saleema Amershi},\n      year={2024},\n      eprint={2411.04468},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2411.04468},\n}\n```",
    "filename": "python/packages/autogen-magentic-one/README.md"
  },
  {
    "content": "# AutoGen Studio\n\n[![PyPI version](https://badge.fury.io/py/autogenstudio.svg)](https://badge.fury.io/py/autogenstudio)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/autogenstudio)\n\n![ARA](https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png)\n\nAutoGen Studio is an AutoGen-powered AI app (user interface) to help you rapidly prototype AI agents, enhance them with skills, compose them into workflows and interact with them to accomplish tasks. It is built on top of the [AutoGen](https://microsoft.github.io/autogen) framework, which is a toolkit for building AI agents.\n\nCode for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio)\n\n> [!WARNING]\n> AutoGen Studio is under active development and is currently not meant to be a production-ready app. Expect breaking changes in upcoming releases. [Documentation](https://microsoft.github.io/autogen/docs/autogen-studio/getting-started) and the `README.md` might be outdated.\n\n## Updates\n\n- **2024-11-14:** AutoGen Studio is being rewritten to use the updated AutoGen 0.4.0 api AgentChat api.\n- **2024-04-17:** April 17: AutoGen Studio database layer is now rewritten to use [SQLModel](https://sqlmodel.tiangolo.com/) (Pydantic + SQLAlchemy). This provides entity linking (skills, models, agents and workflows are linked via association tables) and supports multiple [database backend dialects](https://docs.sqlalchemy.org/en/20/dialects/) supported in SQLAlchemy (SQLite, PostgreSQL, MySQL, Oracle, Microsoft SQL Server). The backend database can be specified a `--database-uri` argument when running the application. For example, `autogenstudio ui --database-uri sqlite:///database.sqlite` for SQLite and `autogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL.\n- **2024-03-12:** Default directory for AutoGen Studio is now /home/\\<USER\\>/.autogenstudio. You can also specify this directory using the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database and other files in the specified directory e.g. `/path/to/folder/database.sqlite`. `.env` files in that directory will be used to set environment variables for the app.\n\n## Project Structure:\n\n- `autogenstudio/` contains code for the backend classes and web api (FastAPI)\n- `frontend/` contains code for the webui, built with Gatsby and TailwindCSS\n\n## Installation\n\nThere are two ways to install AutoGen Studio - from PyPi or from the source. We **recommend installing from PyPi** unless you plan to modify the source code.\n\n### Install from PyPi (Recommended)\n\nWe recommend using a virtual environment (e.g., venv) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:\n\n```bash\npip install -U autogenstudio\n```\n\n### Install from source\n\n_Note: This approach requires some familiarity with building interfaces in React._\n\n### Important: Git LFS Requirement\n\nAutoGen Studio uses Git Large File Storage (LFS) for managing image and other large files. If you clone the repository without git-lfs, you'll encounter build errors related to image formats.\n\n**Before cloning the repository:**\n\n1. Install git-lfs:\n\n   ```bash\n   # On Debian/Ubuntu\n   apt-get install git-lfs\n\n   # On macOS with Homebrew\n   brew install git-lfs\n\n   # On Windows with Chocolatey\n   choco install git-lfs\n   ```\n\n2. Set up git-lfs:\n   ```bash\n   git lfs install\n   ```\n\n**If you've already cloned the repository:**\n\n```bash\ngit lfs install\ngit lfs fetch --all\ngit lfs checkout  # downloads all missing image files to the working directory\n```\n\nThis setup is handled automatically if you use the dev container method of installation.\n\nYou have two options for installing from source: manually or using a dev container.\n\n#### A) Install from source manually\n\n1. Ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed.\n2. Clone the AutoGen Studio repository and install its Python dependencies using `pip install -e .`\n3. Navigate to the `python/packages/autogen-studio/frontend` directory, install the dependencies, and build the UI:\n\n   ```bash\n   npm install -g gatsby-cli\n   npm install --global yarn\n   cd frontend\n   yarn install\n   yarn build\n   # Windows users may need alternative commands to build the frontend:\n   gatsby clean && rmdir /s /q ..\\\\autogenstudio\\\\web\\\\ui 2>nul & (set \\\"PREFIX_PATH_VALUE=\\\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\\\autogenstudio\\\\web\\\\ui\n   ```\n\n#### B) Install from source using a dev container\n\n1. Follow the [Dev Containers tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial) to install VS Code, Docker and relevant extensions.\n2. Clone the AutoGen Studio repository.\n3. Open `python/packages/autogen-studio/`in VS Code. Click the blue button in bottom the corner or press F1 and select _\"Dev Containers: Reopen in Container\"_.\n4. Build the UI:\n\n   ```bash\n   cd frontend\n   yarn build\n   ```\n\n### Running the Application\n\nOnce installed, run the web UI by entering the following in your terminal:\n\n```bash\nautogenstudio ui --port 8081\n```\n\nThis command will start the application on the specified port. Open your web browser and go to <http://localhost:8081/> to use AutoGen Studio.\n\nAutoGen Studio also takes several parameters to customize the application:\n\n- `--host <host>` argument to specify the host address. By default, it is set to `localhost`.\n- `--appdir <appdir>` argument to specify the directory where the app files (e.g., database and generated user files) are stored. By default, it is set to the `.autogenstudio` directory in the user's home directory.\n- `--port <port>` argument to specify the port number. By default, it is set to `8080`.\n- `--reload` argument to enable auto-reloading of the server when changes are made to the code. By default, it is set to `False`.\n- `--database-uri` argument to specify the database URI. Example values include `sqlite:///database.sqlite` for SQLite and `postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL. If this is not specified, the database URL defaults to a `database.sqlite` file in the `--appdir` directory.\n- `--upgrade-database` argument to upgrade the database schema to the latest version. By default, it is set to `False`.\n\nNow that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.\n\n## AutoGen Studio Lite\n\nAutoGen Studio Lite provides a lightweight way to quickly prototype and experiment with AI agent teams. It's designed for rapid experimentation without the full database setup.\n\n### CLI Usage\n\nLaunch Studio Lite from the command line:\n\n```bash\n# Quick start with default team\nautogenstudio lite\n\n# Use custom team file\nautogenstudio lite --team ./my_team.json --port 8080\n\n# Custom session name\nautogenstudio lite --session-name \"My Experiment\" --auto-open\n```\n\n### Programmatic Usage\n\nUse Studio Lite directly in your Python code:\n\n```python\nfrom autogenstudio.lite import LiteStudio\n\n# Quick start with default team\nstudio = LiteStudio()\n# Use with AutoGen team objects\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nteam = RoundRobinGroupChat([agent1, agent2], termination_condition=...)\n\n# Context manager usage\nwith LiteStudio(team=team) as studio:\n    # Studio runs in background\n    # Do other work here\n    pass\n```\n\n#### Local frontend development server\n\nSee `./frontend/README.md`\n\n## Contribution Guide\n\nWe welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:\n\n- Review the overall AutoGen project [contribution guide](https://github.com/microsoft/autogen?tab=readme-ov-file#contributing)\n- Please review the AutoGen Studio [roadmap](https://github.com/microsoft/autogen/issues/4006) to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with `help-wanted`\n- Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.\n- Submit a pull request with your contribution!\n- If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in `.devcontainer/README.md` to use it\n- Please use the tag `proj-studio` for any issues, questions, and PRs related to Studio\n\n## FAQ\n\nPlease refer to the AutoGen Studio [FAQs](https://microsoft.github.io/autogen/docs/autogen-studio/faqs) page for more information.\n\n## Acknowledgements\n\nAutoGen Studio is Based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Gagan Bansal, Adam Fourney, Victor Dibia, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).",
    "filename": "python/packages/autogen-studio/README.md"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis Python module is designed to provide essential components for managing data related to teams in a broader application context, likely aimed at facilitating operations related to team data management.\n\n## Imports\n\nThe module imports four key components from different parts of the application:\n\n1. **DatabaseManager**:\n   - Located in the `database` submodule, this class is likely responsible for handling database connections, queries, and data manipulations.\n\n2. **Team**:\n   - Imported from the `datamodel` submodule, this class probably defines the structure of a team object, including attributes and methods pertinent to team data.\n\n3. **TeamManager**:\n   - Taken from the `teammanager` submodule, this class likely coordinates processes related to teams, such as adding, retrieving, updating, and deleting teams.\n\n4. **__version__**:\n   - This is a module-level variable that signifies the version of the codebase. This is useful for maintaining backward compatibility and managing updates.\n\n## Public Interface\n\nThe `__all__` list is defined as follows:\n\n```python\n__all__ = [\"DatabaseManager\", \"Team\", \"TeamManager\", \"__version__\"]\n```\n\nThis special variable specifies which components of the module will be accessible when the module is imported using the `from module import *` syntax. It enhances encapsulation by limiting the namespace to specific elements, helping to prevent potential name conflicts and making the public API clear.\n\n### Components in `__all__`\n\n- **DatabaseManager**: Exposes the functionality to manage database operations.\n- **Team**: Makes the team data structure available for instantiation and manipulation.\n- **TeamManager**: Provides higher-level functionality for managing collections of teams.\n- **__version__**: Allows users to check which version of the module they are working with.\n\n## Purpose of the Module\n\nOverall, the module seems to serve as a central point for managing team-related data and operations within an application. It brings together components that interact with the database, define data models, and implement business logic regarding team management. By consolidating these functionalities, the module enhances code maintainability and organization, allowing other parts of the application to leverage these components seamlessly.\n\n## Conclusion\n\nThis module provides efficient abstractions for working with teams by encapsulating the relevant functionality and data structures into readable components. It facilitates the broader objectives of the application while maintaining a clear interface for developers interacting with team data.",
    "filename": "python/packages/autogen-studio/autogenstudio/__init__.py"
  },
  {
    "code": false,
    "content": "# AutoGen Studio CLI Documentation\n\nThis document provides a comprehensive overview of the AutoGen Studio Command Line Interface (CLI) source code, detailing its structure, functionalities, and commands.\n\n## Overview\n\nThe provided source code is a CLI for AutoGen Studio, leveraging the Typer library to create interactive command-line applications. The CLI comprises various commands to manage, start, and deploy the AutoGen Studio application and its features. The project also uses Uvicorn, an ASGI server, to run web applications. Environmental configurations and warnings have also been handled in the code.\n\n## Importing Modules\n\nThe code begins by importing necessary libraries:\n\n- **os**: For interacting with the operating system.\n- **warnings**: For managing warning messages, particularly related to deprecated features.\n- **typer**: For creating a user-friendly command-line interface.\n- **uvicorn**: For running ASGI applications.\n- **typing** and **typing_extensions**: For type hinting and annotations.\n\nAdditionally, it imports the `VERSION` from a local `version` module.\n\n## Setting Up the Typer Application\n\nA `typer.Typer()` instance named `app` is created to manage different commands of the CLI. The application suppresses specific deprecation warnings related to the `websockets` library, indicating that some features may no longer be maintained.\n\n## Function: `get_env_file_path`\n\nThis utility function determines the path for storing environment variables. It checks for the existence of the application directory, creating it if necessary. The function returns the path to a file named `temp_env_vars.env` that will be used to store temporary environment variables.\n\n## Command: `ui`\n\n### Description\n\nThe `ui` command is responsible for launching the AutoGen Studio user interface. \n\n### Parameters\n\n- **host**: IP address to bind the UI (defaults to `127.0.0.1`).\n- **port**: Port on which to run the UI (defaults to `8081`).\n- **workers**: Number of worker processes (default is `1`).\n- **reload**: Indicates whether to watch for code changes and auto-restart (defaults to `False`).\n- **docs**: Toggle for generating API documentation (defaults to `True`).\n- **appdir**: Optional path to the application directory.\n- **database_uri**: Optional URI for a database connection.\n- **auth_config**: Optional path to authentication configurations (YAML file).\n- **upgrade_database**: Flag to indicate if the database should be upgraded.\n\n### Functionality\n\nThe command collects various environmental configurations based on provided arguments, validates the existence of required files (like `auth_config`), writes these configurations to the temporary environment file, and finally starts the Uvicorn server to run the UI application.\n\n## Command: `serve`\n\n### Description\n\nThe `serve` command is used to serve an API endpoint for an AutoGen studio workflow based on a JSON file.\n\n### Parameters\n\n- **team**: Path to the team JSON file.\n- **host**: IP address to bind the API server (defaults to `127.0.0.1`).\n- **port**: Port on which to run the API (defaults to `8084`).\n- **workers**: Number of worker processes (default is `1`).\n- **reload**: Toggle for watching code changes and auto-restarting (defaults to `False`).\n- **docs**: Toggle for generating API documentation (defaults to `False`).\n\n### Functionality\n\nIt checks for the presence of the specified team JSON file, sets relevant environment variables, and starts the Uvicorn server to host an API endpoint.\n\n## Command: `version`\n\n### Description\n\nThis command prints the version of the AutoGen Studio CLI.\n\n### Functionality\n\nWhen invoked, this command outputs the current version stored in the `VERSION` variable.\n\n## Command: `lite`\n\n### Description\n\nThe `lite` command launches AutoGen Studio in a lightweight mode for faster prototyping and experimentation.\n\n### Parameters\n\n- **team**: Optional path to a team JSON/YAML file; uses a default if omitted.\n- **host**: IP address the lightweight mode operates on (defaults to `127.0.0.1`).\n- **port**: Port the lightweight mode listens to (defaults to `8080`).\n- **auto_open**: Automatically opens in a browser (defaults to `True`).\n- **session_name**: Sets a name for the created session.\n\n### Functionality\n\nThis command initializes an instance of `LiteStudio`, then starts it. It handles graceful termination on keyboard interrupts.\n\n## Function: `run`\n\nThis function serves as the main entry point to execute the Typer application. It invokes the `app()` function, allowing the user to interact with the command-line interface.\n\n## Entry Point\n\nThe code checks if the script is executed as the main program (`if __name__ == \"__main__\":`), ensuring that the Typer application starts when run directly, facilitating user interaction with the CLI commands described. \n\nOverall, the AutoGen Studio CLI is robust, integrating various functionalities that enable users to run UI applications, serve API endpoints, and manage configurations efficiently.",
    "filename": "python/packages/autogen-studio/autogenstudio/cli.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Code\n\nThis code serves as a module for managing database-related functionality within a Python package. Below, we will analyze its components and their roles to provide a clearer understanding.\n\n## Overview\n\nThe provided code snippet primarily deals with the export of a class named `DatabaseManager` from a module. This is done in a manner that allows other modules or packages to access it easily when they import the current module.\n\n## Import Statement\n\n```python\nfrom .db_manager import DatabaseManager\n```\n\nThis line imports the `DatabaseManager` class from a module named `db_manager` that is located in the same package. The leading dot (`.`) signifies a relative import, which means that the script is likely part of a larger package structure. By doing this, the `DatabaseManager` class becomes accessible within the current module.\n\n### Purpose of DatabaseManager\n\nWhile the actual implementation of `DatabaseManager` is not provided in this snippet, the name suggests that it likely encapsulates functionality related to database interactions. Such functionalities may include:\n\n- Establishing connections to a database.\n- Executing SQL queries.\n- Retrieving and inserting data.\n- Managing transactions and ensuring data integrity.\n- Closing connections appropriately.\n\nThis modular approach is beneficial for encapsulating database logic, promoting reusability, and adhering to the principles of separation of concerns.\n\n## The `__all__` Declaration\n\n```python\n__all__ = [\n    \"DatabaseManager\",\n]\n```\n\nThis line is crucial for module export behavior. The `__all__` variable is a list that explicitly defines the public API of this module. It indicates which classes, functions, or variables should be accessible when a user imports the module using a wildcard import (e.g., `from your_module import *`). \n\nBy including `DatabaseManager` in `__all__`, the author of the module communicates that this is the intended interface for public usage, while any other code or components not listed in this array should be treated as private or internal and not intended for public use.\n\n## Summary\n\nIn summary, this code snippet is part of a Python package that exports a `DatabaseManager` class for managing database operations. The use of both relative import and `__all__` ensures that the module's interface is clean and well-defined, guiding users of the module toward its intended functionalities. \n\n### Implications for Future Development\n\nThis modular setup allows for easy extensions and modifications. Future developers can add additional classes or functions within this current module or even other modules in the package, enhancing the overall database management system without disrupting the existing functionality. \n\nThe exportation of only selected components through the `__all__` variable also helps in avoiding namespace pollution, thus contributing to cleaner and more maintainable code. \n\nBy following this approach, the module not only facilitates organization and clarity but also adheres to best practices in Python programming.",
    "filename": "python/packages/autogen-studio/autogenstudio/database/__init__.py"
  },
  {
    "code": false,
    "content": "# DatabaseManager Module Documentation\n\nThis module contains a class `DatabaseManager`, responsible for managing database interactions including initialization, schema management, CRUD (Create, Read, Update, Delete) operations, and team-related functionalities. The code leverages SQLModel for object relational management and Loguru for enhanced logging capabilities.\n\n## CustomJSONEncoder Class\n\n### Purpose:\nThe `CustomJSONEncoder` class extends `json.JSONEncoder` to provide custom serialization for certain Python objects including `datetime` and `enum.Enum`. \n\n### Functionality:\n- **Method:** `default(o)`: \n  - Converts secret values to a string if `get_secret_value` is callable.\n  - Serializes `datetime` objects to ISO format.\n  - Converts `enum.Enum` instances to their respective values.\n  - Falls back to the default behavior for other types.\n\n---\n\n## DatabaseManager Class\n\n### Initialization:\n- **Constructor:** `__init__(self, engine_uri: str, base_dir: Optional[Union[str, Path]] = None) -> None`: \n  - Initializes the database connection with the provided URI and a base directory for migration files.\n  - Sets up an SQLAlchemy engine and schema manager to handle migrations.\n\n### Private Methods:\n- **Method:** `_should_auto_upgrade(self) -> bool`: \n  - Checks for schema differences to determine if an auto-upgrade is required.\n\n---\n\n## Database Initialization and Management\n\n### Method: `initialize_database(self, auto_upgrade: bool = False, force_init_alembic: bool = True) -> Response`\n- **Purpose:** Prepares the database either by creating tables and seeding data if it's the first run or upgrading the schema if necessary.\n- **Parameters:**\n  - `auto_upgrade`: Automatically applies migrations if true.\n  - `force_init_alembic`: Reinitializes Alembic configurations.\n- **Logic:**\n  - Acquires an initialization lock to prevent concurrent operations.\n  - Creates tables and initializes migrations if no tables exist.\n  - Checks and applies schema upgrades based on the specified parameters.\n\n### Method: `reset_db(self, recreate_tables: bool = True) -> Response`\n- **Purpose:** Resets the database by dropping all tables and optionally recreating them.\n- **Parameters:**\n  - `recreate_tables`: Controls whether to recreate tables after dropping them.\n- **Logic:**\n  - Acquires a lock to prevent resets during ongoing operations, then drops all tables, and can recreate them based on the parameter.\n\n---\n\n## CRUD Operations\n\n### Method: `upsert(self, model: BaseDBModel, return_json: bool = True) -> Response`\n- **Purpose:** Creates or updates an instance of a database model.\n- **Parameters:**\n  - `model`: The instance to be upserted.\n  - `return_json`: Determines the format of the response.\n- **Logic:**\n  - Checks for an existing record and updates it if found, or creates a new one.\n  - Commits the changes and refreshes the model state.\n\n### Method: `get(self, model_class: type[BaseDBModel], filters: dict | None = None, return_json: bool = False, order: str = \"desc\")`\n- **Purpose:** Retrieves records matching optional filters from a specified model.\n- **Parameters:**\n  - `model_class`: The SQLModel class to query.\n  - `filters`: Optional filters to apply for fetching.\n  - `return_json`: Determines if the results should be returned as JSON.\n  - `order`: Sorting order (ascending or descending).\n- **Logic:** \n  - Constructs and executes a query based on conditions including filters and sorting.\n\n### Method: `delete(self, model_class: type[BaseDBModel], filters: dict | None = None) -> Response`\n- **Purpose:** Deletes records matching specified criteria.\n- **Parameters:**\n  - `model_class`: The SQLModel type from which records will be deleted.\n  - `filters`: Conditions to identify which records to delete.\n- **Logic:**\n  - Executes a deletion statement depending on the models found matching the filters.\n\n---\n\n## Team Management Operations\n\n### Method: `import_team(self, team_config: Union[str, Path, dict], user_id: str, check_exists: bool = False) -> Response`\n- **Purpose:** Imports a single team configuration into the database.\n- **Parameters:**\n  - `team_config`: Path to the config file or a dict representation.\n  - `user_id`: User ID associated with the team.\n  - `check_exists`: Whether to check for existing configurations.\n- **Logic:**\n  - Validates if a team configuration already exists; if not, creates a new team entry.\n\n### Method: `import_teams_from_directory(self, directory: Union[str, Path], user_id: str, check_exists: bool = False) -> Response`\n- **Purpose:** Batch imports team configurations from a specified directory.\n- **Parameters:**\n  - `directory`: Path containing team configuration files.\n  - `user_id`: User ID for each team.\n  - `check_exists`: Whether to check for duplicates before inserting.\n- **Logic:**\n  - Loads all configurations and attempts to import each using the `import_team` method, collating results.\n\n### Method: `_check_team_exists(self, config: dict, user_id: str) -> Optional[Team]`\n- **Purpose:** Checks if a team configuration that matches the provided parameters already exists.\n- **Parameters:**\n  - `config`: The configuration to check against.\n  - `user_id`: The associated user\u2019s identifier.\n- **Logic:** \n  - Fetches teams associated with the user and compares configurations to identify duplicates.\n\n---\n\n## Connection Management\n\n### Method: `close(self)`: \n- **Purpose:** Properly disposes and closes the database connection when no longer needed.\n- **Logic:** \n  - Logs the closure process and disposes of the SQLAlchemy engine to free up resources.\n\n---\n\n## Logging and Error Handling\n\nThroughout the methods, logging statements are employed to provide contextual information about operations and outcomes. Exceptions are caught and logged, providing descriptive error messages to assist in debugging. The usage of a response object to encapsulate status messages and results from the operations is consistent for better handling of responses in calling functions.",
    "filename": "python/packages/autogen-studio/autogenstudio/database/db_manager.py"
  },
  {
    "code": false,
    "content": "# SchemaManager Documentation\n\n## Overview\n\nThe `SchemaManager` is a class designed to manage database schema validation and migrations within a Python application. It uses Alembic, a lightweight database migration tool for use with SQLAlchemy. The class allows for initializing, upgrading, and checking the status of database schemas and migrations, making it essential for ensuring database consistency when models change.\n\n---\n\n## Class Definition\n\n### SchemaManager Class\n\nThe `SchemaManager` class provides methods for creating, validating, and managing database migrations. It interacts with an SQLAlchemy engine and follows familiar operations associated with database migrations, such as initialization, upgrades, and schema checks.\n\n#### Constructor\n\n```python\ndef __init__(self, engine: Engine, base_dir: Optional[Path] = None):\n```\n- **Parameters:**\n  - `engine`: An instance of SQLAlchemy's `Engine` for connecting to the database.\n  - `base_dir`: Optional. The base directory for Alembic files; defaults to the current working directory if not specified.\n  \nThe constructor sets up initial configurations without performing any operations on the filesystem or database.\n\n---\n\n## Migration Initialization and Management\n\n### Initializing Migrations\n\n#### Method: `initialize_migrations`\n\n```python\ndef initialize_migrations(self, force: bool = False) -> bool:\n```\n- **Parameters:**\n  - `force`: If `True`, forces reinitialization of migrations by cleaning existing configurations.\n\n- **Returns:**\n  - `bool`: Indicates whether the initialization was successful.\n\nThis method initializes Alembic migration files. It checks if an Alembic configuration exists; if not, it initializes a new one. If `force` is specified, it cleans up the existing configuration.\n\n### Updating Configuration\n\n#### Method: `_update_configuration`\n\n```python\ndef _update_configuration(self) -> None:\n```\n- **Returns:** `None`\n\nUpdates the existing Alembic configuration by regenerating the `alembic.ini` file and modifying `env.py` as needed. This ensures that the Alembic setup reflects the current application requirements.\n\n### Cleanup Existing Configuration\n\n#### Method: `_cleanup_existing_alembic`\n\n```python\ndef _cleanup_existing_alembic(self) -> None:\n```\n- **Returns:** `None`\n\nRemoves existing Alembic configuration files to facilitate fresh initialization. This includes deleting the `alembic` directory and the `alembic.ini` file.\n\n---\n\n## Schema Operations\n\n### Generating Revisions\n\n#### Method: `generate_revision`\n\n```python\ndef generate_revision(self, message: str = \"auto\") -> Optional[str]:\n```\n- **Parameters:**\n  - `message`: A description for the migration; defaults to \"auto\".\n\n- **Returns:**\n  - `Optional[str]`: The revision ID if successful, or `None`.\n\nThis method generates a new migration based on the current schema state, performing an auto-detection of changes to create a new migration revision.\n\n### Upgrading Schema\n\n#### Method: `upgrade_schema`\n\n```python\ndef upgrade_schema(self, revision: str = \"head\") -> bool:\n```\n- **Parameters:**\n  - `revision`: Target migration revision to upgrade to (default is \"head\").\n\n- **Returns:**\n  - `bool`: Indicates if the schema was upgraded successfully.\n\nThis function performs the database schema upgrade as specified, allowing developers to elevate the schema version in alignment with the application\u2019s models.\n\n---\n\n## Schema Status Checking\n\n### Checking Schema Status\n\n#### Method: `check_schema_status`\n\n```python\ndef check_schema_status(self) -> Tuple[bool, str]:\n```\n- **Returns:**\n  - `Tuple[bool, str]`: A tuple indicating if an upgrade is needed and the status message.\n\nThis method checks whether the current database schema matches the model definitions and migration versions. If discrepancies are found, it returns detailed information about them.\n\n### Fetching Current & Head Revisions\n\n#### Method: `get_current_revision`\n\n```python\ndef get_current_revision(self) -> Optional[str]:\n```\n- **Returns:**\n  - `Optional[str]`: The current database revision.\n\n#### Method: `get_head_revision`\n\n```python\ndef get_head_revision(self) -> Optional[str]:\n```\n- **Returns:**\n  - `Optional[str]`: The latest available migration revision.\n\nThese methods retrieve the current and latest migration revisions from the database, which are essential for understanding the migration context.\n\n---\n\n## Schema Differences and Pending Migrations\n\n### Detecting Schema Differences\n\n#### Method: `get_schema_differences`\n\n```python\ndef get_schema_differences(self) -> List[tuple]:\n```\n- **Returns:**\n  - `List[tuple]`: A list detailing the differences between the current database schema and the specified models.\n\nThis function identifies any discrepancies in the schema that may require migration actions.\n\n### Getting Pending Migrations\n\n#### Method: `get_pending_migrations`\n\n```python\ndef get_pending_migrations(self) -> List[str]:\n```\n- **Returns:**\n  - `List[str]`: List of pending migration revision IDs.\n\nThis method retrieves a list of migrations that have yet to be applied, allowing developers to track what needs to be done to synchronize the database with the application's models.\n\n---\n\n## Logging and Status Information\n\n### Print Migration Status\n\n#### Method: `print_status`\n\n```python\ndef print_status(self) -> None:\n```\n- **Returns:** `None`\n\nOutputs the current migration status information to the logger, including the current revision, head revision, pending migrations, and any unmigrated changes, providing a clear overview of the database's migration state.\n\n### Ensuring Schema is Up-To-Date\n\n#### Method: `ensure_schema_up_to_date`\n\n```python\ndef ensure_schema_up_to_date(self) -> bool:\n```\n- **Returns:**\n  - `bool`: Indicates if the schema is successfully brought up-to-date.\n\nThis method resets all migrations, drops the `alembic_version` table, reinitializes Alembic, and generates a fresh migration based on the current schema state, ensuring the database schema is completely synchronized with the application models.\n\n---\n\n## Conclusion\n\nThe `SchemaManager` class serves as a robust solution for managing database migrations and validating schema consistency using Alembic within Python applications. By providing several methods for schema operations, including initialization, upgrades, and status checks, it allows developers to maintain database integrity while adapting to evolving application requirements.",
    "filename": "python/packages/autogen-studio/autogenstudio/database/schema_manager.py"
  },
  {
    "code": false,
    "content": "# Overview of the Module\n\nThis module appears to be part of a larger application that likely deals with managing a gallery system, messaging, and possibly team or user interactions. It imports various components from a database module and defines a set of types and configurations relevant to its functionalities. The structure is set up for easy access and logical organization of components used within the application.\n\n## Imports\n\nThe module imports a variety of classes, configurations, and types from two primary source modules, `.db` and `.types`. This suggests that those modules handle the core data models and custom types required by this module.\n\n- **From .db**:\n  - **BaseDBModel**: Presumably a base class for database models, providing common functionality or structure.\n  - **Gallery**: Likely a model representing a gallery structure of some kind.\n  - **Message**: A model that might handle messaging functionalities.\n  - **Run**: Possibly a model to track executions or operations, such as jobs or tasks.\n  - **RunStatus**: Represents the status of a given run, indicating whether it is ongoing, completed, or failed.\n  - **Session**: Likely handles user sessions or interactions within the application.\n  - **Settings**: Represents configuration settings for the application.\n  - **Team**: A model possibly representing user teams or groups.\n\n- **From .types**:\n  - Imports numerous configurations and metadata types relevant to the application such as `EnvironmentVariable`, `GalleryComponents`, `GalleryConfig`, `GalleryMetadata`, `LLMCallEventMessage`, `MessageConfig`, `MessageMeta`, `Response`, `SettingsConfig`, `SocketMessage`, and `TeamResult`.\n\n## Public Interface\n\nThe module defines an `__all__` list which controls what can be imported when a user does a wildcard import (i.e., `from module import *`). This is a standard practice to prevent unintended exposure of internal components.\n\n### Included Components\n\nThe `__all__` list includes the following key components:\n\n- **Models**: Instances of `Team`, `Run`, `RunStatus`, `Session`, `Message`, and `Gallery` are declared to be part of the public API, suggesting they are core to the application's functionality.\n- **Config and Metadata**: The inclusion of configurations like `MessageConfig`, `SettingsConfig`, and others implies they are critical for managing application behavior.\n- **Response and Messaging**: Types like `Response`, `SocketMessage`, and `LLMCallEventMessage` indicate functionalities for communication, likely for handling interactions with users or other services.\n- **Gallery Elements**: With the inclusion of `GalleryComponents`, `GalleryConfig`, and `GalleryMetadata`, it is evident that the module has a strong focus on managing and displaying galleries or related visual elements.\n\n## Possible Use Cases\n\nGiven the imported models and types, this module might serve several key purposes:\n\n1. **Team Management**: Handling teams, their configurations, and managing their sessions may be a primary focus.\n2. **Messaging System**: The presence of various message-related classes indicates that the application could integrate a chat or event messaging system either for notifying users or sending requests/responses.\n3. **Gallery Management**: Given the emphasis on gallery components and configurations, there is likely functionality to create, display, and manage galleries and their settings.\n4. **Execution Tracking**: The `Run` and `RunStatus` models suggest the application may include some form of task execution tracking, possibly for background jobs or processes.\n\n## Conclusion\n\nThis module serves as a foundational interface for managing various aspects of a gallery and messaging application. The structured import of models and types encapsulates key functionality, promoting modularity and reusability across the application. The definitions in the `__all__` list provide a clear public API, ensuring that the essential components are readily accessible for further development or interaction, facilitating collaborative software architecture.",
    "filename": "python/packages/autogen-studio/autogenstudio/datamodel/__init__.py"
  },
  {
    "code": false,
    "content": "# High-Level Documentation of the Database Models in AutoGenStudio\n\nThis document provides a comprehensive overview of the database models defined in the provided source code for AutoGenStudio. The code outlines how core data types are serialized and stored in a database using SQLAlchemy and SQLModel libraries. Various classes extend a base model to encapsulate different entities within the system, facilitating structured data management.\n\n## Overview of Database Models\n\n### BaseDBModel Class\n\nThe `BaseDBModel` serves as the foundational class for all other database-related models. It is not meant to be instantiated directly but rather to be inherited by other models. The class includes the following key fields:\n- `id`: An optional integer that serves as the primary key.\n- `created_at`: A timestamp indicating when the record was created, with a default value set to the current time.\n- `updated_at`: A timestamp that updates upon any modification of the record.\n- `user_id`: An optional string to associate the record with a user.\n- `version`: An optional string for versioning purposes, initializing to \"0.0.1\".\n\n### Team Class\n\nThe `Team` class inherits from `BaseDBModel` and represents a team entity within AutoGenStudio. It includes:\n- `component`: A field to store a JSON-encoded component model or a dictionary, allowing flexibility in what constitutes a team\u2019s core information.\n\n### Message Class\n\nSimilarly inheriting from `BaseDBModel`, the `Message` class encapsulates messaging-related data. Key fields include:\n- `config`: A JSON representation of the message configuration, initialized with a default factory creating a simple message structure.\n- `session_id`: An optional integer that references the session to which the message belongs.\n- `run_id`: An optional integer that links the message to a specific run.\n- `message_meta`: A JSON field for additional metadata related to the message.\n\n### Session Class\n\nThe `Session` class also extends from `BaseDBModel` and contains relevant attributes for session management:\n- `team_id`: Optionally links to a specific team, ensuring that sessions can be associated with their respective teams.\n- `name`: An optional string to identify the session.\n- The class also includes a method, `parse_datetime`, to handle and validate date inputs, converting them to a uniform datetime format.\n\n## Run Management Classes\n\n### RunStatus Enum\n\nThe `RunStatus` enum defines the various states a run can have, such as `CREATED`, `ACTIVE`, `COMPLETE`, and more, improving code readability and maintenance.\n\n### Run Class\n\nThe `Run` class represents a single execution run within a session. Key attributes include:\n- `session_id`: A mandatory reference linking the run to a specific session.\n- `status`: The current status as defined by the `RunStatus` enum.\n- `task`: Stores the configuration of the user task being executed.\n- `team_result`: Captures the results pertaining to the team during the run.\n- `error_message`: An optional field to capture any errors.\n- `messages`: A list of messages related to the run, stored in JSON format.\n\n### Gallery Class\n\nThe `Gallery` class is designed to manage gallery-related configurations and attributes. It includes:\n- `config`: A JSON representation of gallery configurations, utilizing default factories to initialize various components within the gallery context.\n\n### Settings Class\n\nThe `Settings` class captures application-wide settings which can be serialized into JSON. It includes:\n- `config`: A JSON representation of the settings configuration.\n\n## Evaluation System Models\n\n### EvalTaskDB Class\n\nThe `EvalTaskDB` class is aimed at storing evaluation tasks. Notable attributes include:\n- `name`: A default string identifier for the task.\n- `description`: A field for further details on the task.\n- `config`: A JSON representation of the evaluation task configuration.\n\n### EvalCriteriaDB Class\n\nRepresenting evaluation criteria, the `EvalCriteriaDB` class includes:\n- `name` and `description` for identifying the criteria.\n- `config`: A JSON-encoded configuration structure for the criteria.\n\n### EvalRunDB Class\n\nThe `EvalRunDB` class is focused on tracking evaluation runs. Important fields comprise:\n- `name` and `description` for identification purposes.\n- `task_id`: A foreign key linking to a specific evaluation task.\n- `runner_config` and `judge_config`: JSON configurations for the runner and judge components.\n- `status`: Represents the current state of the evaluation run.\n- Metadata fields like `start_time`, `end_time`, `run_result`, and others for comprehensive tracking of the evaluation process.\n\n## Conclusion\n\nIn summary, the provided code defines a structured set of database models that serve various purposes within the AutoGenStudio framework. The architecture facilitates the serialization of complex configurations into a relational database and includes support for essential features such as versioning and timestamping. By adhering to a base model structure, the code promotes consistency and reusability across different entities, making it more maintainable and understandable. The comprehensive handling of data types, relationships, and evaluation components illustrates a robust approach to backend data management in modern applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/datamodel/db.py"
  },
  {
    "code": false,
    "content": "# Documentation for `datamodel/eval.py`\n\n## Overview\n\nThe `eval.py` module defines a set of data models for handling evaluation tasks in a structured way. This module utilizes Pydantic's `BaseModel` for data validation and typing, allowing for robust handling of evaluation-related data, maintaining clarity through the use of enums for status management and rich type hints for data integrity.\n\n## Data Models\n\n### EvalTask\n\n```python\nclass EvalTask(BaseModel):\n    task_id: UUID | str = Field(default_factory=uuid4)\n    input: str | Sequence[str | Image]\n    name: str = \"\"\n    description: str = \"\"\n    expected_outputs: Optional[List[Any]] = None\n    metadata: Dict[str, Any] = {}\n```\n\nThe `EvalTask` class represents a task that needs to be evaluated. Essential attributes include:\n\n- `task_id`: Uniquely identifies the evaluation task, opting to use either a `UUID` or a `str`. It defaults to a newly generated UUID.\n- `input`: Accepts either a string or a sequence of strings and images, representing what will be evaluated.\n- `name` and `description`: Provide additional context about the task.\n- `expected_outputs`: Allows the specification of anticipated results from the evaluation.\n- `metadata`: A dictionary for additional custom information related to the task.\n\n### EvalRunResult\n\n```python\nclass EvalRunResult(BaseModel):\n    result: TaskResult | None = None\n    status: bool = False\n    start_time: Optional[datetime] = Field(default=datetime.now())\n    end_time: Optional[datetime] = None\n    error: Optional[str] = None\n```\n\n`EvalRunResult` captures the results of a specific evaluation run. Key properties include:\n\n- `result`: Holds the result in the form of an instance of `TaskResult` or `None`.\n- `status`: A boolean indicating success or failure.\n- `start_time` and `end_time`: Timestamps marking the duration of the evaluation run.\n- `error`: An optional field for storing error messages if the evaluation fails.\n\n### EvalDimensionScore\n\n```python\nclass EvalDimensionScore(BaseModel):\n    dimension: str\n    score: float\n    reason: str\n    max_value: float\n    min_value: float\n```\n\nThe `EvalDimensionScore` class is used for scoring individual evaluation dimensions. It encapsulates:\n\n- `dimension`: The specific aspect being evaluated.\n- `score`: The numeric score assigned to this evaluation dimension.\n- `reason`: A rationale explaining the given score.\n- `max_value` and `min_value`: Define the scoring boundaries for the dimension, ensuring scores stay within the expected range.\n\n### EvalScore\n\n```python\nclass EvalScore(BaseModel):\n    overall_score: Optional[float] = None\n    dimension_scores: List[EvalDimensionScore] = []\n    reason: Optional[str] = None\n    max_value: float = 10.0\n    min_value: float = 0.0\n    metadata: Dict[str, Any] = {}\n```\n\n`EvalScore` is a composite score holding information about the overall evaluation. Its attributes include:\n\n- `overall_score`: The aggregate score from all dimensions, which can be optional.\n- `dimension_scores`: A list of `EvalDimensionScore` instances representing scores for each evaluation dimension.\n- `reason`: An optional explanation for the overall score.\n- `max_value` and `min_value`: Default scoring boundaries for consistency across evaluations.\n- `metadata`: A dictionary for any additional information.\n\n### EvalJudgeCriteria\n\n```python\nclass EvalJudgeCriteria(BaseModel):\n    dimension: str\n    prompt: str\n    max_value: float = 10.0\n    min_value: float = 0.0\n    metadata: Dict[str, Any] = {}\n```\n\nThe `EvalJudgeCriteria` class defines the criteria used to judge evaluation results. It includes:\n\n- `dimension`: The specific aspect being addressed in the evaluation.\n- `prompt`: A guiding prompt that may assist evaluators in assessing the dimension.\n- `max_value` and `min_value`: Scoring boundaries that define the evaluation range.\n- `metadata`: A structure for including supplementary information.\n\n### EvalRunStatus\n\n```python\nclass EvalRunStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELED = \"canceled\"\n```\n\n`EvalRunStatus` is an enumeration that delineates the different states of an evaluation run. The available statuses are:\n\n- `PENDING`: The evaluation has yet to start.\n- `RUNNING`: The evaluation is currently in progress.\n- `COMPLETED`: The evaluation has finished successfully.\n- `FAILED`: The evaluation encountered errors.\n- `CANCELED`: The evaluation was halted before completion.\n\n### EvalResult\n\n```python\nclass EvalResult(BaseModel):\n    task_id: UUID | str\n    status: EvalRunStatus = EvalRunStatus.PENDING\n    start_time: Optional[datetime] = Field(default=datetime.now())\n    end_time: Optional[datetime] = None\n```\n\nThe `EvalResult` class captures the results after an evaluation run is completed. Key attributes include:\n\n- `task_id`: Identifies the related evaluation task.\n- `status`: Indicates the current state using the `EvalRunStatus` enum.\n- `start_time` and `end_time`: Timestamps for the duration of the evaluation.\n\n## Summary\n\nIn summary, the `eval.py` module provides a cohesive framework for managing evaluation tasks and their outcomes through structured data models. Each class serves a specific purpose in the evaluation life cycle, enhancing clarity and making it easier to handle various evaluation aspects, such as tasks, scoring, and run statuses.",
    "filename": "python/packages/autogen-studio/autogenstudio/datamodel/eval.py"
  },
  {
    "code": false,
    "content": "# Code Documentation for Message and Gallery Configuration\n\nThis documentation outlines the main components of the provided code, which primarily focuses on configuration and messaging systems related to chat applications. The code leverages data classes to structure configuration settings, message types, and other related entities.\n\n## Overview\n\nThe code defines various data classes for managing messaging configurations, gallery metadata, and environment settings used in a chat application. It utilizes Pydantic for data validation and management, ensuring that data structures adhere to specified types and constraints. The key components include message configurations, result management, and UI settings.\n\n## Message Configuration Classes\n\n### `MessageConfig`\n\nThe `MessageConfig` class outlines the basic structure of a message. It includes:\n\n- **source**: A string that indicates the origin of the message.\n- **content**: This can either be a string, a `ChatMessage` instance, a sequence of `ChatMessage` instances, or `None`.\n- **message_type**: An optional string that designates the type of message, defaulting to \"text\".\n\nThis class serves as a foundational model for any message that will be processed within the application.\n\n### `LLMCallEventMessage`\n\n`LLMCallEventMessage` extends the `TextMessage` class to represent specific events related to calls to a Language Model (LLM). Its attributes include:\n\n- **source**: A hard-coded string \"llm_call_event\".\n  \nThis class also defines methods to convert the message to various formats, although the `to_model_message` method raises a `NotImplementedError`, indicating that this type of message does not support conversion to a `UserMessage`.\n\n### `MessageMeta`\n\nThe `MessageMeta` class serves as a container for metadata associated with a message, including:\n\n- **task**: An optional identifier for the task related to this message.\n- **task_result**: An optional result from the task.\n- **summary_method**: The method used for summarization, defaulting to \"last\".\n- **files**, **time**, **log**, and **usage**: Optional fields for additional data related to the message.\n\nThis class is essential for contextualizing the messages within the application.\n\n## Gallery and Components Configuration\n\n### `GalleryMetadata`\n\n`GalleryMetadata` encapsulates metadata about a gallery, including:\n\n- **author**: The creator of the gallery.\n- **version**, **description**, and **tags**: Strings providing contextual information about the gallery.\n- **homepage** and **category**: URLs and categorization for the gallery.\n- **last_synced**: The last datetime when the gallery data was synchronized.\n\nThis class employs custom JSON encoding for `datetime` objects, ensuring proper format handling when serialized.\n\n### `GalleryComponents`\n\nThe `GalleryComponents` class organizes components relevant to the gallery, including:\n\n- **agents**, **models**, **tools**, **terminations**, **teams**, and **workbenches**: Lists of `ComponentModel` instances representing various entities needed for a functioning gallery.\n\nThis structure promotes modularity and reusability of components within the application.\n\n### `GalleryConfig`\n\n`GalleryConfig` binds the metadata and components into a single configuration model, containing:\n\n- **id** and **name**: Identifiers for the gallery configuration.\n- **url**: An optional link to the gallery.\n- **metadata**: An instance of `GalleryMetadata`.\n- **components**: An instance of `GalleryComponents`.\n\nIt also features custom JSON encoding for `datetime` and `SecretStr` objects for secure handling.\n\n## Environment and UI Settings\n\n### `EnvironmentVariable`\n\nThis class represents an environment variable, with attributes like:\n\n- **name**: The name of the environment variable.\n- **value**: The actual value of that variable.\n- **type**: A literal type that specifies its data type (string, number, boolean, or secret).\n- **description** and **required**: Optional fields for additional context and constraints.\n\nIt allows for flexible configuration of application runtime variables.\n\n### `UISettings`\n\n`UISettings` class defines preferences for the user interface, including:\n\n- **show_llm_call_events**: A boolean flag to control the display of LLM call events.\n- **expanded_messages_by_default**: A boolean indicating if messages should be expanded by default.\n- **human_input_timeout_minutes**: An integer with constraints (1-30) defining the timeout period for human input.\n\nThese settings influence the user's experience when interacting with the application.\n\n### `SettingsConfig`\n\nFinally, `SettingsConfig` consolidates various settings, including:\n\n- **environment**: A list of environment variables.\n- **default_model_client**: An optional default model client, initially set to an OpenAI client instance.\n- **ui**: An instance of `UISettings`.\n\nThis class serves as the overarching configuration model that encapsulates all application settings.\n\n## Web Request/Response Data Models\n\n### `Response`\n\nThis class defines the structure for API responses with fields including:\n\n- **message**: A string conveying the response message.\n- **status**: A boolean indicating the success or failure of the operation.\n- **data**: An optional field for any additional data related to the response.\n\n### `SocketMessage`\n\n`SocketMessage` represents messages sent via WebSocket connections, with attributes:\n\n- **connection_id**: The identifier for the connection.\n- **data**: A dictionary containing the actual message data.\n- **type**: A string indicating the type of the socket message.\n\nThese classes facilitate communication between different parts of the chat application, enabling real-time messaging and structured responses.",
    "filename": "python/packages/autogen-studio/autogenstudio/datamodel/types.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\nThis documentation provides an analysis of the provided source code, outlining its structure, functionality, and any specific components such as functions, methods, or classes. The aim is to present a comprehensive understanding of the code\u2019s purpose and workflow.\n\n## General Purpose\nThe code serves a specific function within its application, which could range from data processing, API interaction, web scraping, or image manipulation, among others. The details of functionality will be explored in subsequent sections.\n\n## Functionality Breakdown\nThe main functionalities of the code can typically be categorized into several sections based on operations performed.\n\n### Initialization and Configuration\n- **Global Variables/Constants**: Initial settings or constants are often defined at the top of the code. These may include API keys, file paths, or configuration settings that the rest of the code relies on.\n- **Library Imports**: The code likely imports various libraries or modules necessary for its execution. This might include standard libraries or third-party packages that aid in accomplishing specific tasks (like handling JSON data, performing HTTP requests, etc.).\n\n### Core Logic\n- **Primary Algorithm**: The heart of the code lies in its core logic. This is where the main algorithm or series of steps are executed. The flow might involve loops, conditionals, or data manipulations.\n- **Data Handling**: The code may include routines for reading, processing, and writing data. This could involve parsing CSV files, managing databases, or interfacing with web APIs to retrieve or send data.\n\n### Functions/Methods (if applicable)\n- **Function Definitions**: If the code contains functions, each function has a distinct role:\n  - **Data Processing Functions**: These might perform tasks like cleansing or transforming data.\n  - **Utility Functions**: Independent helpers that perform single tasks, like formatting strings or calculations.\n  - **Main Execution Function**: This function typically orchestrates the entire process, calling other functions in the correct order.\n\n### User Interaction (if applicable)\n- **Input Handling**: The code may manage how users interact with it, whether through command-line inputs, graphical user interface elements, or web forms.\n- **Output Generation**: The results of the code\u2019s execution are typically formatted and displayed, whether as console output, files written to disk, or responses sent back to a user interface.\n\n## Error Handling and Validation\n- **Validation Routines**: The code may include error-checking mechanisms to validate inputs and ensure the program behaves predictably even when faced with unexpected data.\n- **Exception Handling**: The use of try/except blocks (or equivalent structures) may be apparent, enabling the program to handle errors gracefully and provide feedback to the user.\n\n## Conclusion\nThis code provides a specific utility, integrating various programming constructs into a cohesive unit. Understanding each section allows for better maintenance and potential enhancements in future iterations. Through its logical structure, it efficiently manages tasks ranging from data acquisition to processing and output generation, depending on the specific use case within its application.",
    "filename": "python/packages/autogen-studio/autogenstudio/eval/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview of the Evaluation Judge Module\n\nThis module defines an asynchronous evaluation judging framework that utilizes a Large Language Model (LLM) to assess the quality of responses generated for specified tasks. The code is structured around the concepts of components and judges, utilizing abstract classes and concrete implementations to facilitate extensibility and configuration management.\n\n## Base Classes and Configuration\n\n### BaseEvalJudgeConfig\n\n- **Description**: This class serves as the base configuration for different evaluation judges. It holds the judge's name, description, and additional metadata.\n- **Attributes**:\n  - `name`: A string representing the judge's name.\n  - `description`: A string providing information about the judge.\n  - `metadata`: A dictionary storing additional key-value pairs for configuration purposes.\n\n### BaseEvalJudge\n\n- **Description**: An abstract base class defining the structure and contract for all evaluation judges.\n- **Attributes**:\n  - `component_type`: A string that identifies the type of component, set to \"eval_judge\".\n- **Methods**:\n  - `__init__`: Initializes the judge's specifics, such as name, description, and metadata.\n  - `judge`: An abstract method that evaluates a task's result based on provided criteria.\n  - `_to_config`: Converts the current state of the judge into a serialized configuration object.\n\n## Concrete Implementation of Evaluation Judge\n\n### LLMEvalJudgeConfig\n\n- **Description**: Configuration specifically for the `LLMEvalJudge`, extending the base configuration.\n- **Attributes**:\n  - `model_client`: An unspecified component model utilized to connect with the underlying LLM.\n\n### LLMEvalJudge\n\n- **Description**: A concrete implementation of the evaluation judge that employs an LLM to assess results using predefined criteria.\n- **Attributes**:\n  - `component_config_schema`: This specifies the schema for configurations related to `LLMEvalJudge`.\n  - `model_client`: The LLM client responsible for processing evaluation requests.\n  \n- **Methods**:\n  - `__init__`: Initializes the LLM judge, taking the model client, name, description, and metadata.\n  - `judge`: Executes the evaluation of results based on tasks and criteria, returning an `EvalScore` object.\n  - `_judge_dimension`: Assesses each dimension of the evaluation, formatting prompts and parsing responses from the LLM.\n  - `_format_task`: Converts an `EvalTask` object into a string domain suitable for LLM queries.\n  - `_parse_judgment`: Extracts the explanation and score from the LLM's evaluation text.\n  - `_to_config`: Serializes the LLM judge's configuration, including the model client settings.\n  - `_from_config`: Deserializes the judge from a given configuration, re-instantiating the model client.\n\n## Evaluation Process\n\n### The `judge` Method\n\nThis asynchronous method is central to the evaluation process. It receives an `EvalTask`, an `EvalRunResult`, and a list of criteria. The workflow is as follows:\n\n1. Creates an `EvalScore` object to store scores.\n2. Initiates parallel tasks to evaluate each dimension specified in the criteria.\n3. Waits for all dimension evaluations to complete using `asyncio.gather`.\n4. Averages the valid scores and assigns that to the `overall_score` of the `EvalScore` object.\n\n### The `_judge_dimension` Method\n\nThis method evaluates a specific criterion, executing these steps:\n\n- Formats the task and result for LLM consumption.\n- Constructs a prompt to solicit a judgment from the LLM.\n- Sends the prompt to the model client, receiving a structured response.\n- Attempts to parse the response; in case of error, it defaults to a score of 0 with an explanation.\n\n## Utility Methods\n\n### Task Formatting\n\nThe `_format_task` method takes an `EvalTask` and converts it into a string. This transformation organizes the task's description and input into a coherent message format for LLM evaluation.\n\n### Parsing Judgments\n\nThe `_parse_judgment` method processes the LLM's textual output to retrieve the evaluation's explanation and score. This involves basic string manipulation and validation to ensure that scores fall within expected boundaries.\n\n## Configuration Serialization\n\nBoth `LLMEvalJudge` and its base class implement methods for translating the internal state into a configuration format. The `dump_component` and `load_component` methods provide a way to serialize and deserialize these judges, supporting ease of use and integration into larger systems.\n\n## Usage Example (Commented Out in Code)\n\nThe commented-out example at the end of the module showcases how to instantiate and use the `LLMEvalJudge`:\n\n1. Create a model client (e.g., an OpenAI client).\n2. Instantiate the `LLMEvalJudge`.\n3. Configure evaluation criteria.\n4. Define a mock evaluation task and result.\n5. Execute the evaluation and log the score.\n\nThis illustrates the practical application of the components and their methods in a real-world scenario, outlining how the framework can be integrated into a broader evaluation system. \n\nOverall, this module provides a structured, modular approach for leveraging advanced language models in evaluative tasks, paving the way for automated assessments across diverse applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/eval/judges.py"
  },
  {
    "code": false,
    "content": "# Evaluation Orchestrator Documentation\n\n## Overview\n\nThe `EvalOrchestrator` class provides a comprehensive framework for managing evaluation tasks, criteria, and runs in an asynchronous environment. It facilitates the creation, retrieval, and management of evaluation tasks, criteria, and runs. The orchestrator can operate in two modes: in-memory (when no database manager is provided) and persistent using a database manager for data storage.\n\n## Key Features\n\n- **Task Management**: Creation, retrieval, and listing of evaluation tasks.\n- **Criteria Management**: Handling of evaluation criteria necessary for judging tasks.\n- **Run Management**: Configuration, execution, and status tracking of evaluation runs.\n- **Error Handling and Logging**: Robust mechanisms to log and handle errors throughout the evaluation process.\n- **Result Tabulation**: Compact data organization for visual representation, useful for analytics.\n\n## Class Structure\n\n### Attributes\n\n- `_db_manager`: An instance of `DatabaseManager` for interacting with a database, if provided.\n- `_tasks`: A dictionary storing evaluation tasks if the DB manager is not present.\n- `_criteria`: A dictionary storing evaluation criteria if the DB manager is not present.\n- `_runs`: A dictionary for tracking evaluation runs if the DB manager is not present.\n- `_active_runs`: A dictionary of currently running evaluation tasks to prevent duplicate executions.\n\n### Constructor\n\n```python\ndef __init__(self, db_manager: Optional[DatabaseManager] = None)\n```\n\n#### Parameters\n\n- `db_manager`: The optional database manager to enable storage persistence.\n\n## Task Management\n\n### create_task\n\n```python\nasync def create_task(self, task: EvalTask) -> str\n```\n\nCreates a new evaluation task. If a database manager exists, the task is stored in the database; otherwise, it is stored in memory.\n\n#### Parameters\n\n- `task`: An instance of `EvalTask` representing the task to create.\n\n#### Returns\n\n- The ID of the created task.\n\n### get_task\n\n```python\nasync def get_task(self, task_id: str) -> Optional[EvalTask]\n```\n\nRetrieves an evaluation task by its ID. It checks in the database or memory based on the availability of a DB manager.\n\n#### Parameters\n\n- `task_id`: The ID of the task to retrieve.\n\n#### Returns\n\n- The corresponding `EvalTask` object or `None` if not found.\n\n### list_tasks\n\n```python\nasync def list_tasks(self) -> List[EvalTask]\n```\n\nLists all existing evaluation tasks, either from the database or in-memory store.\n\n#### Returns\n\n- A list of `EvalTask` instances.\n\n## Criteria Management\n\n### create_criteria\n\n```python\nasync def create_criteria(self, criteria: EvalJudgeCriteria) -> str\n```\n\nCreates new evaluation criteria and stores it in the appropriate location based on the presence of the DB manager.\n\n#### Parameters\n\n- `criteria`: An instance of `EvalJudgeCriteria`.\n\n#### Returns\n\n- The ID of the created criteria.\n\n### get_criteria\n\n```python\nasync def get_criteria(self, criteria_id: str) -> Optional[EvalJudgeCriteria]\n```\n\nRetrieves evaluation criteria by its identifier.\n\n#### Parameters\n\n- `criteria_id`: The unique identifier for the criteria.\n\n#### Returns\n\n- The corresponding `EvalJudgeCriteria` object or `None`.\n\n### list_criteria\n\n```python\nasync def list_criteria(self) -> List[EvalJudgeCriteria]\n```\n\nLists all available evaluation criteria, either from the database or in-memory store.\n\n#### Returns\n\n- A list of `EvalJudgeCriteria`.\n\n## Run Management\n\n### create_run\n\n```python\nasync def create_run(self, task: Union[str, EvalTask], runner: BaseEvalRunner, judge: BaseEvalJudge, criteria: List[Union[str, EvalJudgeCriteria]], name: str = \"\", description: str = \"\") -> str\n```\n\nConfigures and creates new evaluation runs, linking them to a task, runner, judge, and criteria.\n\n#### Parameters\n\n- `task`: Task to evaluate (can be a string ID or an `EvalTask`).\n- `runner`: The runner class to use for the evaluation.\n- `judge`: The judging class for evaluation.\n- `criteria`: List of evaluation criteria.\n- `name`: Optional name for the run.\n- `description`: Optional description for the run.\n\n#### Returns\n\n- The ID of the created run.\n\n### start_run\n\n```python\nasync def start_run(self, run_id: str) -> None\n```\n\nStarts an existing evaluation run identified by its ID.\n\n#### Parameters\n\n- `run_id`: The unique identifier for the run.\n\n### _execute_run\n\n```python\nasync def _execute_run(self, run_id: str) -> None\n```\n\nInternal method to execute the evaluation run, managing runners and judges as needed.\n\n#### Parameters\n\n- `run_id`: The ID of the run to execute.\n\n### get_run_status\n\n```python\nasync def get_run_status(self, run_id: str) -> Optional[EvalRunStatus]\n```\n\nFetches the status of an evaluation run.\n\n#### Parameters\n\n- `run_id`: The ID of the run.\n\n#### Returns\n\n- The status of the run, or `None` if not found.\n\n### get_run_result\n\n```python\nasync def get_run_result(self, run_id: str) -> Optional[EvalRunResult]\n```\n\nRetrieves the result of a completed evaluation run.\n\n#### Parameters\n\n- `run_id`: The ID of the run.\n\n#### Returns\n\n- The result of the run, or `None` if not found.\n\n### list_runs\n\n```python\nasync def list_runs(self) -> List[Dict[str, Any]]\n```\n\nEnumerates all available evaluation runs, providing status and other relevant info.\n\n#### Returns\n\n- A list of dictionaries with run information.\n\n## Helper Methods\n\n### _get_run_config\n\n```python\nasync def _get_run_config(self, run_id: str) -> Optional[Dict[str, Any]]\n```\n\nFetches the complete configuration for a specific evaluation run.\n\n#### Parameters\n\n- `run_id`: The ID of the run.\n\n#### Returns\n\n- The configuration for the run, or `None` if not found.\n\n### tabulate_results\n\n```python\nasync def tabulate_results(self, run_ids: List[str], include_reasons: bool = False) -> TabulatedResults\n```\n\nGenerates a structured representation of evaluation results suitable for visualization.\n\n#### Parameters\n\n- `run_ids`: List of run IDs to include in the tabulation.\n- `include_reasons`: Flag to indicate whether to include reasons for scores.\n\n#### Returns\n\n- A dictionary structured for easy visualization.\n\n## Conclusion\n\nThe `EvalOrchestrator` class is a robust and flexible framework for managing evaluation tasks, criteria, and runs in an asynchronous manner. It supports both in-memory operations and persistence through a database, providing a solid foundation for evaluations in a range of applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/eval/orchestrator.py"
  },
  {
    "code": false,
    "content": "# Evaluation Runner Framework Documentation\n\nThis document provides a high-level overview of an evaluation runner framework designed for processing evaluation tasks using various evaluation methods. The primary components include abstract base classes, specific implementations using individual model clients or teams, and configurations for these runners.\n\n## Overview of Components\n\nThe framework is built around the concept of evaluation runners, which are responsible for executing tasks and returning results. Two main types of runners are defined:\n1. **ModelEvalRunner**: Utilizes a single language model (LLM) to process tasks.\n2. **TeamEvalRunner**: Uses a team of agents to manage and process tasks.\n\n### Classes and Their Roles\n\n1. **BaseEvalRunnerConfig**: \n   - This class serves as a base for all evaluation runner configurations.\n   - It captures essential attributes like `name`, `description`, and `metadata` for runners.\n\n2. **BaseEvalRunner**:\n   - An abstract base class that implements the core interface for evaluation runners.\n   - Defines an abstract method `run` that subclasses must implement, outlining how specific evaluations are executed.\n\n3. **ModelEvalRunnerConfig**:\n   - Inherits from `BaseEvalRunnerConfig`.\n   - Includes a specific attribute, `model_client`, which represents the model used for evaluation.\n\n4. **ModelEvalRunner**:\n   - A concrete implementation of `BaseEvalRunner` that evaluates tasks by sending them to a single model client.\n   - Implements the `run` method, which prepares the input, invokes the model, and retrieves the evaluation results.\n\n5. **TeamEvalRunnerConfig**:\n   - Similar to `ModelEvalRunnerConfig`, but focuses on configurations for the team-based evaluation runner.\n\n6. **TeamEvalRunner**:\n   - A concrete class for evaluating tasks using a team of agents.\n   - Implements its own version of the `run` method, managing inputs for multiple agents and coordinating their responses to provide a collective evaluation result.\n\n## Evaluation Runner Configurations\n\n### BaseEvalRunnerConfig\n\n```python\nclass BaseEvalRunnerConfig(BaseModel):\n    \"\"\"Base configuration for evaluation runners.\"\"\"\n    name: str\n    description: str = \"\"\n    metadata: Dict[str, Any] = {}\n```\n- This configuration class holds basic metadata about the evaluation runner, providing a structure to serialize information that may be necessary for configuration management and runtime.\n\n### ModelEvalRunnerConfig\n\n```python\nclass ModelEvalRunnerConfig(BaseEvalRunnerConfig):\n    \"\"\"Configuration for ModelEvalRunner.\"\"\"\n    model_client: ComponentModel\n```\n- Inherits from `BaseEvalRunnerConfig`, adding the `model_client` attribute for specifying the model component that the runner will use.\n\n### TeamEvalRunnerConfig\n\n```python\nclass TeamEvalRunnerConfig(BaseEvalRunnerConfig):\n    \"\"\"Configuration for TeamEvalRunner.\"\"\"\n    team: ComponentModel\n```\n- Functions similarly to `ModelEvalRunnerConfig`, but targets a team of agents for processing evaluation tasks.\n\n## Implementation of Evaluation Runners\n\n### ModelEvalRunner\n\n```python\nclass ModelEvalRunner(BaseEvalRunner, Component[ModelEvalRunnerConfig]):\n    \"\"\"Evaluation runner that uses a single LLM to process tasks.\"\"\"\n```\n- Implements the abstract methods of `BaseEvalRunner`, specifically focusing on how a single language model will process tasks.\n\n#### Key Methods\n\n- **`run`**: \n   ```python\n   async def run(self, task: EvalTask, cancellation_token: Optional[CancellationToken] = None) -> EvalRunResult:\n   ```\n   - Processes the input task (either as a string or a list). \n   - It constructs appropriate message formats and sends them to the model client, capturing the response in an `EvalRunResult` object.\n\n### TeamEvalRunner\n\n```python\nclass TeamEvalRunner(BaseEvalRunner, Component[TeamEvalRunnerConfig]):\n    \"\"\"Evaluation runner that uses a team of agents to process tasks.\"\"\"\n```\n- Similar to `ModelEvalRunner`, this class evaluates tasks using multiple agents working as a team.\n\n#### Key Methods\n\n- **`run`**: \n   ```python\n   async def run(self, task: EvalTask, cancellation_token: Optional[CancellationToken] = None) -> EvalRunResult:\n   ```\n   - Implements task processing through a team, handling different message formats (text and multimodal) and managing the orchestration of agent responses.\n\n## Configuration Management\n\n### `_to_config` and `_from_config` Methods\n\nBoth evaluation runners implement methods to serialize configurations to/from their respective configuration classes:\n\n- **`_to_config`**:\n   - Converts the current state of the runner to its configuration class, facilitating easy serialization for storage.\n\n- **`_from_config`**:\n   - Creates an instance of the runner from a given configuration, re-instantiating the model or team based on the serialized components.\n\n### Exception Handling\n\nBoth runners include exception handling in their `run` methods:\n- If an exception arises (e.g., during communication with a model or team), it captures the error and updates the `EvalRunResult` accordingly, ensuring robustness and informative error reporting.\n\n## Conclusion\n\nThis evaluation runner framework allows for extensible evaluation task processing using both individual models and collaborative teams. The abstractions provided ensure that different types of evaluation methodologies can be easily integrated and configured, facilitating a wide range of applications in autonomous systems and AI-driven platforms.",
    "filename": "python/packages/autogen-studio/autogenstudio/eval/runners.py"
  },
  {
    "code": false,
    "content": "# Code Analysis and Documentation\n\nThis module appears to be part of a Python package that deals with the creation and management of galleries. It imports specific classes and functions from another module called `builder`, which likely contains the core functionalities required for gallery operations.\n\n## Imports Overview\n\nThe code includes two primary imports from the `builder` module:\n\n```python\nfrom .builder import GalleryBuilder, create_default_gallery\n```\n\n### `GalleryBuilder`\n\n- **Purpose**: This class is likely responsible for constructing gallery objects. It may encapsulate properties and methods needed to define how galleries behave, how they are initialized, and how images or items are added and displayed.\n  \n- **Role**: Primarily, `GalleryBuilder` would manage gallery configurations, layout options, item arrangements, and possibly interactions such as navigation or filtering through gallery items.\n\n### `create_default_gallery`\n\n- **Purpose**: This function likely serves to set up a standard gallery configuration based on predefined settings. It may provide a starting point for gallery creation, ensuring that there is a base structure in place.\n  \n- **Role**: By utilizing this function, users can quickly initialize a gallery with sensible defaults without needing to manually specify every attribute. This can be beneficial for users who want to set up a gallery quickly or for those who are new to the library.\n\n## Exported Components\n\nThe line below delineates what will be provided to any module that imports this one. It specifies the public interface of the module:\n\n```python\n__all__ = [\"GalleryBuilder\", \"create_default_gallery\"]\n```\n\n### Purpose of `__all__`\n\n- **Module Encapsulation**: The `__all__` variable is used to define the public API of the module. This indicates which classes or functions should be directly accessible to anyone importing the module, thereby encapsulating internal components.\n\n- **User Guidance**: By listing `GalleryBuilder` and `create_default_gallery`, the module actively informs users about the primary functionalities they can utilize without delving into the internal workings of the `builder` module.\n\n## General Functionality\n\n1. **Package Structure**: This module is a part of a larger package, indicated by the relative import statement. It suggests an organized structure likely intended to separate functionalities into manageable components.\n   \n2. **Modular Design**: The reference to `builder` underscores a modular design philosophy where specific components are handled in isolation, allowing for easier testing, maintenance, and updates.\n\n3. **Reusability**: The design allows for reusability of `GalleryBuilder` and `create_default_gallery`. Other parts of the application or external modules can import and utilize these components without having to understand their implementations, promoting encapsulation.\n\n4. **Potential for Extension**: Given that there are likely other imports, classes, or functions not shown in this snippet, it is reasonable to infer that this module may serve as an extensible foundation for future enhancements or features.\n\n## Conclusion\n\nIn summary, this module provides a structured interface to deal with galleries by exposing essential building components. By utilizing `GalleryBuilder` and `create_default_gallery`, users are allowed to quickly set up gallery interfaces while keeping the underlying logic organized and contained within the `builder` module. The inclusion of `__all__` helps maintain a clear public API, promoting better usability and understanding of the module's capabilities for other developers working with the library.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation Overview of the Provided Code\n\nThe provided code is a Python script designed for building a gallery of conversational components and creating configurations in an AI-driven, multi-agent system. It structures various components, such as agents, termination conditions, tools, teams, and workbenches, into organized configurations that facilitate interactions in an automated dialogue environment.\n\n## Imports and Dependencies\n\nThe script begins with several import statements that gather required modules and classes. These imports include:\n\n- **System Libraries**: `os`, `tempfile`, and type definitions such as `List` and `Optional`.\n- **AutoGen Classes**: Classes from the `autogen_agentchat` and `autogen_core` libraries are imported to manage agents, conditions, and models.\n- **External Classes**: Classes for specific functionalities like the web surfer agent and code executors are also imported to enhance capabilities.\n\nThe framework allows for parallel processing, modular construction, and handling tasks in a conversational context.\n\n## `GalleryBuilder` Class\n\nThe main class, `GalleryBuilder`, serves as the foundation for creating a collection of components, providing various methods to add entities like teams, agents, models, tools, termination conditions, and workbenches.\n\n### Initialization\n\nThe `__init__` method initializes the class with parameters such as `id`, `name`, and an optional `url`. It sets up lists to hold components and establishes default metadata for the gallery. \n\n#### Methods Overview\n\nThe class includes several key methods:\n- **`set_metadata()`**: Updates the gallery's metadata.\n- **`add_team()`**, **`add_agent()`**, **`add_model()`**, **`add_tool()`**, **`add_termination()`**, **`add_workbench()`**: Each of these methods allows users to add respective components to the gallery, optionally specifying labels and descriptions.\n- **`_update_component_metadata()`**: A helper method to update the metadata of individual components.\n- **`build()`**: Returns a fully configured `GalleryConfig` object that aggregates all added components and metadata.\n\n## Creating a Default Gallery\n\nThe `create_default_gallery()` function sets up a default gallery filled with predefined components, including agents, models, tools, and termination conditions.\n\n### Setting Environment Variables\n\nAt the start of this function, the script checks for necessary API keys (e.g., OpenAI, Azure) in the environment variables. If not found, they are set to \"test\" for debugging.\n\n### Component Initialization\n\n- **Model Clients**: It instantiates various model clients based on available engines like OpenAI, Anthropic, and Azure. It also includes local clients for specific configurations and establishes models with different labeling and purposes.\n- **Tools**: Various tools, such as a calculator and web fetching tools, are added for functionality during conversations.\n- **Agents**: \n  - Assistant agents are created to provide support and conduct tasks using the tools, with distinct roles and systems messages.\n  - Other agents include verification and research assistants to ensure accurate task completion.\n  \n### Termination Conditions\n\nThe function defines multiple termination conditions that can end a conversation based on different triggers, such as timeouts, token usage, or specific text commands.\n\n### Team Creation\n\n- Various teams are set up to manage conversations, including a round-robin team, a selector team with critique capabilities, and a swarm team with handoff functions based on agent specialty.\n\n## Additional Functionality\n\n1. **Web Surfer Team Creation**: A dedicated team comprising a web surfer, a verification assistant, and a user proxy is created to manage internet-based tasks where human input is sometimes necessary.\n2. **Deep Research Teams**: The script introduces agents dedicated to in-depth research, including roles for searching, verifying, and summarizing findings. It uses a selector prompt to determine which agent speaks next based on prior exchanges in the conversation.\n\n## Workbench Setup\n\nThe `create_default_gallery()` function also introduces several workbench configurations:\n- **Static Workbench**: A basic setup for routine tasks like calculation and content fetching.\n- **MCP Workbenches**: Several configurations that interact with MCP servers to access varied capabilities such as filesystem operations and web content fetching.\n\n## `create_default_lite_team` Function\n\nThe `create_default_lite_team()` function allows the creation of a simplified team. This function contains logic similar to `create_default_gallery()`, but focuses solely on minimal components such as a basic assistant agent to perform arithmetic using the calculator tool.\n\n## Main Execution Block\n\nAt the end of the script, if the file is executed as the main program, it will invoke the `create_default_gallery` function and save the output in a JSON file named `gallery_default.json`. This allows the configuration to be retrieved and utilized later.\n\n```python\nif __name__ == \"__main__\":\n    # Create and save the gallery\n    gallery = create_default_gallery()\n\n    # Save to file\n    with open(\"gallery_default.json\", \"w\") as f:\n        f.write(gallery.model_dump_json(indent=2))\n```\n\nThis targeted output enables convenient loading and integration of the built gallery configuration into other systems or applications.\n\nOverall, the script provides a robust framework for developing and orchestrating an AI-driven conversational system, encapsulating a variety of interactive components under a unified configuration methodology.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/builder.py"
  },
  {
    "code": false,
    "content": "# Module Overview\n\nThis module serves as a high-level interface for a variety of tools that facilitate web searching, mathematical calculations, webpage fetching, and image generation. Each tool is imported from its respective module and collected into a single interface, which aids in organizing and simplifying access to these functionalities.\n\n## Tools and Their Purposes\n\nThe module imports five distinct tools, each designed to serve a specific purpose within a larger application. These tools are:\n\n1. **bing_search_tool**: \n   - This tool likely interfaces with the Bing search engine, allowing users to send queries and retrieve search results from Bing. It could be used to gather information, conduct research, or find specific web content based on user input.\n\n2. **calculator_tool**: \n   - This tool is intended for performing mathematical calculations. It might include functionalities for simple arithmetic operations or more complex calculations, making it a versatile component for applications that require numerical data processing.\n\n3. **fetch_webpage_tool**:\n   - As the name suggests, this tool retrieves content from web pages. It can scrape text, images, or other data from specified URLs, enabling programmatic access to online information for further processing or display.\n\n4. **generate_image_tool**:\n   - This tool is designed to create images programmatically. It could be utilized for generating graphics, editing existing images, or producing visual content based on certain parameters or user specifications.\n\n5. **google_search_tool**: \n   - Similar to the Bing search tool, this tool interfaces with Google\u2019s search engine. It allows users to perform searches and obtain results from Google, leveraging its extensive search capabilities.\n\n## Exporting Tools\n\nAt the end of the module, the `__all__` list is defined. This special list specifies what is exported when the module is imported using the syntax `from module_name import *`. By including tool names in this list, the module restricts access to only those specified tools, promoting cleaner and more controlled usage in other parts of the application. \n\nThe tools included in `__all__` are:\n\n- `bing_search_tool`\n- `calculator_tool`\n- `google_search_tool`\n- `generate_image_tool`\n- `fetch_webpage_tool`\n\n## Conclusion\n\nThe module acts as a consolidated toolkit that promotes easy access to various functionalities useful for web research, data processing, and content generation. By encapsulating these tools and explicitly managing the accessible names, the module not only enhances organization but also improves usability and maintainability in the broader application context. Each tool contributes uniquely to the overall capability of the module, facilitating a wide range of applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/__init__.py"
  },
  {
    "code": false,
    "content": "# Bing Search API Integration Documentation\n\nThis documentation provides an overview of the Python script that integrates with the Bing Web Search API, allowing users to perform web searches and retrieve results in various formats. The script utilizes asynchronous programming principles and libraries to handle HTTP requests and process HTML content.\n\n## Overview\n\nThe primary function in the code is `bing_search`, which allows users to interact with the Bing Search API. It fetches search results based on user-defined parameters such as query, number of results, and type of content (webpages, news, images, or videos). It also includes functionality to fetch the full content of webpages in markdown format. \n\n## Dependencies\n\nThe script relies on several external libraries:\n- `httpx`: A powerful HTTP client for making asynchronous requests.\n- `BeautifulSoup` from the `bs4` package: For parsing HTML and extracting necessary elements.\n- `html2text`: A utility to convert HTML to markdown format.\n- Standard libraries such as `json`, `os`, and `urllib.parse`.\n\nMake sure to install these libraries using pip before executing the script.\n\n## Function: `bing_search`\n\n### Purpose\n`bing_search` performs a search query against the Bing Web Search API and returns a list of results as dictionaries. Each dictionary contains detailed information about the search result, such as title, link, snippet, and optionally content.\n\n### Parameters\n- **query**: The search phrase that users want to find results for.\n- **num_results**: The maximum number of results to return (default is 3, maximum is 50).\n- **include_snippets**: A flag indicating whether to include brief descriptions (snippets) in the output.\n- **include_content**: A flag to choose whether to include the full webpage content in markdown format.\n- **content_max_length**: The maximum character length of the content to fetch.\n- **language**: The language code for the search results (e.g., 'en' for English).\n- **country**: An optional parameter for specifying the market code (e.g., 'us' for the United States).\n- **safe_search**: Controls the level of safe search filtering applied to results.\n- **response_filter**: Specifies the type of results to retrieve (webpages, news, images, videos).\n\n### Returns\n- A list of dictionaries containing the search results.\n\n### Exceptions\n- The method raises `ValueError` for various reasons such as invalid API credentials, exceeding quota, or incorrect parameter values.\n\n## Internal Function: `fetch_page_content`\n\n### Purpose\nThis helper function is nested within `bing_search` and is responsible for fetching the full content of a webpage and converting it to markdown format using `html2text`.\n\n### Parameters\n- **url**: The URL of the webpage to fetch.\n- **max_length**: An optional parameter to limit the length of the retrieved content.\n\n### Returns\n- The content of the webpage in markdown format, truncated if it exceeds the specified length.\n\n## HTTP Request Handling\n\n### Creating Request\nThe script constructs HTTP headers and query parameters required for launching the request to the Bing Search API. This includes setting the appropriate headers for authentication and formatting.\n\n### Error Handling\nThe function includes comprehensive error handling for:\n- Authentication issues (e.g., invalid or expired API key).\n- Access restrictions (e.g., quota exceeded).\n- Formatting issues with the API response (e.g., JSON decode errors).\nIt raises meaningful exceptions that can guide the user to rectify the issues.\n\n## Result Processing\n\nUpon receiving the response from the Bing Search API, the script processes it based on the specified `response_filter`. It handles different result types (webpages, news, images, videos) and extracts relevant information such as title, link, snippet, and duration (for videos).\n\nEach result is structured into a dictionary that is appended to a results list, which will be returned to the user after limiting to the `num_results` specified.\n\n## Creating a Bing Search Tool\n\nAfter defining the `bing_search` function, a `FunctionTool` instance is created. This instance wraps the `bing_search` function, providing a description and the necessary imports for its execution.\n\n### Properties of `bing_search_tool`\n- **func**: The function to call for Bing searching.\n- **description**: A concise description of the tool's functionality.\n- **global_imports**: A list of modules and libraries the tool relies upon.\n\nThis setup enables the function to be used as part of a larger system while maintaining clarity regarding its dependencies and behavior.\n\n## Conclusion\n\nThe script encapsulates the capability to interact with Bing's search API effectively. It offers robust search functionalities while internally managing complexities such as error handling and HTML content extraction. Users need to ensure that they have a valid API key and libraries before using the `bing_search` tool for their application.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/bing_search.py"
  },
  {
    "code": false,
    "content": "# Calculator Function Documentation\n\n## Overview\n\nThis code defines a simple calculator tool that can perform basic arithmetic operations: addition, subtraction, multiplication, and division. The calculator accepts two floating-point numbers and an operator as input, returning the result as a string. The implementation ensures error handling for invalid inputs and arithmetic exceptions, making it user-friendly.\n\n## Function: `calculator`\n\n### Purpose\n\nThe `calculator` function serves as the core logic to carry out arithmetic operations. It takes three parameters: two numbers (`a` and `b` of type `float`) and an operator (`operator` of type `str`) indicating the desired operation.\n\n### Parameters\n\n- `a` (float): The first operand in the arithmetic operation.\n- `b` (float): The second operand in the arithmetic operation.\n- `operator` (str): A string representing the arithmetic operation. The accepted values are:\n  - `\"+\"`: Addition\n  - `\"-\"`: Subtraction\n  - `\"*\"`: Multiplication\n  - `\"/\"`: Division\n\n### Return Value\n\n- Returns a string:\n  - The result of the arithmetic operation if successful.\n  - An error message if:\n    - The operator is invalid.\n    - The operation involves division by zero.\n    - Any other unforeseen error occurs during execution.\n\n### Error Handling\n\nThe function includes a try-except block that catches exceptions that may arise during arithmetic operations. This allows for graceful failure and informative feedback to the user.\n\n### Logic Breakdown\n\n1. **Addition**: If the operator is `\"+\"`, the function returns the sum of `a` and `b`.\n2. **Subtraction**: If the operator is `\"-\"`, the function returns the difference.\n3. **Multiplication**: If the operator is `\"*\"`, the function returns the product.\n4. **Division**: \n   - If the operator is `\"/\"` and `b` is zero, it returns an error message stating division by zero is not allowed.\n   - Otherwise, it returns the quotient of `a` divided by `b`.\n5. **Invalid Operator**: If the operator does not match any of the accepted values, the function returns an error message.\n6. **General Exceptions**: Catches any other exceptions and returns the error message.\n\n## Tool Creation: `calculator_tool`\n\n### Purpose\n\nFollowing the function definition, a tool named `calculator_tool` is created using the `FunctionTool` class from the `autogen_core.tools` module. This tool organizes the `calculator` function for integration and usage potentially within a larger framework.\n\n### Parameters\n\n- `name` (str): Specifies the name of the tool as \"calculator\".\n- `description` (str): Provides a brief summary of what the tool does, mentioning its functionality as a basic arithmetic calculator.\n- `func` (function): References the `calculator` function to be executed when the tool is invoked.\n- `global_imports` (list): An empty list in this case, indicating that there are no additional global imports required for this tool.\n\n### Role in Larger Context\n\nBy defining `calculator_tool`, the code positions the calculator function as part of a toolset that could be utilized in automated environments, user interfaces, or as part of a more extensive software application. This modular approach enhances reusability and maintainability. \n\n## Summary\n\nIn conclusion, the provided code sets up a functional arithmetic calculator capable of performing basic operations while handling various potential errors. The use of the `FunctionTool` class allows the calculator function to be integrated into broader applications, facilitating ease of access and usability for arithmetic calculations.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/calculator.py"
  },
  {
    "code": false,
    "content": "# Webpage Fetcher Documentation\n\nThis documentation provides an overview of a Python script that fetches a webpage, processes its content, and converts it into Markdown format. The main functionalities are encapsulated in the `fetch_webpage` asynchronous function. Additionally, there is an integration with a tool class, `FunctionTool`, which makes the function available for further usage in a tool-based architecture.\n\n## Overview\n\nThe script aims to provide a convenient way to retrieve HTML content from a specified URL, convert it to Markdown format, and manage additional options like image inclusion and content length truncation. It employs asynchronous HTTP requests for efficient network communication, alongside BeautifulSoup for HTML parsing, and html2text for converting HTML to Markdown.\n\n## Key Functionality\n\n### `fetch_webpage` Function\n\n#### Purpose\n\nThe `fetch_webpage` function is responsible for:\n\n1. Making an HTTP GET request to fetch the webpage content.\n2. Cleaning the HTML by removing script and style elements.\n3. Converting all relative URLs to absolute URLs for links and images.\n4. Transforming the cleaned HTML into Markdown format using the `html2text` library.\n5. Optionally, truncating the Markdown output to a specified maximum length.\n\n#### Arguments\n\n- **url** (`str`): The target URL of the webpage to be fetched.\n- **include_images** (`bool`): Allows the inclusion of image references in the converted Markdown. Defaults to `True`.\n- **max_length** (`Optional[int]`): Limits the output Markdown length. If set to `None`, there is no size restriction.\n- **headers** (`Optional[Dict[str, str]]`): Dictionary of HTTP headers to customize the request. Default headers are used if none are provided.\n\n#### Return Value\n\nThe function returns a string containing the Markdown representation of the fetched webpage.\n\n#### Error Handling\n\nThe function raises a `ValueError` in two scenarios:\n- If the URL is invalid or the HTTP request fails (handled via `httpx.RequestError`).\n- For any exceptions encountered during processing (caught at the end of the try-except block).\n\n## Internal Logic\n\n1. **Setting Default Headers**: If no headers are provided, a default \"User-Agent\" header is set to mimic browser behavior.\n  \n2. **Asynchronous Request**: An `httpx.AsyncClient` is employed to perform the GET request asynchronously, enabling the retrieval of the webpage without blocking the event loop.\n\n3. **HTML Parsing**: The response is parsed using BeautifulSoup, which provides a structured way to navigate and manipulate the HTML DOM.\n\n4. **Clean Up**: Script and style tags are removed from the DOM to prevent them from interfering with Markdown conversion.\n\n5. **URL Normalization**: Relative links and image sources are converted to absolute URLs using `urljoin`, allowing for consistent links in the Markdown output.\n\n6. **Markdown Conversion**: The `html2text` library\u2019s `HTML2Text` class is configured:\n   - Line wrapping is disabled (`body_width = 0`).\n   - The function respects the `include_images` flag to decide on image inclusion in the final Markdown output.\n\n7. **Markdown Truncation**: If `max_length` is provided and the generated Markdown content exceeds this length, it is truncated, and a notation is added to indicate this.\n\n8. **Final Return**: The Markdown string is returned, stripped of leading and trailing whitespace.\n\n## Integration with `FunctionTool`\n\nThe script defines a `fetch_webpage_tool` using the `FunctionTool` class to facilitate the integration of the `fetch_webpage` function into a larger toolset or application. This includes:\n\n- Providing a description of its functionality to users.\n- Specifying necessary global imports, ensuring that all dependencies required for the function are readily available.\n\nThe `FunctionTool` setup strengthens the modularity of the code, allowing the `fetch_webpage` function to be invoked efficiently within different contexts.\n\n## Usage Scenario\n\nThis script is particularly useful for applications or systems that require scraping or summarizing web content in Markdown format. Possible usage scenarios include:\n\n- Blog generation tools that convert web articles into Markdown for further editing.\n- Integration into web applications that serve summarized content to users.\n- Utility scripts for content curation and archiving websites as Markdown files. \n\nThe combination of async requests, HTML parsing, and Markdown conversion makes it a powerful tool for various web content processing needs.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/fetch_webpage.py"
  },
  {
    "code": false,
    "content": "# Image Generation Script Documentation\n\nThis script provides functionality to generate images based on text descriptions using OpenAI's DALL-E model. It defines a primary asynchronous function for image generation and wraps this functionality in a tool for further integration into applications. Below is a detailed breakdown of the components and their roles.\n\n## Overview of Imports\n\n```python\nimport base64\nimport io\nimport uuid\nfrom pathlib import Path\nfrom typing import List, Literal, Optional\n\nfrom autogen_core.code_executor import ImportFromModule\nfrom autogen_core.tools import FunctionTool\nfrom openai import OpenAI\nfrom PIL import Image\n```\n\nThe script begins by importing necessary libraries and modules:\n\n- **Base64**: For decoding base64 encoded image data.\n- **Io**: For handling streams of binary data.\n- **Uuid**: To generate unique identifiers for image files.\n- **Path**: To handle file paths conveniently.\n- **Typing**: To utilize type hinting for better code clarity.\n- **autogen_core**: Imports a custom code executor and tools for enhanced functionality.\n- **OpenAI**: The library allowing interaction with OpenAI's API.\n- **PIL (Pillow)**: A library for opening, manipulating, and saving various image file formats.\n\n## Function: `generate_image`\n\n```python\nasync def generate_image(\n    query: str, output_dir: Optional[Path] = None, image_size: Literal[\"1024x1024\", \"512x512\", \"256x256\"] = \"1024x1024\"\n) -> List[str]:\n```\n\n### Purpose and Parameters\n\nThis asynchronous function generates images based on a natural language description provided by the user. Its parameters are:\n\n- **query**: A string description of the image to generate.\n- **output_dir**: An optional parameter that specifies the directory where the images will be saved. If not provided, the current working directory will be used.\n- **image_size**: A literal specifying the size of the generated image. Accepted values are \"1024x1024\", \"512x512\", or \"256x256\" (default is \"1024x1024\").\n\n### Returns\n\nThe function returns a list of strings representing the file paths of the saved images.\n\n## Image Generation Logic\n\n```python\n    # Initialize the OpenAI client\n    client = OpenAI()\n```\n\nAn instance of the OpenAI client is created to interact with the DALL-E model.\n\n```python\n    response = client.images.generate(model=\"dall-e-3\", prompt=query, n=1, response_format=\"b64_json\", size=image_size)\n```\n\nThe `generate` method is called on the client, which submits the image generation request to the DALL-E model. It sends the prompt (query), specifies the number of images to generate (1), requests the response format as base64 JSON, and defines the image size.\n\n## Response Processing\n\n```python\n    saved_files = []\n\n    # Process the response\n    if response.data:\n        for image_data in response.data:\n            # Generate a unique filename\n            file_name: str = f\"{uuid.uuid4()}.png\"\n```\n\nAn empty list is initialized to keep track of the saved image file paths. The script checks if the response contains image data; if so, it iterates through this data:\n\n- A unique filename for each image is generated using `uuid.uuid4()`, ensuring each file saved has a different name.\n\n### Saving the Image\n\n```python\n            base64_str = image_data.b64_json\n            if base64_str:\n                img = Image.open(io.BytesIO(base64.decodebytes(bytes(base64_str, \"utf-8\"))))\n                # Save the image to a file\n                img.save(file_path)\n                saved_files.append(str(file_path))\n```\n\nFor each image data entry:\n- The base64 string representing the image is extracted.\n- This string is decoded, and an image object is created using the Pillow library.\n- Finally, the generated image is saved to the specified path, and the path is added to the `saved_files` list.\n\n## Function Tool Creation\n\n```python\n# Create the image generation tool\ngenerate_image_tool = FunctionTool(\n    func=generate_image,\n    description=\"Generate images using DALL-E based on text descriptions.\",\n    global_imports=[\n        \"io\",\n        \"uuid\",\n        \"base64\",\n        ImportFromModule(\"typing\", (\"List\", \"Optional\", \"Literal\")),\n        ImportFromModule(\"pathlib\", (\"Path\",)),\n        ImportFromModule(\"openai\", (\"OpenAI\",)),\n        ImportFromModule(\"PIL\", (\"Image\",)),\n    ],\n)\n```\n\n### FunctionTool Initialization\n\nAt the end of the script, the `generate_image` function is wrapped in a `FunctionTool`, which makes it easier to incorporate into larger frameworks or applications. The tool includes:\n\n- A reference to the `generate_image` function.\n- A description summarizing its purpose.\n- A list of global imports necessary for the function to operate correctly.\n\nThis tool allows the image generation functionality to be reused in different contexts easily, promoting modularity and maintainability. \n\n## Conclusion\n\nIn summary, this script encapsulates the functionality to generate and save images based on text descriptions using OpenAI's DALL-E model. It leverages various Python libraries and wraps the core function in a tool, facilitating easier integration into larger systems. The use of asynchronous programming enables efficient handling of requests and image generation tasks.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/generate_image.py"
  },
  {
    "code": false,
    "content": "# Google Search Tool Documentation\n\n## Overview\n\nThis Python script provides functionality to perform Google searches via the Custom Search API. It allows users to retrieve search results along with optional snippets and webpage content. The results are formatted in markdown, which is useful for applications that present textual content or generate reports. It leverages libraries such as `httpx` for asynchronous HTTP requests, `BeautifulSoup` for HTML parsing, and `html2text` for converting HTML to markdown.\n\n## Environment Variables\n\nThe script requires two environment variables to be set:\n- `GOOGLE_API_KEY`: This is essential for authenticating against the Google Custom Search API.\n- `GOOGLE_CSE_ID`: This is the ID of the Google Custom Search Engine used to perform the search queries.\n\nFailure to set these variables results in a `ValueError`, prompting users to configure their environment correctly.\n\n## Functionality\n\n### `google_search`\n\nThe main function, `google_search`, executes the search and processes the results. It takes several parameters to customize the search, including:\n\n- `query`: A string representing the search terms.\n- `num_results`: The number of search results to return (default is 3, max is 10).\n- `include_snippets`: A boolean flag to decide whether to include snippets from the search results.\n- `include_content`: A boolean flag to include the full webpage content in markdown format.\n- `content_max_length`: An optional integer specifying the maximum length for the webpage content (if included).\n- `language`: A string defining the language code for filtering search results.\n- `country`: An optional string for specifying the country for localized results.\n- `safe_search`: A boolean to enable or disable safe search features.\n\nThe function returns a list of dictionaries, each representing a search result with fields for `title`, `link`, `snippet`, and optionally `content`.\n\n### Fetching Page Content\n\nWithin `google_search`, a nested asynchronous function, `fetch_page_content`, is defined to fetch content from a webpage. This helper function performs the following tasks:\n\n1. Sends an HTTP GET request to the specified URL using `httpx.AsyncClient`.\n2. Parses the HTML response with `BeautifulSoup`, removing script and style tags to clean up the content.\n3. Converts relative URLs to absolute ones to ensure hyperlinks and media sources remain valid.\n4. Transforms the cleaned HTML into markdown format using the `html2text` library.\n5. Checks if the converted markdown exceeds the specified maximum length and truncates it if necessary.\n\nIf any errors are encountered during the fetching process, a descriptive error message is returned.\n\n### API Interaction\n\nThe main function prepares the parameters for the Google Custom Search API call based on provided arguments. It constructs a dictionary to specify the API key, search engine ID, query, number of results, language, and safe search parameters.\n\nSimilar to the content fetching, the API call is made asynchronously with error handling to manage `httpx.RequestError` and other exceptions. If the response is successful, it processes the JSON data to extract relevant fields and appends them to the results list.\n\n## Error Handling\n\nThe script includes robust error handling:\n- It raises a `ValueError` if essential environment variables are missing.\n- It manages HTTP request errors using try-except blocks, providing clear feedback on failures.\n- It also checks for key errors in the response format, ensuring that users are informed if the response is not as expected.\n\n## Tool Integration\n\nAt the end of the script, an instance of the `FunctionTool` is created, encapsulating the `google_search` function along with its description and required imports. This integration allows for easier usage of the tool within other systems, providing a clear API description to potential consumers.\n\n- **Description**: The `FunctionTool` provides an overview for users regarding its function, emphasizing the need for the necessary environment variables.\n- **Global Imports**: The tool lists all necessary modules, ensuring that they are included when the tool is used in other contexts.\n\n## Conclusion\n\nThis script is a comprehensive tool for leveraging Google's Custom Search API to perform searches programmatically. It handles results systematically while providing optional webpage content, making it useful for applications in data retrieval, report generation, or content analysis. The structured design ensures that it can be easily integrated into larger systems or used standalone for quick searches.",
    "filename": "python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py"
  },
  {
    "code": false,
    "content": "# Module Documentation for LiteStudio\n\n## Overview\n\nThis module is part of a package and serves as an interface for importing the `LiteStudio` class from the `studio` module. The use of `__all__` indicates that the module intends to expose only certain elements when imported using the wildcard syntax (`from module import *`). In this case, the only exposed element is `LiteStudio`.\n\n## Import Statement\n\n```python\nfrom .studio import LiteStudio\n```\n\n### Purpose\n\nThe import statement is designed to pull the `LiteStudio` class from a sibling module named `studio`. The leading dot in `.studio` signifies that `studio` is located in the same package as the current module. This structure allows for organized code by compartmentalizing functionality into different modules.\n\n## Exposed Elements\n\n```python\n__all__ = [\"LiteStudio\"]\n```\n\n### Explanation\n\nThe `__all__` variable is a convention in Python used to define the public interface of a module. By including `LiteStudio` in `__all__`, the code specifies that only this class should be imported when the following syntax is used:\n\n```python\nfrom module_name import *\n```\n\n### Benefits\n\nThis approach provides several benefits:\n\n1. **Clarity**: It makes clear which parts of the module are intended for public use.\n2. **Encapsulation**: It helps to prevent accidental imports of elements that are meant to be private or internal to the module.\n3. **Control**: It gives the developer control over the API exposed to other users or developers.\n\n## Class: LiteStudio\n\nAlthough the implementation details of `LiteStudio` are not present in the code snippet, we can infer a few things based on the module's structure.\n\n### Role\n\n`LiteStudio` is likely a class that encapsulates functionalities related to a lightweight version of a studio application or tool. It could serve various roles such as:\n\n- Providing a user interface for certain functionalities.\n- Managing resources or sessions in an application.\n- Acting as a controller or service layer for specific operations.\n\n### Suggested Functions and Features\n\nThe actual features and functionalities of `LiteStudio` can range widely based on its intended purpose. They might include:\n\n- Initialization and configuration of studio settings.\n- Methods for manipulating data or performing tasks relevant to studio operations.\n- Integrations with other components or services within the larger application.\n\n## Conclusion\n\nThe provided module is a straightforward interface for exposing the `LiteStudio` class from a sibling module called `studio`. While the specific functionalities of `LiteStudio` are not detailed in the current code snippet, its encapsulation within this module indicates an organized structure for building a potentially complex application. \n\nAs a best practice, developers working on this codebase should refer to the `studio` module for detailed information about the `LiteStudio` class to fully understand its capabilities and intended use within the overall application.",
    "filename": "python/packages/autogen-studio/autogenstudio/lite/__init__.py"
  },
  {
    "code": false,
    "content": "# LiteStudio Documentation\n\n## Overview\n\n`LiteStudio` is a Python class designed for managing instances of AutoGen Studio in a lightweight mode. It allows users to configure and run a server that can handle both file-based and programmatic team configurations. This utility primarily facilitates development environments for applications leveraging AutoGen's capabilities.\n\n## Initialization\n\n### `__init__` Method\n\nThe constructor initializes an instance of `LiteStudio` with parameters crucial for its operation:\n\n- **team**: Specifies how the team configuration should be loaded. It can accept various types:\n  - A path to a JSON file (string or `Path`).\n  - A dictionary representing the team configuration.\n  - An instance of `ComponentModel`.\n  - None, resulting in the creation of a default team.\n  \n- **host** and **port**: Set the server's host and port, defaulting to `127.0.0.1` and `8080` respectively.\n\n- **session_name**: Names the session that will be auto-created.\n\n- **auto_open**: A boolean that indicates whether to automatically open a web browser pointing to the running server upon startup.\n\n### Team Loading\n\n#### `_load_team` Method\n\nThis private method is responsible for loading the team configuration based on the provided argument. Depending on the type of the `team` argument, it performs different actions:\n\n- If `None`, it creates a default team using an external function.\n- If a file path is provided, it checks for its existence and raises an error if not found.\n- If a dictionary is provided, it serializes it to a temporary JSON file.\n- If a `ComponentModel` is provided, it dumps its model to a temporary file.\n- Lastly, it attempts to serialize other types by checking for specific attributes (`dump_component`, `model_dump`, or `dict`) and saving the validated data into a temporary file.\n\nThis method finally returns the absolute path of the team JSON file.\n\n## Environment Setup\n\n### `_get_env_file_path` Method\n\nThis private method creates a directory to store environment variable files if it does not already exist, and returns the path to a specified environment file, which holds necessary configuration settings.\n\n### `_setup_environment` Method\n\nThis method establishes the environment variables crucial for running the lite mode of the studio and saves them to a file. Required environment variables include:\n\n- Host and port configurations\n- Flags for enabling lite mode, disabling API documentation, and authentication\n- The session name and the path to the team file\n\nAfter writing these key settings to a file, it returns the path to that file.\n\n## Server Management\n\n### `_setup_browser_opening` Method\n\nThis method creates a separate thread to automatically open a web browser pointing to the studio's URL after a slight delay, allowing the server time to start properly before the browser attempts to access it.\n\n### `start` Method\n\nThe `start` method launches the lite studio server. It can run the server in either the foreground (blocking) or in the background. The following steps are carried out:\n\n1. It checks whether the server is already running and raises an error if so.\n2. It sets up the environment variables and the browser opening.\n3. The server is started using `uvicorn`, either in a separate thread or blocking the main thread.\n\n### `stop` Method \n\nThis method safely stops the lite studio server. If running in the background, it attempts to join the thread, ensuring a graceful shutdown. Note that this method's handling of server termination can be limited, especially in production scenarios.\n\n## Context Management\n\n### `__enter__` and `__exit__` Methods\n\nThe `LiteStudio` class implements context management functionality:\n- The `__enter__` method starts the server in the background upon entering the context manager.\n- The `__exit__` method stops the server when exiting the context.\n\nThis design allows seamless integration with `with` statements, ensuring proper resource handling.\n\n## Utility Function\n\n### `shutdown_port` Class Method\n\nThis utility method shuts down any process running on a specified port. It uses the `lsof` command to find and kill the processes, handling errors gracefully in case the command is not available or if other execution issues occur. \n\nThis feature is particularly useful for developers who may frequently start and stop servers during testing and development, preventing port conflicts.\n\n## Summary\n\nIn summary, the `LiteStudio` class provides a comprehensive solution for managing lightweight instances of the AutoGen Studio, facilitating easy configuration, server management, and environment setup through a well-structured API. Through its methods, it streamlines the workflow for developers while accounting for potential issues like process management and user configuration.",
    "filename": "python/packages/autogen-studio/autogenstudio/lite/studio.py"
  },
  {
    "code": false,
    "content": "# MCP WebSocket Bridge Callbacks Documentation\n\nThis document provides a high-level overview of the provided source code, which is primarily designed to manage and handle interactions with an MCP (message communication protocol) WebSocket. The code consists of several asynchronous callback functions that deal with specific operations such as message creation, sampling, and elicitation requests. \n\n## Overview of Imports\n\nThe code utilizes various modules and types from libraries and local utilities to facilitate its functionality:\n\n- **asyncio**: For handling asynchronous operations.\n- **uuid**: To generate unique identifiers for requests.\n- **datetime**: To manage time-related tasks, although it isn\u2019t explicitly used in the visible code.\n- **loguru**: For logging error and information messages.\n- **mcp.shared.context** and **mcp.shared.session**: To define request contexts and session handlers.\n- **mcp.types**: To define various data types used throughout the callbacks.\n\nAdditionally, it imports utility functions such as `extract_real_error` for error handling and `serialize_for_json` for data serialization. \n\n## Message Handler Callback\n\n### `create_message_handler`\n\nThis function generates an asynchronous message handler that processes incoming messages from the MCP protocol. \n\n#### Purpose\n- It routes different types of messages (e.g., `ServerRequest`, `ClientResult`, `ServerNotification`, and exceptions) and invokes appropriate logging and processing mechanisms.\n\n#### Detailed Functionality\n1. **Error Handling**: If the incoming message is an exception, it logs the error message and details after extracting the real error using `extract_real_error`.\n  \n2. **Protocol Messages Processing**: If the message has a `method` attribute, it logs the method and its parameters, serializing parameters where applicable. \n\n3. **Generic Message Handling**: For any other message type, it logs the message type and serializes its content if necessary.\n\n4. **Error Logging**: If an exception is raised during processing, it captures and logs this error as well.\n\nThis handler ensures the uniform handling of all incoming protocol messages and errors, enhancing the overall robustness of the WebSocket interactions.\n\n## Sampling Callback\n\n### `create_sampling_callback`\n\nThis function defines a callback that responds to AI sampling requests from tools.\n\n#### Purpose\n- To manage requests for AI-driven responses by logging relevant information and returning a placeholder response.\n\n#### Detailed Functionality\n1. **Request Identification**: It generates a new unique identifier for the incoming request to keep track of interactions.\n  \n2. **Logging Request Details**: It logs the number of messages included in the request and their corresponding parameters for monitoring purposes.\n\n3. **Placeholder Response Handling**: The callback constructs a default dummy response to simulate AI sampling. This response serves as a placeholder until a proper AI model is configured.\n\n4. **Additional Logging**: It logs the provision of the dummy response, showing when the request was handled successfully.\n\n5. **Error Handling**: In the event of an exception, it captures the error, logs it, and returns an `ErrorData` object with a descriptive message.\n\nThis callback is central to providing immediate feedback to tools that request AI sampling, while also facilitating future enhancements by indicating where integrations with AI can occur.\n\n## Elicitation Callback\n\n### `create_elicitation_callback`\n\nThis function sets up a callback to manage user input requests from tools.\n\n#### Purpose\n- To facilitate the solicitation of additional information from the user based on tool requests by logging activities and managing user responses effectively.\n\n#### Detailed Functionality\n1. **Unique Request Handling**: It generates a unique request ID for tracking response completion or timeout events.\n\n2. **Logging of Request Parameters**: It logs the message requesting user input, along with any associated schema.\n\n3. **User Input Management**: The callback sends the elicitation request via the `bridge.on_elicitation_request`, expecting a response from the user.\n\n4. **Response Handling with Future**: It utilizes an `asyncio.Future` to await the user response, allowing other tasks to run concurrently until the response comes in or a timeout occurs.\n\n5. **Timeout Handling**: If no response is received within a specified duration (60 seconds), it logs a timeout warning and cleans up by removing the pending request.\n\n6. **Error Management**: Similar to the previous callbacks, any errors encountered during this process are logged, and appropriate error data is returned.\n\nThis callback plays a crucial role in ensuring that user requests for information are processed smoothly, providing timely feedback and handling potential errors adeptly.\n\n## Conclusion\n\nThe provided code implements critical callback functionalities for managing an MCP WebSocket bridge. Each function has a defined purpose, ensuring effective communication between tools, AI models, and users while integrating robust error handling and logging mechanisms at every step. The overall design supports extensibility for future AI integration, improving the user experience in interactive applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/mcp/callbacks.py"
  },
  {
    "code": false,
    "content": "# MCP Client Documentation\n\nThis documentation covers the functionality and structure of an MCP (Modular Communication Protocol) client implemented in Python. The code defines an event handler interface and a main client class responsible for handling various operations within an MCP session.\n\n## MCP Event Handler Interface\n\n### Overview\nThe `MCPEventHandler` class serves as a protocol interface for handling events related to MCP operations. This protocol defines several asynchronous methods that must be implemented by any class intending to handle MCP events.\n\n### Methods\n\n- **`on_initialized(self, session_id: str, capabilities: Any) -> None`**  \n  This method is called when the MCP session is successfully initialized. It receives the session ID and the capabilities of the session, which can be used for further communication.\n\n- **`on_operation_result(self, operation: str, data: Dict[str, Any]) -> None`**  \n  This method is invoked when an MCP operation completes successfully. It provides the name of the operation that was executed along with the resulting data.\n\n- **`on_operation_error(self, operation: str, error: str) -> None`**  \n  This method handles errors that occur during MCP operations. It provides the name of the operation that failed and the corresponding error message.\n\n- **`on_mcp_activity(self, activity_type: str, message: str, details: Dict[str, Any]) -> None`**  \n  This method is called for general MCP protocol activities, such as logging or tracking events that may not fit into the other categories.\n\n- **`on_elicitation_request(self, request_id: str, message: str, requested_schema: Any) -> None`**  \n  This method is called when the MCP requests user input, allowing the handler to respond appropriately based on the request ID and message.\n\n## MCP Client Class\n\n### Overview\nThe `MCPClient` class is designed to manage an MCP session and execute various operations. It abstracts the transport layer, enabling a clean interface for performing operations and handling their results.\n\n### Constructor\n\n- **`__init__(self, session: ClientSession, session_id: str, event_handler: MCPEventHandler)`**  \n  The constructor initializes the client with a session object, the session identifier, and an event handler that implements the `MCPEventHandler` protocol. It sets up internal states for tracking initialization and capabilities.\n\n### Initialization\n\n- **`async def initialize(self) -> None`**  \n  This asynchronous method initializes the MCP session. It attempts to call the `initialize` method on the `ClientSession`. If successful, it updates the capabilities and notifies the event handler about the session's initialization. If any errors occur, it reports them to the event handler.\n\n### Handling Operations\n\n- **`async def handle_operation(self, operation: Dict[str, Any]) -> None`**  \n  This asynchronous method processes incoming MCP operations. It checks the operation type and executes the corresponding action. The method includes cases for various operations such as listing tools, calling a tool, and reading resources. For each operation, it serializes the result and informs the event handler of the operation's success or failure.\n\n### Supported Operations\n\n1. **List Tools**  \n   This retrieves a list of tools available in the MCP session.\n\n2. **Call Tool**  \n   This operation invokes a specified tool with provided arguments. It requires a tool name and may raise an error if not specified.\n\n3. **List Resources**  \n   This operation retrieves available resources within the MCP environment.\n\n4. **Read Resource**  \n   This fetches a specific resource identified by its URI, which must be provided.\n\n5. **List Prompts**  \n   This operation retrieves a list of prompts available in the session.\n\n6. **Get Prompt**  \n   This retrieves a specific prompt by name, requiring the prompt's name as input. Like the call tool operation, it raises an error if the name is not provided.\n\n### Error Handling\nError handling is a crucial aspect of the `handle_operation` method. The code captures exceptions and extracts meaningful error messages, allowing for robust communication of issues to the event handler.\n\n## Capabilities Property\n\n- **`@property def capabilities(self)`**  \n  The `capabilities` property exposes the capabilities of the MCP session that were initialized earlier. This allows external components to query the capabilities without breaking encapsulation. \n\nOverall, the MCP Client is designed to facilitate seamless interaction with an MCP session, ensuring that operations are handled efficiently and errors are properly communicated.",
    "filename": "python/packages/autogen-studio/autogenstudio/mcp/client.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code provides utility functions and error handling mechanisms for a system that utilizes WebSockets and JSON serialization. It defines a custom exception, `McpOperationError`, and implements functions to handle error extraction, object serialization for JSON compatibility, and specific checks for WebSocket disconnection exceptions. The code is structured in a way that enhances the usability and robustness of an application likely built on the FastAPI framework.\n\n## Custom Exception: `McpOperationError`\n\n```python\nclass McpOperationError(Exception):\n    \"\"\"Raised when MCP operation fails\"\"\"\n    pass\n```\n\nThe `McpOperationError` is a custom exception derived from Python\u2019s built-in `Exception` class. This exception is likely intended to be raised when operations related to an MCP (Multi-Channel Processor or similar) encounter an error. By defining this custom exception, developers can catch specific errors related to MCP functionality in their code more clearly.\n\n## Error Extraction: `extract_real_error`\n\n```python\ndef extract_real_error(e: Exception) -> str:\n    \"\"\"Extract the real error message from potentially wrapped exceptions\"\"\"\n```\n\nThe `extract_real_error` function is designed to analyze an exception object and produce a comprehensive string representation of its underlying causes. Given the possibility of nested exceptions in Python, this function handles multiple scenarios:\n\n- **ExceptionGroup (Python 3.11+):** It checks if the exception has multiple wrapped exceptions and extracts information from them.\n- **Chained Exceptions:** If the exception has a cause, it traverses through the chain to construct a detailed message.\n- **Contextual Exceptions:** It examines the context of the exception to provide additional insights.\n- **Default Case:** If none of the above cases are met, it simply returns the type and message of the exception.\n\nThe output is a concatenated string that includes key pieces of information about the error, formatted for ease of understanding.\n\n## JSON Serialization: `serialize_for_json`\n\n```python\ndef serialize_for_json(obj: Any) -> Any:\n    \"\"\"Convert objects to JSON-serializable format\"\"\"\n```\n\nThe `serialize_for_json` function serves to convert various types of objects into a format suitable for JSON serialization. It achieves this through the following checks and transformations:\n\n- **AnyUrl:** If the object is of type `AnyUrl`, it converts it to a string.\n- **Dictionaries:** If it\u2019s a dictionary, the function recursively serializes its keys and values for JSON compatibility.\n- **Lists:** Similarly, it serializes each item in a list.\n- **Pydantic Models:** If the object has a `model_dump` method (common with Pydantic models), it recursively serializes the dictionary returned by this method.\n- **Default Case:** For all other object types, it returns the object as is.\n\nThis function is helpful for preparing data that needs to be transmitted over the network or stored, ensuring it complies with JSON standards.\n\n## WebSocket Disconnect Check: `is_websocket_disconnect`\n\n```python\ndef is_websocket_disconnect(e: Exception) -> bool:\n    \"\"\"Check if an exception (potentially nested) is a WebSocket disconnect\"\"\"\n```\n\nThe `is_websocket_disconnect` function checks if a given exception, which could be nested, signifies a WebSocket disconnection event. The function uses an inner function `check_exception` to perform several checks:\n\n- **Direct Check:** It first checks if the exception is an instance of `WebSocketDisconnect`.\n- **Name/Message Check:** It examines the exception name and message for indications of WebSocket disconnection.\n- **Nested Exceptions:** The function recursively analyzes any grouped exceptions, chained exceptions, or context-related exceptions to determine if they signal a WebSocket disconnect.\n\nThe result is a Boolean flag indicating the presence of a WebSocket disconnect within the exception hierarchy.\n\n## Conclusion\n\nOverall, this code enhances error handling and data serialization in applications that utilize WebSockets and Pydantic models, potentially in a FastAPI context. It includes robust mechanisms for extracting detailed error messages, converting objects to JSON-compatible formats, and specifically identifying WebSocket disconnection scenarios. This functionality lays a solid foundation for developing resilient and user-friendly web applications.",
    "filename": "python/packages/autogen-studio/autogenstudio/mcp/utils.py"
  },
  {
    "code": false,
    "content": "# MCPWebSocketBridge Documentation\n\n## Overview\n\nThe `MCPWebSocketBridge` class serves as a bridge between WebSocket connections and Multi-Channel Protocol (MCP) operations. This asynchronous class handles WebSocket messages, communicates with an underlying `MCPClient`, and sends responses or notifications back through the WebSocket.\n\n## Class Definition\n\n### MCPWebSocketBridge\n\nThe `MCPWebSocketBridge` class inherits from `MCPEventHandler`, which implies it implements certain event handling methods pertinent to the MCP protocol.\n\n#### Initialization\n\n```python\ndef __init__(self, websocket: WebSocket, session_id: str):\n```\n\n- **Parameters**:\n  - `websocket`: Establishes the WebSocket connection.\n  - `session_id`: Unique identifier for the session associated with this WebSocket connection.\n\n- **Attributes**:\n  - `self.mcp_client`: Optional attribute for an instance of `MCPClient`.\n  - `self.pending_elicitations`: Dictionary for tracking pending elicitation responses.\n  - `self._running`: Boolean flag to manage the running state of the bridge.\n\n## WebSocket Messaging\n\n### Sending Messages\n\n```python\nasync def send_message(self, message: Dict[str, Any]) -> None:\n```\n\n- **Purpose**: This private method sends JSON-serialized messages through the WebSocket connection, verifying the connection state.\n- **Error Handling**: Logs errors that may occur during the message-sending process using the `loguru` logger.\n\n### Handling Incoming Messages\n\n```python\nasync def handle_websocket_message(self, message: Dict[str, Any]) -> None:\n```\n\n- **Purpose**: This method processes incoming JSON messages to perform actions based on the message type.\n\n- **Message Types**:\n  - **operation**: Triggers an operation through the `MCPClient`.\n  - **ping**: Replies with a \"pong\" message.\n  - **elicitation_response**: Invokes a separate handler for elicitation responses.\n  - **unknown**: Sends an error for unrecognized message types.\n\n## Event Handling Methods\n\nThe class implements several methods corresponding to different events that can occur during MCP operations. They all utilize the `send_message` method to communicate various statuses back to the WebSocket:\n\n### 1. Initialization Event\n\n```python\nasync def on_initialized(self, session_id: str, capabilities: Any) -> None:\n```\n\n- Notifies the client that the bridge has been initialized along with session capabilities.\n\n### 2. Operation Result Event\n\n```python\nasync def on_operation_result(self, operation: str, data: Dict[str, Any]) -> None:\n```\n\n- Sends the result of an MCP operation back to the client.\n\n### 3. Operation Error Event\n\n```python\nasync def on_operation_error(self, operation: str, error: str) -> None:\n```\n\n- Notifies the client of any errors that occurred during the execution of an MCP operation.\n\n### 4. MCP Activity Event\n\n```python\nasync def on_mcp_activity(self, activity_type: str, message: str, details: Dict[str, Any]) -> None:\n```\n\n- Communicates information about specific activities occurring within MCP.\n\n### 5. Elicitation Request Event\n\n```python\nasync def on_elicitation_request(self, request_id: str, message: str, requested_schema: Any) -> None:\n```\n\n- Sends requests for elicitation responses back to clients, including all relevant details.\n\n## Elicitation Response Handling\n\n### Processing User Responses to Elicitation Requests\n\n```python\nasync def _handle_elicitation_response(self, message: Dict[str, Any]) -> None:\n```\n\n- **Purpose**: This method processes responses to elicitation requests, determining if a response has been accepted, declined, or canceled.\n- **Completes**: Updates the corresponding `Future` in `pending_elicitations` based on the response.\n\n## Main Message Loop\n\n### Run Method\n\n```python\nasync def run(self) -> None:\n```\n\n- **Purpose**: The main loop that continuously listens for incoming messages from the WebSocket, while also managing the asynchronous nature of the connections.\n- **Error Handling**: Handles errors related to message reception and logs potential issues, specifically focusing on JSON parse errors.\n\n## Stopping the Bridge\n\n### Stop Method\n\n```python\ndef stop(self) -> None:\n```\n\n- **Purpose**: Safely terminates the message loop by updating the `_running` state to `False`.\n\n## Conclusion\n\nThe `MCPWebSocketBridge` class is a critical component for managing WebSocket communications in an asynchronous environment while adhering to the guidelines of the MCP protocol. Through a well-structured set of methods, it ensures that operations are handled efficiently, and relevant information is communicated back to clients consistently.",
    "filename": "python/packages/autogen-studio/autogenstudio/mcp/wsbridge.py"
  },
  {
    "code": false,
    "content": "# Module Documentation: TeamManager\n\n## Overview\n\nThis module serves as an interface for the `TeamManager` class, which is imported from the `teammanager` module. Its primary role is to make the `TeamManager` class available for use in other parts of the application by specifying it in the `__all__` list. This indicates which objects the module exports when using the wildcard import.\n\n## Purpose\n\nThe main purpose of this module is to provide access to the functionality encapsulated within the `TeamManager` class. By defining `__all__`, it promotes a clean API for users of the module, allowing them to understand which components are intended for public use.\n\n## Import Statement\n\n```python\nfrom .teammanager import TeamManager\n```\n\nThis line imports the `TeamManager` class from a sibling module named `teammanager`. The dot (`.`) indicates a relative import, suggesting that `teammanager` is located in the same package as the current module.\n\n### Implications of Relative Import\n\nUsing a relative import signifies that this module is part of a larger package structure. This organization can help keep related components together and facilitates modular design, which is particularly useful in larger projects.\n\n## Exposing the `TeamManager` Class\n\n```python\n__all__ = [\"TeamManager\"]\n```\n\nThis line defines the `__all__` variable, which is a conventional Python construct used to specify a list of public objects of that module. \n\n### Role of `__all__`\n\n- **Public Interface Specification**: By including `TeamManager` in `__all__`, the module indicates that this is the primary object intended for use by external code.\n- **Control Over Imports**: This setting influences behaviors in wildcard imports (e.g., `from module import *`), allowing the module to control which symbols are exported and can help avoid namespace pollution and confusion.\n\n## TeamManager Class\n\nWhile this module does not define the `TeamManager` class itself, its architecture implies that `TeamManager` encapsulates certain functionalities related to team management\u2014possibly involving creating, modifying, or organizing teams within an application.\n\n### Potential Functionality of TeamManager\n\nThough the specifics of `TeamManager` are not detailed in this module, typical functionality might include:\n\n- **Creating Teams**: Facilitating the establishment of new teams with designated members.\n- **Managing Team Members**: Offering methods to add or remove members from the teams.\n- **Tracking Team Performance**: Potentially includes features for assessing the productivity or output of teams.\n- **Collaboration Tools**: Could offer methods and utilities for managing communication and collaboration within teams.\n\n## Conclusion\n\nThis module imports and exposes the `TeamManager` class for reuse in other portions of the application. While it does not provide additional features or functions, its structure sets the stage for a well-organized and modular codebase. By clearly defining the public API through `__all__`, it intends to streamline interaction with the team management functionalities encapsulated within the `TeamManager` class.",
    "filename": "python/packages/autogen-studio/autogenstudio/teammanager/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Code\n\nThis code is a component of an asynchronous program designed to manage and execute group chat agents. It utilizes the `asyncio` library for handling asynchronous I/O operations and includes logging functionality to track events during the execution of tasks. Below is a detailed breakdown of its functionalities, organized into logical sections.\n\n## Imports and Initialization\n\nThe code begins with various import statements, bringing in necessary libraries and modules:\n\n- `asyncio`, `json`, `logging`, `os`, `time`, `pathlib`, and `typing`: Standard libraries for handling asynchronous tasks, data manipulation, logging, and type annotations.\n- `aiofiles`: An async file handling library.\n- `yaml`: For loading YAML configurations.\n- Custom modules from `autogen_agentchat` and `autogen_core`: Providing functionality for agent communication, task management, and logging events.\n\nA logger instance is created for tracking events within the module.\n\n## Type Definitions\n\nSeveral type definitions are declared to facilitate input handling:\n\n- `SyncInputFunc`: A synchronous function type that takes a string argument and returns a string.\n- `AsyncInputFunc`: An asynchronous function type, accepting a string and an optional `CancellationToken`, returning a future string.\n- `InputFuncType`: A union type that can be either `SyncInputFunc` or `AsyncInputFunc`.\n\n## RunEventLogger Class\n\n### Purpose\n\n`RunEventLogger` is a custom logging handler designed to capture events related to LLM calls (likely representing a machine learning model invocation). It queues these events to be streamed later.\n\n### Methods\n\n- **`__init__`**: Initializes the logger and an asynchronous queue to store `LLMCallEventMessage` instances.\n- **`emit`**: Overrides the default logging handler behavior to put LLM call event messages into the event queue for later retrieval.\n\n## TeamManager Class\n\n### Overview\n\nThe `TeamManager` class is responsible for managing operations related to teams of chat agents. This includes loading configurations and executing tasks within a group chat context.\n\n### Attributes\n\n- `_team`: An optional attribute for storing the current group chat instance.\n- `_run_context`: An instance of `RunContext` to manage execution context and dependencies.\n\n### Methods\n\n#### Configuration Loading\n\n- **`load_from_file`**: Asynchronously loads a team configuration from a specified JSON or YAML file. It validates the file format and raises errors for unsupported formats or if the file does not exist.\n  \n```python\nasync def load_from_file(path: Union[str, Path]) -> Any:\n```\n\n- **`load_from_directory`**: Asynchronously loads all valid team configuration files from a specified directory. It supports JSON and YAML formats and handles loading errors by logging them.\n\n```python\nasync def load_from_directory(directory: Union[str, Path]) -> List[Any]:\n```\n\n#### Team Creation\n\n- **`_create_team`**: This private method constructs a team instance based on provided configuration data. It can accept different types of inputs: file paths, dictionaries, or a `ComponentModel`. It sets environment variables if provided and initializes agents within the team.\n\n```python\nasync def _create_team(\n    self,\n    team_config: Union[str, Path, Dict[str, Any], ComponentModel],\n    input_func: Optional[InputFuncType] = None,\n    env_vars: Optional[List[EnvironmentVariable]] = None,\n) -> BaseGroupChat:\n```\n\n### Task Execution\n\n#### Streaming Execution\n\n- **`run_stream`**: Allows for asynchronous execution of tasks within a created team. It streams the results, yields them one-by-one, and monitors for any cancellation requests. This method integrates the logger to capture events relating to LLM calls during execution.\n\n```python\nasync def run_stream(\n    self,\n    task: str | BaseChatMessage | Sequence[BaseChatMessage] | None,\n    team_config: Union[str, Path, Dict[str, Any], ComponentModel],\n    input_func: Optional[InputFuncType] = None,\n    cancellation_token: Optional[CancellationToken] = None,\n    env_vars: Optional[List[EnvironmentVariable]] = None,\n) -> AsyncGenerator[Union[BaseAgentEvent | BaseChatMessage | LLMCallEvent, BaseChatMessage, TeamResult], None]:\n```\n\n#### Synchronous Execution\n\n- **`run`**: This method executes a task synchronously. It follows a similar workflow to `run_stream`, but it returns a single `TeamResult`, containing the final outcome of the executed task.\n\n```python\nasync def run(\n    self,\n    task: str | BaseChatMessage | Sequence[BaseChatMessage] | None,\n    team_config: Union[str, Path, Dict[str, Any], ComponentModel],\n    input_func: Optional[InputFuncType] = None,\n    cancellation_token: Optional[CancellationToken] = None,\n    env_vars: Optional[List[EnvironmentVariable]] = None,\n) -> TeamResult:\n```\n\n## Clean-Up and Resource Management\n\nBoth `run` and `run_stream` methods ensure that at the end of their execution, proper clean-up is performed. Agents in the team are closed if they have a close method, ensuring that resources are released and that the application does not hang after execution. This meticulous management highlights the attention to resource handling, which is essential in asynchronous programming.\n\n## Summary\n\nThis code defines a framework for managing group chat agents, enabling the loading of configurations from both files and directories, creating teams of agents, and executing tasks in both asynchronous streaming and synchronous manners. It employs logging to track events and provides mechanisms to ensure robust resource management throughout the execution of tasks. Overall, it is well-structured for its intended asynchronous context, making it suitable for building complex interactions in chat-based systems.",
    "filename": "python/packages/autogen-studio/autogenstudio/teammanager/teammanager.py"
  },
  {
    "code": false,
    "content": "# Code Analysis and Documentation\n\n## Overview\n\nThis document provides a high-level description and analysis of the given source code. It outlines the purpose of the code, breaks down its components, and details any functions, methods, or classes that are defined within. The goal is to give readers clarity regarding what the code does and how it functions overall.\n\n## Functionality\n\nThe code appears to be a script designed to perform a specific task involving data processing. It likely involves reading input data, transforming it in some manner, and producing an output, whether that be to the console, a file, or another data structure. The script is structured to handle various operations, possibly including cleaning data, performing calculations, or iterating over datasets.\n\n## Key Components\n\n### Libraries and Imports\n\nThe code likely begins with a series of import statements, which are essential for utilizing external libraries and modules. These libraries may include packages for data manipulation such as `pandas` or `numpy`, or libraries for data visualization like `matplotlib` or `seaborn`. Understanding which libraries are imported offers insights into the functionalities that the script is designed to leverage.\n\n### Data Input\n\nThe first major step in the script is typically the data input phase. This may involve reading from files (CSV, JSON, Excel, etc.), querying databases, or accepting command-line arguments. The methods used to gather data are crucial, as they determine the format and structure of the dataset to be processed.\n\n### Data Processing\n\nFollowing the input stage, the script likely includes a data processing section. Here, various operations are performed on the dataset. This could include cleaning the data (removing null values, converting data types), transforming it (normalizing, aggregating), or analyzing it (calculating statistics, identifying trends). This processing stage is critical as it prepares the data for final output.\n\n### Function Definitions\n\nIf the code defines functions, each would typically encapsulate a specific operation. These functions may be designed for:\n- **Data Cleaning**: Handling missing or invalid entries.\n- **Computation**: Performing certain calculations or algorithms.\n- **Visualization**: Creating plots or graphs to represent the data visually.\n\nEach function should have a clear purpose and would be reusable within the script, allowing for a modular approach to coding.\n\n### Output Generation\n\nOnce the data has been processed, the script concludes with an output generation phase. This may involve:\n- Printing results to the console for immediate viewing.\n- Writing transformed data to a new file for further use.\n- Generating plots or charts if visualization is part of the script's purpose.\n\nUnderstanding how the output is handled is essential, as it dictates how users or other systems interact with the results of the script's execution.\n\n## Error Handling\n\nAn essential aspect of robust scripting is error handling. The code may include try/except blocks or conditional checks to manage potential runtime errors, such as file not found errors, data format mismatches, or empty data structures. This enhances the script's usability and stability.\n\n## Conclusion\n\nIn summary, the provided code operates as a scripted solution to perform data-related tasks efficiently. Through a structured flow of libraries, input handling, processing, and output generation, it facilitates a comprehensive approach to managing and analyzing data. The use of functions enhances modularity and reusability, making the script easier to maintain and expand upon in future versions or implementations.",
    "filename": "python/packages/autogen-studio/autogenstudio/utils/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for `construct_task` Function\n\n## Overview\nThe `construct_task` function is designed to create a series of `ChatMessage` objects based on user input in the form of a text query and a list of files. The function is part of a larger system, likely involved in handling chat interactions, possibly in a customer support or automated assistant context. It processes both text and image files and generates messages that can be used by an AI agent for further processing.\n\n## Imports\nThe function relies on several modules:\n1. **base64**: Used for decoding base64 encoded content, particularly for text files.\n2. **typing.Sequence**: For type hints, specifying that the function returns a sequence of `ChatMessage` objects.\n3. **autogen_agentchat.messages**: Imported `ChatMessage`, `MultiModalMessage`, and `TextMessage`, which are likely classes used to represent different types of chat messages.\n4. **autogen_core**: Imports `Image`, suggesting it handles image-related functionalities.\n5. **autogen_core.models**: Imports `UserMessage`, potentially for representing messages from user interactions.\n6. **loguru.logger**: A logging module used for logging warnings and errors encountered during file processing.\n\n## Function Signature\n```python\ndef construct_task(query: str, files: list[dict] | None = None) -> Sequence[ChatMessage]:\n```\n\n### Parameters\n- **query (str)**: A string containing the user's message or question.\n- **files (list[dict] | None)**: An optional list of dictionaries, where each dictionary represents a file and contains metadata such as `name`, `content`, and `type`. If not provided, defaults to an empty list.\n\n### Returns\n- The function returns a sequence of `ChatMessage` objects. These can include instances of `TextMessage` for text inputs and `MultiModalMessage` for image files.\n\n## Function Logic\n1. **Initialize Variables**: The function starts by initializing a list called `messages` to store the messages created from the user's input.\n2. **Process Query**: If a valid query string is provided, a `TextMessage` object is created and appended to the `messages` list, indicating the source is the user.\n\n3. **Handle Files**: The function then iterates through each file in the `files` list:\n   - For **Image Files**:\n     - The function checks if the `type` of the file starts with `image/`.\n     - It decodes the image content from base64 using `Image.from_base64`.\n     - A `MultiModalMessage` is created with the image and metadata about the filename.\n   \n   - For **Text Files**:\n     - If the `type` starts with `text/`, the content is decoded from base64 to UTF-8.\n     - A `TextMessage` is generated and appended, including the decoded content and filename metadata.\n\n   - For **Unsupported Types**:\n     - The function logs a warning using `logger.warning` if the file type is unsupported.\n     - If the type starts with `application/`, it attempts to decode it as text, treating it similarly to text files. A `TextMessage` is created based on this content.\n\n4. **Error Handling**: The function includes a try-except block that captures any exceptions encountered during file processing. If an error occurs with one file, it logs the error and continues processing other files.\n\n## Logging\nThe use of `logger` from the `loguru` library enhances the robustness of the function. It logs warnings for unsupported file types and errors during processing, which can be invaluable for debugging and operational monitoring.\n\n## Summary\nIn summary, the `construct_task` function is a utility for converting user queries and associated files into structured chat messages for an automated system. It adeptly handles both text and image files, making it versatile for various input types. The function is built with error handling and logging, ensuring that it can process input as smoothly as possible while providing meaningful feedback on issues encountered.",
    "filename": "python/packages/autogen-studio/autogenstudio/utils/utils.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThe provided source code is a script that performs specific actions based on its logic. It likely involves operations depending on certain inputs and may incorporate functions or classes to organize its functionality. The code consists of various components that work together to achieve a defined goal, processing data or performing tasks as required.\n\n## Main Functionality\n\nAt a high level, this script appears to be designed for a specific application, possibly involving data manipulation, input/output operations, or algorithmic processing. While the exact details depend on the contents of the code, we can outline some common characteristics often found in scripts like this one.\n\n### Input Handling\n\nThe script likely begins by gathering input data, which can come from various sources such as user input, files, or external APIs. This phase is critical as it sets up the context for the operations that follow. If input validation is involved, it ensures that the subsequent processes are based on accurate and expected data.\n\n### Data Processing\n\nFollowing the input stage, there is usually a segment of code that processes the data. This could involve transformations, computations, or filtering actions designed to manipulate the input into a desired format or state. Depending on the requirements, this section may utilize different algorithms or functions to carry out its objectives efficiently.\n\n### Function Definitions\n\nIf the code contains functions, each function is likely defined to encapsulate a specific task or operation. This modular approach allows for code reuse and enhances readability. For example, a function might handle data validation, another could perform calculations, and yet another might format the output for display.\n\n### Conditional Logic\n\nThe script may include conditional statements (like if-else blocks) to control the flow of operations based on specific criteria. This logic is essential for making decisions within the code, allowing it to respond dynamically to varying inputs or states. For example, certain actions may only occur if particular conditions are met, thus enabling tailored functionality.\n\n## Output Generation\n\nAs the script completes its processing, it probably generates output, which may be displayed to the user or written to a file. This output is crucial as it provides the results of the script\u2019s operations. Formatting the output correctly ensures that information is conveyed clearly and effectively, making it easier for users to understand the results.\n\n### Error Handling\n\nGood practice in coding includes error handling to manage potential issues that may arise during execution. The script might implement try-catch blocks or similar mechanisms to ensure that errors are gracefully handled, providing feedback without crashing the application. This aspect is vital for maintaining user experience and ensuring reliability.\n\n## Conclusion\n\nIn summary, the code serves to perform a series of tasks that start from gathering input, processing the data, making decisions based on conditions, and finally generating output. The design might include functions for organization, with processes clearly delineated to enhance readability and maintainability. Further details would be derived from examining specific functions and logic within the script, which would provide a more in-depth understanding of its workings. \n\nThis high-level overview serves as an initial assessment of the code's purpose and structure, setting the stage for deeper analysis if needed.",
    "filename": "python/packages/autogen-studio/autogenstudio/validation/__init__.py"
  },
  {
    "code": false,
    "content": "# Component Test Service Documentation\n\nThis documentation outlines the functionality of the `ComponentTestService`, which is designed to test various types of components like agents, models, tools, teams, and termination components within an automated generation environment. The service utilizes asynchronous programming to perform tests, allowing for non-blocking execution and better resource management.\n\n## Import Statements\n\nThe code begins by importing the required modules:\n\n```python\nimport asyncio\nfrom typing import Any, Dict, List, Optional\nfrom autogen_core import ComponentModel\nfrom autogen_core.models import ChatCompletionClient, UserMessage\nfrom pydantic import BaseModel\n```\n\n- **asyncio**: Used for asynchronous programming.\n- **typing**: Provides type hints for better code clarity and type checking.\n- **autogen_core**: Contains the `ComponentModel` used to represent different components.\n- **autogen_core.models**: Contains `ChatCompletionClient` and `UserMessage`, which are used for testing components that interact with chat-based models.\n- **pydantic**: Used to define data models with validation capabilities.\n\n## Data Models\n\n### `ComponentTestResult`\n\nThis class, inheriting from `BaseModel`, defines the structure of the result returned from component tests:\n\n```python\nclass ComponentTestResult(BaseModel):\n    status: bool\n    message: str\n    data: Optional[Any] = None\n    logs: List[str] = []\n```\n\n- **status**: Indicates whether the test was successful.\n- **message**: Provides a textual summary of the test result.\n- **data**: Optional field to contain any relevant data returned from the test.\n- **logs**: A list of strings to track logs generated during the testing process.\n\n### `ComponentTestRequest`\n\nThis class encapsulates the information required to initiate a test on a component:\n\n```python\nclass ComponentTestRequest(BaseModel):\n    component: ComponentModel\n    model_client: Optional[Dict[str, Any]] = None\n    timeout: Optional[int] = 30\n```\n\n- **component**: The component to be tested.\n- **model_client**: An optional dictionary specifying the model client configuration.\n- **timeout**: The maximum time (in seconds) allowed for the test to complete.\n\n## ComponentTestService Class\n\nThe `ComponentTestService` class contains several static methods targeted at testing different types of components. Each method returns a `ComponentTestResult`.\n\n### `test_agent`\n\nThis method tests an agent component:\n\n```python\nasync def test_agent(component: ComponentModel, model_client: Optional[ChatCompletionClient] = None) -> ComponentTestResult\n```\n\n1. Attempts to load the agent from the given component.\n2. If successful, sends a predefined question to the agent and waits for a response.\n3. Validates the response, updating the logs accordingly.\n4. Returns a `ComponentTestResult` containing the outcome of the test.\n\n### `test_model`\n\nThis method is for testing model components:\n\n```python\nasync def test_model(component: ComponentModel, model_client: Optional[ChatCompletionClient] = None) -> ComponentTestResult\n```\n\n1. Uses the component itself as a model client.\n2. Sends a predefined query and evaluates the response.\n3. Logs success or failure, returning the relevant result.\n\n### `test_tool`, `test_team`, and `test_termination`\n\nThese methods are placeholders for testing tool, team, and termination components, respectively:\n\n```python\nasync def test_tool(component: ComponentModel) -> ComponentTestResult\nasync def test_team(component: ComponentModel, model_client: Optional[ChatCompletionClient] = None) -> ComponentTestResult\nasync def test_termination(component: ComponentModel) -> ComponentTestResult\n```\n\n- Each of these functions acknowledges that these tests are not yet implemented, returning a default result indicating the respective component was loaded successfully.\n\n## `test_component`\n\nThe primary interface to test various component types:\n\n```python\nasync def test_component(cls, component: ComponentModel, timeout: int = 60, model_client: Optional[ChatCompletionClient] = None) -> ComponentTestResult\n```\n\n1. Identifies the type of component being tested.\n2. Selects the appropriate testing method based on the component type.\n3. Determines if the method accepts a model client for additional configuration.\n4. Executes the test method within a specified timeout.\n5. Handles any exceptions that occur, providing meaningful error messages.\n\n### Error Handling\n\nThroughout the methods, exceptions are caught and handled gracefully by returning a `ComponentTestResult` with a status of `False`, along with a message describing the error. This approach adds a layer of robustness to the testing service.\n\n## Conclusion\n\nThe `ComponentTestService` is a well-defined framework for dynamically testing different types of components in an automated generation system. It utilizes asynchronous calls to maximize efficiency and provides a structured way to handle errors and log results. The placeholders for tool, team, and termination tests indicate areas for future expansion, making this service adaptable and scalable as new component types are developed.",
    "filename": "python/packages/autogen-studio/autogenstudio/validation/component_test_service.py"
  },
  {
    "code": false,
    "content": "# Validation Service Documentation\n\nThis document provides an overview of the `validation_service.py` file located in the `validation` module. The file defines various classes and a validation service specifically aimed at validating components in the context of a broader software architecture.\n\n## Overview\n\nThe `validation_service.py` file primarily includes a validation service that verifies components' configurations against predefined rules. The main purpose of this service is to ensure that components are correctly defined, compatible, and able to be instantiated without errors. The service employs the Pydantic library for data validation and makes use of reflection to dynamically import classes and validate their configurations.\n\n## Data Models\n\n### ValidationRequest\n\n```python\nclass ValidationRequest(BaseModel):\n    component: ComponentModel\n```\n\n- **Purpose**: Represents a request for validation, encapsulating the component model that needs validation.\n- **Attributes**: \n  - `component`: A `ComponentModel` instance representing the component to validate.\n\n### ValidationError\n\n```python\nclass ValidationError(BaseModel):\n    field: str\n    error: str\n    suggestion: Optional[str] = None\n```\n\n- **Purpose**: Describes an error that occurred during validation.\n- **Attributes**: \n  - `field`: The name of the field that has an issue.\n  - `error`: A description of the validation problem.\n  - `suggestion`: Optional guidance on how to resolve the error.\n\n### ValidationResponse\n\n```python\nclass ValidationResponse(BaseModel):\n    is_valid: bool\n    errors: List[ValidationError] = []\n    warnings: List[ValidationError] = []\n```\n\n- **Purpose**: Represents the response returned after validation.\n- **Attributes**: \n  - `is_valid`: A boolean indicating whether the component is valid.\n  - `errors`: A list of `ValidationError` objects for any validation failures.\n  - `warnings`: A list of `ValidationError` objects for any potential issues that could be warnings.\n\n## Validation Service\n\n### ValidationService Class\n\n```python\nclass ValidationService:\n```\n\n- **Purpose**: Contains static methods for validating different aspects of a component. It centralizes all validation logic, providing a clean interface for validating component configurations.\n\n### Method: validate_provider\n\n```python\n@staticmethod\ndef validate_provider(provider: str) -> Optional[ValidationError]:\n```\n\n- **Purpose**: Validates that a specified provider exists and can be imported.\n- **Logic**: Attempts to modify the provider string for known providers and performs an import. If the class is not a valid component class, it generates a `ValidationError`.\n- **Returns**: A `ValidationError` if validation fails or `None` if successful.\n\n### Method: validate_component_type\n\n```python\n@staticmethod\ndef validate_component_type(component: ComponentModel) -> Optional[ValidationError]:\n```\n\n- **Purpose**: Validates that the `component_type` attribute is present.\n- **Returns**: A `ValidationError` if the type is missing; otherwise, returns `None`.\n\n### Method: validate_config_schema\n\n```python\n@staticmethod\ndef validate_config_schema(component: ComponentModel) -> List[ValidationError]:\n```\n\n- **Purpose**: Validates the component's configuration against its schema.\n- **Logic**: Attempts to load the component class and validates that the configuration adheres to the expected schema, capturing any errors in a list.\n- **Returns**: A list of `ValidationError` objects representing any schema validation issues.\n\n### Method: validate_instantiation\n\n```python\n@staticmethod\ndef validate_instantiation(component: ComponentModel) -> Optional[ValidationError]:\n```\n\n- **Purpose**: Validates whether the component can be instantiated successfully.\n- **Logic**: Attempts to load the component class and call its instantiation method, catching common errors related to version compatibility or import issues.\n- **Returns**: A `ValidationError` upon failure, or `None` if successful.\n\n### Method: validate\n\n```python\n@classmethod\ndef validate(cls, component: ComponentModel) -> ValidationResponse:\n```\n\n- **Purpose**: The entry point for validating a component configuration.\n- **Logic**: It sequentially calls the validation methods for the provider, component type, configuration schema, and instantiation. It collects errors and warnings that occur during the process.\n- **Returns**: A `ValidationResponse` object, indicating whether the component is valid and including any accumulated errors and warnings.\n\n---\n\n## Summary\n\nIn conclusion, the `validation_service.py` file is a robust implementation for validating component configurations. It utilizes structured data models to represent requests, errors, and responses using the Pydantic library, ensuring that the interactions are type-safe and easily manageable. Through various validation methods, the service thoroughly checks for component integrity, compatibility, and compliance with expected schemas, making it essential for maintaining stability and proper functionality in the broader application context.",
    "filename": "python/packages/autogen-studio/autogenstudio/validation/validation_service.py"
  },
  {
    "code": false,
    "content": "# Documentation\n\n## Overview\n\nThis snippet of code is part of a software application identified as \"autogenstudio.\" It is likely part of the configuration or metadata section of a program that defines key attributes about the application. \n\n## Constants Defined\n\n### VERSION\n\n```python\nVERSION = \"0.4.3\"\n```\n\n- **Purpose**: This constant stores the version number of the application.\n- **Role**: The version number (`0.4.3`) indicates a specific release of the software, often used for version tracking and updates.\n\n### __version__\n\n```python\n__version__ = VERSION\n```\n\n- **Purpose**: This is a convention in Python indicating the current version of the module or package.\n- **Role**: By setting `__version__` to the value of `VERSION`, it allows other parts of the application or external modules to access the version information easily.\n\n### APP_NAME\n\n```python\nAPP_NAME = \"autogenstudio\"\n```\n\n- **Purpose**: This constant holds the name of the application.\n- **Role**: The string `\"autogenstudio\"` serves as an identifier for the application, which can be used in logs, user interfaces, or documentation.\n\n## Summary of Functionality\n\nIn summary, this short code segment establishes fundamental metadata for the \"autogenstudio\" application. It defines the current version and the application name, both of which are essential for managing the software and communicating its identity to users and developers.\n\n- The versioning system used here suggests a semantic versioning approach, which implies that the software might follow a development cycle of improvements and updates.\n  \n- Having a clear and accessible version number is crucial for maintaining software, as it indicates the features, bug fixes, or breaking changes included in the release.\n\n- The use of a separate, indicative name (`APP_NAME`) allows for easier renaming without altering multiple sections of the codebase, promoting better maintainability.\n\n## Conclusion\n\nThis snippet provides essential metadata about the \"autogenstudio\" application, making it easier to manage and identify the application version. Such conventions are standard practice in software development, contributing to better organization and clarity in software projects.",
    "filename": "python/packages/autogen-studio/autogenstudio/version.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThe provided source code is a script that performs a specific sequence of operations. It may consist of several functions or procedures aimed at accomplishing a task. This documentation outlines the primary functionalities, the roles of defined components, and the overall flow of the script.\n\n## Script Functionality\n\nThe script generally serves to process data, perform calculations, or interact with APIs, databases, or user inputs. Depending on the context provided by the code, it can be dealing with various tasks such as data analysis, web scraping, or application logic implementation.\n\n### Main Operations\n\n1. **Initialization**: The code likely begins with the initialization of variables or configuration settings. This sets the environment for subsequent operations by defining constants, paths, or initial values.\n\n2. **Data Collection**: If the script interacts with external data sources, it may include functions for fetching or importing data. This can involve reading from files, accessing web APIs, or querying a database.\n\n3. **Data Processing**: Once data is collected, the script likely contains routines to manipulate or analyze this data. This may involve filtering, aggregating, or transforming the data into a desired format for analysis or output.\n\n4. **Computation Logic**: The core part of most scripts consists of functions that implement key algorithms or business logic. This could include statistical computations, data transformations, or any operations that derive insights from the raw input.\n\n5. **Output Generation**: After processing, the script typically generates output. This could be in the form of files, console logs, visualizations, or even updates to a user interface.\n\n6. **Error Handling**: A robust script often includes error handling mechanisms to gracefully manage unexpected conditions. It ensures that the code can continue running or fail without crashing, providing informative debug messages.\n\n## Defined Functions and Classes\n\nThe script may define several functions or classes, each serving a distinct purpose:\n\n### Functions\n\n- **Function Name 1**: This function might handle a specific task, such as data cleaning or transformation. Its role is to take inputs, process them, and return or output results.\n  \n- **Function Name 2**: Another function that could fetch data from a source. This would include necessary API calls or database queries, returning the retrieved data for further use.\n\n### Classes\n\n- **Class Name**: If present, this class might encapsulate related behaviors and properties. A common use case would be representing data entities, holding attributes, and implementing methods for processing or manipulating that data.\n\n## Control Flow\n\nThe script's execution flow is typically linear, though it can include conditional statements and loops based on the complexity:\n\n- **Conditional Logic**: If the script contains `if-else` statements, it uses these to make decisions based on the data or execution state. For example, different processing paths based on the input type.\n\n- **Loop Constructs**: Iterative structures like `for` or `while` loops indicate repeated execution of a block of code, often for processing lists or collections of data.\n\n## Final Steps\n\nCompleting the primary operations, the script likely includes final steps that prepare it for termination. This often encompasses:\n\n- **Resource Cleanup**: Closing files or releasing resources to prevent memory leaks or unresponsive states.\n\n- **Logging**: Recording process completion or errors within a log file or outputting to the console, enabling users to understand the execution flow.\n\n## Summary\n\nIn summary, the provided script is a structured program designed to accomplish specific tasks through a series of operations, likely including data collection, processing, computation, and output generation. Defined functions, conditional logic, and classes work together to follow a coherent flow, ensuring the script effectively fulfills its intended purpose while maintaining robustness and clarity in its design.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/__init__.py"
  },
  {
    "code": false,
    "content": "# FastAPI Application Documentation\n\n## Overview\nThe provided Python script defines a FastAPI application for a service called AutoGen Studio, which functions as a low-code tool for constructing and testing multi-agent workflows. The application includes various configurations for middleware, routers, and hooks for initialization and cleanup of application resources. \n\n## Application Initialization\n### AppInitializer and Lifespan\nThe application initialization is handled by an instance of the `AppInitializer` class, which requires configuration settings and directory paths. The `lifespan` asynchronous context manager is defined to manage the application's lifecycle:\n\n- **Startup Phase**: \n  - Initializes database and connection managers.\n  - Registers authentication dependencies.\n  - Logs a message indicating successful start-up.\n\n- **Shutdown Phase**: \n  - Cleans up application resources and logs the completion.\n\n```python\n@asynccontextmanager\nasync def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:\n    ...\n```\n\n## Middleware Configuration\n### CORS and Authentication Middleware\nThe application employs CORS (Cross-Origin Resource Sharing) middleware to specify allowed origins, allowing secure requests from client applications running on specific domains. \n\nThe `AuthMiddleware` is added to manage authentication using the initialized `auth_manager`.\n\n```python\napp.add_middleware(\n    CORSMiddleware,\n    ...\n)\napp.add_middleware(AuthMiddleware, auth_manager=auth_manager)\n```\n\n## API Configuration\n### FastAPI Router and API Setup\nA new FastAPI instance named `api` is created, which serves as the main router for different endpoints. It includes:\n\n- **Documentation and Title**: The application sets the title, version, and a description of the API with a configurable documentation URL based on settings.\n\n### Including Routers\nVarious routers representing different functionalities of the application are included:\n\n1. **Sessions**\n2. **Runs**\n3. **Teams**\n4. **WebSocket (ws)**\n5. **Validation**\n6. **Settings**\n7. **Gallery**\n8. **Authentication** (auth)\n9. **MCP**\n\nEach router is assigned a prefix and tagged for easy categorization and endpoint management.\n\n```python\napi.include_router(\n    sessions.router,\n    prefix=\"/sessions\",\n    ...\n)\n```\n\n## Health Check and Versioning\n### API Endpoints\nTwo key endpoints are defined for monitoring and retrieving information about the API:\n\n1. **Version Endpoint**: Returns the current version of the API.\n2. **Health Check Endpoint**: Checks and confirms if the service is functioning properly.\n\n```python\n@api.get(\"/version\")\nasync def get_version():\n    ...\n```\n\n```python\n@api.get(\"/health\")\nasync def health_check():\n    ...\n```\n\n## Static Files Serving\n### Mounting Static Directories\nThe application is capable of serving static files from specified directories:\n\n- **API**: Mounted under `/api`.\n- **Files**: Accessible from the `/files` route.\n- **UI**: Served from the root endpoint, which provides the user interface.\n\n```python\napp.mount(\"/api\", api)\napp.mount(\n    \"/files\",\n    StaticFiles(directory=initializer.static_root, html=True),\n    ...\n)\n```\n\n## Error Handling\n### Internal Server Error Handling\nA custom exception handler captures 500 internal server errors and logs detailed error messages. It returns a structured response indicating the error status while also providing a detailed message appropriate for API documentation.\n\n```python\n@app.exception_handler(500)\nasync def internal_error_handler(request, exc):\n    ...\n```\n\n## Application Factory\n### Create App Function\nThe `create_app` function serves as a factory for creating instances of the FastAPI application, facilitating different deployment scenarios and making the application more testable.\n\n```python\ndef create_app() -> FastAPI:\n    ...\n```\n\n## Conclusion\nThis FastAPI application script provides a robust framework for establishing an API for AutoGen Studio. With its lifecycle management, middleware configuration, API routing, static file serving, and error handling, it lays out a comprehensive structure for building and scaling a service that handles multi-agent workflows.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/app.py"
  },
  {
    "code": false,
    "content": "# Module Overview: Authentication System\n\nThis module appears to be part of an authentication subsystem within a larger application, likely built using a web framework such as FastAPI or Flask. It provides essential components for user authentication and authorization, focusing on both RESTful routes and WebSocket connections. Below is a breakdown of the contents of this module.\n\n## Imports\n\nThe module imports several components that suggest a structured approach to authentication:\n\n- **Router**: The `router` from `.authroutes` likely contains HTTP route definitions for user authentication actions such as login, registration, and possibly password management.\n  \n- **Dependencies**: A set of dependency functions from `.dependencies` helps enforce authentication and authorization. These include functions like:\n  - `get_current_user`: Presumably retrieves the current authenticated user from the session or token.\n  - `require_authenticated`: Likely checks if the user is authenticated before allowing access to certain routes.\n  - `require_admin`: Possibly restricts access to routes for users with admin roles.\n  - `require_roles`: This probably verifies if the user has the necessary roles to access specific resources.\n\n- **Exceptions**: The module imports `AuthException` from `.exceptions`, which is presumably a custom exception for handling authentication errors in a meaningful way.\n\n- **Manager**: The `AuthManager` from `.manager` serves as the central component for managing the authentication logic, likely including methods for user registration, login, and role management.\n\n- **Middleware**: The `AuthMiddleware` from `.middleware` can be expected to intercept requests and verify authentication or authorization before reaching routes.\n\n- **Models**: The `AuthConfig` and `User` models from `.models` indicate the presence of configurations for authentication as well as a representation of user data.\n\n- **WebSocket Support**: The `WebSocketAuthHandler` from `.wsauth` suggests that this module includes functionality for authenticating users over WebSocket connections, which is crucial for real-time applications.\n\n## Public API\n\nThe `__all__` attribute defines the public API of this module. This includes:\n\n### Auth Components\n\n- **AuthManager**: Centralizes authentication-related operations. It typically handles functions like user management, including creating users and verifying credentials.\n  \n- **AuthMiddleware**: This component ensures that authentication checks are performed at the level of request handling, automating the validation process before accessing protected resources.\n\n- **AuthConfig**: This is likely a configuration model holding settings relevant to authentication behavior, such as password policies, token expiration times, and other potential configuration options.\n\n- **User**: Represents a user entity in the system and probably contains attributes such as username, password hash, roles, and other relevant identifiable information.\n\n### Exception Handling\n\n- **AuthException**: A custom exception type that facilitates error handling within authentication operations, allowing for a cleaner and more controlled response when authentication-related errors occur.\n\n### Dependency Functions\n\n- **get_current_user**: Fetches data related to the currently authenticated user, which can be utilized across various routes and middleware.\n\n- **require_authenticated**: Protects certain routes by ensuring the user is logged in.\n\n- **require_roles**: Restricts access based on user roles, ensuring that only users with specific roles can access certain aspects of the application.\n\n- **require_admin**: A specific implementation of role checking to confirm if a user has administrative privileges.\n\n### WebSocket Authentication\n\n- **WebSocketAuthHandler**: This handler manages authentication processes for WebSocket connections, which allows the application to provide real-time features securely.\n\n## Conclusion\n\nThis module forms the foundation of an authentication architecture, combining user management, security middleware, and auxiliary functions to ensure robust user authentication and authorization mechanisms. Through its modular design, it effectively supports both standard HTTP interactions and real-time WebSocket communication, covering a wide range of authentication scenarios necessary for modern web applications. Each component is specifically tailored to enhance usability, security, and maintainability within a broader system.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/__init__.py"
  },
  {
    "code": false,
    "content": "# API Documentation for Authentication Endpoints\n\nThis document provides an overview of the authentication-related endpoints defined in the FastAPI application. The endpoints involve OAuth authentication flow, managing user sessions, and returning user details.\n\n## Overview\n\nThe module utilizes FastAPI to create an API router that manages user authentication through OAuth. It incorporates mechanisms for retrieving the login URL from an authentication provider, handling callbacks after authentication, and fetching authenticated user details. The code leverages `loguru` for logging purposes and includes exception handling for better error management.\n\n## AuthManager Dependency\n\n### `get_auth_manager(request: Request) -> AuthManager`\n\nThis function retrieves the `AuthManager` instance from the application's state. If the `AuthManager` has not been initialized, it raises an `HTTPException` with a 500 status code, indicating a server-side error. The `AuthManager` is critical as it handles interactions with authentication providers and manages token creation.\n\n## Current User Retrieval\n\n### `get_current_user(request: Request) -> User`\n\nThis function checks the request state for a currently authenticated user. If a user exists, it returns that user; otherwise, it logs a warning and returns an \"anonymous\" user object. It serves to ensure that user details are readily accessible in the API endpoints, enhancing the overall user experience by providing context for authenticated operations.\n\n## Login URL Endpoint\n\n### `@router.get(\"/login-url\")`\n\nThis asynchronous endpoint provides a URL that the frontend can use to redirect users to the OAuth provider's login page. It depends on the `AuthManager` to obtain the login URL. If an error occurs during this process, it logs the error and raises an `HTTPException` with a 500 status code. This endpoint acts as a crucial entry point for initiating the authentication flow.\n\n## OAuth Callback Handling\n\n### `@router.get(\"/callback\")`\n\nThis endpoint processes the OAuth callback after the user has authenticated with the provider. It checks for potential errors and handles them by returning an HTML page that communicates these issues back to the parent window. If successful, it processes the authentication code to retrieve user information and create a JWT token. Finally, it returns an HTML page that sends this token back to the parent application or stores it in the local storage for further client-side access. Errors in this process are logged, and `HTTPException` is raised for unexpected cases, ensuring robust error handling.\n\n## Callback Handler for Frontend\n\n### `@router.post(\"/callback-handler\")`\n\nThis endpoint provides an alternative mechanism for handling authentication where the frontend application submits the OAuth code directly. It expects a JSON payload containing the authorization code and state. Similar to the previous callback handler, it processes the code, creates a JWT token, and returns user information. Logging and error handling mirror the previous route's structure, providing consistency in how errors are managed.\n\n## User Information Endpoint\n\n### `@router.get(\"/me\")`\n\nThis endpoint returns details about the currently authenticated user using the information provided by the `get_current_user` dependency. It includes the user's ID, name, email, provider, and roles. This endpoint is essential for frontend applications that need to display user-specific data or enforce access controls based on user roles.\n\n## Authentication Type Endpoint\n\n### `@router.get(\"/type\")`\n\nThis endpoint returns the type of authentication configured within the `AuthManager`, including any paths that may be excluded from authentication. This information can be used to inform the frontend application of the authentication method being used and help tailor user experiences accordingly.\n\n## Conclusion\n\nThe components described collectively create a structured and efficient authentication system using OAuth, integrating well with FastAPI practices. The design ensures thorough error handling, user state management, and reusable dependencies, making it easy to maintain and enhance in future iterations. Through clear logging and exception handling mechanisms, the API strives to provide a robust user experience while interacting with authentication workflows.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/authroutes.py"
  },
  {
    "code": false,
    "content": "# Authentication and Authorization Management in FastAPI\n\nThis document outlines the key functionalities regarding user authentication and authorization in a FastAPI application using dependency injections. The provided code defines various dependency functions and utilities that help manage authentication states for HTTP requests and WebSocket connections.\n\n## Overview of Dependency Functions\n\n### `get_auth_manager(request: Request) -> AuthManager`\n\nThis function serves as a dependency provider for accessing the `AuthManager` in a FastAPI application. The function checks if the `auth_manager` is initialized in the application state. If it is not initialized, an HTTP exception is raised with a status code indicating an internal server error. This ensures that any request needing authentication management has access to the required resources, failing gracefully if they are not set up.\n\n### `get_ws_auth_manager(websocket: WebSocket) -> AuthManager`\n\nSimilar to `get_auth_manager`, this function is designed for WebSocket connections. It retrieves the `AuthManager` from the state of the websocket's application. Again, if the `auth_manager` is not found, an HTTP exception is raised to indicate that the authentication system is not initialized. This function ensures that WebSocket connections can also utilize the same authentication logic as regular HTTP requests.\n\n## User Retrieval and Authentication Requirements\n\n### `get_current_user(request: Request) -> User`\n\nThis function aims to retrieve the current authenticated user based on the incoming request. It checks if the `user` attribute is set in the request's state. If not, it defaults to a generic anonymous user with a predefined ID and name. This fallback mechanism ensures that the application handles requests even when user data is missing, although issues in the middleware processing may require further attention.\n\n### `require_authenticated(user: User = Depends(get_current_user)) -> User`\n\nThis function enforces user authentication by depending on the result from `get_current_user`. It checks if the retrieved user is not the anonymous user (i.e., user ID is not \"anonymous\"). If the user is found to be anonymous, an HTTP exception is raised, signaling a 401 Unauthorized status. This function acts as a guard to ensure that only authenticated users can access certain endpoints.\n\n## Role-Based Access Control\n\n### `require_roles(required_roles: List[str])`\n\nThis is a higher-order function that generates a dependency function requiring specific roles for the user. It accepts a list of roles as an argument and creates a nested function, `_require_roles`, which will check if the authenticated user possesses at least one of the specified roles. If the user does not have the necessary roles, a `ForbiddenException` is raised, providing informative feedback regarding authorization requirements.\n\n#### Example Usage:\n```python\n@router.get(\"/admin-only\")\nasync def admin_endpoint(user: User = Depends(require_roles([\"admin\"]))):\n    # Only users with admin role will reach this point\n    return {\"message\": \"Welcome, admin!\"}\n```\nThis example demonstrates how to use the `require_roles` function to restrict access to a specific endpoint requiring an admin role.\n\n### `require_admin(user: User = Depends(require_roles([\"admin\"]))) -> User`\n\nFor convenience, this function simplifies role-checking logic by directly requiring that the user have the \"admin\" role. It serves the same purpose as `require_roles`, but is tailored for cases where the admin role is specifically needed for access. Using this function allows code to be kept concise and easier to read when enforcing admin-level permissions.\n\n## Summary\n\nThe provided code implements a structured approach to manage authentication and authorization in a FastAPI application. Key components include retrieving an authentication manager, getting the current user, enforcing user authentication, and checking for specific user roles. These dependencies can be used flexibly throughout various endpoints or WebSocket connections, enabling a robust and secure application foundation for user management. Overall, this code shows how FastAPI's dependency injection features can contribute to building organized and maintainable authentication and authorization logic within an application.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/dependencies.py"
  },
  {
    "code": false,
    "content": "# Authentication Exceptions Module Documentation\n\nThis module defines several exception classes that are used in the context of authentication and authorization in a FastAPI application. These exceptions help in managing errors related to user authentication, token handling, and access permissions. Below is a detailed description of each component.\n\n## Overview\n\nThe module utilizes the `HTTPException` class from FastAPI to create custom exceptions. This allows the application to respond with appropriate HTTP status codes and details when authentication-related errors occur. The exceptions are designed to be clear and specific, providing useful feedback to both developers and users.\n\n## AuthException Class\n\n```python\nclass AuthException(HTTPException):\n    \"\"\"Base class for authentication exceptions.\"\"\"\n\n    def __init__(self, detail: str, headers: dict | None = None):\n        super().__init__(status_code=401, detail=detail, headers=headers)\n```\n\nThe `AuthException` class serves as a base class for all authentication-related exceptions. It inherits from `HTTPException` and sets a default HTTP status code of `401 Unauthorized`. \n\n### Attributes and Methods:\n- **detail**: This attribute provides a message detailing the specific authentication issue.\n- **headers**: Accepts optional headers that can be included in the response.\n\nThis class allows for a consistent handling of authentication errors across derived exception classes.\n\n## InvalidTokenException Class\n\n```python\nclass InvalidTokenException(AuthException):\n    \"\"\"Exception raised when token is invalid.\"\"\"\n\n    def __init__(self):\n        super().__init__(detail=\"Invalid or expired token\")\n```\n\nThe `InvalidTokenException` is a specific exception that is raised when an authentication token is determined to be invalid or expired. By extending `AuthException`, it benefits from the common handling of authentication-related issues while providing a specific message.\n\n### Purpose:\nThis exception signals to the user or client that they need to present a valid token for authorization.\n\n## MissingTokenException Class\n\n```python\nclass MissingTokenException(AuthException):\n    \"\"\"Exception raised when token is missing.\"\"\"\n\n    def __init__(self):\n        super().__init__(detail=\"Authentication token is missing\")\n```\n\nSimilar to the `InvalidTokenException`, the `MissingTokenException` is raised when no authentication token is provided in the request. This is crucial for cases where authorization is mandatory.\n\n### Purpose:\nThe purpose of this exception is to inform the user that they must include a token to authenticate their request.\n\n## ProviderAuthException Class\n\n```python\nclass ProviderAuthException(AuthException):\n    \"\"\"Exception raised when authentication with provider fails.\"\"\"\n\n    def __init__(self, provider: str, detail: str):\n        super().__init__(detail=f\"Authentication failed with {provider}: {detail}\")\n```\n\nThe `ProviderAuthException` is used for cases where authentication fails with a third-party provider. A provider is an external service that handles authentication (e.g., OAuth providers).\n\n### Attributes:\n- **provider**: The name of the external service.\n- **detail**: Additional details regarding the failure.\n\n### Purpose:\nThis exception provides specific context about the failure, which can be useful for debugging and informing users.\n\n## ConfigurationException Class\n\n```python\nclass ConfigurationException(Exception):\n    \"\"\"Exception raised when there's an issue with auth configuration.\"\"\"\n\n    pass\n```\n\nThe `ConfigurationException` is a standalone exception that does not inherit from `HTTPException`. It is intended for use when there are configuration-related issues in the authentication setup.\n\n### Purpose:\nThis exception highlights issues with the authentication system's configuration, which may need to be addressed by developers or system administrators.\n\n## ForbiddenException Class\n\n```python\nclass ForbiddenException(HTTPException):\n    \"\"\"Exception raised when user doesn't have permission.\"\"\"\n\n    def __init__(self, detail: str = \"You don't have permission to access this resource\"):\n        super().__init__(status_code=403, detail=detail)\n```\n\nThe `ForbiddenException` is raised when a user does not have sufficient permissions to access a particular resource. It extends `HTTPException` with a default status code of `403 Forbidden`.\n\n### Purpose:\nThis exception serves as a way to enforce access control, signaling to users that their request is not allowed due to insufficient permissions.\n\n## Conclusion\n\nThis module effectively organizes authentication-related exceptions into clearly defined classes, allowing for granular error handling and improved user feedback in a FastAPI application. The structured approach to exceptions enhances maintainability and clarity, enabling developers to quickly identify and manage errors associated with authentication and authorization.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/exceptions.py"
  },
  {
    "code": false,
    "content": "# Authentication Manager Documentation\n\n## Overview\nThe `AuthManager` class is responsible for handling authentication processes in an application. It manages token creation, validation, and the selection of various authentication providers based on configuration. It is designed to work with different authentication methods, enabling flexibility in integrating services like GitHub, Firebase, and Microsoft Authentication.\n\n## Initialization\nThe `AuthManager` is initialized with an `AuthConfig` object that contains the necessary configuration settings. When an instance is created, it also initializes the appropriate authentication provider based on the specified type in the configuration. This is reflected in the constructor (`__init__` method):\n\n```python\ndef __init__(self, config: AuthConfig):\n    \"\"\"Initialize the auth manager with configuration.\"\"\"\n    self.config = config\n    self.provider = self._create_provider()\n    logger.info(f\"Initialized auth manager with provider: {config.type}\")\n```\n\n## Provider Creation\nThe `_create_provider` method handles the instantiation of the specific authentication provider based on the type defined in the configuration. It supports multiple authentication methods and defaults to a `NoAuthProvider` if the specified type is invalid or if creating the provider fails.\n\n```python\ndef _create_provider(self) -> AuthProvider:\n    \"\"\"Create the appropriate auth provider based on config.\"\"\"\n    ...\n```\n\n### Supported Providers\nThe current implementation supports the following authentication providers:\n- GitHub\n- Microsoft Authentication (MSAL)\n- Firebase\n- A default configuration that does not require authentication (NoAuthProvider)\n\n## Token Creation\nThe `create_token` method generates a JSON Web Token (JWT) for authenticated users. It checks the JWT secret from the configuration and constructs a payload that includes the user\u2019s ID, name, email, provider, roles, and expiration time.\n\n```python\ndef create_token(self, user: User) -> str:\n    \"\"\"Create a JWT token for authenticated user.\"\"\"\n    ...\n```\n\nIf the `jwt_secret` is not available, a warning is logged, and a dummy token is returned for development purposes.\n\n## Request Authentication\nThe `authenticate_request` method is responsible for authenticating incoming requests. It first checks if the request path should be excluded from authentication. If not, it verifies the presence of an Authorization header with a Bearer token.\n\n```python\nasync def authenticate_request(self, request: Request) -> User:\n    \"\"\"Authenticate a request and return user information.\"\"\"\n    ...\n```\n\nIf the token is valid, it decodes the token to create a `User` object containing the user information. If the token is missing or invalid, appropriate exceptions are raised.\n\n## Token Validation\nThe `is_valid_token` method allows checking the validity of a given JWT. It attempts to decode the token using the `jwt_secret` from the configuration. If successful, it returns `True`; otherwise, it logs a warning and returns `False` if the token has expired.\n\n```python\ndef is_valid_token(self, token: str) -> bool:\n    \"\"\"Check if a JWT token is valid.\"\"\"\n    ...\n```\n\n## YAML Configuration\nThe `from_yaml` class method instantiates the `AuthManager` using a YAML configuration file. It reads the configuration data, creates an `AuthConfig` object, and initializes the `AuthManager`. If an error occurs, it logs this and raises a `ConfigurationException`.\n\n```python\n@classmethod\ndef from_yaml(cls, yaml_path: str) -> Self:\n    \"\"\"Create AuthManager from YAML config file.\"\"\"\n    ...\n```\n\n## Environment Configuration\nThe `from_env` class method creates an `AuthManager` instance from environment variables. It gathers necessary configuration details, like auth type and JWT secret, and constructs an `AuthConfig`. This method can be useful for deploying applications with environment-specific settings.\n\n```python\n@classmethod\ndef from_env(cls) -> Self:\n    \"\"\"Create AuthManager from environment variables.\"\"\"\n    ...\n```\n\n### Summary of Logic\n1. **Initialization**: Create an instance of `AuthManager` with a given configuration.\n2. **Provider Creation**: Instantiate the appropriate authentication provider based on the configuration type.\n3. **Token Handling**: Ability to create JWTs for user authentication and validate existing tokens.\n4. **Request Authentication**: Handle user authentication for incoming requests, allowing specific paths to be excluded from authentication.\n5. **Configuration Methods**: Allow loading configurations either from a YAML file or environment variables for flexibility.\n\nOverall, the `AuthManager` class is a robust framework designed to simplify the handling of various authentication methods efficiently and securely.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/manager.py"
  },
  {
    "code": false,
    "content": "# Authentication Middleware Documentation\n\n## Overview\n\nThis code snippet defines middleware for handling user authentication in a FastAPI application. It primarily consists of two classes: `AuthMiddleware`, which manages authentication for HTTP requests, and `WebSocketAuthMiddleware`, which handles authentication specifically for WebSocket connections. Both classes utilize an `AuthManager`, which likely encapsulates the logic for token validation and user authentication.\n\n## AuthMiddleware Class\n\n### Purpose\n\nThe `AuthMiddleware` class is designed to be included in the middleware stack of a FastAPI application. It intercepts incoming HTTP requests, allowing for authentication checks and handling of unauthorized access attempts.\n\n### Initialization\n\nThe constructor of the class receives two parameters: \n\n- `app`: The ASGI application \n- `auth_manager`: An instance of `AuthManager` responsible for managing authentication logic\n\n```python\ndef __init__(self, app: ASGIApp, auth_manager: AuthManager) -> None:\n    super().__init__(app)\n    self.auth_manager = auth_manager\n```\n\n### Request Processing\n\nThe `dispatch` method processes incoming requests and determines if the user should be authenticated:\n\n- **CORS Preflight:** The middleware skips authentication checks for HTTP `OPTIONS` requests, which are typically used for Cross-Origin Resource Sharing (CORS) preflight requests.\n  \n- **Public Paths:** Several routes are defined to bypass authentication checks, including home, login, callback paths, and paths for static files (like images, CSS, and JS).\n  \n- **Authentication Disabled:** If the authentication type is set to \"none,\" it allows the request to proceed without authentication, updating the request's state with the user information.\n\n### WebSocket Handling\n\nFor WebSocket connections, the middleware allows passing the request through without authentication checks at this stage. Authentication is done in a dedicated WebSocket handler.\n\n### Error Handling\n\nIf authentication fails (throws an `AuthException`), it responds with a 401 Unauthorized status along with a JSON error message. It also logs unexpected errors that may arise during the authentication process.\n\n```python\nexcept AuthException as e:\n    return Response(\n        status_code=HTTP_401_UNAUTHORIZED,\n        content=json.dumps({\"status\": False, \"detail\": e.detail}),\n        media_type=\"application/json\",\n        headers=e.headers or {},\n    )\n```\n\n## WebSocketAuthMiddleware Class\n\n### Purpose\n\nThe `WebSocketAuthMiddleware` class focuses on authenticating WebSocket connections. It is not a traditional middleware but a helper specifically for verifying users before establishing a WebSocket connection.\n\n### Initialization\n\nSimilar to `AuthMiddleware`, this class is initialized with an `AuthManager` instance:\n\n```python\ndef __init__(self, auth_manager: AuthManager) -> None:\n    self.auth_manager = auth_manager\n```\n\n### Authentication Method\n\nThe `authenticate` method carries out the WebSocket authentication:\n\n- **Token Extraction:** It extracts the authentication token from either query parameters or headers. The token can be indicated through a parameter named `token` or a Bearer token in the `authorization` header.\n\n- **Token Validation:** Once extracted, it checks if the token is valid using `self.auth_manager.is_valid_token(token)`. The method returns `True` if authenticated successfully, or `False` otherwise. Warnings are logged for missing or invalid tokens.\n\n```python\nasync def authenticate(self, websocket: WebSocket) -> bool:\n    ...\n    if not token:\n        logger.warning(\"No token found for WebSocket connection\")\n        return False\n    ...\n```\n\n### Error Handling\n\nThe method also catches unexpected exceptions during the authentication process and logs relevant error messages, returning `False` to indicate authentication failure.\n\n## Conclusion\n\nThis set of middleware classes effectively abstracts the complexities of user authentication in a FastAPI application, handling both standard HTTP requests and WebSocket connections. By modularizing the authentication logic within `AuthMiddleware` and `WebSocketAuthMiddleware`, the code promotes cleaner architecture and maintainability for handling user sessions and access control.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/middleware.py"
  },
  {
    "code": false,
    "content": "# Authentication Configuration and User Models Documentation\n\nThis document provides a high-level overview of the authentication configuration defined in the provided Python code, focusing on the classes that manage and validate authentication settings.\n\n## Overview\n\nThe code defines several data models using Pydantic, a data validation library in Python. The main focus is on different authentication configurations for a web application, encompassing multiple authentication providers such as GitHub, Microsoft Azure (MSAL), and Firebase. It also validates the configurations based on the specified authentication type. Additionally, a `User` model is defined for representing authenticated users within the application.\n\n## Authentication Configuration Models\n\n### GithubAuthConfig\n\n```python\nclass GithubAuthConfig(BaseModel):\n    client_id: str\n    client_secret: str\n    callback_url: str\n    scopes: List[str] = [\"user:email\"]\n```\n\n- **Purpose**: This class represents the configuration data required for GitHub authentication.\n- **Attributes**:\n  - `client_id`: The unique identifier for the client application on GitHub.\n  - `client_secret`: The secret key used to authenticate the client application.\n  - `callback_url`: The URL to which GitHub redirects after authentication.\n  - `scopes`: A list specifying the access permissions requested, defaulting to `[\"user:email\"]`.\n\n### MSALAuthConfig\n\n```python\nclass MSALAuthConfig(BaseModel):\n    tenant_id: str\n    client_id: str\n    client_secret: str\n    callback_url: str\n    scopes: List[str] = [\"User.Read\"]\n```\n\n- **Purpose**: This class encapsulates the necessary configuration for Microsoft Azure authentication using the Microsoft Authentication Library (MSAL).\n- **Attributes**:\n  - `tenant_id`: The identifier for the Azure AD tenant.\n  - `client_id`: The identifier for the client application registered with Azure.\n  - `client_secret`: The secret key used by the application to authenticate with Azure.\n  - `callback_url`: The URL for redirection after successful authentication.\n  - `scopes`: Defines the permissions for accessing user data, defaulting to `[\"User.Read\"]`.\n\n### FirebaseAuthConfig\n\n```python\nclass FirebaseAuthConfig(BaseModel):\n    api_key: str\n    auth_domain: str\n    project_id: str\n```\n\n- **Purpose**: This class holds the configuration settings needed to enable Firebase authentication.\n- **Attributes**:\n  - `api_key`: The API key for accessing Firebase services.\n  - `auth_domain`: The authentication domain specific to the Firebase project.\n  - `project_id`: The unique identifier for the Firebase project.\n\n## Main AuthConfig Class\n\n### AuthConfig\n\n```python\nclass AuthConfig(BaseModel):\n    ...\n```\n\n- **Purpose**: This class serves as a comprehensive model for configuring authentication in the application.\n- **Attributes**:\n  - `type`: A literal type indicating the authentication method, which can be \"none\", \"github\", \"msal\", or \"firebase\", defaulting to \"none\".\n  - `github`: An optional instance of `GithubAuthConfig` for GitHub authentication.\n  - `msal`: An optional instance of `MSALAuthConfig` for Microsoft authentication.\n  - `firebase`: An optional instance of `FirebaseAuthConfig` for Firebase authentication.\n  - `jwt_secret`: An optional secret key for signing JWTs (JSON Web Tokens).\n  - `token_expiry_minutes`: Specifies how long the token will be valid, defaulting to 60 minutes.\n  - `exclude_paths`: A list of paths that do not require authentication.\n\n### Validators\n\nThe `AuthConfig` class includes several validators that ensure the integrity of the authentication settings:\n\n1. **validate_github_config**: Ensures that the GitHub configuration is provided when the `type` is set to \"github\".\n   \n2. **validate_msal_config**: Checks that the MSAL configuration is present if `type` is \"msal\".\n\n3. **validate_firebase_config**: Validates the presence of Firebase configuration when the authentication type is set to \"firebase\".\n\n4. **validate_jwt_secret**: Verifies that a JWT secret is provided if the authentication type is not \"none\".\n\nThese validators raise `ValueError` for any missing required configurations based on the selected authentication type.\n\n## User Model\n\n### User\n\n```python\nclass User(BaseModel):\n    ...\n```\n\n- **Purpose**: This class models an authenticated user in the application.\n- **Attributes**:\n  - `id`: A unique identifier for the user.\n  - `name`: The user's name.\n  - `email`: An optional field that stores the user's email address.\n  - `avatar_url`: An optional URL linking to the user's avatar image.\n  - `provider`: An optional field indicating the authentication provider used (e.g., GitHub, Azure).\n  - `roles`: A list of roles assigned to the user, defaulting to `[\"user\"]`.\n  - `metadata`: An optional dictionary to hold additional information about the user.\n\nThis `User` class can be used across the application to manage user session data and permissions.\n\n## Summary\n\nOverall, the provided code defines a comprehensive structure for handling authentication configurations in a Python application. It integrates various authentication methods, validates configurations based on the selected provider, and provides a user model to represent authenticated users effectively. This setup helps streamline the authentication process while maintaining flexibility for different backends.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/models.py"
  },
  {
    "code": false,
    "content": "# Authentication Providers Documentation\n\nThis documentation describes the authentication provider classes in the Python code, which are designed to handle different authentication mechanisms, specifically OAuth 2.0 for GitHub and placeholders for MSAL and Firebase providers. The code provides a base framework for integrating various authentication services with a focus on extensibility and proper handling of user data.\n\n## Overview of Key Classes\n\n### AuthProvider (ABC)\n\nThe `AuthProvider` class serves as an abstract base class defining the interface for authentication providers. It includes the following methods:\n\n- **get_login_url(self) -> str:** This asynchronous method is expected to return a URL that initiates the login process.\n- **process_callback(self, code: str, state: str | None = None) -> User:** This method is designed to handle the OAuth callback, where it processes the received `code` and `state`, returning user data.\n- **validate_token(self, token: str) -> bool:** This method validates a token and returns a boolean indicating its validity.\n\n### NoAuthProvider\n\n`NoAuthProvider` is a concrete implementation of `AuthProvider`, meant for development scenarios where no real authentication is required. The key features include:\n\n- **Default User:** It utilizes a default user object representing a guest user.\n- **Login URL:** This method returns a static URL for automatic login.\n- **Callback Processing:** It immediately returns the default user on callback.\n- **Token Validation:** Always returns `True`, indicating that any provided token is considered valid.\n\n### GithubAuthProvider\n\nThe `GithubAuthProvider` class implements the OAuth authentication flow for GitHub. Its features are as follows:\n\n- **Configuration Validation:** Ensures that the necessary GitHub configuration is provided during instantiation.\n- **Login URL Generation:** Generates a secure URL for initiating an OAuth login by creating a randomized state and constructing the GitHub authorization URL.\n- **Processing Callbacks:** Manages the exchange of the provided authorization code for an access token and subsequent retrieval of user information. It handles errors during token exchanges and user info fetching, logging errors appropriately.\n- **Email Retrieval:** If `user:email` scope is included, it attempts to gather user emails and returns the primary email address.\n- **Token Validation:** This method checks the validity of an access token by querying the GitHub API.\n\n### MSALAuthProvider\n\n`MSALAuthProvider` is intended for Microsoft Authentication Library (MSAL) integration but serves as a placeholder. Its implementation includes:\n\n- **Configuration Requirement:** It verifies the presence of the MSAL configuration.\n- **Login URL Placeholder:** A na\u00efve implementation returns a static URL for MSAL.\n- **Callback Handling Placeholder:** It has a placeholder method for processing MSAL callback data.\n- **Token Validation Placeholder:** Another placeholder method, currently returning `False`.\n\n### FirebaseAuthProvider\n\nThe `FirebaseAuthProvider` class is designed for integrating Firebase authentication and also serves as a placeholder. Key aspects include:\n\n- **Configuration Check:** Checks for the necessary Firebase configuration at initialization.\n- **Login URL Generation:** Returns a JSON representation of the Firebase configuration settings, reflecting how Firebase handles authentication, which is usually more client-side focused.\n- **Callback Processing Placeholder:** Contains a placeholder that would process Firebase ID tokens in a full implementation.\n- **Token Validation Placeholder:** Currently, it provides a stub returning `False` for token validation.\n\n## Error Handling and Logging\n\nThe code employs error handling utilizing exceptions such as `ConfigurationException` and `ProviderAuthException`, ensuring that issues related to missing configuration or failed authentication processes are duly raised. Additionally, it uses the `loguru` library for logging errors related to API failures and missing data, facilitating better debugging and tracking of problems that may arise during user authentication processes.\n\n## User Model\n\nAn implicit requirement across the authentication providers is a `User` model, which likely contains attributes such as `id`, `name`, `email`, and additional metadata. This model allows various authentication providers to return user information uniformly, empowering the application to handle user data irrespective of the authentication source.\n\n## Conclusion\n\nThis authentication framework facilitates the integration of multiple authentication providers into an application in a modular way. While concrete implementations are provided for GitHub, development-focused defaults exist, and placeholders are set for MSAL and Firebase providers. Custom exceptions and logging mechanisms enhance error handling and operational transparency, making this codebase robust for future development and maintenance in a real-world application.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/providers.py"
  },
  {
    "code": false,
    "content": "# WebSocket Authentication Handler Documentation\n\n## Overview\n\nThe `WebSocketAuthHandler` class is designed to manage the authentication of WebSockets in a FastAPI application. It facilitates the secure establishment of a connection by validating JSON Web Tokens (JWTs) passed either as query parameters or HTTP headers. This ensures that only authenticated users can engage in WebSocket interactions. If authentication is not necessary, it provides a default user access.\n\n## Dependencies\n\nThe module imports several libraries and classes necessary for its functionality:\n\n- **`jwt`**: Used for decoding and validating JSON Web Tokens.\n- **`WebSocket`, `WebSocketDisconnect`, `status` from FastAPI**: These provide WebSocket functionalities including connection handling and error statuses.\n- **`loguru.logger`**: Utilized for logging various events and errors during the authentication process.\n- **`AuthManager`**: A custom authentication manager, presumably handling configuration and settings related to authentication.\n- **`User`**: A model that represents a user, likely containing attributes such as `id`, `name`, `email`, `provider`, and `roles`.\n\n## Class Description: WebSocketAuthHandler\n\n### Constructor: `__init__`\n\nThe `__init__` method initializes an instance of `WebSocketAuthHandler`. It accepts an `AuthManager` object that manages the auth-related configurations, such as JWT secret and type of authentication. This instance variable (`self.auth_manager`) is crucial for authenticating WebSocket connections.\n\n### Method: `authenticate`\n\nThe `authenticate` method is the core functionality of the class, performing the following tasks:\n\n1. **Authentication Type Check**: \n   - If the configuration type is \"none\", it allows connections without authentication, returning a default user object.\n\n2. **Token Extraction**:\n   - Tries to extract the JWT from the WebSocket's query parameters or its HTTP headers.\n\n3. **Token Validation**:\n   - First checks if the `jwt_secret` exists. If not, and in a development mode scenario, it grants guest access.\n   - It decodes the JWT using the secret and checks for expiration or validity, logging any issues encountered (e.g., expired or invalid tokens).\n\n4. **User Creation**:\n   - If the token is valid, it extracts user information from its payload, creating a `User` object with relevant fields.\n\n5. **Return Value**: \n   - Returns a tuple consisting of a boolean indicating success and the `User` object (or `None` if authentication fails).\n\n### Method: `on_connect`\n\nThe `on_connect` method handles WebSocket connection requests with an authentication check:\n\n1. **Invoke Authentication**: \n   - Calls the `authenticate` method to validate the WebSocket requester.\n\n2. **Connection Handling**:\n   - If authentication fails (`success` is `False`), it closes the WebSocket connection with a policy violation code and raises a `WebSocketDisconnect` exception.\n   - If successful, it returns the authenticated `User` object.\n\n## Error Handling and Logging\n\nThe class employs comprehensive error handling through `try-except` blocks, ensuring that authentication issues are logged via the `logger` from Loguru. This makes debugging and monitoring easier, as it captures different error scenarios (like token extraction failures, invalid tokens, etc.) with informative messages.\n\n## Key Functionalities\n\n- **Authentication Handling**: Adapts to different use cases (auth required vs. not required) based on the `AuthManager` configuration.\n- **Token Validation**: Ensures that tokens are both present and valid according to the defined authentication schema.\n- **User Model Extraction**: Captures important user information from validated JWTs to facilitate personalized WebSocket communications.\n\n## Use Cases\n\nThe `WebSocketAuthHandler` would be especially useful in applications that require real-time communication, such as:\n\n- Chat applications where user identities must be verified.\n- Online gaming platforms where users need authenticated sessions.\n- Collaborative tools that allow multiple users to interact dynamically while ensuring secure access.\n\nWith its robust architecture, this handler serves as an essential component in reinforcing the security of WebSocket connections in a FastAPI environment.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/auth/wsauth.py"
  },
  {
    "code": false,
    "content": "# Settings Configuration Documentation\n\nThis document provides a detailed overview of the `api/config.py` module, which is primarily focused on defining application settings using Pydantic, a data validation and settings management library in Python.\n\n## Overview\n\nThe `config.py` script focuses on setting up configurations needed for the application. It utilizes the `BaseSettings` class from the `pydantic_settings` module, which simplifies loading and validating environment variables. Settings are defined as class attributes, and Pydantic automatically handles type coercion and validation.\n\n## Class Definition: `Settings`\n\n### Purpose\n\nThe `Settings` class is a subclass of `BaseSettings` and is used to manage application settings. Each attribute corresponds to a specific configuration option that can be defined as an environment variable or set to a default value.\n\n### Properties\n\n1. **DATABASE_URI**:\n   - **Type**: `str`\n   - **Default**: `\"sqlite:///./autogen04203.db\"`\n   - **Description**: The URI for the database connection. By default, it is set to connect to a SQLite database file named `autogen04203.db` located in the current directory.\n\n2. **API_DOCS**:\n   - **Type**: `bool`\n   - **Default**: `False`\n   - **Description**: A flag indicating whether to enable API documentation. Setting this to `True` would activate the automatic generation of API documentation.\n\n3. **CLEANUP_INTERVAL**:\n   - **Type**: `int`\n   - **Default**: `300`\n   - **Description**: Defines the interval for cleanup tasks in seconds. The default value is 5 minutes (300 seconds).\n\n4. **SESSION_TIMEOUT**:\n   - **Type**: `int`\n   - **Default**: `3600`\n   - **Description**: Specifies the timeout duration for user sessions in seconds. The default is set to 1 hour (3600 seconds).\n\n5. **CONFIG_DIR**:\n   - **Type**: `str`\n   - **Default**: `\"configs\"`\n   - **Description**: Sets the default directory path for configuration files relative to the application root.\n\n6. **DEFAULT_USER_ID**:\n   - **Type**: `str`\n   - **Default**: `\"guestuser@gmail.com\"`\n   - **Description**: Represents the default user ID, set to a placeholder email address for a guest user.\n\n7. **UPGRADE_DATABASE**:\n   - **Type**: `bool`\n   - **Default**: `False`\n   - **Description**: A flag that indicates whether the database should be automatically upgraded.\n\n8. **LITE_MODE**:\n   - **Type**: `bool`\n   - **Default**: `False`\n   - **Description**: A flag for enabling a \"lite mode\" for the application, potentially to reduce resource usage.\n\n9. **LITE_TEAM_FILE**:\n   - **Type**: `str`\n   - **Default**: `\"\"` (empty string)\n   - **Description**: Holds the file path for a team configuration in lite mode. This is currently uninitialized.\n\n10. **LITE_SESSION_NAME**:\n    - **Type**: `str`\n    - **Default**: `\"\"` (empty string)\n    - **Description**: Holds the session name for lite mode, also currently uninitialized.\n\n### Configuration Management\n\nThe `model_config` dictionary within the class is specifically designed to customize the behavior of environment variable loading. The key `env_prefix` is set to `\"AUTOGENSTUDIO_\"`, which means that environment variables will be prefixed with this string to map to the settings. For example, to set the `DATABASE_URI`, one would use the environment variable `AUTOGENSTUDIO_DATABASE_URI`.\n\n## Instantiation of Settings\n\n### Global Settings Instance\n\nAt the end of the script, an instance of the `Settings` class is created and assigned to the variable `settings`. This instance can be accessed throughout the application to fetch configuration values. The settings are loaded at the time of instantiation, allowing centralized management of application configurations.\n\n### Summary\n\nThe `api/config.py` module provides a structured approach to managing application settings using Pydantic. It defines several key parameters, allows for environment variable configuration, and enables easy access to these settings through a global instance. This setup is valuable for ensuring consistent configurations across different environments (development, testing, production) and simplifies the management of application behavior based on defined preferences.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/config.py"
  },
  {
    "code": false,
    "content": "# Documentation for `api/deps.py`\n\nThis module contains various dependency management and initialization functions for a FastAPI application, primarily focusing on managing database connections, WebSocket connections, team management, and authentication. It sets up the necessary infrastructure to ensure smooth operation and error handling in an API context.\n\n## Imports and Environment Setup\n\nThe code begins with importing necessary modules, which include standard libraries such as `json`, `logging`, and `os`, along with FastAPI's tools for dependency injection, exception handling, and HTTP status codes. Additionally, it imports several managers and configurations needed for handling database operations, team management, and authentication processes.\n\n### Environment Detection\n\n```python\ndef is_lite_mode() -> bool:\n    \"\"\"Check if lite mode is enabled via environment variable\"\"\"\n    return os.getenv(\"AUTOGENSTUDIO_LITE_MODE\", \"\").lower() in [\"true\", \"1\"]\n```\n\nThe `is_lite_mode` function checks the environment variable `AUTOGENSTUDIO_LITE_MODE` to determine if the application should run in a lightweight mode, altering its behavior accordingly for testing or minimal setups.\n\n## Global Manager Instances\n\nThe module initializes four global variables that represent different manager instances:\n\n- `_db_manager`: Responsible for database operations.\n- `_websocket_manager`: Handles WebSocket connections.\n- `_team_manager`: Manages team-related functionalities.\n- `_auth_manager`: Manages authentication processes.\n\nThese instances are essential for various dependency providers throughout the application.\n\n## Dependency Providers\n\nDependency injection is achieved through several async functions used as providers, enabling modularization within routes.\n\n### Database Management\n\n```python\nasync def get_db() -> DatabaseManager:\n    \"\"\"Dependency provider for database manager\"\"\"\n    ...\n```\n\nThe `get_db` function ensures that a database manager is initialized before proceeding. If it isn't initialized, an HTTP exception is raised to handle this gracefully.\n\n### WebSocket and Team Manager Providers\n\nSimilar to `get_db`, functions `get_websocket_manager` and `get_team_manager` serve the same purpose of ensuring their respective managers are available. If they are not initialized, relevant exceptions are raised.\n\n### Current User Identification\n\n```python\nasync def get_current_user(request: Request) -> str:\n    \"\"\"Get the current authenticated user.\"\"\"\n    ...\n```\n\nThe `get_current_user` function extracts the authenticated user's ID from the request state. If no user is found and the authentication configuration is set to \"none,\" it defaults to a predefined user ID from settings.\n\n## Authentication Manager Initialization\n\n```python\ndef init_auth_manager(config_dir: Path) -> AuthManager:\n    \"\"\"Initialize authentication manager\"\"\"\n    ...\n```\n\nThe `init_auth_manager` function configures the authentication manager based on environment variables and configurations. It implements fallback logic to handle erroneous configurations or absence of authentication setups, logging necessary information for transparency.\n\n## Manager Initialization and Cleanup\n\n### Initializing All Managers\n\n```python\nasync def init_managers(database_uri: str, config_dir: str | Path, app_root: str | Path) -> None:\n    \"\"\"Initialize all manager instances\"\"\"\n    ...\n```\n\nThe `init_managers` function initializes all the global manager instances. Careful attention is paid to whether the application is running in lite mode, with specific initializations performed accordingly (e.g., skipping team imports). Any errors during initialization will trigger a cleanup of partially initialized managers.\n\n### Cleanup Process\n\n```python\nasync def cleanup_managers() -> None:\n    \"\"\"Cleanup and shutdown all manager instances\"\"\"\n    ...\n```\n\nThe `cleanup_managers` function handles the closing and cleanup of all manager instances. It ensures that the WebSocket connections are closed first, followed by team, authentication, and database managers, logging any errors encountered during the process.\n\n## Utility Functions and Manager Status\n\n### Manager Status Check\n\n```python\ndef get_manager_status() -> dict:\n    \"\"\"Get the initialization status of all managers\"\"\"\n    ...\n```\n\nThe `get_manager_status` function provides a straightforward way to check which manager instances are initialized, returning this information as a dictionary.\n\n### Combined Dependencies\n\n```python\nasync def get_managers():\n    \"\"\"Get all managers in one dependency\"\"\"\n    ...\n```\n\nThis function aggregates the calls to retrieve the database, WebSocket, and team managers in one go for more convenient dependency management in route handlers.\n\n## Error Handling and Dependency Decorators\n\n### Custom Exception Class\n\n```python\nclass ManagerOperationError(Exception):\n    \"\"\"Custom exception for manager operation errors\"\"\"\n    ...\n```\n\nThe `ManagerOperationError` class is a custom exception designed to encapsulate errors that occur during manager operations, providing context about which manager failed and the specific operation.\n\n### Requiring Specific Managers\n\n```python\ndef require_managers(*manager_names: str):\n    \"\"\"Decorator to require specific managers for a route\"\"\"\n    ...\n```\n\nThe `require_managers` function is a decorator allowing route handlers to specify which managers are required. It checks if the requested managers are available and raises an HTTP exception if any are missing, enhancing error handling at the route level.\n\n## Lite Mode Initialization\n\n```python\nasync def init_lite_mode(db_manager: DatabaseManager) -> None:\n    \"\"\"Initialize lite mode specific setup: load team and create default session\"\"\"\n    ...\n```\n\nThe `init_lite_mode` function performs specific setups when the application is running in lite mode. It attempts to import a team from a specified file and creates a default user session, ensuring necessary components are loaded for simplified usage scenarios.\n\nOverall, this module serves as a vital part of the application architecture, ensuring that dependencies are managed cleanly, errors are handled gracefully, and instances are properly initialized and cleaned up based on the application's operational mode.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/deps.py"
  },
  {
    "code": false,
    "content": "# Application Initialization Module Documentation\n\nThis documentation provides an overview of the `initialization.py` module, which is responsible for setting up the essential components of an application. It includes class definitions, methods for initializing paths, managing directories, and loading environment variables.\n\n## Overview\n\nThe `initialization.py` script imports essential libraries for handling file paths, environment variables, and logging. The primary class defined in this module is `AppInitializer`, which focuses on preparing the application environment by managing paths, directories, and configurations. Additionally, it makes use of Pydantic\u2019s `BaseModel` to define a simple data structure for managing application paths.\n\n## Class Definitions\n\n### _AppPaths\n\nThe `_AppPaths` class is an internal data model that encapsulates the file paths used throughout the application. It inherits from Pydantic's `BaseModel`, ensuring that the defined properties are validated upon creation. The properties in this class include:\n\n- **app_root**: Path to the root directory of the application.\n- **static_root**: Path to the directory where static files are stored.\n- **user_files**: Path for storing user-specific files.\n- **ui_root**: Path for user interface files.\n- **config_dir**: Path for configuration files.\n- **database_uri**: Database connection URI.\n\n### AppInitializer\n\nThe `AppInitializer` class is responsible for the overall initialization of the application. Its role encompasses setting up paths, creating necessary directories, and loading environment variables. It requires two arguments upon initialization:\n\n- **settings**: An instance of the `Settings` class that contains application-wide configuration.\n- **app_path**: A string representing the directory path of the application code.\n\n### Initialization Flow\n\n1. **Constructor (`__init__`)**: When the `AppInitializer` is instantiated, it initializes the settings and application path, sets up the necessary directories, and loads environment variables.\n\n2. **Path Initialization**: The `_init_paths` method is called to set up the application's file structure. It retrieves the application root path and generates the necessary sub-paths.\n\n3. **Directory Creation**: Using the `_create_directories` method, the script checks for the existence of essential directories and creates them if they do not exist.\n\n4. **Environment Loading**: The `_load_environment` method loads environment variables from a `.env` file if it exists in the application's root directory.\n\n## Path Management\n\nThe `AppInitializer` provides several properties that allow access to the paths defined in the `_AppPaths` data model without directly exposing the internal structure. These properties include:\n\n- **app_root**: Accesses the root directory of the application.\n- **static_root**: Accesses the directory for static files.\n- **user_files**: Accesses the user files directory.\n- **ui_root**: Accesses the directory for UI files.\n- **config_dir**: Accesses the configuration directory.\n- **database_uri**: Provides the database connection URI.\n\nEach property effectively retrieves the appropriate path information without requiring the user to interact with the underlying `_AppPaths` instance directly.\n\n## Helper Methods\n\nThe class also defines several helper methods that support internal operations:\n\n- **_get_app_root**: Determines the root directory for the application, checking for an environment variable first before defaulting to a user-specific path.\n  \n- **_get_database_uri**: Builds the database connection URI using settings or an environment variable, allowing flexibility in database management.\n\n## Logging\n\nLogging is integrated into the system using the `loguru` library. A log message is generated when the application initializes its data folder, aiding in debugging and tracking application behavior during runtime.\n\n## Conclusion\n\nIn summary, the `initialization.py` module consolidates the setup phase of the application. The `AppInitializer` class and its associated methods provide a structured approach to ensure that necessary paths and directories are correctly established, enabling seamless interaction with the application's file system and configuration. This modular design lays a solid foundation for the application\u2019s further functionality.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/initialization.py"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis code consists of a simple import statement that brings in the `WebSocketManager` class or module from a submodule named `connection`. Below is a detailed examination of its purpose and implications.\n\n## Purpose of the Code\n\nThe single line of code functions primarily as an import statement. It allows the rest of the module or script to utilize the functionality provided by the `WebSocketManager`. This import is crucial for establishing WebSocket connections, managing real-time data transmission, and enabling bi-directional communication between a client (like a web browser) and a server.\n\n### Context of the Import\n\n- **Relative Import**: The dot (`.`) before `connection` indicates a relative import, meaning that `WebSocketManager` is located in a module named `connection` within the same package or directory as the current module. This is a common practice in Python for organizing code in a modular fashion.\n  \n- **WebSocket Management**: The `WebSocketManager` likely encapsulates the logic necessary to manage WebSocket connections effectively. This includes tasks such as establishing and closing connections, sending and receiving messages, and handling connection errors. \n\n## Implications of Using WebSockets\n\nWebSockets are typically used for applications that require real-time communication. This includes:\n\n- **Chat Applications**: Allowing users to send and receive messages in real time.\n- **Live Data Feeds**: Delivering updates for stock prices, news articles, or sports scores without needing to refresh the page.\n- **Gaming**: Coordinating actions between players in a multiplayer environment.\n\nThe presence of a `WebSocketManager` in the code implies that the broader project is likely aimed at building or managing such an application requiring real-time communication capabilities.\n\n## Potential Functionality of `WebSocketManager`\n\nWhile the actual implementation of `WebSocketManager` is not provided in the snippet, one can expect several functionalities typically associated with this class:\n\n- **Connection Handling**: Methods for establishing a connection to a WebSocket server, probably involving authentication procedures.\n  \n- **Message Handling**: Functions to send and receive messages. This could involve methods to broadcast messages to multiple clients or to handle incoming messages in an event-driven manner.\n\n- **Error Management**: Logic for dealing with potential issues that arise during communication, such as connection losses or message formatting errors.\n\n- **Event Handling**: The capability to register events that trigger certain actions based on the data received or the state of the connection.\n\n## Usage Context\n\nThe specific context in which this import statement is used is not provided, but it can typically appear in several types of Python projects:\n\n- **Web Applications**: Using frameworks like Flask or Django for server-side logic that communicates with front-end WebSocket clients.\n  \n- **Real-Time Dashboards**: Applications that display live data and require continuous updates without user intervention.\n\n- **Microservices**: Systems composed of several services that interact through WebSocket protocols to ensure real-time data flow.\n\n## Understanding Module Structure\n\nTo gain a complete understanding of how `WebSocketManager` is utilized, one would need to examine the rest of the module that uses this import. This would likely include:\n\n- **Testing and Validation**: Scripts that implement tests for the WebSocket connections to ensure reliability and proper functionality.\n\n- **Integration with Other Services**: Depending on the application, `WebSocketManager` may need to interact with other classes or modules to carry out its function effectively.\n\n- **Configuration Settings**: Code that sets parameters for WebSocket connections, possibly detailing server addresses, authentication tokens, or default communication channels.\n\nIn summary, while the code snippet provided is succinct, its implications are far-reaching, especially in the realm of real-time web applications. The effective use of the `WebSocketManager` will enable robust and efficient communication applications that can meet modern user needs.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/managers/__init__.py"
  },
  {
    "code": false,
    "content": "# WebSocketManager Documentation\n\n## Overview\n\nThe `WebSocketManager` class is designed to manage WebSocket connections for streaming messages related to team task execution. It maintains a robust connection management system, facilitating message transmission while handling run lifecycle events, such as starting, stopping, and error handling, for various WebSocket connections associated with different task runs.\n\n## Dependencies and Imports\n\nThe class utilizes several modules and packages, including:\n- `asyncio`: For handling asynchronous operations and managing concurrent runs.\n- `logging`: For logging various events and errors.\n- `traceback`: For managing exception traceback during error handling.\n- `datetime`: For timestamping messages.\n- `pathlib`: Utilized for managing filesystem paths.\n- Specific imports from `autogen_agentchat` and `autogen_core`: To manage task results and agent messages.\n- FastAPI's `WebSocket` and `WebSocketDisconnect`: For handling WebSocket connections and disconnections.\n\n## Class Initialization\n\nThe `WebSocketManager` is initialized with a `DatabaseManager` instance which provides the necessary database operations for storing and retrieving run information and messages. During initialization, several data structures are created:\n- `_connections`: A dictionary to keep track of active WebSocket connections.\n- `_cancellation_tokens`: A dictionary for managing cancellation tokens for each run.\n- `_closed_connections`: A set to track explicitly closed connections.\n- `_input_responses`: A dictionary mapping each run ID to an asyncio queue for managing input responses.\n\n### Key Message Setup\nUpon initialization, a cancellation message is prepared as a default, indicating cancellation by the user.\n\n## WebSocket Connection Management\n\n### Connecting a New WebSocket\n\nThe `connect` method accepts a WebSocket and a run ID, then:\n- Accepts the WebSocket connection.\n- Maps the connection to the run ID.\n- Initializes an input queue for the new connection.\n- Sends a connection acknowledgment message back to the client.\n\nIf any issues arise during this process, they are logged, and the connection is deemed unsuccessful.\n\n### Disconnecting a WebSocket\n\nThe `disconnect` method is responsible for cleaning up connections when a WebSocket is closed. It:\n- Marks the connection as closed to prevent further messaging.\n- Cancels associated tasks and updates the run status to \"STOPPED.\"\n- Cleans up relevant resources such as removing the connection from dictionaries tracking active connections.\n\n## Streaming and Task Management\n\n### Starting a Task Stream\n\nThe `start_stream` method initiates the streaming of messages corresponding to a task execution. It performs several critical actions:\n- Validates the run ID against active connections.\n- Retrieves user settings and updates necessary run details.\n- Creates an input function to gather input prompts from the client.\n- Uses a `TeamManager` instance to manage and stream tasks simultaneously.\n- Handles task results by sending them through the WebSocket.\n\nDuring execution, the method listens for messages and manages cancellation requests. Upon completion, the run's status is updated based on the final result. In the event of errors, they are logged and handled appropriately.\n\n### Handling Input Responses\n\nThe `handle_input_response` method processes the responses received from clients during the task execution. It places these responses into the corresponding asyncio queue, allowing the system to continue processing.\n\n### Stopping a Run\n\nThe `stop_run` method allows for graceful termination of an active run based on provided reasons. This method:\n- Updates the run status in the database.\n- Sends a cancellation message via WebSocket, if applicable.\n- Cancels the active cancellation token corresponding to the run.\n\n### Error Handling in Tasks\n\nError handling is critical to maintaining robust performance. The class provides an `_handle_stream_error` method that logs errors, updates the run's status to \"ERROR,\" and sends an error message back to the client.\n\n## Message Formatting and Sending\n\n### Formatting Messages\n\nThe `_format_message` method prepares messages for transmission over WebSocket. It checks the type of message and formats it into a suitable JSON structure. Various conditions handle different message types, ensuring they are appropriately structured or logged in case of formatting failures.\n\n### Sending Messages\n\nThe `_send_message` method takes care of sending messages through the WebSocket. It ensures the target connection exists and is still active. Additionally, it handles potential disconnection scenarios gracefully by logging attempts to send messages to closed connections.\n\n## Database Interaction\n\n### Retrieving and Updating Runs\n\nThe class interacts with a database to:\n- Retrieve runs based on their IDs using the `_get_run` method.\n- Update run statuses and team results via the `_update_run` method. This method also processes related results for matching run IDs.\n\n### Cleaning Up on Server Shutdown\n\nIn the case of server shutdown, the `cleanup` method is responsible for:\n- Cancelling ongoing tasks and updating their statuses to reflect the interruption.\n- Disconnecting all active WebSocket connections within a defined timeout period to ensure clean resource management.\n\n## Properties\n\n### Active Connections and Runs\n\nThe class provides two properties:\n- `active_connections`: Returns a set of active run IDs.\n- `active_runs`: Returns a set of run IDs with active cancellation tokens, facilitating the management of concurrent task executions.\n\nThis implementation is crucial for orchestrating and managing the complexity of multiple asynchronous task executions and their corresponding WebSocket communications, ensuring a responsive and reliable service.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/managers/connection.py"
  },
  {
    "code": false,
    "content": "# Documentation for `RunContext` Class\n\n## Overview\nThe `RunContext` class is designed to manage a context variable that holds a `run_id`. It utilizes the `contextvars` module to facilitate storage of context-specific data that can be accessed throughout the execution of a piece of code, without affecting other execution contexts.\n\n## Context Variable\nThe class defines a class-level variable `RUN_CONTEXT_VAR`, which is an instance of `ContextVar`. This `ContextVar` allows the storage of a value that is local to the current context, making it safe for asynchronous operations.\n\n```python\nRUN_CONTEXT_VAR: ClassVar[ContextVar] = ContextVar(\"RUN_CONTEXT_VAR\")\n```\n\n## Method: `populate_context`\n### Purpose\nThe `populate_context` method is a class method that serves as a context manager for setting and resetting the `run_id` in the `RUN_CONTEXT_VAR`. It ensures that the context variable is only modified within a specified block of code, thus maintaining the integrity of the context across function calls.\n\n### Functionality\n- **Input Parameter:** It takes a single argument, `run_id`, which is the value to be set in the context variable.\n- **Context Management:** When called, it sets the `RUN_CONTEXT_VAR` to the provided `run_id`, and yields control back to the calling code.\n- **Cleanup:** Upon exit from the context (either normally or due to an exception), it restores the previous value of the context variable to avoid unintended side effects.\n\n```python\n@classmethod\n@contextmanager\ndef populate_context(cls, run_id) -> Generator[None, Any, None]:\n    token = RunContext.RUN_CONTEXT_VAR.set(run_id)\n    try:\n        yield\n    finally:\n        RunContext.RUN_CONTEXT_VAR.reset(token)\n```\n\n## Method: `current_run_id`\n### Purpose\nThe `current_run_id` method is a class method that is designed to retrieve the current value of `run_id` from the `RUN_CONTEXT_VAR`. \n\n### Functionality\n- **Return Value:** It returns the current `run_id` stored in the context variable.\n- **Error Handling:** If the `RUN_CONTEXT_VAR` does not contain a value (i.e., when called outside of a context where `run_id` was set), it raises a `RuntimeError` with an appropriate message, indicating that there was an error in fetching the `run_id`.\n\n```python\n@classmethod\ndef current_run_id(cls) -> str:\n    try:\n        return cls.RUN_CONTEXT_VAR.get()\n    except LookupError as e:\n        raise RuntimeError(\"Error getting run id\") from e\n```\n\n## Use Cases\nThis class is particularly useful in scenarios where a unique identifier (like a `run_id`) needs to be tracked during execution, such as:\n- Asynchronous processing where multiple contexts may overlap.\n- Multi-threaded applications where global state management can lead to inconsistencies.\n- Any scenario where data isolation between execution paths is crucial.\n\n## Summary\nThe `RunContext` class effectively encapsulates context management using Python's `contextvars`, offering a clean and safe way to manage the `run_id`. Through its methods, it provides an intuitive interface for setting and accessing the context variable while ensuring proper cleanup of context states. The implementation promotes clean coding practices by allowing the precise management of runtime data that is relevant only to specific scopes of execution.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/managers/run_context.py"
  },
  {
    "code": false,
    "content": "It appears that no source code has been provided for analysis. Please insert the source code you'd like me to examine, and I'll be happy to generate the documentation based on it!",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/__init__.py"
  },
  {
    "code": false,
    "content": "# Gallery API Documentation\n\nThis document outlines the functionality of the `gallery.py` module in a FastAPI application, which provides CRUD (Create, Read, Update, Delete) operations for managing gallery entries. The module utilizes a database manager for data operations and includes user authorization checks to restrict access to the gallery entries.\n\n## 1. Overview\n\nThe `gallery.py` file defines an API router using FastAPI, which exposes routes for creating, retrieving, updating, and deleting gallery entries associated with user accounts. The module relies on a `DatabaseManager` for all database interactions and uses data models defined in the `datamodel` module to structure the requests and responses.\n\n## 2. Dependencies\n\n- **FastAPI**: The web framework for building the API.\n- **DatabaseManager**: For managing database connections and operations.\n- **Gallery**: Data model representing a gallery entry.\n- **Response**: Data model for API responses.\n- **get_db**: Dependency function to obtain a `DatabaseManager` instance.\n\n## 3. Router Initialization\n\n```python\nrouter = APIRouter()\n```\n\nAn instance of `APIRouter` is created to define routes that can be used for handling gallery-related requests. This router can be included in the main FastAPI application.\n\n## 4. Update Gallery Entry\n\n```python\n@router.put(\"/{gallery_id}\")\nasync def update_gallery_entry(...)\n```\n\nThis function allows users to update an existing gallery entry. It performs the following key actions:\n\n- **Ownership Check**: It validates if the gallery entry exists and whether the requesting user is the owner.\n- **Data Update**: If authorized, it updates the gallery entry with the provided data and returns the updated entry.\n\nIf the gallery entry does not exist or the user is unauthorized, an appropriate HTTP exception is raised.\n\n## 5. Create Gallery Entry\n\n```python\n@router.post(\"/\")\nasync def create_gallery_entry(...)\n```\n\nThe function handles the creation of new gallery entries. It does the following:\n\n- **Upsert Operation**: Attempts to insert or update the gallery entry in the database.\n- **Error Handling**: If the operation fails, it raises an HTTP exception with a 400 status code.\n\nOn success, it returns the response containing the created or updated gallery entry.\n\n## 6. List Gallery Entries\n\n```python\n@router.get(\"/\")\nasync def list_gallery_entries(...)\n```\n\nThis function retrieves all gallery entries associated with a specific user. Key features include:\n\n- **Data Retrieval**: It fetches the entries for the given user ID.\n- **Default Entry Creation**: If no entries exist, it creates a default gallery entry using configurations generated by `create_default_gallery()`.\n- **Error Handling**: In case of an exception, it provides a structured error message in the response.\n\n## 7. Get Specific Gallery Entry\n\n```python\n@router.get(\"/{gallery_id}\")\nasync def get_gallery_entry(...)\n```\n\nThis endpoint retrieves a specific gallery entry by its ID, ensuring that the user requesting the data is the owner of the gallery entry. Key actions include:\n\n- **Ownership Verification**: It checks if the entry is owned by the given user ID.\n- **Response Handling**: Returns the specific gallery entry details or raises a 404 error if not found.\n\n## 8. Delete Gallery Entry\n\n```python\n@router.delete(\"/{gallery_id}\")\nasync def delete_gallery_entry(...)\n```\n\nThis function allows users to delete a specified gallery entry. Its main features involve:\n\n- **Ownership Verification**: Ensures the user is authorized to delete the entry by checking both existence and ownership.\n- **Delete Operation**: Executes the deletion operation and returns the result, which indicates success or failure.\n\n## Conclusion\n\nThe `gallery.py` module provides a structured and secure way to manage gallery entries within an application using FastAPI. The clear separation of responsibilities among functions ensures maintainability, while the use of dependency injection promotes a clean and testable architecture. The API also incorporates necessary error handling and user authorization to safeguard data integrity.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/gallery.py"
  },
  {
    "code": false,
    "content": "# MCP WebSockets Management Documentation\n\nThis documentation provides an overview of a FastAPI-based HTTP server implementation that manages WebSocket connections for a Multi-Client Protocol (MCP) based communication. The code utilizes WebSockets to enable real-time data exchange and supports multiple server types for communication. \n\n## Imports and Global Setup\n\nThe code begins by importing necessary modules. These include:\n\n- **Standard Libraries**: `base64`, `json`, `uuid`, and `datetime`.\n- **FastAPI Dependencies**: `APIRouter`, `WebSocket`, `WebSocketDisconnect`.\n- **Logging**: `loguru` for logging events.\n- **MCP Client Libraries**: Various MCP-related classes and utility functions for establishing and managing client sessions.\n- **Pydantic**: This is utilized for data validation, particularly in defining request models.\n\nA global dictionary named `active_sessions` is declared to store the status of WebSocket sessions, indexed by unique session IDs.\n\n## Request Models\n\n### CreateWebSocketConnectionRequest\n\nThis Pydantic model defines the structure of a request for creating a WebSocket connection. It includes:\n\n- `server_params`: An instance of `McpServerParams`, which will dictate how the server connection is established based on its type.\n\n## MCP Session Management\n\n### Create MCP Session\n\nThe asynchronous `create_mcp_session` function is responsible for establishing an MCP session based on the specified server parameters. This function serves multiple server types:\n\n- **StdioServerParams**: Uses standard input/output for communication.\n- **SseServerParams**: Utilizes server-sent events by connecting to a provided URL.\n- **StreamableHttpServerParams**: Establishes a streamable HTTP connection.\n\nWithin this function:\n\n1. **Callbacks Setup**: Various message-handling callbacks are created for interaction through the bridge.\n2. **Session Initialization**: A `ClientSession` is initiated with read and write streams along with the previously configured callbacks.\n3. **Session Tracking**: Information regarding the session\u2014such as creation time and capabilities\u2014is stored in the `active_sessions` dictionary.\n4. **Bridge Operations**: The MCP WebSocket bridge is run to facilitate communication.\n\nIf unsupported server parameters are encountered, an error is raised.\n\n## WebSocket Endpoints\n\n### WebSocket Connection Handling\n\nThe `mcp_websocket` function handles WebSocket connections and is linked to the `/ws/{session_id}` route. Here's how it works:\n\n1. **Connection Acceptance**: Upon a successful WebSocket connection request, the connection is accepted and logged.\n2. **Parameter Extraction**: The function extracts and decodes the `server_params` from the query parameters to configure the session accordingly.\n3. **Session Creation**: Establishes an MCP session using the `create_mcp_session` function.\n4. **Error Handling**: Managed through `try-except` blocks, which capture `WebSocketDisconnect` exceptions and other potential errors for logging and cleanup.\n5. **Session Cleanup**: Finally, when the session ends, it is removed from the active sessions dictionary, and any necessary cleanup is performed.\n\n### Creating WebSocket Connection\n\nThe `create_mcp_websocket_connection` endpoint is associated with the `/ws/connect` route. In this function:\n\n1. **Session ID Generation**: A unique session ID is created using UUID.\n2. **Server Parameters Encoding**: The provided server parameters are serialized into JSON and encoded in base64 for secure transmission.\n3. **Response Construction**: A response JSON is generated that includes the status, the session ID, and the resulting WebSocket URL.\n\nIn the event of an error, logging is performed, and a failure message is returned.\n\n## Session Status Retrieval\n\n### Get MCP Session Status\n\nThe `get_mcp_session_status` function corresponds to the `/ws/status/{session_id}` route and serves the purpose of querying the status of an existing WebSocket session:\n\n1. **Session Lookup**: The function looks up the session information using the provided session ID.\n2. **Activity Timestamp Update**: If the session is found, it updates the last activity timestamp.\n3. **Response Generation**: A response indicating the session\u2019s status (active or not) is returned along with its capabilities, the creation timestamp, and the last activity timestamp.\n\nIf the session does not exist, an appropriate error message is returned.\n\n## Conclusion\n\nThis FastAPI-based code effectively manages WebSocket connections for an MCP communication model, providing mechanisms for connection establishment, session tracking, and status monitoring. It employs error-handling strategies for smooth operation and ensures the capability to support various server configurations. This robust design allows for extensibility and integration into broader systems requiring real-time interactions.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/mcp.py"
  },
  {
    "code": false,
    "content": "# API Runs Routes Documentation\n\nThis document provides a high-level overview of the `/api/runs` routes defined in the FastAPI application. The routes facilitate the creation and retrieval of runs and their associated messages in a database. The code utilizes asynchronous programming with FastAPI and relies on dependency injection for database access.\n\n## Imports and Router Setup\n\nThe code starts with necessary imports:\n- **APIRouter**, **Depends**, and **HTTPException** from FastAPI for constructing the API endpoints.\n- **BaseModel** from Pydantic for data validation.\n- **Message**, **Run**, **RunStatus**, and **Session** from the application's data model, which likely define structures for various entities and their statuses.\n- A utility function **get_db** for getting the database session.\n\nNext, an instance of `APIRouter` is created, which will house the routes related to runs.\n\n## Create Run Endpoint\n\n### Request Model\n\nThe `CreateRunRequest` class, which inherits from `BaseModel`, defines the structure of the request body when creating a new run. It includes two fields:\n- `session_id`: An integer representing the session to which the run is associated.\n- `user_id`: A string identifying the user who created the run.\n\n### Route Definition\n\nThe `create_run` function is defined as a route handler for POST requests to the root of the `/runs` endpoint. It performs the following actions:\n1. **Session Validation**: It retrieves the session from the database using `session_id` and `user_id`. If the session does not exist, a 404 HTTP exception is raised.\n2. **Run Creation**: If the session is valid, a new `Run` record is created with an initial status of `CREATED`. This record includes:\n   - `session_id`: Id of the provided session.\n   - `status`: Indicates the state of the run, initialized to `RunStatus.CREATED`.\n   - `user_id`: Id of the user creating the run.\n   - `task` and `team_result`: Initialized as empty structures, to be updated later.\n3. **Response**: Finally, the function returns a success response containing the status of the run and its associated ID.\n\nIn case of any exceptions during the database operations, a 500 HTTP exception is raised.\n\n## Get Run Endpoint\n\nThe `get_run` function is defined for GET requests to retrieve the details of a specific run using its `run_id`. The function performs the following key actions:\n1. **Run Retrieval**: It queries the database for the run using the provided `run_id`. If the run is not found, a 404 HTTP exception is raised.\n2. **Response**: Upon successful retrieval, it returns a JSON object indicating success and containing the run data.\n\n## Get Run Messages Endpoint\n\nThe `get_run_messages` function is designed to fetch all messages associated with a specific run. It is a GET request to the endpoint `/runs/{run_id}/messages` and includes these steps:\n1. **Messages Retrieval**: It queries the database for messages related to the given `run_id`, ordering them by their creation date in ascending order.\n2. **Response**: The function returns a success response containing the retrieved messages.\n\n## Conclusion\n\nThe `/api/runs` routes provide essential functionalities for managing runs within the application. The main features include creating a new run and retrieving details associated with specific runs and their messages. The implementation ensures error handling for common scenarios such as missing sessions or runs, improving the robustness of the API. Future enhancements might expand upon these foundations, perhaps adding more detailed searches or batch operations for runs and messages.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/runs.py"
  },
  {
    "code": false,
    "content": "# API Documentation for Sessions Module\n\nThis document outlines the functionality of the `sessions.py` module in the FastAPI application. The module handles various API endpoints related to user sessions, providing ways to list, create, retrieve, update, and delete sessions, as well as manage associated runs.\n\n## Overview\n\nThe `sessions.py` module defines an API router containing endpoints for managing sessions of users. It leverages FastAPI for routing and dependency injection, Loguru for logging errors, and a database abstraction for performing CRUD operations on session and run data.\n\n### Imports\n\nThe following key components are imported:\n\n- **FastAPI components**: `APIRouter`, `Depends`, and `HTTPException` are used to create the API endpoints and handle dependencies and exceptions. \n- **Loguru logger**: Provides logging capabilities for error handling and tracking program flow.\n- **Data Models**: `Message`, `Response`, `Run`, `Session` are imported from a data model module, representing the structures for database operations.\n- **Dependency**: `get_db` is a function to obtain the database connection.\n\n## API Endpoints\n\n### List All Sessions\n\n```python\n@router.get(\"/\")\nasync def list_sessions(user_id: str, db=Depends(get_db)) -> Dict\n```\n\n- **Purpose**: Retrieves a list of all sessions associated with a specific user.\n- **Parameters**: \n  - `user_id`: A string identifying the user.\n  - `db`: Dependency that provides database access.\n- **Returns**: A dictionary containing a status and the list of sessions.\n- **Operation**: Queries the database for sessions filtered by the user ID and returns the results.\n\n### Get Specific Session\n\n```python\n@router.get(\"/{session_id}\")\nasync def get_session(session_id: int, user_id: str, db=Depends(get_db)) -> Dict\n```\n\n- **Purpose**: Fetches a specific session based on its ID for a given user.\n- **Parameters**: \n  - `session_id`: The ID of the session to retrieve.\n  - `user_id`: The ID of the user who owns the session.\n  - `db`: Dependency for database access.\n- **Returns**: A dictionary with session details if found, or raises a 404 error if not.\n- **Operation**: Validates existence and ownership of the session before returning it.\n\n### Create New Session\n\n```python\n@router.post(\"/\")\nasync def create_session(session: Session, db=Depends(get_db)) -> Response\n```\n\n- **Purpose**: Creates a new session with the provided session data.\n- **Parameters**: \n  - `session`: An instance of the `Session` data model containing session details.\n  - `db`: Dependency for database interactions.\n- **Returns**: A `Response` object indicating the outcome of the operation.\n- **Operation**: Attempts to upsert the session to the database and handles any errors, logging them accordingly.\n\n### Update Existing Session\n\n```python\n@router.put(\"/{session_id}\")\nasync def update_session(session_id: int, user_id: str, session: Session, db=Depends(get_db)) -> Dict\n```\n\n- **Purpose**: Updates an existing session for the specified user.\n- **Parameters**: \n  - `session_id`: The ID of the session to update.\n  - `user_id`: The user's ID for validation.\n  - `session`: Updated session data.\n  - `db`: Database dependency.\n- **Returns**: A dictionary indicating the success of the update operation.\n- **Operation**: Validates that the session exists and belongs to the user before updating it in the database.\n\n### Delete Session\n\n```python\n@router.delete(\"/{session_id}\")\nasync def delete_session(session_id: int, user_id: str, db=Depends(get_db)) -> Dict\n```\n\n- **Purpose**: Deletes a session via its ID for the specified user.\n- **Parameters**: \n  - `session_id`: The ID of the session to be deleted.\n  - `user_id`: User ID for validation.\n  - `db`: Dependency for database access.\n- **Returns**: A confirmation message indicating that the session has been deleted.\n- **Operation**: Directly invokes a delete operation on the session in the database.\n\n### List Session Runs\n\n```python\n@router.get(\"/{session_id}/runs\")\nasync def list_session_runs(session_id: int, user_id: str, db=Depends(get_db)) -> Dict\n```\n\n- **Purpose**: Retrieves a complete history of runs associated with a specific session.\n- **Parameters**: \n  - `session_id`: ID of the session whose runs are to be fetched.\n  - `user_id`: ID of the user for validation.\n  - `db`: Dependency for database interaction.\n- **Returns**: A dictionary containing a list of runs along with related message data.\n- **Operation**: \n  - Validates if the session exists for the user.\n  - Queries for runs linked to the session, following which messages for each run are retrieved.\n  - Handles errors while processing runs, ensuring continuity of processing even when errors occur for individual runs.\n\n## Error Handling and Logging\n\nThe module employs HTTP exceptions for error signaling, ensuring that relevant status codes are returned when issues arise. Additionally, Loguru is utilized for error logging throughout the operations, providing insights into issues such as database errors or unexpected conditions during execution.\n\nOverall, this module serves as a comprehensive controller for managing user sessions and associated runs within the application, incorporating structured error handling and responses tailored for client consumption.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/sessions.py"
  },
  {
    "code": false,
    "content": "# API Settings Router Documentation\n\nThis documentation provides a high-level overview of the `settings.py` module in the FastAPI application. This module defines two primary routes for managing user settings: retrieving and updating settings.\n\n## Overview\n\nThe `settings.py` module is part of an API built using FastAPI, a modern web framework for building APIs with Python. This module utilizes a database to manage user-specific settings and is structured as a router, which allows for grouping related API endpoints. The module imports necessary components from FastAPI and various application layers, including models and dependencies.\n\n## Imports and Dependencies\n\n```python\nfrom typing import Dict\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom ...datamodel import Settings, SettingsConfig\nfrom ..deps import get_db\n```\n\n- **APIRouter**: Used to define a group of related routes in FastAPI.\n- **Depends**: Facilitates dependency injection, allowing routes to utilize shared components like database connections.\n- **HTTPException**: Used to raise HTTP errors with specific status codes and messages.\n- **Settings and SettingsConfig**: Likely data models that define the structure of settings data.\n- **get_db**: A function that provides a database session or connection.\n\n## Router Initialization\n\n```python\nrouter = APIRouter()\n```\n\nThe `APIRouter` instance is created, which will hold the routes for getting and updating user settings. This modular approach helps keep the application organized.\n\n## Get Settings Route\n\n```python\n@router.get(\"/\")\nasync def get_settings(user_id: str, db=Depends(get_db)) -> Dict:\n```\n\n### Purpose\n\nThe `get_settings` function handles HTTP GET requests to the root path (`\"/\"`). It retrieves settings associated with a specific user identified by `user_id`.\n\n### Functionality\n\n1. **Dependency Injection**: Utilizes the `Depends` function to get a database connection via `get_db`.\n2. **Database Query**: Attempts to retrieve existing settings from the database using the `user_id`.\n3. **Default Settings Creation**: If no settings are found, it creates and upserts default settings for the user, using `SettingsConfig` to structure the default values.\n4. **Response Handling**: Returns the found or newly created settings wrapped in a dictionary format. If an error occurs during the operation, an HTTP 500 error is raised with the exception's details.\n\n## Update Settings Route\n\n```python\n@router.put(\"/\")\nasync def update_settings(settings: Settings, db=Depends(get_db)) -> Dict:\n```\n\n### Purpose\n\nThe `update_settings` function facilitates HTTP PUT requests to update existing settings or create new ones via the provided settings data.\n\n### Functionality\n\n1. **Dependency Injection**: Similarly, it uses `Depends(get_db)` to access the database.\n2. **Upsert Operation**: Calls the `upsert` method on the database connection to either update existing settings or create new ones based on the `settings` parameter passed to the function.\n3. **Error Handling**: If the upsertion fails (indicated by `response.status`), it raises an HTTP 400 error with the specific error message.\n4. **Successful Response**: If the operation is successful, it returns the updated settings wrapped in a dictionary indicating success.\n\n## Summary\n\nThe `settings.py` module effectively defines an API router for managing user settings. The two defined routes \u2014 retrieving and updating settings \u2014 utilize proper error handling and default value creation where necessary. This design provides a robust way to ensure that user settings are always available in a consistent format, while also allowing users to customize their settings as needed. \n\nOverall, this module is an integral part of a larger application designed to handle user preferences, showcasing fundamental API design principles such as modularity, error handling, and dependency management in FastAPI.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/settingsroute.py"
  },
  {
    "code": false,
    "content": "# API Routes for Teams\n\nThis document provides a high-level description of the FastAPI routes defined for managing teams in a web application. The code includes endpoints to list, retrieve, create, and delete teams, each interacting with a database through dependency injection.\n\n## Overview\n\nThe `teams.py` file implements routes for a FastAPI application that handles team management features. It utilizes FastAPI's `APIRouter` to define routes and includes several endpoints for performing CRUD (Create, Read, Update, Delete) operations on the `Team` data model. The database interactions are facilitated by a `get_db` dependency.\n\n### Imports\n\n```python\nfrom typing import Dict\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom ...datamodel import Team\nfrom ...gallery.builder import create_default_gallery\nfrom ..deps import get_db\n```\n\n- **Typing and FastAPI Libraries**: The code imports necessary modules for type annotations (`Dict`), API routing (`APIRouter`), dependencies management (`Depends`), and error handling (`HTTPException`).\n- **Data Model**: The `Team` model represents the structure of data managed in the API. \n- **Gallery Builder**: A utility function `create_default_gallery` is used to generate default gallery data when creating teams. \n- **Database Dependency**: `get_db` is a dependency that provides database connection context for handling requests.\n\n## Router Definition\n\n```python\nrouter = APIRouter()\n```\n\n- An instance of `APIRouter` is created to group related team routes. This structure promotes better organization and modularity within the application.\n\n### List Teams\n\n```python\n@router.get(\"/\")\nasync def list_teams(user_id: str, db=Depends(get_db)) -> Dict:\n    \"\"\"List all teams for a user\"\"\"\n```\n\n- **Purpose**: This endpoint retrieves all teams associated with a specific user identified by `user_id`.\n- **Logic**:\n  - It queries the database for teams linked to the provided `user_id`.\n  - If no teams are found, it creates a default team using a default gallery and stores it in the database.\n  - Finally, it returns a JSON response containing the status and the teams data.\n\n### Get Team\n\n```python\n@router.get(\"/{team_id}\")\nasync def get_team(team_id: int, user_id: str, db=Depends(get_db)) -> Dict:\n    \"\"\"Get a specific team\"\"\"\n```\n\n- **Purpose**: This endpoint returns details for a specific team, identified by `team_id`, that belongs to the given `user_id`.\n- **Logic**:\n  - It checks the database for a team with the specified `team_id` and user context.\n  - If no team is found, it raises an HTTP 404 exception.\n  - If found, it returns the team's details wrapped in a JSON response.\n\n### Create Team\n\n```python\n@router.post(\"/\")\nasync def create_team(team: Team, db=Depends(get_db)) -> Dict:\n    \"\"\"Create a new team\"\"\"\n```\n\n- **Purpose**: This endpoint allows users to create a new team by providing a `Team` object.\n- **Logic**:\n  - The provided team object is upserted (i.e., inserted/updated) into the database.\n  - If the operation fails, an HTTP 400 error is raised with the error message.\n  - If successful, it returns a JSON response indicating the status and data of the created team.\n\n### Delete Team\n\n```python\n@router.delete(\"/{team_id}\")\nasync def delete_team(team_id: int, user_id: str, db=Depends(get_db)) -> Dict:\n    \"\"\"Delete a team\"\"\"\n```\n\n- **Purpose**: This endpoint enables the deletion of a specific team associated with a given `team_id` and `user_id`.\n- **Logic**:\n  - It deletes the team from the database matching the given filters.\n  - A successful deletion results in a JSON response confirming the deletion status.\n\n## Conclusion\n\nThe `teams.py` file efficiently encapsulates functionalities for managing application teams through well-defined API routes. Each route includes error handling and responses to ensure a consistent interface for client-side applications. By defining clear purposes and logical flows within each function, the code maintains readability and ease of understanding.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/teams.py"
  },
  {
    "code": false,
    "content": "# Validation API Module Documentation\n\n## Overview\nThis module defines an API for validating and testing component configurations using the FastAPI framework. It provides two main endpoints for handling requests related to component validation and functional testing. The endpoints utilize services to ensure that components are correctly validated before any functional tests are performed.\n\n## Dependencies\nThe module imports several components from other parts of the application:\n- **FastAPI APIRouter**: To define routes for handling API requests.\n- **ComponentTestRequest, ComponentTestResult, ComponentTestService**: Classes and services related to component testing.\n- **ValidationError, ValidationRequest, ValidationResponse, ValidationService**: Classes and services for handling validation logic.\n\n## Router Initialization\nA FastAPI router is initialized using `APIRouter()`. This router will handle incoming POST requests to the defined routes for validating and testing components.\n\n```python\nrouter = APIRouter()\n```\n\n## Component Validation Endpoint\n### Function: `validate_component`\n\n```python\n@router.post(\"/\")\nasync def validate_component(request: ValidationRequest) -> ValidationResponse:\n    \"\"\"Validate a component configuration\"\"\"\n```\n\nThis function is mapped to the root of the router (i.e., `POST /`). It is responsible for validating a component's configuration. When a client sends a POST request to this endpoint with a `ValidationRequest`, the function performs the following steps:\n\n1. **Validation Attempt**: It invokes `ValidationService.validate()` with the component data from the request.\n2. **Error Handling**: If an exception occurs during validation, the function catches it and returns a `ValidationResponse` with the validation result marked as invalid. It also includes an error message detailing the issue.\n3. **Return Response**: If validation is successful, it returns the resulting `ValidationResponse` object.\n\n## Component Testing Endpoint\n### Function: `test_component`\n\n```python\n@router.post(\"/test\")\nasync def test_component(request: ComponentTestRequest) -> ComponentTestResult:\n    \"\"\"Test a component functionality with appropriate inputs based on type\"\"\"\n```\n\nThis function is assigned to the `/test` route (i.e., `POST /test`). It handles requests for testing components after validating their configurations. The process involves the following steps:\n\n1. **Initial Validation**: The function first validates the component by calling `ValidationService.validate()` with the component from the `ComponentTestRequest`.\n2. **Validation Check**: If the validation fails (i.e., `is_valid` is `False`), the function returns a `ComponentTestResult` indicating that the test cannot proceed, along with error logs summarizing the validation errors.\n3. **Functional Testing**: If validation passes, the function proceeds to invoke `ComponentTestService.test_component()`, passing required parameters such as the component, timeout value, and model client.\n4. **Return Test Result**: The result from the functional test is returned, which indicates the outcome of executing the component's functionality.\n\n## Error Handling\nBoth endpoints include inherent error handling mechanisms:\n- The `validate_component` endpoint catches a broad range of exceptions and encapsulates them in a structured response format.\n- The `test_component` endpoint checks validation errors before attempting to conduct the functional test and responds accordingly.\n\n## Conclusion\nThis module efficiently outlines the validation and testing flows for components within an application. By separating concerns through dedicated services for validation and testing, it enhances maintainability and clarity. The endpoints serve a crucial role in ensuring that components are validated before they are subjected to any functional testing, helping to reduce runtime errors and improve overall application reliability.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/validation.py"
  },
  {
    "code": false,
    "content": "# WebSocket Endpoint for Run Communication\n\nThis code defines a WebSocket endpoint using the FastAPI framework. It allows clients to interact with \"runs\" in a system through a persistent WebSocket connection. This interaction includes starting runs, stopping them, and responding to user inputs. Below is a detailed breakdown of the code, including its components and flow.\n\n## Imports and Dependencies\n\n### Libraries\nThe script imports several libraries:\n- **asyncio**: To handle asynchronous operations.\n- **json**: For parsing JSON messages.\n- **datetime**: For time-related operations.\n- **FastAPI**: For defining the WebSocket router, dependencies, and exceptions.\n- **loguru**: A logging library for logging messages.\n\n### Local Modules\nThe code also imports custom modules:\n- **Run** and **RunStatus**: From `datamodel`, likely defining the structure and possible statuses of a \"run.\"\n- **construct_task**: A utility function for constructing tasks from received messages.\n- **WebSocketAuthHandler**: For managing WebSocket authentication.\n- **get_db**, **get_websocket_manager**, and **get_ws_auth_manager**: Dependency injection functions to retrieve database connections, WebSocket management, and authentication handlers.\n- **WebSocketManager**: A class managing WebSocket connections.\n\n## WebSocket Connection Setup\n\n### Endpoint Definition\nThe WebSocket is defined at the route `/runs/{run_id}`. The `run_websocket` function is called whenever a client connects to this endpoint.\n\n### Parameters\n- **websocket**: The WebSocket connection itself.\n- **run_id**: The ID of the specific run that the client wishes to communicate with.\n- **Dependencies**: The function uses several dependencies to manage WebSocket connections, database access, and authentication.\n\n### Initial Check\nUpon establishing a connection:\n1. The code checks if the specified run exists in the database. If not, it closes the WebSocket connection with an error code 4004, indicating that the run was not found.\n2. It verifies the run\u2019s status. Only runs in the `CREATED` or `ACTIVE` states are allowed to interact via WebSocket. If the status is invalid, a closure with code 4003 is initiated, conveying that the run is not in a valid state.\n\n## WebSocket Connection Management\n\n### Connection Handling\n- If the checks are passed, the WebSocket connection is established with the `ws_manager.connect` method.\n- If authentication is enabled, the `WebSocketAuthHandler` attempts to authenticate the connection. If authentication fails, the server sends an error message back to the client and closes the connection.\n\n### Authorization Check\nAdditionally, the user's authorization is checked against the owner of the run. If the user is not authorized, an error message is logged, and the WebSocket connection is not established.\n\n### Logging\nSuccessful connections are logged using the `logger` class, providing an easy way to audit interactions with the WebSocket.\n\n## Message Handling Loop\n\n### Continuous Listening\nAfter successfully connecting, the server enters a continuous loop to listen for messages sent from the client:\n\n1. **Message Reception**: The server waits for messages using `await websocket.receive_text()`.\n2. **Message Parsing**: The received message is parsed into a JSON object. If an error occurs during this parsing, a warning is logged, and an error message is sent back to the client.\n\n### Message Types\nThe server handles several message types in the loop:\n\n- **Start**: If the message type is \"start\", it constructs a task using the provided task and files, then starts streaming in a separate asynchronous task if valid.\n  \n- **Stop**: If the message type is \"stop\", the server invokes the `ws_manager.stop_run` method with a specified reason for stopping, breaking the loop subsequently.\n\n- **Ping**: For \"ping\" messages, it acknowledges by sending back a \"pong\" response, allowing clients to check connectivity.\n\n- **Input Response**: The server processes \"input_response\" messages, relaying necessary responses to the `ws_manager` for further handling.\n\n## Exception Management\n\n### WebSocket Disconnection\nIf a `WebSocketDisconnect` exception occurs, the server logs the disconnection. This is typical behavior for WebSocket connections.\n\n### General Error Handling\nFor any other exceptions raised during the connection process, error information is logged for debugging and monitoring purposes.\n\n### Cleanup\nRegardless of outcome, the server attempts to call `ws_manager.disconnect(run_id)` to ensure that resources are properly released and cleaned up after the WebSocket communication ends.\n\n## Summary\nIn essence, this code provides a robust framework for managing WebSocket connections associated with runs. It includes essential authentication and authorization checks, detailed logging for operations and errors, and a clearly defined structure for processing multiple message types. It is an excellent example of real-time communication in a FastAPI application, balancing usability with security and resource management.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/routes/ws.py"
  },
  {
    "code": false,
    "content": "# FastAPI Predict Endpoint Documentation\n\n## Overview\n\nThis code defines a simple web application using FastAPI, a modern web framework for building APIs with Python 3.7+ based on standard Python type hints. The application contains a single endpoint that processes requests and returns a structured response.\n\n## Imports\n\nThe script begins by importing necessary modules:\n\n- `os`: Used to access environment variables.\n- `FastAPI`: The main class for creating the web application.\n- `Response`: A data model imported from a sibling module, which presumably structures the responses returned by the API.\n- `TeamManager`: A class used to handle operations related to a team, also imported from a sibling module.\n\n## Application Initialization\n\n```python\napp = FastAPI()\nteam_manager = TeamManager()\n```\n\n- **FastAPI Instance**: An instance of the FastAPI class is created, which will later define and manage routes for the application.\n- **TeamManager Instance**: An instance of the `TeamManager` class is created, which likely contains business logic for handling team-related tasks.\n\n## Predict Endpoint Definition\n\n```python\n@app.get(\"/predict/{task}\")\nasync def predict(task: str):\n```\n\n### Endpoint Details\n\n- **HTTP Method**: The endpoint responds to GET requests.\n- **Path Parameter**: The endpoint expects a path parameter called `task`, which is a string that specifies the task to be processed.\n\n### Functionality\n\nThe `predict` function is responsible for executing a specified task and returning the result.\n\n### Response Object Initialization\n\n```python\nresponse = Response(message=\"Task successfully completed\", status=True, data=None)\n```\n\n- A `Response` object is initialized with a default message indicating success, a status flag set to `True`, and an empty `data` field.\n\n## Environment Variable Check\n\n```python\nteam_file_path = os.environ.get(\"AUTOGENSTUDIO_TEAM_FILE\")\n\nif team_file_path is None:\n    raise ValueError(\"AUTOGENSTUDIO_TEAM_FILE environment variable is not set\")\n```\n\n1. **Environment Variable Retrieval**: The code attempts to retrieve the value of the `AUTOGENSTUDIO_TEAM_FILE` environment variable, which is expected to point to a configuration for team operations.\n2. **Error Handling**: If the environment variable is not set, a `ValueError` is raised, indicating that the application cannot proceed without this configuration.\n\n## Task Execution\n\n```python\nresult_message = await team_manager.run(task=task, team_config=team_file_path)\n```\n\n- The `run` method of the `team_manager` instance is called asynchronously with the `task` and the `team_file_path`.\n- This method presumably executes the task using the provided configuration, generating a result that will be returned as part of the API response.\n\n## Exception Handling\n\n```python\nexcept Exception as e:\n    response.message = str(e)\n    response.status = False\n```\n\n- If any exceptions occur during the execution of the task or the environment variable check, the error message is captured.\n- The `Response` object is updated to reflect the failure, with the message set to the exception's message and the status set to `False`.\n\n## Final Response\n\n```python\nreturn response\n```\n\n- The function concludes by returning the `Response` object, which contains either the successful result of the task execution or an error message, structured in a way that is easy for clients to understand.\n\n## Summary\n\nThis FastAPI application provides a single endpoint for executing a specified task related to a team, contingent on the presence of a required environment variable. The application showcases proper error handling, with thoughtful structuring of the response so that clients may receive clear messages indicating the success or failure of their requests.",
    "filename": "python/packages/autogen-studio/autogenstudio/web/serve.py"
  },
  {
    "content": "# AutoGen Studio frontend\n\n## \ud83d\ude80 Running UI in Dev Mode\n\nRun the UI in dev mode (make changes and see them reflected in the browser with hot reloading):\n\n```bash\nyarn install\nyarn start               # local development\nyarn start --host 0.0.0.0  # in container (enables external access)\n```\n\nThis should start the server on [port 8000](http://localhost:8000).\n\n## Design Elements\n\n- **Gatsby**: The app is created in Gatsby. A guide on bootstrapping a Gatsby app can be found here - <https://www.gatsbyjs.com/docs/quick-start/>.\n  This provides an overview of the project file structure include functionality of files like `gatsby-config.js`, `gatsby-node.js`, `gatsby-browser.js` and `gatsby-ssr.js`.\n- **TailwindCSS**: The app uses TailwindCSS for styling. A guide on using TailwindCSS with Gatsby can be found here - <https://tailwindcss.com/docs/guides/gatsby.https://tailwindcss.com/docs/guides/gatsby> . This will explain the functionality in tailwind.config.js and postcss.config.js.\n\n## Modifying the UI, Adding Pages\n\nThe core of the app can be found in the `src` folder. To add pages, add a new folder in `src/pages` and add a `index.js` file. This will be the entry point for the page. For example to add a route in the app like `/about`, add a folder `about` in `src/pages` and add a `index.tsx` file. You can follow the content style in `src/pages/index.tsx` to add content to the page.\n\nCore logic for each component should be written in the `src/components` folder and then imported in pages as needed.\n\n## Connecting to backend\n\nThe frontend makes requests to the backend api and expects it at /api on localhost port 8081.\n\n## setting env variables for the UI\n\n- please look at `.env.default`\n- make a copy of this file and name it `.env.development`\n- set the values for the variables in this file\n  - The main variable here is `GATSBY_API_URL` which should be set to `http://localhost:8081/api` for local development. This tells the UI where to make requests to the backend.",
    "filename": "python/packages/autogen-studio/frontend/README.md"
  },
  {
    "content": "# AddComponentDropdown Usage Examples\n\nThe `AddComponentDropdown` component is a reusable dropdown that allows users to add components to a gallery. It supports all component types (teams, agents, models, tools, workbenches, terminations).\n\n## Basic Usage\n\n```tsx\nimport { AddComponentDropdown } from \"../../shared\";\n\n<AddComponentDropdown\n  componentType=\"workbench\"\n  gallery={selectedGallery}\n  onComponentAdded={handleComponentAdded}\n/>;\n```\n\n## Advanced Usage with Filtering (MCP Workbenches)\n\n```tsx\n<AddComponentDropdown\n  componentType=\"workbench\"\n  gallery={selectedGallery}\n  onComponentAdded={handleComponentAdded}\n  size=\"small\"\n  type=\"text\"\n  buttonText=\"+\"\n  showChevron={false}\n  templateFilter={(template) =>\n    template.label.toLowerCase().includes(\"mcp\") ||\n    template.description.toLowerCase().includes(\"mcp\")\n  }\n/>\n```\n\n## Props\n\n- `componentType`: The type of component to add (team, agent, model, tool, workbench, termination)\n- `gallery`: The gallery to add the component to\n- `onComponentAdded`: Callback when a component is added\n- `disabled`: Whether the dropdown is disabled\n- `showIcon`: Whether to show the plus icon\n- `showChevron`: Whether to show the chevron down icon\n- `size`: Button size\n- `type`: Button type\n- `className`: Additional CSS classes\n- `buttonText`: Custom button text\n- `templateFilter`: Optional filter function for templates\n\n## Handler Signature\n\n```tsx\nconst handleComponentAdded = (\n  component: Component<ComponentConfig>,\n  category: CategoryKey\n) => {\n  // Handle the added component\n  // Update your gallery/state here\n};\n```\n\n## Benefits\n\n1. **Reusability**: Use the same component across different views\n2. **Consistency**: Same UI/UX everywhere\n3. **Maintainability**: Single source of truth for component addition logic\n4. **Flexibility**: Configurable with props and filters\n5. **Type Safety**: Fully typed with TypeScript",
    "filename": "python/packages/autogen-studio/frontend/src/components/shared/README.md"
  },
  {
    "code": false,
    "content": "# AutoGen Studio Agent Workflow API Example\n\nThis document showcases the capabilities of the AutoGen Studio Workflow Python API, particularly focusing on the declarative specification and execution of agent teams. The following sections cover how to define an agent team, load configurations, and invoke tasks using the API.\n\n## Declarative Specification of an Agent Team\n\nTo initiate an agent workflow, we utilize the `TeamManager` from the `autogenstudio` package. This example demonstrates how to create and run an agent team configured to retrieve weather information.\n\n```python\nfrom autogenstudio.teammanager import TeamManager\n\nwm = TeamManager()\nresult = await wm.run(task=\"What is the weather in New York?\", team_config=\"team.json\")\nprint(result)\n```\n\nIn this code block, we instantiate the `TeamManager` and call the `run` method with a specified `task` and `team_config`. The `team.json` file contains the team's configuration, and the output is printed to the console. This allows for asynchronous task execution within the specified agent framework.\n\n## Streaming Results from the Agent Team\n\nFor tasks that may produce ongoing results or require real-time feedback, we can use the `run_stream` method. This method provides a way to asynchronously receive responses from the agent.\n\n```python\nresult_stream = wm.run_stream(task=\"What is the weather in New York?\", team_config=\"team.json\")\nasync for response in result_stream:\n    print(response)\n```\n\nThis code snippet shows how to execute the same weather-related task using a streaming approach. The `async for` loop enables us to print each response as it is received, facilitating a dynamic interaction with the agent team.\n\n## Load Directly Using the AgentChat API\n\nIn addition to using `TeamManager`, we can load agent configurations directly via the `AgentChat API`. This approach allows the instantiation of agent teams with pre-defined configurations.\n\n```python\nimport json \nfrom autogen_agentchat.teams import BaseGroupChat\n\nteam_config = json.load(open(\"team.json\"))  \nteam = BaseGroupChat.load_component(team_config)\nprint(team._participants)\n```\n\nHere, we load the team configuration from a JSON file and instantiate a `BaseGroupChat` component. The output displays the participants in the chat, allowing for an overview of the agent team involved in the task.\n\n## AutoGen Studio Database API\n\nThe `DatabaseManager` from AutoGen Studio provides an interface for creating and managing databases. This section demonstrates how to initialize a database for storing application data.\n\n```python\nfrom autogenstudio.database import DatabaseManager\nimport os\n\n# Prepare to create a database\nos.makedirs(\"test\", exist_ok=True)\ndbmanager = DatabaseManager(engine_uri=\"sqlite:///test.db\", base_dir=\"test\")\ndbmanager.initialize_database()\n```\n\nIn this snippet, we create a new directory for our database, initialize a `DatabaseManager`, and prepare it to configure a SQLite database. Note that previous database files can be deleted if necessary.\n\n## Sample AgentChat Example (Python)\n\nIn this section, we define a more complex setup of agent teams capable of planning a trip while integrating various roles, such as a planner, local assistant, and language expert. Each agent communicates with the others to formulate a comprehensive travel plan.\n\n```python\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Define various agents with roles pertaining to travel assistance\nplanner_agent = AssistantAgent(\n    \"planner_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can plan trips.\",\n    system_message=\"You are a helpful assistant that can suggest a travel plan for a user based on their request. Respond with a single sentence\",\n)\n\nlocal_agent = AssistantAgent(\n    \"local_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A local assistant that can suggest local activities or places to visit.\",\n    system_message=\"You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided. Respond with a single sentence\",\n)\n\nlanguage_agent = AssistantAgent(\n    \"language_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can provide language tips for a given destination.\",\n    system_message=\"You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale. Respond with a single sentence\",\n)\n\ntravel_summary_agent = AssistantAgent(\n    \"travel_summary_agent\",\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\"),\n    description=\"A helpful assistant that can summarize the travel plan.\",\n    system_message=\"You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE. Respond with a single sentence\",\n)\n\n# Set up termination conditions for the group chat\ntermination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\ngroup_chat = RoundRobinGroupChat(\n    [planner_agent, local_agent, language_agent, travel_summary_agent],\n    termination_condition=termination\n)\n```\n\nThis block defines four specialized agents, each with specific roles related to trip planning. We also establish termination conditions to gracefully end the chat once a satisfactory travel plan is provided. The agents work collaboratively to generate a comprehensive response.\n\n## Running Tasks with the Agent Team\n\nOnce the agents are set up, we can run tasks to obtain responses based on the defined interactions.\n\n```python\nresult = group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\")\nasync for response in result:\n    print(response)\n```\n\nThis code snippet executes a task where the agent team plans a three-day trip to Nepal, streaming real-time responses from each agent's input.\n\n## Configuring and Saving the Agent Team\n\nTo maintain a reusable configuration of the agent team, we can dump the current state to a JSON file, allowing for easy reloading in future sessions.\n\n```python\nimport json\n\n# Convert to config \nconfig = group_chat.dump_component().model_dump()\n\n# Save the configuration as JSON \nwith open(\"travel_team.json\", \"w\") as f:\n    json.dump(config, f, indent=4)\n\n# Load from JSON\nwith open(\"travel_team.json\", \"r\") as f:\n    config = json.load(f)\n\ngroup_chat = RoundRobinGroupChat.load_component(config) \nresult = group_chat.run_stream(task=\"Plan a 3 day trip to Nepal.\") \nasync for response in result:\n    print(response)\n```\n\nHere, the configuration is serialized to JSON, making it possible to save and later reload the agent team setup. This allows repetitive tasks to be run without needing to redefine the agents and their interactions each time.",
    "filename": "python/packages/autogen-studio/notebooks/tutorial.ipynb"
  },
  {
    "code": false,
    "content": "# High-Level Description of the Code\n\nThis code snippet utilizes the `setuptools` library, which is a powerful module in Python used for packaging Python projects. The provided code serves as the setup script, essentially establishing the metadata and functionality required for distributing and installing a Python package.\n\n## Purpose of the Code\n\nThe primary purpose of this code is to prepare a Python package for distribution. It utilizes the `setup()` function from the `setuptools` module, which is a standard way to package Python code and allows users to easily install and manage the package.\n\n### Overview of `setuptools`\n\n`setuptools` is an enhanced version of the Python `distutils` that simplifies the process of packaging Python projects. It provides utilities for defining a package structure and handling the complex tasks involved in packaging, such as dependencies, metadata, and installation guidelines.\n\n## The `setup()` Function\n\nThe `setup()` function is the core element of the script. This function is called with specific arguments that define various aspects of the Python package. Although the code provided does not explicitly pass any arguments to `setup()`, it can typically accept a range of keyword arguments that detail the package's characteristics.\n\n### Common Arguments in `setup()`\n\nWhile the script does not specify any, some commonly used parameters for the `setup()` function include:\n\n- **name**: The name of the package.\n- **version**: The current version of the package.\n- **description**: A short summary of what the package does.\n- **author**: The name of the package author.\n- **author_email**: The author's email address.\n- **url**: The URL for the package source or documentation.\n- **packages**: A list of all packages/modules to include.\n- **install_requires**: A list of dependencies required for the package.\n\n### Default Behavior\n\nIf no arguments are provided, the `setup()` function will still execute without error. However, it may not adequately inform users or package managers about the package's purpose or requirements. Hence, the utility of the `setup()` call without any parameters is limited.\n\n## Use Cases\n\nThis code snippet can be the starting point for anyone looking to package their Python project. It simplifies the initial setup for a user but needs additional arguments to fulfill functional requirements for distribution.\n\n### Project Structure\n\nIn a typical Python package project, this setup script (`setup.py`) would reside at the root of the project directory along with other essential files, such as:\n\n- `__init__.py`: Indicating the presence of a package.\n- README file: Providing details about the project.\n- LICENSE file: Specifying the licensing terms.\n- `requirements.txt`: Listing dependencies (especially for libraries).\n\n## Packaging and Distribution\n\nOnce this setup script is appropriately configured, you can use it to create a distributable package. This can be achieved via command-line instructions such as:\n\n```bash\npython setup.py sdist\n```\n\nThis command generates a source distribution in a `dist` directory, which can then be uploaded to package indexes like PyPI (Python Package Index).\n\n### Installation of the Package\n\nUsers can install the packaged project using pip, shorthand for \"Pip Installs Packages,\" with a command like:\n\n```bash\npip install package_name\n```\n\nThis command locates the specified package and installs it, utilizing the metadata defined in the setup script.\n\n## Conclusion\n\nIn summary, this small snippet encapsulates a significant aspect of Python package management. While it only provides a skeleton for packaging, it is integral for making a project installable and distributable. Properly populating the `setup()` function with details about the package can enhance usability and visibility in the Python ecosystem.",
    "filename": "python/packages/autogen-studio/setup.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\nThe provided source code implements a program that operates within a specified domain, performing a series of operations tailored towards a specific functionality. The overall structure includes variable definitions, function declarations, and execution logic.\n\n## Variable Definitions\nAt the outset, the code defines several variables that are crucial for the program's operations. These may include configurations, values for computational logic, and state indicators, which collectively set the context for the functions that follow. Proper management of these variables is key to ensuring that the intended operations are performed accurately.\n\n## Function Definitions\nThe code contains multiple functions, each serving a distinct purpose:\n\n- **Function A**: Responsible for initializing settings or preparing the environment. This function may take in parameters that determine how the rest of the code behaves.\n  \n- **Function B**: This function processes data or performs a computational task. Its outputs can be consumed by other components of the program, and it might include various checks or transformations of input data.\n  \n- **Function C**: Focused on outputting results or managing user interactions. This could encompass displaying information, writing to files, or sending data to external systems.\n\n### Function A: Initialization\nThis function lays the groundwork by establishing necessary configurations and setting variables to default values. It might check for prerequisites and handle any errors or exceptions during setup.\n\n### Function B: Data Processing\nFunction B plays a central role in the program. It likely encapsulates the main logic necessary to achieve the program's objectives, transforming input into meaningful output. Error handling in this function ensures robustness against invalid data.\n\n### Function C: User Interaction\nThis function may present results back to the user, or take input in a form that is user-friendly. It could also handle any command-line arguments or graphical user interface elements as needed.\n\n## Main Script Execution\nFollowing the function definitions, the script enters its execution phase. Typically, the script starts by calling the initialization function, preparing for further operations. Subsequent lines will call the core data processing function(s) as required. Logic here often includes conditionals to guide the flow of execution based on user inputs or configurations.\n\n## Control Flow and Logic\nThe control flow of the program is likely managed through conditionals and loops. Depending on the outcomes produced by various functions, the script directs execution towards different paths, such as repeating processes, handling errors, or completing the program.\n\n## Error Handling and Validation\nThe code should incorporate mechanisms for error handling, which are essential for preventing crashes and ensuring smooth operation. This includes checking for valid inputs and managing exceptions gracefully, providing users with helpful feedback in case of errors.\n\n## Output Generation\nNear the end of the script, there may be a segment dedicated to generating the final output. This could involve formatting the results in a clear, readable manner or aggregating data to present a comprehensive summary.\n\n## Conclusion\nIn summary, the code is structured to facilitate specific functionality through a series of defined functions that execute a set of operations, managing state through variable definitions and ensuring robustness through error handling. The execution flow is straightforward, guiding the process from initialization to final output, accommodating user interactions along the way.",
    "filename": "python/packages/autogen-studio/tests/__init__.py"
  },
  {
    "code": false,
    "content": "# MCP Callback Functions Test Suite Documentation\n\nThis document provides an overview of a test suite for the refactored callback functions related to the MCP (Message Communication Protocol). The tests are written using `pytest`, and utilize asyncio for asynchronous operations. The primary focus is on testing callback mechanisms, including error handling and successful operations.\n\n## Imports and Dependencies\n\nThe script begins with necessary imports:\n- **asyncio**: For handling asynchronous operations.\n- **pytest**: A testing framework to facilitate test writing and execution.\n- **uuid**: For unique identifier generation.\n- **unittest.mock**: To mock objects and functions for testing.\n- **datetime**: For handling date and time objects.\n- **mcp.types**: Importing various types related to MCP, including message parameters and results.\n- **autogenstudio.mcp.callbacks**: Importing callback handler functions that need testing. \n- **autogenstudio.mcp.wsbridge**: Importing the WebSocket bridge used for communication.\n\n## Mock Bridge Class\n\n### `MockBridge`\n\n```python\nclass MockBridge(MCPWebSocketBridge):\n    \"\"\"Mock bridge for testing callbacks\"\"\"\n```\n\nThis class simulates the `MCPWebSocketBridge`, serving as a stand-in to facilitate testing without requiring actual WebSocket connections. It implements crucial methods to capture events:\n\n- **`__init__`**: Initializes the mock bridge with a test session ID and empty lists for pending elicitation and events.\n- **`on_mcp_activity`**: Appends MCP activities to the `events` list for tracking.\n- **`on_elicitation_request`**: Logs elicitation requests similarly.\n\nThis mock aids in verifying that callback functions invoke the correct behaviors.\n\n## Test Class Definition\n\n### `TestMCPCallbacks`\n\n```python\nclass TestMCPCallbacks:\n    \"\"\"Test MCP callback functions\"\"\"\n```\n\nThis class encapsulates all test cases related to MCP callbacks. Each method corresponds to a specific test scenario, verifying different aspects of callback mechanisms.\n\n### Fixture for Mock Bridge\n\n```python\n@pytest.fixture\ndef mock_bridge(self):\n    \"\"\"Create a mock bridge\"\"\"\n```\n\nThis fixture instantiates a `MockBridge` before each test. It ensures a fresh environment for tests, avoiding any cross-contamination between scenarios.\n\n## Test Cases Overview\n\n### Message Handler Tests\n\n#### `test_message_handler_with_exception`\n\n```python\nasync def test_message_handler_with_exception(self, mock_bridge):\n    \"\"\"Test message handler with exception\"\"\"\n```\n\nThis test checks how the message handler reacts to an exception. It verifies that an error is logged correctly within the mock bridge when an exception is encountered.\n\n#### `test_message_handler_with_method_message`\n\n```python\nasync def test_message_handler_with_method_message(self, mock_bridge):\n    \"\"\"Test message handler with method-based message\"\"\"\n```\n\nThis scenario tests the handling of a message containing a method attribute. The test confirms that the correct activity is recorded in the event logs.\n\n#### `test_message_handler_with_other_message`\n\n```python\nasync def test_message_handler_with_other_message(self, mock_bridge):\n    \"\"\"Test message handler with other message types\"\"\"\n```\n\nThis case examines how the message handler manages other message types, particularly those without a method attribute. It checks for appropriate event logging.\n\n### Sampling Callback Tests\n\n#### `test_sampling_callback_success`\n\n```python\nasync def test_sampling_callback_success(self, mock_bridge):\n    \"\"\"Test sampling callback success case\"\"\"\n```\n\nThis method tests a successful sampling callback scenario. It verifies the expected output and checks for corresponding activity logs for both the request and response.\n\n#### `test_sampling_callback_exception`\n\n```python\nasync def test_sampling_callback_exception(self, mock_bridge):\n    \"\"\"Test sampling callback with exception\"\"\"\n```\n\nThis test verifies the error handling of a sampling callback when an exception occurs. It checks that an `ErrorData` result is returned and logged accurately.\n\n### Elicitation Callback Tests\n\n#### `test_elicitation_callback_success`\n\n```python\nasync def test_elicitation_callback_success(self, mock_bridge):\n    \"\"\"Test elicitation callback success case\"\"\"\n```\n\nIn this test, the success of the elicitation callback is assessed. The scenario includes simulating a user response and confirms that the resulting content matches expectations.\n\n#### `test_elicitation_callback_timeout`\n\n```python\nasync def test_elicitation_callback_timeout(self, mock_bridge):\n    \"\"\"Test elicitation callback timeout\"\"\"\n```\n\nThis method tests the behavior of the elicitation callback during a timeout situation. It ensure that the proper error handling and logging occur in such cases.\n\n#### `test_elicitation_callback_exception`\n\n```python\nasync def test_elicitation_callback_exception(self, mock_bridge):\n    \"\"\"Test elicitation callback with exception\"\"\"\n```\n\nThe final test evaluates how the elicitation callback handles exceptions during processing. Similar to previous tests, it verifies that an `ErrorData` result is returned and logged correctly.\n\n## Main Execution\n\nAt the end of the script, if it is run as the main program, it triggers the execution of `pytest` to run all defined tests:\n\n```python\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n```\n\nThis ensures that the tests can be run in an isolated environment, confirming the functionality and robustness of the MCP callback implementations. \n\nIn conclusion, this test suite provides comprehensive coverage for the MCP callback functions, ensuring they behave as expected under various scenarios, including error conditions.",
    "filename": "python/packages/autogen-studio/tests/mcp/test_mcp_callbacks.py"
  },
  {
    "code": false,
    "content": "# MCP Client Test Documentation\n\nThis document provides a detailed description of the Python test module designed to validate the implementation of the MCP (Multi-Channel Protocol) client. The code utilizes `pytest` for testing asynchronous methods and employs mocks to simulate the behavior of the MCP session and event handling.\n\n## Overview\n\nThe test module verifies the functionality of `MCPClient`, which interacts with an MCP session, handles various operations, and processes events. It uses mocking to simulate expected responses from the MCP server, ensuring the client behaves as intended. Each test verifies a specific aspect of the client's operations, including initialization, tool listing, and error handling.\n\n## Imports and Dependencies\n\nAt the beginning of the script, necessary modules and classes are imported, including:\n\n- **asyncio**: For asynchronous programming.\n- **pytest**: For testing frameworks and fixtures.\n- **unittest.mock**: For creating mocked objects and asynchronous mocks.\n- **datetime**: For handling date and time, although it is not directly used in testing.\n- **MCP client and types**: Classes that represent the MCP protocol's different entities like tools, resources, and prompts.\n\n## Mock Event Handler\n\n### Class: `MockEventHandler`\n\nThis class extends `MCPEventHandler` and serves to capture events triggered by the `MCPClient`. The purpose of this mock is to verify that the correct events are generated during testing. It maintains a list of events and provides methods to handle different types of MCP events such as:\n\n- **on_initialized**: Logs when the client is initialized.\n- **on_operation_result**: Logs the result of any operation performed by the client.\n- **on_operation_error**: Logs any errors encountered during operations.\n- **on_mcp_activity**: Logs general MCP activity.\n- **on_elicitation_request**: Logs requests for elicitation from the server.\n\n## Test Class\n\n### Class: `TestMCPClient`\n\nThis is the main test class that contains all test cases for the `MCPClient`. It is structured using pytest's testing framework and includes several asynchronous test methods.\n\n### Fixtures\n\n1. **mock_session**: \n   - Creates a mocked MCP session using `AsyncMock`.\n   - Sets up return values for various methods that simulate server responses when the client calls operations like initialization, listing tools, calling tools, listing resources, and handling prompts.\n\n2. **mock_event_handler**: \n   - Instantiates the previously defined `MockEventHandler` to capture events during testing.\n\n## Test Methods\n\n### Initialization Tests\n\n1. **`test_client_initialization`**:\n   - Initializes `MCPClient` with mock session and event handler.\n   - Asserts that the session, session ID, and event handler are correctly assigned and verifies that the client is not initialized at this point.\n\n2. **`test_client_initialize`**:\n   - Calls the `initialize` method on `MCPClient`.\n   - Asserts that the session's `initialize` method was called and checks the client's internal state to ensure it has been initialized correctly.\n\n### Tool Operations\n\n3. **`test_list_tools_operation`**:\n   - Tests the operation for listing tools available on the server.\n   - Validates that the `list_tools` method of the session is called and checks the event generated.\n\n4. **`test_call_tool_operation`**:\n   - Tests the invocation of a tool with the necessary arguments.\n   - Verifies the method call on the session and checks the resulting event.\n\n5. **`test_call_tool_missing_name`**:\n   - Simulates an error by invoking a tool without specifying its name.\n   - Checks that the expected error event is logged properly.\n\n### Error Handling\n\n6. **`test_unknown_operation`**:\n   - Tests how the client handles an unrecognized operation.\n   - Validates that the appropriate error event is logged when encountering an unknown operation.\n\n7. **`test_operation_exception_handling`**:\n   - Simulates an exception thrown by the `list_tools` method to check error handling.\n   - Asserts that the correct error event is captured.\n\n## Test Execution\n\nThe script concludes with a conditional check that executes the test cases when the script is run directly. The `pytest.main` function is called with command-line arguments, enabling detailed output during test execution.\n\n## Conclusion\n\nThis test module is a comprehensive framework for verifying the functionality and robustness of the `MCPClient` class within the MCP protocol implementation. By utilizing mocks and structured tests, it ensures that the client correctly handles various operations and error scenarios, reflecting the intended design and capabilities of the protocol.",
    "filename": "python/packages/autogen-studio/tests/mcp/test_mcp_client.py"
  },
  {
    "code": false,
    "content": "# MCP Integration Tests Documentation\n\nThis document provides a high-level overview and explanation of the MCP Integration Tests source code. The tests validate various components and functionalities of the MCP system, specifically focusing on the interaction between the system\u2019s client and WebSocket integration. The code structure comprises several test classes using the `pytest` framework and asynchronous programming with `asyncio`.\n\n## Classes Overview\n\n### TestMCPIntegration\nThe `TestMCPIntegration` class focuses on testing the overall operational flow within the MCP system between the WebSocket and the MCP client. It includes two main tests that assess the functionality and proper integration of key system components.\n\n#### 1. `test_end_to_end_operation_flow`\nThis test simulates a complete operation flow:\n- **Mocking WebSocket**: A mock WebSocket is created to imitate a connected client, with an asynchronous `send_json` method for handling messages.\n- **MCP WebSocket Bridge**: An instance of `MCPWebSocketBridge` is created to facilitate communication between the WebSocket and the MCP client.\n- **Mock MCP Session**: It initializes mock session methods for the MCP client, particularly for listing available tools. \n- **Client Initialization**: The mock client is set, and the operation flow is triggered by sending a JSON operation message requesting tools.\n- **Assertions**: The test validates that the appropriate methods were called and that the correct number of messages were sent through the WebSocket.\n\n#### 2. `test_elicitation_integration`\nThis test is focused on the elicitation flow:\n- **Mocking WebSocket**: Similar to the previous method, a mock WebSocket is created.\n- **Elicitation Callback**: A callback for elicitation is established using the bridge, ensuring that it connects properly to the bridge's pending elicitation dictionary.\n- **Assertions**: The test checks for the existence and type of `pending_elicitations` within the bridge.\n\n### TestMCPUtils\nThe `TestMCPUtils` class verifies several utility functions used throughout the MCP system. It contains tests for error extraction and JSON serialization.\n\n#### 1. Error Extraction Tests\nVarious tests examine the functionality of `extract_real_error`:\n- Tests handle simple errors, chained exceptions, and context exceptions to ensure complete error handling and accurate message retrieval.\n\n#### 2. JSON Serialization Tests\nThe tests for `serialize_for_json` confirm that various data types (simple, dictionaries, lists, and complex objects) can be serialized correctly:\n- Each test evaluates the output against expected results to guarantee consistent execution across different data inputs.\n\n#### 3. WebSocket Disconnect Validation\nTests are present to verify the accurate detection of WebSocket disconnections, including:\n- Regular and nested exceptions, addressing the robustness needed for handling different error scenarios.\n\n### TestMCPRouteIntegration\nThe `TestMCPRouteIntegration` class is dedicated to testing the integration with route components, particularly focusing on server parameters and their serialization.\n\n#### 1. `test_server_params_serialization`\nThis test checks the serialization capabilities of `StdioServerParams`:\n- It confirms that server parameters can be efficiently converted into a dictionary form while maintaining their structure.\n\n#### 2. `test_websocket_url_encoding`\nThis test examines the encoding process used when passing server parameters through the WebSocket:\n- Serializes the parameters to JSON, encodes them to base64, then decodes and verifies that the original data remains intact after encoding and decoding.\n\n## Conclusion\nThe source code consists of structured integration tests for an MCP (Multi-Component Platform). By leveraging `pytest` and `asyncio`, the tests assure seamless operation and error handling across various system components, notably the WebSocket and MCP client interactions. The tests are comprehensive, covering essential utility functions and routing mechanisms, contributing to a robust and reliable software system.",
    "filename": "python/packages/autogen-studio/tests/mcp/test_mcp_integration.py"
  },
  {
    "code": false,
    "content": "# MCP WebSocket Functionality Tests Documentation\n\nThis documentation describes the updated test suite for the MCP WebSocket functionality, which has been refactored to work with a new architecture. The tests are designed to verify that the newly implemented components (like `MCPClient` and `MCPWebSocketBridge`) perform as expected.\n\n## Overview\n\nThe code contains a series of unit tests utilizing the `pytest` framework, specifically designed to check the functionality of WebSocket operations, such as listing tools, calling tools, and managing resources through the MCP WebSocket API. Legacy tests have been replaced to reflect the updates in the architecture. The tests employ mocking techniques to isolate dependencies and simulate behavior.\n\n## Test Class Structure\n\n### `TestMCPWebSocketUpdated`\n\nThis class encapsulates the tests for the MCP WebSocket functionality. Each test utilizes fixtures to provide necessary mocks and setup required for testing. It performs operations on the client and verifies responses over WebSockets.\n\n### Fixtures\n\n#### `mock_server_params`\n\nCreates a mock instance of `StdioServerParams` for testing WebSocket connections. This includes command-line arguments and environment variables necessary for initiating the server.\n\n```python\n@pytest.fixture\ndef mock_server_params(self):\n    \"\"\"Create mock server parameters\"\"\"\n    ...\n```\n\n#### `mock_client_session`\n\nConstructs a mock for the MCP client session that includes various method stubs, such as `initialize`, `list_tools`, `call_tool`, etc. This fixture simulates the behavior of actual backend resources, allowing tests to verify interaction without needing actual resources.\n\n```python\n@pytest.fixture\ndef mock_client_session(self):\n    \"\"\"Create a mock MCP client session with all necessary methods\"\"\"\n    ...\n```\n\n#### `mock_websocket`\n\nCreates a mock WebSocket instance, initialized to simulate a connected state. This mock is used to verify messages being sent through the WebSocket.\n\n```python\n@pytest.fixture\ndef mock_websocket(self):\n    \"\"\"Create a mock WebSocket\"\"\"\n    ...\n```\n\n## Test Cases\n\n### `test_websocket_bridge_send_message`\n\nThis asynchronous test checks that messages can be sent through the `MCPWebSocketBridge`. It creates a bridge instance using the mocked WebSocket and sends a test message. The expectation is that the `send_json` method of the mocked WebSocket was called exactly once with the correct message.\n\n```python\n@pytest.mark.asyncio\nasync def test_websocket_bridge_send_message(self, mock_websocket):\n    \"\"\"Test WebSocket message sending via MCPWebSocketBridge\"\"\"\n    ...\n```\n\n### `test_mcp_client_list_tools_operation`\n\nThis test verifies the handling of the 'list_tools' operation by the `MCPClient`. It initializes the client and calls the operation, then checks to ensure that `list_tools` was invoked on the client session and that the correct response was sent via WebSocket.\n\n```python\n@pytest.mark.asyncio\nasync def test_mcp_client_list_tools_operation(self, mock_websocket, mock_client_session):\n    \"\"\"Test handling list_tools operation via MCPClient\"\"\"\n    ...\n```\n\n### `test_mcp_client_call_tool_operation`\n\nChecks the execution of a specific tool via the 'call_tool' operation. Similar to the previous test, it verifies that the appropriate session method is called with correct parameters and that the WebSocket sends the expected response.\n\n```python\n@pytest.mark.asyncio\nasync def test_mcp_client_call_tool_operation(self, mock_websocket, mock_client_session):\n    \"\"\"Test handling call_tool operation via MCPClient\"\"\"\n    ...\n```\n\n### `test_mcp_client_list_resources_operation`\n\nThis operation tests the 'list_resources' functionality, following the same pattern: the method is invoked, and checks are made to ensure correct calls were made and responses sent.\n\n```python\n@pytest.mark.asyncio\nasync def test_mcp_client_list_resources_operation(self, mock_websocket, mock_client_session):\n    \"\"\"Test handling list_resources operation via MCPClient\"\"\"\n    ...\n```\n\n### `test_mcp_client_read_resource_operation`\n\nVerifies the 'read_resource' operation on the MCP client, asserting that it correctly interacts with the mock session to fetch resource data and sends the appropriate result through the WebSocket.\n\n```python\n@pytest.mark.asyncio\nasync def test_mcp_client_read_resource_operation(self, mock_websocket, mock_client_session):\n    \"\"\"Test handling read_resource operation via MCPClient\"\"\"\n    ...\n```\n\n### `test_mcp_client_error_handling`\n\nTests the client\u2019s error handling capabilities by forcing an exception during the execution of an operation. It verifies that the correct error message is sent over the WebSocket when an error occurs.\n\n```python\n@pytest.mark.asyncio\nasync def test_mcp_client_error_handling(self, mock_websocket, mock_client_session):\n    \"\"\"Test error handling in MCP operations via MCPClient\"\"\"\n    ...\n```\n\n## Connection URL Generation and Active Sessions\n\n### `test_websocket_connection_url_generation`\n\nThis test checks the generation of WebSocket connection URLs, ensuring that the serialized server parameters are correctly encoded and can be decoded back to their original structure.\n\n```python\ndef test_websocket_connection_url_generation(self, mock_server_params):\n    \"\"\"Test WebSocket connection URL generation (preserved from original tests)\"\"\"\n    ...\n```\n\n### `test_active_sessions_structure`\n\nExplores the internal structure of active sessions, verifying that sessions can be added to a dictionary and asserting the expected data structure is maintained.\n\n```python\ndef test_active_sessions_structure(self):\n    \"\"\"Test active sessions data structure (preserved from original tests)\"\"\"\n    ...\n```\n\n## `TestMCPRouteIntegrationUpdated`\n\n### Integration Tests for Routes\n\nThis class details integration tests focused on the MCP routes, ensuring that the associated components such as the router and WebSocket connection requests are correctly implemented and functional.\n\n#### `test_router_exists`\n\nConfirms that the MCP router exists and is correctly configured, validating the type of router being used.\n\n```python\ndef test_router_exists(self):\n    \"\"\"Test that the MCP router exists and is properly configured\"\"\"\n    ...\n```\n\n#### `test_create_websocket_connection_request_model`\n\nThis test creates a WebSocket connection request to validate that the proper request model is correctly structured.\n\n```python\ndef test_create_websocket_connection_request_model(self):\n    \"\"\"Test the request model for creating WebSocket connections\"\"\"\n    ...\n```\n\n## Conclusion\n\nThis updated test suite for the MCP WebSocket functionality is thorough, ensuring that all aspects of the WebSocket communication, client operations, and error handling behavior are covered. It effectively uses mocking for isolation, focusing on the expected behaviors and interactions with the underlying architectural components. The tests are structured to provide clear expectations about the functionality, lending confidence to the robustness of the code.",
    "filename": "python/packages/autogen-studio/tests/mcp/test_mcp_websocket.py"
  },
  {
    "code": false,
    "content": "# MCPWebSocketBridge Unit Tests Documentation\n\nThis document provides an overview of the unit tests implemented for the `MCPWebSocketBridge` class. The tests utilize the `pytest` framework along with asynchronous capabilities from `asyncio`. They verify the functionality of the `MCPWebSocketBridge` class using a mock WebSocket to simulate the necessary interactions.\n\n## Overview of the Code\n\nThe code includes a series of tests aimed at ensuring that the `MCPWebSocketBridge` functions as intended when communicating over a WebSocket. The tests cover various scenarios, including initialization, message handling, error conditions, and different message types.\n\n## MockWebSocket Class\n\n```python\nclass MockWebSocket:\n    \"\"\"Mock WebSocket for testing\"\"\"\n    ...\n```\n\n### Purpose\n\n`MockWebSocket` serves as a dummy implementation of a WebSocket connection, enabling the simulation of sending and receiving messages without needing a real WebSocket server. It tracks messages sent and messages that are ready to be received.\n\n### Key Methods\n\n- **send_json(data)**: Simulates sending a JSON message by appending it to the `messages_sent` list.\n- **receive_text()**: Mimics receiving a text message, incrementing the `receive_index` to keep track of which message to return. Raises an exception if there are no messages left to receive.\n- **add_message(message)**: Adds a message to the list of messages to be received, ensuring it is in JSON string format.\n\n## Test Class Setup\n\n```python\nclass TestMCPWebSocketBridge:\n    \"\"\"Test the MCPWebSocketBridge class\"\"\"\n    ...\n```\n\n### Purpose\n\nThis class encapsulates all the unit tests for the `MCPWebSocketBridge`. It uses fixtures to set up the environment and maintain separation of different test scenarios.\n\n### Test Fixtures\n\n- **mock_websocket**: Creates an instance of `MockWebSocket` for use in the tests.\n- **bridge**: Instantiates the `MCPWebSocketBridge` using the mock WebSocket, allowing tests to interact with the bridge without dependency on an actual WebSocket connection.\n\n## Test Cases\n\n### Initialization Test\n\n```python\n@pytest.mark.asyncio\nasync def test_bridge_initialization(self, bridge, mock_websocket):\n    \"\"\"Test bridge initialization\"\"\"\n    ...\n```\n\n#### Purpose\n\nVerifies that the `MCPWebSocketBridge` initializes correctly with the expected parameters, including the WebSocket instance, session ID, and initial conditions.\n\n### Message Sending Test\n\n```python\n@pytest.mark.asyncio\nasync def test_send_message(self, bridge, mock_websocket):\n    \"\"\"Test message sending through WebSocket\"\"\"\n    ...\n```\n\n#### Purpose\n\nChecks if a message can be sent through the WebSocket and that the correct data format is used.\n\n### Event Handling Tests\n\nSeveral tests are implemented to verify the handling of different event types.\n\n#### On Initialized Event\n\n```python\n@pytest.mark.asyncio\nasync def test_on_initialized_event(self, bridge, mock_websocket):\n    \"\"\"Test on_initialized event handler\"\"\"\n    ...\n```\n\n#### On Operation Result Event\n\n```python\n@pytest.mark.asyncio\nasync def test_on_operation_result_event(self, bridge, mock_websocket):\n    \"\"\"Test on_operation_result event handler\"\"\"\n    ...\n```\n\n#### On Operation Error Event\n\n```python\n@pytest.mark.asyncio\nasync def test_on_operation_error_event(self, bridge, mock_websocket):\n    \"\"\"Test on_operation_error event handler\"\"\"\n    ...\n```\n\n#### On Elicitation Request Event\n\n```python\n@pytest.mark.asyncio\nasync def test_on_elicitation_request_event(self, bridge, mock_websocket):\n    \"\"\"Test on_elicitation_request event handler\"\"\"\n    ...\n```\n\n### MCP Client Management Test\n\n```python\n@pytest.mark.asyncio\nasync def test_set_mcp_client(self, bridge):\n    \"\"\"Test setting MCP client\"\"\"\n    ...\n```\n\n#### Purpose\n\nTests the ability to set an instance of the `MCPClient` on the bridge, asserting that the internal reference now points to the new client.\n\n### Message Handling Tests\n\nSeveral tests validate the bridge's ability to process messages received over the WebSocket.\n\n#### Handling Ping Message\n\n```python\n@pytest.mark.asyncio\nasync def test_handle_ping_message(self, bridge, mock_websocket):\n    \"\"\"Test handling ping message\"\"\"\n    ...\n```\n\n#### Handling Operation Messages\n\nTwo scenarios are tested here: one with a client and one without.\n\n- **Without Client**: Confirms that an error is produced when attempting to handle an operation message without an initialized MCP client.\n- **With Client**: Ensures that the WebSocket communicates the message as expected and tasks are created properly.\n\n### Handling Elicitation Responses\n\nTests for processing user responses to elicitation requests, checking both acceptance and declination actions.\n\n### Message Loop Tests\n\nThese tests examine the overall behavior of the message processing loop in the bridge.\n\n#### Valid JSON Handling\n\n```python\n@pytest.mark.asyncio\nasync def test_message_loop_with_valid_json(self, bridge, mock_websocket):\n    \"\"\"Test message loop with valid JSON messages\"\"\"\n    ...\n```\n\n#### Invalid JSON Handling\n\n```python\n@pytest.mark.asyncio\nasync def test_message_loop_with_invalid_json(self, bridge, mock_websocket):\n    \"\"\"Test message loop with invalid JSON\"\"\"\n    ...\n```\n\n### Bridge Stopping Test\n\nTests the ability to stop the bridge correctly, ensuring that its running state is toggled appropriately.\n\n```python\ndef test_stop_bridge(self, bridge):\n    \"\"\"Test stopping the bridge\"\"\"\n    ...\n```\n\n## Conclusion\n\nThis code serves as a comprehensive unit test suite for the `MCPWebSocketBridge`. It covers various aspects of its functionality, ensuring robust error handling and interactions with the WebSocket. By utilizing the `pytest` framework, the tests facilitate easy extension and maintenance in the future. \n\nTo execute the tests, simply run the file as a script, which invokes the `pytest` testing framework automatically.",
    "filename": "python/packages/autogen-studio/tests/mcp/test_mcp_wsbridge.py"
  },
  {
    "code": false,
    "content": "# Documentation for `test_llm_call_event_message.py`\n\n## Overview\n\nThis script is a test case designed to validate the functionality of the `LLMCallEventMessage` class from the `autogenstudio.datamodel.types` module. It utilizes the `pytest` framework, allowing for organized testing of individual components and their behaviors. The primary focus is on the inner functions of the `LLMCallEventMessage` class, ensuring they return expected results and correctly handle unsupported operations.\n\n## Imports\n\n```python\nimport pytest\nfrom autogenstudio.datamodel.types import LLMCallEventMessage\n```\n\n- **pytest**: A testing framework for Python that allows for easy and scalable test creation. It provides rich assertions and reporting capabilities.\n- **LLMCallEventMessage**: A data model class presumably representing a specific type of message in a system, which includes various methods intended to manipulate or convert the message content.\n\n## Test Function\n\n### `test_LLMCallEventMessage_inner_funcs`\n\n```python\ndef test_LLMCallEventMessage_inner_funcs():\n    \"\"\"Test the inner functions of LLMCallEventMessage\"\"\"\n```\n\n- The main purpose of this function is to test the inner functionality of the `LLMCallEventMessage` class.\n- The docstring succinctly describes the intent of the test, confirming it aims to assess internal methods' behaviors.\n\n### Mock Object Creation\n\n```python\n    message = LLMCallEventMessage(\n        content=\"Test message\"\n    )\n```\n\n- A mock instance of `LLMCallEventMessage` is created with a predefined `content` string, `\"Test message\"`. \n- This setup allows the test to interact with a control scenario, making it easier to verify the accuracy of the class methods.\n\n## Method Testing\n\n### Testing `to_text` and `to_model_text`\n\n```python\n    assert message.to_text() == \"Test message\"\n    assert message.to_model_text() == \"Test message\"\n```\n\n- Two assertions are made here:\n  - `to_text()`: Expected to return the original content of the message, confirming that this method correctly retrieves the message text.\n  - `to_model_text()`: Similarly, it should return the same string, validating that this method can convert the message content appropriately.\n- Both assertions test the expected outputs against the input provided during object initialization.\n\n### Handling Unsupported Operations\n\n```python\n    with pytest.raises(NotImplementedError, match=\"This message type is not supported.\"):\n        message.to_model_message()\n```\n\n- This block tests the behavior of the `to_model_message()` method, which is expected to raise a `NotImplementedError`.\n- The assertion ensures that when the method is invoked on the mock message object, it raises the correct exception with an appropriate error message, indicating that this specific message conversion is not implemented.\n\n## Conclusion\n\nThis test case provides a clear and concise validation of the `LLMCallEventMessage` class's inner functions. By employing both successful assertions and error handling, the functionality and robustness of the message handling are thoroughly examined. The use of `pytest` allows for quick identification of potential issues, making the code maintainable and reliable over time. The structured testing approach ensures that future modifications to the `LLMCallEventMessage` class can be evaluated against these predefined behaviors, enhancing code quality assurance.",
    "filename": "python/packages/autogen-studio/tests/test_datamodel_types.py"
  },
  {
    "code": false,
    "content": "# Documentation for Database Operations Testing\n\nThis module contains tests designed to verify the correct behavior of database operations using SQLModel and pytest. It utilizes a temporary SQLite database for isolated testing to ensure code functionality without affecting any production database.\n\n## Overview\n\nThe tests focus on performing essential database operations such as basic setup, entity creation, upsert (update or insert), deletion, cascade deletion, and database initialization scenarios. Several fixtures are defined to prepare the test environment, including a temporary database instance and a sample team entity for testing.\n\n### Dependencies\n\nThe test framework relies on the following libraries:\n- `asyncio`: For asynchronous execution of database cleanup tasks.\n- `pytest`: For testing functionality.\n- `sqlmodel`: For defining and interacting with the database.\n\nThe implementation includes importing crucial components from the autogen studio and agent chat modules that are integral for the tests.\n\n## Fixtures\n\n### `test_db`\n\n```python\n@pytest.fixture\ndef test_db(tmp_path) -> Generator[DatabaseManager, None, None]:\n```\n\nThis fixture sets up a temporary SQLite database instance for testing. It uses the provided `tmp_path` to create a unique database file, initializes the database schema without upgrading existing tables, and safely cleans up after tests by closing the database connection.\n\n### `test_user`\n\n```python\n@pytest.fixture\ndef test_user() -> str:\n```\n\nThis fixture provides a simple string representation of a test user's email address, which is used in the tests to create entities associated with a user.\n\n### `sample_team`\n\n```python\n@pytest.fixture\ndef sample_team(test_user: str) -> Team:\n```\n\nThis fixture creates a sample team consisting of an `AssistantAgent` using OpenAI's chat completion client. The team is configured to use a round-robin chat strategy, with a specific termination condition for agent responses.\n\n## Test Class: `TestDatabaseOperations`\n\nThis class contains several methods for testing various database operations. Each method represents a separate test scenario.\n\n### Test: `test_basic_setup`\n\n```python\ndef test_basic_setup(self, test_db: DatabaseManager):\n```\n\nThis test checks the basic setup and connection to the database. It executes simple SQL queries to verify successful interaction with the database and ensure the connection is operational.\n\n### Test: `test_basic_entity_creation`\n\n```python\ndef test_basic_entity_creation(self, test_db: DatabaseManager, sample_team: Team):\n```\n\nThis test assesses the ability to create entities in the database. A sample team entity is upserted (inserted or updated), and the test verifies that the operation was successful and that the entity is retrievable from the database.\n\n### Test: `test_upsert_operations`\n\n```python\ndef test_upsert_operations(self, test_db: DatabaseManager, sample_team: Team):\n```\n\nThis test evaluates the upsert functionality for both insert and update operations. Initially, it checks for successful creation and then conducts an update to verify that the changes are correctly persisted in the database.\n\n### Test: `test_delete_operations`\n\n```python\ndef test_delete_operations(self, test_db: DatabaseManager, sample_team: Team):\n```\n\nThis test examines the delete functionality for various cases. After inserting a team entity, it deletes the team and asserts that the entity does not exist in the database post-deletion.\n\n### Test: `test_cascade_delete`\n\n```python\ndef test_cascade_delete(self, test_db: DatabaseManager, test_user: str):\n```\n\nThis test focuses on verifying the cascade delete functionality across multiple levels of related entities (Run and Message) when a parent entity (Team) is deleted. The test ensures that associated records are also removed correctly.\n\n### Test: `test_initialize_database_scenarios`\n\n```python\ndef test_initialize_database_scenarios(self, tmp_path, monkeypatch):\n```\n\nThis test verifies various scenarios related to database initialization, including handling schema migrations and ensuring that the database can be initialized correctly under different configurations. It uses mocking to manipulate schema checking behavior during testing.\n\n## Conclusion\n\nThis module comprehensively tests different database operations such as creation, update, deletion, and initialization using a fully isolated setup. Each test case is designed to validate crucial functionality, ensuring that the underlying database management system behaves as expected under various conditions. This paves the way for stable and reliable database interactions in the application.",
    "filename": "python/packages/autogen-studio/tests/test_db_manager.py"
  },
  {
    "code": false,
    "content": "# LiteStudio Test Suite Documentation\n\nThis document provides a high-level overview of the test suite for the `LiteStudio` class, which is part of the `autogenstudio.lite` module. The tests are meant to validate the behavior of `LiteStudio` across various scenarios, including initialization with different team data types, configuration, and starting/stopping the studio.\n\n## Test Fixtures\n\n### `sample_team_file`\n```python\n@pytest.fixture\ndef sample_team_file(tmp_path: Path) -> str:\n    ...\n```\nThis fixture creates a temporary JSON file representing a team specification. The data structure includes the `type`, `name`, `description`, and `agents`. It writes this data to a file and returns the file path, which is used in various tests to initialize `LiteStudio` with valid team data.\n\n### `sample_team_object`\n```python\n@pytest.fixture\ndef sample_team_object() -> Dict[str, Union[str, List[Any]]]:\n    ...\n```\nThis fixture provides a basic dictionary structure representing a team without the need for file I/O. It specifies the `type`, `name`, and an empty list for `agents`. It is useful for tests that need to directly pass a team object to `LiteStudio`.\n\n## Team Loading and Initialization Tests\n\n### Loading Team from File\n```python\ndef load_team_from_file(file_path: str) -> Dict[str, Any]:\n    ...\n```\nThis helper function reads team data from the specified JSON file path and returns it as a dictionary. It abstracts file reading logic, simplifying assertions in the tests related to file content verification.\n\n### Initialization with File\n```python\ndef test_init_with_file_team(sample_team_file: str) -> None:\n    ...\n```\nThis test validates that `LiteStudio` can be initialized using a valid team file. It checks that all expected parameters (like `host`, `port`, and `session_name`) are set correctly and verifies the existence of the team file.\n\n### Initialization with Team Object\n```python\ndef test_init_with_team_object(sample_team_object: Dict[str, Union[str, List[Any]]]) -> None:\n    ...\n```\nThis test ensures that `LiteStudio` can be initialized with a dictionary representing a team without writing to a file. It checks for temporary file creation and validates that the file's contents match the input team object.\n\n### Initialization with Default Team\n```python\n@patch('autogenstudio.gallery.builder.create_default_lite_team')\ndef test_init_with_no_team(mock_create_default: MagicMock) -> None:\n    ...\n```\nThis test checks the behavior of `LiteStudio` when no team configuration is provided. It utilizes mocking to simulate the creation of a default team and verifies that appropriate default parameters are assigned.\n\n### Initialization with Invalid File\n```python\ndef test_init_with_invalid_file() -> None:\n    ...\n```\nThis scenario tests `LiteStudio`\u2019s response to an invalid file path. It expects to throw a `FileNotFoundError`, confirming that error handling is accurately implemented.\n\n## Environment Setup and Execution Tests\n\n### Environment Variable Setup\n```python\ndef test_setup_environment(sample_team_file: str) -> None:\n    ...\n```\nThis test assesses the environment variable configuration generated by `LiteStudio`. It asserts the presence and correctness of key environment variables related to the studio's operation.\n\n### Starting in Foreground Mode\n```python\n@patch('uvicorn.run')\n@patch('threading.Thread')\n@patch('webbrowser.open')\ndef test_start_foreground(mock_browser: MagicMock, mock_thread: MagicMock, mock_uvicorn: MagicMock, sample_team_file: str) -> None:\n    ...\n```\nThis test checks if `LiteStudio` can initiate operation in the foreground, verifying that the expected functions for running the server and opening the browser are called correctly.\n\n### Starting in Background Mode\n```python\n@patch('uvicorn.run')\n@patch('threading.Thread')\ndef test_start_background(mock_thread_class: MagicMock, mock_uvicorn: MagicMock, sample_team_file: str) -> None:\n    ...\n```\nThis test similarly verifies the successful startup of `LiteStudio` in the background, confirming that threads are created and started as expected.\n\n## Additional Functionality Tests\n\n### Context Manager Functionality\n```python\ndef test_context_manager(sample_team_file: str) -> None:\n    ...\n```\nThis test evaluates the context manager behavior of `LiteStudio`, ensuring that resources are initialized when entering a context and properly cleaned up upon exit.\n\n### Stopping the Studio\n```python\ndef test_stop_with_server_thread(sample_team_file: str) -> None:\n    ...\n```\nThis test assesses the stop functionality when a server thread is active. It checks that the thread joins correctly, ensuring graceful shutdown.\n\n### Port Shutdown\n```python\n@patch('subprocess.run')\ndef test_shutdown_port(mock_subprocess: MagicMock) -> None:\n    ...\n```\nThis test verifies that the port shutdown functionality works correctly by simulating external process termination on a specific port.\n\n### Error Handling for Double Start\n```python\n@patch('uvicorn.run')\ndef test_start_twice_raises_error(mock_uvicorn: MagicMock, sample_team_file: str) -> None:\n    ...\n```\nThe test checks the error response when an attempt is made to start an already running instance of `LiteStudio`, ensuring that the appropriate `RuntimeError` is raised.\n\n## Serialization Tests\n\n### Initialization with Serialization Capabilities\n```python\ndef test_init_with_team_object_with_serialization_methods() -> None:\n    ...\n```\nThis test investigates the initialization of `LiteStudio` with objects capable of serialization. It ensures that the serialized data is correctly captured in the created team file.\n\n### Initialization with Pydantic Model\n```python\ndef test_init_with_team_object_model_dump() -> None:\n    ...\n```\nThis test focuses on using objects following Pydantic v2 style, verifying that they are handled correctly during initialization by confirming the expected file contents.\n\n### Handling Unsupported Types\n```python\ndef test_init_with_unsupported_team_object() -> None:\n    ...\n```\nThis scenario tests how `LiteStudio` reacts to unsupported team objects. It confirms that an appropriate error message is raised when serialization fails.\n\n### Initialization with Path Objects\n```python\ndef test_init_with_path_object(tmp_path: Path) -> None:\n    ...\n```\nThis test checks if `LiteStudio` can accept `Path` objects directly, verifying that file paths are handled properly and the content is verified.\n\n### Initialization with Component Model\n```python\ndef test_init_with_component_model() -> None:\n    ...\n```\nThis test ensures that `LiteStudio` properly initializes when given a team defined by a mock component model, maintaining the expected serialization process.\n\nBy structuring the tests in this way, the suite covers a range of functionalities and edge cases that `LiteStudio` may encounter, helping ensure robust and reliable behavior for users of the API.",
    "filename": "python/packages/autogen-studio/tests/test_lite_studio.py"
  },
  {
    "code": false,
    "content": "# Test Suite for TeamManager\n\nThis document provides an overview of the test suite designed to validate the functionality of the `TeamManager` class from the `autogenstudio` module. The tests leverage `pytest` for structure and `unittest.mock` for mocking dependencies, aimed at ensuring that the TeamManager operates correctly when handling team configurations and execution tasks.\n\n## Overview\n\nThe test suite contains several fixtures and functions to conduct a series of tests focused on loading configurations, creating teams, and running asynchronous tasks. The configurations are mainly concerned with teams defined using agents within the chat environment. By utilizing both JSON and YAML formats for configuration files, the tests guarantee flexibility in handling various data formats.\n\n## Fixtures\n\n### sample_config\n\nThe `sample_config` fixture establishes a preliminary setup that creates an agent and its associated team. The agent in this case is the `AssistantAgent`, using an `OpenAIChatCompletionClient` with a model specified. This fixture constructs a `RoundRobinGroupChat` team, which is aimed at managing chat agents in a round-robin fashion. \n\nThis configuration is then dumped into a dictionary format, making it retrievable for subsequent tests, ensuring that all tests run with a consistent configuration.\n\n### config_file\n\nThe `config_file` fixture creates a temporary JSON configuration file utilizing the `sample_config` fixture. This file serves as a testing resource, allowing individual tests to validate file loading functionality. The temporary nature ensures that the test environment remains clean and is disposed of after the tests are completed.\n\n### config_dir\n\nThe `config_dir` fixture extends the capabilities of `config_file` by creating a temporary directory containing multiple configuration files. It includes both a JSON and a YAML file derived from `sample_config`. By modifying a field in the YAML file, the fixture enables the tests to verify that different formats can be handled correctly, allowing for thorough validation of the loading functions.\n\n## Test Class: TestTeamManager\n\nThis class encapsulates several asynchronous tests focusing on the `TeamManager` class function.\n\n### test_load_from_file\n\nThis method tests the loading of configuration data from a JSON file created by the `config_file` fixture. After asserting that the loaded configuration matches the expected `sample_config`, the test checks for appropriate error handling. It ensures that a `FileNotFoundError` is raised when a nonexistent file is referenced and verifies that unsupported file formats trigger a `ValueError`.\n\n### test_load_from_directory\n\nIn this test, the `TeamManager.load_from_directory()` method is examined to ensure it can successfully load configurations from a directory populated by the `config_dir` fixture. The test confirms that two configurations are loaded, checking if one of the loaded teams contains a label that matches either \"RoundRobinGroupChat\" or \"YamlTeam\". This verifies that diverse formats are recognized by the loading function.\n\n### test_create_team\n\nThis test evaluates the `TeamManager`'s ability to create a team based on a configuration dictionary. It mocks the `Team.load_component` method to confirm that the `_create_team()` method correctly constructs a team using the provided configuration, and it verifies that the mock is invoked with the expected parameters. This ensures that proper team instantiation is performed without reliance on the actual `load_component` implementation.\n\n### test_run_stream\n\nThis test case focuses on the invocation of the `run_stream()` method of the `TeamManager`. Similar to the previous tests, it mocks the `_create_team` method and the `run_stream` operation of a team. This allows effective testing of the asynchronous streaming of results. Messages are generated and collected to ensure that all expected outputs, including a final task result, are accurately yielded and returned. It verifies consistency in the process by checking the number of streamed messages and confirming that the last message corresponds to a `TeamResult`.\n\n## Conclusion\n\nThis test suite provides comprehensive coverage for the `TeamManager` class in the `autogenstudio` module, ensuring that the core functionalities related to team configuration loading, team creation, and the asynchronous execution of tasks through streams are rigorously assessed. By utilizing both fixtures and mocks, the tests guarantee that various scenarios are handled appropriately, reinforcing the reliability and robustness of the underlying system.",
    "filename": "python/packages/autogen-studio/tests/test_team_manager.py"
  },
  {
    "content": "# test-utils",
    "filename": "python/packages/autogen-test-utils/README.md"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code defines a set of classes that leverage asynchronous programming patterns to create various agents for message handling, including `LoopbackAgent`, `CascadingAgent`, and components to manage configuration in a structured manner. The code utilizes Python's asyncio library for concurrency, dataclasses for structured data storage, and Pydantic for data validation and settings management.\n\n---\n\n## Message Types and Data Classes\n\n### MessageType Placeholder\n\n```python\n@dataclass\nclass MessageType: ...\n```\n\nThe `MessageType` class serves as a placeholder for defining message types used within different agents. It is a dataclass, though no attributes are currently defined. This class would typically represent messages that agents can handle.\n\n### CascadingMessageType\n\n```python\n@dataclass\nclass CascadingMessageType:\n    round: int\n```\n\nThe `CascadingMessageType` class holds a single attribute, `round`, which is an integer. This class is used specifically in the `CascadingAgent` to manage the progression through multiple rounds of message handling.\n\n### ContentMessage\n\n```python\n@dataclass\nclass ContentMessage:\n    content: str\n```\n\n`ContentMessage` has a single attribute `content`, which is a string. This dataclass is used to encapsulate messages containing textual information, allowing agents to handle generic content.\n\n---\n\n## Agent Classes\n\n### LoopbackAgent\n\n```python\nclass LoopbackAgent(RoutedAgent):\n    ...\n```\n\nThe `LoopbackAgent` extends `RoutedAgent` and is designed to process incoming messages. It tracks the number of calls made to its message handler and stores received messages in a list. \n\n- **Initialization**: The constructor initializes the number of calls and an empty list for received messages, along with an event for asynchronous notification.\n  \n- **Message Handling**: The asynchronous method `on_new_message` updates the call count, stores the message, and sets an event to signal that a new message has been processed. It returns the received message to its caller.\n\n### LoopbackAgentWithDefaultSubscription\n\n```python\n@default_subscription\nclass LoopbackAgentWithDefaultSubscription(LoopbackAgent): ...\n```\n\n`LoopbackAgentWithDefaultSubscription` is a subclass of `LoopbackAgent` decorated with `@default_subscription`, which implies it has a predefined subscription behavior. This enhances the `LoopbackAgent` by automatically subscribing to a topic without additional configuration.\n\n### CascadingAgent\n\n```python\n@default_subscription\nclass CascadingAgent(RoutedAgent):\n    ...\n```\n\nSimilar to `LoopbackAgent`, `CascadingAgent` is designed to handle messages. However, it specializes in cascading behavior across rounds:\n\n- **Initialization**: Takes `max_rounds` as a parameter, setting it to limit how many times the agent can cascade messages.\n\n- **Message Handling**: The `on_new_message` method processes messages of type `CascadingMessageType`. It increments the call count and checks the current round. If it hasn\u2019t reached the maximum, it publishes a new message for the next round, effectively creating a chain of messages until the round limit is reached.\n\n### NoopAgent\n\n```python\nclass NoopAgent(BaseAgent):\n    ...\n```\n\nThe `NoopAgent` is a basic agent that inherits from `BaseAgent`. Its primary function is to exist without performing any operations:\n\n- **Message Handling**: It implements `on_message_impl` but raises a `NotImplementedError`, serving as a placeholder for future implementations or for deriving other functionality.\n\n---\n\n## Component Classes\n\n### MyInnerConfig\n\n```python\nclass MyInnerConfig(BaseModel):\n    inner_message: str\n```\n\n`MyInnerConfig` is a Pydantic model defining the configuration schema for an inner component. It validates that `inner_message` is a string, providing structured settings for inner components.\n\n### MyInnerComponent\n\n```python\nclass MyInnerComponent(ComponentBase[MyInnerConfig], Component[MyInnerConfig]):\n    ...\n```\n\n`MyInnerComponent` is a component class that uses the `MyInnerConfig` schema for its configuration:\n\n- **Initialization**: It constructs an instance using the inner message provided.\n\n- **Configuration Management**: The `_to_config` method returns an instance of `MyInnerConfig`, and `_from_config` takes a configuration object to recreate an instance of `MyInnerComponent`.\n\n### MyOuterConfig\n\n```python\nclass MyOuterConfig(BaseModel):\n    outer_message: str\n    inner_class: ComponentModel\n```\n\nThis Pydantic model defines the configuration schema for an outer component. It contains `outer_message` as a string and `inner_class`, which is a model representing an inner component.\n\n### MyOuterComponent\n\n```python\nclass MyOuterComponent(ComponentBase[MyOuterConfig], Component[MyOuterConfig]):\n    ...\n```\n\nThe `MyOuterComponent` class manages both its own configuration and an instance of `MyInnerComponent`. \n\n- **Initialization**: It takes an outer message and an instance of `MyInnerComponent` as arguments.\n\n- **Configuration Management**: Similarly, it provides methods for converting to and from configuration, allowing structured and validated settings to be reapplied to the component.\n\n--- \n\n## Conclusion\n\nThe code illustrates the architecture of asynchronous agents and structured components for message passing in a system. Each agent has distinct responsibilities, managing different types of messages while complying with a defined structure and validation framework. The combination of asyncio, dataclasses, and Pydantic promotes efficient, type-safe, and organized code, making it suitable for scalable applications.",
    "filename": "python/packages/autogen-test-utils/src/autogen_test_utils/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation for OpenTelemetry Custom Span Exporter\n\n## Overview\nThis Python script implements a custom span exporter for use with OpenTelemetry, a set of APIs, libraries, and agents aimed at observability in software applications. The custom exporter, `MyTestExporter`, captures exported spans for testing purposes. Additionally, there is a function to create a tracer provider configured with this exporter.\n\n## Classes\n\n### MyTestExporter\n`MyTestExporter` is a custom class that extends the `SpanExporter` interface provided by OpenTelemetry. This class is designed to store spans that are exported for further examination or testing.\n\n#### Attributes\n- **exported_spans**: A list that holds instances of `ReadableSpan`. This list is initialized as empty when an instance of `MyTestExporter` is created and will be populated with exported spans.\n\n#### Methods\n1. **__init__()**\n    - Initializes a new instance of `MyTestExporter` and initializes the `exported_spans` list.\n  \n2. **export(spans: Sequence[ReadableSpan]) -> SpanExportResult**\n    - Takes a sequence of `ReadableSpan` objects as input and appends them to the `exported_spans` list.\n    - Returns a `SpanExportResult` indicating the success of the operation.\n  \n3. **shutdown() -> None**\n    - A placeholder method that currently does nothing. It can be overridden in the future for cleanup tasks, if necessary.\n\n4. **clear() -> None**\n    - Clears the `exported_spans` list, effectively resetting the storage of exported spans.\n  \n5. **get_exported_spans() -> List[ReadableSpan]**\n    - Returns the current list of `exported_spans`. This allows for inspection of spans that have been captured by the exporter.\n\n## Function\n\n### get_test_tracer_provider\nThe function `get_test_tracer_provider` is responsible for creating a tracer provider that utilizes the custom exporter.\n\n#### Parameters\n- **exporter (MyTestExporter)**: An instance of `MyTestExporter`, which will be used to handle the exporting of spans.\n\n#### Returns\n- **TracerProvider**: A newly created instance of `TracerProvider` that has the `SimpleSpanProcessor` configured with the provided exporter.\n\n#### Functionality\n- The function begins by instantiating a new `TracerProvider`. \n- It configures the tracer provider to utilize the `SimpleSpanProcessor` that processes spans using the specified `exporter`.\n- The configured `TracerProvider` allows for span creation and tracing activities in an application, using the custom exporter to store exported spans for testing.\n\n## Summary\nIn summary, this script defines a custom span exporter, `MyTestExporter`, which captures spans exported during tracing and exposes methods for manipulation and retrieval of those spans. The provided function, `get_test_tracer_provider`, allows developers to create a tracer provider that integrates this custom exporter, thereby enabling testing and analysis of tracing information generated in an application. This setup is useful in scenarios where developers need to validate and test tracing implementations without relying on external storage or networked backends.",
    "filename": "python/packages/autogen-test-utils/src/autogen_test_utils/telemetry_test_utils.py"
  },
  {
    "code": false,
    "content": "It appears there was no source code provided for analysis. Please share the source code you'd like me to review, and I'll be happy to create a clear, high-level description of its functionality.",
    "filename": "python/packages/autogen-test-utils/tests/test.py"
  },
  {
    "content": "# component-schema-gen\n\nThis is a tool to generate schema for built in components.\n\nSimply run `gen-component-schema` and it will print the schema to be used.",
    "filename": "python/packages/component-schema-gen/README.md"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThis document analyzes a block of source code, providing a high-level summary of its functionality and breaking down the components defined within it. The analysis covers the purpose of functions, classes, or the flow of the script step-by-step.\n\n## Purpose of the Code\n\nThe code is designed to perform a specific set of tasks, likely aimed at processing data, interfacing with a system, or automating a repetitive operation. Each section or component plays a vital role in achieving the overall goal. \n\n## Structure of the Code\n\n### Key Components\n\nThe code consists of functions, classes, or both. Each section serves a distinctive purpose contributing to the main functionality. Here, we will identify and describe these individual components.\n\n### Functions and Methods\n\nIf the code defines any functions or methods, each will typically encapsulate a particular action or computation. For instance:\n\n- **Function1** might take input parameters and return a processed result, adhering to the principles of modular programming. \n- **Function2** may handle error checking or validation, ensuring the reliability and robustness of the code by checking for issues before proceeding to further processing.\n\nThe utilization of functions promotes reusability and maintainability of the script.\n\n### Classes (if any)\n\nIn case there are classes in the code, they likely represent objects or entities related to the problem domain. For example:\n\n- **Class1** may encapsulate a set of attributes and methods pertinent to a specific entity, such as a user or a transaction. \n- The methods within the class could manage behaviors, initializing class instances, or handling transformations and operations.\n\nDefining classes helps in organizing code logically, making it easier to understand and extend.\n\n## Code Flow\n\n### Initial Setup\n\nAt the beginning of the script, any necessary imports or initial configurations will be established. This section sets the groundwork for the code to run effectively, potentially loading libraries required for various functions.\n\n### Main Execution Flow\n\nThe core of the code will typically execute through a main block or a series of function calls:\n\n1. **Input Gathering**: \n   - The code might begin by obtaining input, either from user prompts, files, or external sources (e.g., databases or APIs). This operation might involve reading data into an appropriately structured format.\n\n2. **Processing Logic**:\n   - The next segment often contains the primary processing logic where the essential computational tasks occur. Each function or method might be invoked here to handle different aspects of the data or task.\n\n3. **Output Generation**:\n   - Following the processing, the code may direct the results to an output, such as displaying information on a console, writing to a file, or sending data back to an external system.\n\n### Error Handling\n\nA robust code will often include mechanisms for error handling, ensuring that any unexpected conditions or problems that arise during execution are managed gracefully, likely involving try-catch blocks or condition checks.\n\n## Summary\n\nThe source code serves as a structured document comprising potentially multiple functions and classes, each performing a specific role in executing a broader objective. Through well-organized logic, including setups, processing, and outputs, the code achieves its tasks while maintaining clarity and modularity. Knowledge of classes and functions significantly enhances the understanding and reusability of the code.",
    "filename": "python/packages/component-schema-gen/src/component_schema_gen/__init__.py"
  },
  {
    "code": false,
    "content": "# Documentation: Component Schema Builder Script\n\nThis script builds a JSON schema for various components, particularly aimed at integrations with OpenAI and Azure platforms. The core of its functionality is encapsulated in the `build_specific_component_schema` function, which generates schema details for different components based on their configuration.\n\n## Imports\n\nThe script starts by importing essential libraries and modules:\n\n- `json`: For JSON data handling and representation.\n- `sys`: To access system-specific parameters and functions.\n- `DefaultDict`, `Dict`, `List`, `TypeVar`: For type hinting and creating dictionaries with default behaviors.\n- Components of the `autogen_core` library, such as `ComponentModel`, `WELL_KNOWN_PROVIDERS`, and configuration utilities.\n- Azure-related authentication components and OpenAI clients for interaction with these services.\n- `BaseModel` from `pydantic`: To create and validate structured data models.\n\n## Global Variables\n\nA global dictionary, `all_defs`, is initialized to hold various definitions that may be constructed during the script execution. This is empty at the start and serves as a placeholder for component schemas.\n\n## Type Definitions\n\nThe `TypeVar` definition `T` binds to any type that is a subclass of `BaseModel`. This creates a generic type that can be reused throughout the script for better type safety and clarity.\n\n## Function: `build_specific_component_schema`\n\nThis function generates a detailed schema for a specific component based on provided type and provider strings.\n\n### Parameters:\n- `component`: A class type of `ComponentSchemaType[T]`, representing the component for which the schema is being created.\n- `provider_str`: A string representing the component's provider.\n\n### Workflow:\n1. **Schema Generation**:\n   - The function first retrieves the JSON schema of the component configuration model.\n   - It ensures that the component is a subclass of `ComponentToConfig`.\n   \n2. **Schema Customization**:\n   - Defines a section for `$defs` if it is absent.\n   - Merges the component's schema with the main model schema and establishes a `$ref` for the configuration within the overall structure.\n   \n3. **Provider Information**:\n   - Establishes a canonical provider string and integrates it into the component's schema and model properties.\n   - Adds information about the provider and component type.\n   \n4. The resulting schema is returned as a structured dictionary.\n\n## Function: `main`\n\nThis function kickstarts the schema building process and outputs the final schema structure.\n\n### Workflow:\n1. **Initialize Outer Schema**: \n   - An `outer_model_schema` dictionary is initialized to hold the overall schema definition.\n   \n2. **Provider Lookup Table**:\n   - A `reverse_provider_lookup_table` is created using `DefaultDict` to relate known provider types with their identifiers from `WELL_KNOWN_PROVIDERS`.\n   \n3. **Defining Inner Function: `add_type`**:\n   - This inner function accepts a component type, verifies its subclass status, and constructs the corresponding JSON schema using `build_specific_component_schema`.\n   - It ensures existing definitions do not overwrite existing entries and properly appends new definitions to the outer schema.\n\n4. **Adding Component Types**:\n   - The function adds specific component types (`OpenAIChatCompletionClient`, `AzureOpenAIChatCompletionClient`, `AzureTokenProvider`) using the `add_type` inner function. This creates schema definitions for these components.\n\n5. **Outputting Schema**:\n   - Finally, it prints the generated outer model schema in a readable JSON format.\n\n## Execution Check\n\nThe script resides within an `if __name__ == \"__main__\":` block ensuring that the `main` function executes only when the script is run directly, not when imported as a module.\n\n## Conclusion\n\nThe provided script is a flexible utility for dynamically generating JSON schema definitions for various components used in cloud-based applications. By leveraging typed models and well-defined component structures, it allows multiple integrations with sophisticated schema validation based on configuration data. The use of Pydantic enhances data quality and integrity, while clear structuring with dicts facilitates easy extension and maintenance.",
    "filename": "python/packages/component-schema-gen/src/component_schema_gen/__main__.py"
  },
  {
    "content": "# magentic-one-cli",
    "filename": "python/packages/magentic-one-cli/README.md"
  },
  {
    "code": false,
    "content": "It seems that no source code was provided for analysis. To help you effectively, please share the piece of code you would like me to analyze. Once I have the code, I can create a clear, high-level description of its functionality and roles of its components.",
    "filename": "python/packages/magentic-one-cli/src/magentic_one_cli/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview\n\nThe provided code snippet is a straightforward Python script that imports a function or a class named `main` from a module located in the same package (indicated by the leading dot) named `._m1`. After the import, it immediately invokes the `main` function or class. This document provides a high-level analysis of what the code does and its structure.\n\n## Module Import\n\n### Importing `main`\n\n```python\nfrom ._m1 import main\n```\n\n- The code begins with an import statement which retrieves the `main` entity from the `._m1` module. \n- The leading dot (`.`) signifies that `_m1` is a relative import, meaning that it is located within the same package as the script itself. This is a common practice in structured Python projects to manage dependencies elegantly between modules within the same package.\n\n### `main` Function or Class\n\n- The `main` entity being imported is not explicitly defined in this snippet. However, it is typical for `main` to be a function or class designed to serve as the entry point for executable scripts or applications. \n- The precise behavior, functionality, and implementation of `main` would need to be examined in the `_m1` module for a complete understanding. It would typically encapsulate the core logic or orchestrate the execution flow of the application.\n\n## Execution Flow\n\n### Invoking `main`\n\n```python\nmain()\n```\n\n- The script invokes the `main` function/class immediately after importing it.\n- The immediate call to `main()` suggests that this script is intended to run as a standalone program, potentially performing operations defined within `main`. \n- Without additional context or the implementation details of `main`, it\u2019s impossible to determine what specific tasks are performed when it is called. \n\n### Anticipated Behavior\n\n- Based on conventions in Python programming, it can be anticipated that the `main` function might handle tasks such as:\n  - Initializing application components.\n  - Accepting user input through command-line arguments.\n  - Executing main application logic which could involve computations, data processing, or service initiation.\n  - Handling exceptions or errors gracefully.\n  \n### Application Context\n\n- The snippet does not include any context on what kind of application or functionality it supports, such as whether it is a command-line tool, a web application, or a library.\n- Future users or developers must refer to the `_m1` module documentation or its source code for clarity on how `main` operates and what dependencies or configurations might be necessary for it to function correctly.\n\n## Conclusion\n\nThe given code snippet demonstrates a fundamental aspect of Python programming where modularity and reuse are emphasized through import statements. \n\n- It defines a minimal entry point for executing the imported `main` method from a local module. \n- Users seeking to understand the impact of running this script will need to consult the `_m1` module for further details on the `main` function/class, its parameters, exceptions, and overall functionality within the broader application context. \n- Clear naming conventions and modular architecture contribute to the maintainability and readability of code, which can be crucial for collaborative projects and future enhancements.",
    "filename": "python/packages/magentic-one-cli/src/magentic_one_cli/__main__.py"
  },
  {
    "code": false,
    "content": "# Documentation for `magentic_one_cli.py`\n\n## Overview\nThe `magentic_one_cli.py` script is a command-line interface (CLI) designed to execute complex tasks using the `MagenticOne` class. Users can provide specific tasks as input, toggle human-in-the-loop (HIL) mode, enable rich console output, and use custom configuration files for the task execution.\n\n## Dependencies\nThis script imports several modules required for its operation:\n- **argparse**: For parsing command-line arguments.\n- **asyncio**: To handle asynchronous operations.\n- **os** and **sys**: For file system interactions and system-specific parameters.\n- **yaml**: For reading YAML configuration files.\n- Several components from `autogen_agentchat`, `autogen_core`, and `autogen_ext` packages are imported to facilitate task execution, user interaction, and console output.\n\n## Default Configuration\nThe script defines a default configuration, `DEFAULT_CONFIG_FILE`, which points to a YAML file named `config.yaml`. In the absence of this file, it provides fallback content in `DEFAULT_CONFIG_CONTENTS`. This YAML config specifies the `client` provider and the model to be used (defaulting to `gpt-4o`).\n\n## Function: `cancellable_input`\n```python\nasync def cancellable_input(prompt: str, cancellation_token: Optional[CancellationToken]) -> str:\n```\nThis asynchronous function is designed to manage user input with cancellation support:\n- **Parameters**:\n  - `prompt`: A string to display when requesting input.\n  - `cancellation_token`: An optional token that can be linked to an asyncio task to allow cancellation.\n- It creates an asyncio task to handle input in a separate thread.\n- If provided, it links the cancellation token to this task to enable cancellation.\n\n## Main Function: `main`\n```python\ndef main() -> None:\n```\nThis function encapsulates the main logic for the CLI application:\n- It initializes an argument parser with descriptions of expected arguments and functionality.\n- It defines several command-line options including:\n  - `task`: The primary task string to be executed.\n  - `--no-hil`: Flag to disable HIL mode.\n  - `--rich`: Flag to enable rich console display.\n  - `--config`: Flag to specify a custom configuration YAML file.\n  - `--sample-config`: Flag to print a sample configuration.\n  \n### Argument Parsing and Configuration Loading\n- The function checks whether a sample config is requested. If so, it prints the default configuration contents and exits.\n- If no task is provided, it prints usage instructions.\n- It loads the configuration from the specified file or defaults to `DEFAULT_CONFIG_FILE` or `DEFAULT_CONFIG_CONTENTS` if the file is absent.\n\n### Running the Task\n```python\nasync def run_task(task: str, hil_mode: bool, use_rich_console: bool) -> None:\n```\nThe script defines an internal asynchronous function, `run_task`, that executes the specified task:\n- **Parameters**:\n  - `task`: The task to be executed.\n  - `hil_mode`: Boolean indicating whether human-in-the-loop mode is enabled.\n  - `use_rich_console`: Boolean indicating if a rich console output should be used.\n- Inside this function:\n  - A chat completion client is instantiated based on the loaded configuration.\n  - It prepares a user input manager for capturing user input during task execution.\n  - It uses `DockerCommandLineCodeExecutor` to handle code execution in a Docker environment.\n  - Initializes the `MagenticOne` class with the client, input manager, and HIL configuration.\n  - Depending on the `use_rich_console` flag, it invokes either a `RichConsole` or a regular `Console` for output during task execution.\n  - It ensures the client is closed after execution.\n\n## Execution Control\nThe script concludes by checking if it is being run as the main module and calling the `main` function. It uses `asyncio.run` to execute `run_task` asynchronously with the provided arguments.\n\n## Example Usage\nUsers can invoke the script from a command line as follows:\n- Basic task execution:\n  ```bash\n  python magentic_one_cli.py \"example task\"\n  ```\n- Disabling HIL mode:\n  ```bash\n  python magentic_one_cli.py --no-hil \"example task\"\n  ```\n- Enabling rich console:\n  ```bash\n  python magentic_one_cli.py --rich \"example task\"\n  ```\n- Specifying a custom configuration:\n  ```bash\n  python magentic_one_cli.py --config config.yaml \"example task\"\n  ```\n\nTo display a sample configuration, users can run:\n```bash\npython magentic_one_cli.py --sample-config\n```\n\nOverall, this script serves as a flexible tool for executing complex tasks while allowing for configuration and user interaction customization through both command-line options and input management features.",
    "filename": "python/packages/magentic-one-cli/src/magentic_one_cli/_m1.py"
  },
  {
    "content": "# pyautogen\n\n> **NOTE:** This is a proxy package for the latest version of [`autogen-agentchat`](https://pypi.org/project/autogen-agentchat/). If you are looking for the 0.2.x version, please pin to `pyautogen~=0.2.0`.\n> To migrate from 0.2.x to the latest version, please refer to the [migration guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html).\n> Read our [previous clarification regarding to forks](https://github.com/microsoft/autogen/discussions/4217).\n> We have regained admin access to this package.\n\nAutoGen is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n- [Project homepage](https://github.com/microsoft/autogen)\n- [Documentation](https://microsoft.github.io/autogen/)\n- [Discord](https://aka.ms/autogen-discord)\n- [Contact](mailto:autogen@microsoft.com)",
    "filename": "python/packages/pyautogen/README.md"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThis document provides a high-level explanation and analysis of the given piece of source code. The analysis will cover the main functionalities, any defined methods or classes, and a step-by-step summary of the script's actions, if applicable.\n\n## General Purpose\n\nThe code serves to [describe the primary function of the code, e.g., \"perform data analysis on a dataset,\" \"implement a web scraping tool,\" etc.]. It integrates [mention any libraries or frameworks used] and follows certain logical structures or algorithms to achieve its goals.\n\n## Imports and Dependencies\n\nIn the beginning, the code imports several libraries that might include:\n\n- **Library A**: Used for [describe functionality, e.g., data manipulation, numerical operations].\n- **Library B**: Relevant for [another functionality, e.g., web requests, JSON parsing].\n- **Library C**: To facilitate [additional functionality, e.g., data visualization].\n\nThese libraries are essential as they provide the necessary functions and methods that simplify the main operations performed in the code.\n\n## Variables and Configuration\n\nThe script initializes several variables at the outset. These variables can include:\n\n- **Config Variables**: Such as API keys, database connection strings, or file paths.\n- **Constant Values**: Defined constants to represent limits, thresholds, or status codes.\n  \nThese variables allow for easy modification of the script\u2019s behavior without altering the core logic, making it more maintainable.\n\n## Core Functionality\n\nThe code proceeds to execute its primary function through a series of defined operations. The main functionalities could involve:\n\n1. **Data Retrieval**: If the code fetches data from an external source (e.g., database, web API), describe how it handles connections and data fetching.\n2. **Data Processing**: Explain how the data is manipulated, including any transformations, calculations, or aggregations.\n3. **Data Output**: This could involve writing to a file, sending data to an API, or displaying results in a console or a graphical user interface.\n\n## Defined Functions/Methods\n\nIf the script includes functions or methods, these are typically grouped logically, and each has a specific role:\n\n### Function A: [Function Name]\n- **Purpose**: Describe the purpose of this function, e.g., \"To calculate the average value of a dataset.\"\n- **Parameters**: List and describe the input parameters.\n- **Returns**: What data type or structure is returned.\n\n### Function B: [Function Name]\n- **Purpose**: Outline its role, e.g., \"To validate user input.\"\n- **Parameters**: Explanation of input parameters.\n- **Returns**: Result type or output.\n\nContinue with other functions, if applicable, providing the same format for consistency.\n\n## Error Handling\n\nThe code may implement error handling techniques, such as `try` and `except` blocks. These are crucial for:\n\n- Recovering from unexpected issues that may arise during execution.\n- Logging errors for future debugging.\n- Providing user-friendly messages for common problems.\n\n## Summary of Execution Flow\n\nThe execution flow can be summarized in a step-wise manner:\n\n1. **Initialization**: The script starts by importing libraries and setting up initial configurations.\n2. **Data Retrieval**: The code retrieves necessary data based on the configurations set.\n3. **Data Processing**: It processes the data through various functions, applying necessary calculations.\n4. **Output**: The final results are either written to a destination or displayed to the user.\n5. **Error Handling**: If errors arise during the process, they are caught, and appropriate messages are logged or displayed.\n\n## Conclusion\n\nThis analysis provides an overview of the code, outlining its capabilities, functions, and execution flow. The detailed descriptions of methods and core functionalities contribute to a better understanding of the code\u2019s role and structure, enabling further modifications or enhancements as needed. Overall, the code effectively [summarize what the code achieves].",
    "filename": "python/packages/pyautogen/src/pyautogen/__init__.py"
  },
  {
    "code": false,
    "content": "# Project Task Manager\n\nThis Python script serves as a task manager for projects defined in a `pyproject.toml` file. It utilizes the `tomli` library to parse TOML files and integrates with `PoeThePoet`, a tool for running tasks based on the project's configuration. The script primarily focuses on discovering projects, extracting tasks, and executing specified tasks from the command line.\n\n## Imports\n\nThe script begins by importing several modules:\n\n- `glob`: Used for finding pathnames matching a specified pattern.\n- `sys`: Provides access to command-line arguments (i.e., `sys.argv`).\n- `Path`: A class from the `pathlib` module used for manipulating filesystem paths.\n- `List`: A type hint for lists from the `typing` module.\n- `tomli`: A library for loading TOML files.\n- `PoeThePoet`: A class responsible for running tasks defined in the project configuration.\n- `print`: A function from the `rich` library to enable styled terminal output.\n\n## Function: `discover_projects`\n\n```python\ndef discover_projects(workspace_pyproject_file: Path) -> List[Path]:\n```\n\nThe `discover_projects` function accepts a path to a `pyproject.toml` file and returns a list of project paths. \n\n1. **Reading TOML Data**: It opens the provided TOML file and loads its content.\n2. **Extracting Projects**: Retrieves project members from the `tool.uv.workspace.members` section in the TOML data, along with any entries to be excluded.\n3. **Resolving Wildcards**: If any project names contain wildcards (i.e., \"*\"), it glob matches them to find all applicable file paths.\n4. **Excluding Projects**: Excludes any projects listed under the `exclude` section, also resolving potential wildcards for those entries.\n5. **Returns**: Finally, it returns a complete list of project paths.\n\n## Function: `extract_poe_tasks`\n\n```python\ndef extract_poe_tasks(file: Path) -> set[str]:\n```\n\nThis function extracts the tasks defined in a project's `pyproject.toml` file.\n\n1. **File Reading**: Similar to `discover_projects`, it opens the specified TOML file and reads its content.\n2. **Collecting Task Names**: It retrieves the task names under the `tool.poe.tasks` section and stores them in a set.\n3. **Handling Includes**: Checks for any included files defined in the `include` field. If present and valid, it recursively gathers tasks from the included file.\n4. **Returns**: The function returns a set of task names.\n\n## Function: `main`\n\n```python\ndef main() -> None:\n```\n\nThe `main` function serves as the entry point of the script.\n\n1. **Define Primary File Path**: It sets the path to the local `pyproject.toml` file using the current file's directory.\n2. **Discover Projects**: Calls `discover_projects` to get all project paths.\n3. **Argument Check**: Checks if a task name is provided as a command-line argument; if not, it prints an error message and exits.\n4. **Task Execution**: \n   - Iterates over each discovered project.\n   - For each project, it extracts tasks via `extract_poe_tasks`.\n   - It checks if the requested task is found in the project's tasks.\n   - If found, it creates an instance of `PoeThePoet`, runs the task, and exits with the result code returned by the task.\n   - If the task is not found in a project, it prints a message indicating so.\n\n## Script Execution Flow\n\nTo execute the script, users should run the corresponding Python file from the command line, providing the desired task name as an argument. The script will subsequently:\n\n1. Locate the `pyproject.toml` within the script's directory.\n2. Discover all project paths defined in the `pyproject.toml`.\n3. Validate whether the user has input a task name.\n4. Extract and check task names from each project.\n5. Execute the specified task in the project's directory using `PoeThePoet`.\n\n## Error Handling and User Feedback\n\nThe script includes basic error handling, particularly checking for the presence of required command-line arguments. When a task isn't found in any project, it informs the user, enhancing the usability of the tool. The use of `rich.print` allows for better visual output on the console, improving readability.\n\n## Summary\n\nThis script leverages the power of Python's rich ecosystem for project management tasks. By focusing on TOML configuration files and enabling task execution across multiple projects, it provides a flexible solution for developers managing complex workspaces. The integration with `PoeThePoet` allows for streamlined task execution, making it a valuable utility in any modern Python development environment.",
    "filename": "python/run_task_in_pkgs_if_exist.py"
  },
  {
    "content": "# **Multi-Agent PostgreSQL Data Management System with AutoGen and Azure PostgreSQL**\n\n\n<div align=\"center\">\n  <img src=\"https://github.com/mehrsa/MultiAgent_Azure_PostgreSQL_AutoGen0.4/blob/main/misc/Drawing%203.png\" alt=\"Architecture\">\n</div>\n\nGo to below repository to try out a demo demonstrating how to build a **multi-agent AI system** for managing shipment data stored on an Azure PostgreSQL database:\n\n[MultiAgent_Azure_PostgreSQL_AutoGen](https://github.com/Azure-Samples/MultiAgent_Azure_PostgreSQL_AutoGen0.4/tree/main)",
    "filename": "python/samples/agentchat_azure_postgresql/README.md"
  },
  {
    "content": "# Building a Multi-Agent Application with AutoGen and Chainlit\n\nIn this sample, we will demonstrate how to build simple chat interface that\ninteracts with an [AgentChat](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html)\nagent or a team, using [Chainlit](https://github.com/Chainlit/chainlit),\nand support streaming messages.\n\n## Installation\n\nTo run this sample, you will need to install the following packages:\n\n```shell\npip install -U chainlit autogen-agentchat \"autogen-ext[openai]\" pyyaml\n```\n\nTo use other model providers, you will need to install a different extra\nfor the `autogen-ext` package.\nSee the [Models documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html) for more information.\n\n\n## Model Configuration\n\nCreate a configuration file named `model_config.yaml` to configure the model\nyou want to use. Use `model_config_template.yaml` as a template.\n\n## Running the Agent Sample\n\nThe first sample demonstrate how to interact with a single AssistantAgent\nfrom the chat interface.\n\n```shell\nchainlit run app_agent.py -h\n```\n\nYou can use one of the starters. For example, ask \"What the weather in Seattle?\".\n\nThe agent will respond by first using the tools provided and then reflecting\non the result of the tool execution.\n\n## Running the Team Sample\n\nThe second sample demonstrate how to interact with a team of agents from the\nchat interface.\n\n```shell\nchainlit run app_team.py -h\n```\nYou can use one of the starters. For example, ask \"Write a poem about winter.\".\n\nThe team is a RoundRobinGroupChat, so each agent will respond in turn.\nThere are two agents in the team: one is instructed to be generally helpful\nand the other one is instructed to be a critic and provide feedback. \nThe two agents will respond in round-robin fashion until\nthe 'APPROVE' is mentioned by the critic agent.\n\n## Running the Team Sample with UserProxyAgent\n\nThe third sample demonstrate how to interact with a team of agents including\na [UserProxyAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)\nfor approval or rejection.\n\n```shell\nchainlit run app_team_user_proxy.py -h\n```\n\nYou can use one of the starters. For example, ask \"Write code to reverse a string.\".\n\nBy default, the `UserProxyAgent` will request an input action from the user\nto approve or reject the response from the team.\nWhen the user approves the response, the `UserProxyAgent` will send a message\nto the team containing the text \"APPROVE\", and the team will stop responding.\n\n\n## Next Steps\n\nThere are a few ways you can extend this example:\n\n- Try other [agents](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html).\n- Try other [team](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html) types beyond the `RoundRobinGroupChat`.\n- Explore custom agents that sent multimodal messages.",
    "filename": "python/samples/agentchat_chainlit/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script sets up a conversational assistant using the Chainlit framework. The assistant can greet users and provide weather information. It utilizes an external model for responses, working in a streaming context for efficient interaction. The code is structured into distinct asynchronous functions that handle the initiation of chat sessions and message processing, leveraging type hints for better clarity.\n\n## Dependencies\n\nThe script imports several modules:\n\n- **typing**: For type hinting, specifically the `List` type.\n- **chainlit**: A framework for building interactive web applications, particularly focused on chat interfaces.\n- **yaml**: To read configuration files in YAML format.\n- **autogen_agentchat**: Contains classes and methods used for managing the assistant agent and responses.\n- **autogen_core**: Provides functionalities related to model client operations.\n\n## Starter Messages\n\n### Function: `set_starts`\n\n```python\n@cl.set_starters  # type: ignore\nasync def set_starts() -> List[cl.Starter]:\n    ...\n```\n\nThis asynchronous function defines initial starter messages for the chat interface. It returns a list of `Starter` objects that serve as predefined prompts for users. The starters included are:\n\n1. **Greetings**: A prompt asking users how the assistant can help.\n2. **Weather**: A prompt asking for weather information in New York City.\n\nThese starters enhance user engagement by providing quick options for interaction at the beginning of a chat session.\n\n## Weather Inquiry Tool\n\n### Function: `get_weather`\n\n```python\n@cl.step(type=\"tool\")  # type: ignore\nasync def get_weather(city: str) -> str:\n    ...\n```\n\nThis function is defined as a tool within the assistant. It takes a city name as an argument and returns a hardcoded weather response. Specifically, it states that \"the weather in {city} is 73 degrees and Sunny.\" While currently simplified, this function could later be expanded to incorporate real weather data from an API.\n\n## Chat Session Initialization\n\n### Function: `start_chat`\n\n```python\n@cl.on_chat_start  # type: ignore\nasync def start_chat() -> None:\n    ...\n```\n\nThis function is triggered when a chat starts. Its responsibilities include:\n\n1. **Loading Configuration**: It reads a YAML configuration file (`model_config.yaml`) to obtain the necessary settings for the model client.\n2. **Model Client Initialization**: It initializes a `ChatCompletionClient` using the loaded configuration.\n3. **Assistant Agent Creation**: An `AssistantAgent` is instantiated using the `get_weather` tool, allowing it to utilize the weather-checking functionality. \n4. **Session Management**: It sets up the user session by initiating a prompt history and assigning the assistant to the session.\n\nThese steps set the foundation for interactive conversations with the user.\n\n## Message Handling and Streaming Responses\n\n### Function: `chat`\n\n```python\n@cl.on_message  # type: ignore\nasync def chat(message: cl.Message) -> None:\n    ...\n```\n\nThis function processes incoming chat messages sent by the user. It performs several key tasks:\n\n1. **Agent Retrieval**: It retrieves the instance of `AssistantAgent` from the user session.\n2. **Response Construction**: Initializes an empty response message variable.\n3. **Asynchronous Message Streaming**: For every message received from the user, it streams a response from the assistant agent. The response generation is done asynchronously, allowing for real-time interaction.\n\n### Response Types\n\nInside the message processing loop:\n\n- If an incoming message is of the type `ModelClientStreamingChunkEvent`, it streams the content back to the user.\n- If the message is a `Response`, it finalizes the streaming process and sends the complete message to the user.\n\nThis structure allows for a fluid and dynamic response mechanism, enhancing the user experience during interactions.\n\n## Conclusion\n\nThis script effectively sets up a conversational assistant capable of interacting with users through structured prompts and weather inquiries. Leveraging asynchronous programming and streaming responses, it ensures a responsive and engaging chat environment. Each defined function plays a specific role, from initializing the agent to handling user messages, demonstrating good separation of concerns in the coding structure.",
    "filename": "python/samples/agentchat_chainlit/app_agent.py"
  },
  {
    "code": false,
    "content": "# Description of the Code\n\nThis code is a server-side implementation designed for a chat interface using the `chainlit` library. Its main functionality revolves around creating an interactive chat experience with an assistant and a critic agent, both of which use a machine learning model for generating responses. The chat application allows users to engage with AI-generated content such as poems, stories, or code.\n\n## Imports and Dependencies\n\nThe code begins with several import statements that include essential libraries and components:\n\n- `List` and `cast` from `typing`: These are used for type hinting and casting types.\n- `chainlit` as `cl`: A framework for building chat applications. \n- `yaml`: A module for parsing YAML file formats, which is used to load configuration.\n- Components from `autogen_agentchat`: These include agent classes and messaging utilities that facilitate AI interaction.\n- `CancellationToken` from `autogen_core`: A utility for managing cancellation of tasks.\n- `ChatCompletionClient` from `autogen_core.models`: A client class for invoking chat completions from the AI model.\n\n## Function: `start_chat()`\n\n### Overview\n\nThe `start_chat` function is triggered when a chat session commences. It initializes the necessary components for the chat session, specifically an assistant agent, a critic agent, and a termination condition for the session.\n\n### Function Steps\n\n1. **Load Configuration**: The function reads the model configuration from a file named `model_config.yaml`, which defines the parameters for the AI model.\n  \n2. **Create Model Client**: Using the loaded configuration, a `ChatCompletionClient` instance is created, responsible for managing interactions with the AI model.\n\n3. **Initialize Assistant Agent**: An `AssistantAgent` named \"assistant\" is created. This agent is designed to provide helpful responses and utilizes model client streaming for real-time interaction.\n\n4. **Initialize Critic Agent**: Another `AssistantAgent`, named \"critic\", is instantiated to provide feedback, signaling approval when tasks are completed satisfactorily.\n\n5. **Set Termination Condition**: A `TextMentionTermination` condition is defined, which specifies that the session can end when the critic agent sends an \"APPROVE\" message.\n\n6. **Group Chat Creation**: Both agents are managed within a `RoundRobinGroupChat`, which ensures that they will take turns interacting with the user until termination conditions are met.\n\n7. **Session Setup**: The function initializes the user session by setting a prompt history and storing the created team (the group chat with the assistant and critic agents).\n\n## Function: `set_starts()`\n\n### Overview\n\nThe `set_starts` function defines the initial chat starters, providing predefined prompts that guide users on what kind of interaction they can initiate with the chat interface.\n\n### Function Steps\n\n1. **Return Starter Messages**: This function returns a list of `cl.Starter` objects, each containing:\n   - A label (e.g., \"Poem Writing\").\n   - A message body that serves as an initial prompt for the user (e.g., \"Write a poem about the ocean.\").\n\nThe starters align with potential chat topics that encourage users to engage meaningfully with the assistant.\n\n## Function: `chat(message)`\n\n### Overview\n\nThe `chat` function is the primary handler for incoming user messages in the chat interface. This function manages the interaction between the user and the assistant/critic agents, streaming responses dynamically.\n\n### Function Steps\n\n1. **Retrieve Group Chat**: The function retrieves the `RoundRobinGroupChat` team from the user session, which contains the assistant and critic agents.\n\n2. **Initialize Streaming Response**: A variable (`streaming_response`) is initialized to manage the streaming of responses.\n\n3. **Stream Messages**: The function enters an asynchronous loop, running the chat agents' responses based on the incoming user message (`message.content`). The message is packaged as a `TextMessage` and sent as input to the agents.\n\n4. **Handle Streaming Response**: \n   - If the response is a `ModelClientStreamingChunkEvent`, it indicates a chunk of the streaming message from the model. If this is the first chunk, a new message object is created and initiated for streaming.\n   - The chunk is then streamed to the user. \n\n5. **Complete Response Handling**: If the response indicates that streaming is complete (typically indicated by a different message type), the entire message is finalized and sent to the user.\n\n6. **Handle Task Results**: If the message is a `TaskResult`, indicating the end of a task, a termination message is crafted to be sent to the user, providing feedback on the task result and the reason for task termination if any.\n\n7. **Handle Other Message Types**: Any other message types are simply skipped and not processed further.\n\n## Conclusion\n\nOverall, this code forms the backbone of an interactive chat application, combining collaborative AI agents to assist and critique user-generated prompts across various topics. This structured setup allows for a fluid user experience where AI-generated responses are dynamic and responsive to user inputs. The combination of agents and user session management makes it a robust framework for facilitating a supportive AI-driven dialogue.",
    "filename": "python/samples/agentchat_chainlit/app_team.py"
  },
  {
    "code": false,
    "content": "# Chat Application Documentation\n\n## Overview\nThis piece of code implements a chat application using the Chainlit framework. The application enables interaction between a user and two assistant agents: an \"assistant\" agent and a \"critic\" agent. It efficiently manages user input and generates responses through a round-robin group chat mechanism, facilitating a dynamic conversation. The application incorporates YAML configuration for model initialization and user action handling, allowing for an interactive chat experience.\n\n## Imports\nThe code begins by importing necessary modules and classes:\n- **chainlit (cl)**: A framework for building web-based applications.\n- **yaml**: For loading configuration settings.\n- **autogen_agentchat**: Contains classes for agent management, message handling, and condition definitions.\n- **CancellationToken**: Used to handle user cancellation events in chat.\n\n## User Input and Action Functions\nTwo asynchronous functions are defined to manage user interactions:\n\n### `user_input_func`\n```python\nasync def user_input_func(prompt: str, cancellation_token: CancellationToken | None = None) -> str:\n```\n- **Purpose**: To collect textual input from the user interface.\n- **Implementation**: Displays a prompt to the user and awaits a response. If the user does not respond within a time limit, it returns an appropriate message. On receiving input, it returns the user's response.\n\n### `user_action_func`\n```python\nasync def user_action_func(prompt: str, cancellation_token: CancellationToken | None = None) -> str:\n```\n- **Purpose**: Similar to `user_input_func`, but focuses on user actions (approval/rejection).\n- **Implementation**: It prompts the user to choose between approving or rejecting an action. Based on the user's selection, it returns either \"APPROVE.\" (as a termination condition) or \"REJECT.\" If no action is taken within a specified time, it defaults to a prompt message.\n\n## Chat Initialization\nThe `start_chat` function is triggered when a chat session starts.\n\n```python\n@cl.on_chat_start\nasync def start_chat() -> None:\n```\n- **Model Configuration**: It loads model settings from `model_config.yaml`.\n- **Agent Creation**:\n  - **Assistant Agent**: Acts as a helpful assistant.\n  - **Critic Agent**: Provides feedback and responds specifically with \"APPROVE\" if the feedback is addressed.\n  - **User Proxy Agent**: Manages user inputs and actions.\n- **Termination Condition**: The chat will terminate when the user approves an action via the `TextMentionTermination`.\n- **Group Chat Setup**: Initializes a `RoundRobinGroupChat` object that coordinates the interactions between the three agents and sets it in the user session.\n\n## Starter Messages\nThe `set_starts` function defines several starter messages that the user can select to initiate a dialogue.\n\n```python\n@cl.set_starters\nasync def set_starts() -> List[cl.Starter]:\n```\n- **Purpose**: To provide options for the user to begin conversations on specific topics such as poem writing, story writing, or coding tasks.\n- **Implementation**: Returns a list of `Starter` objects with labels and associated messages.\n\n## Message Handling\nThe main chat logic is handled through the `chat` function.\n\n```python\n@cl.on_message \nasync def chat(message: cl.Message) -> None:\n```\n- **Get Team**: Retrieves the `RoundRobinGroupChat` from the user session.\n- **Message Streaming**: Handles messages sent by the user, streaming responses from the different agents. It checks the type of message:\n  - If it's a streaming chunk event, it updates the user with the content progressively.\n  - If a task result indicates that the task has been completed, it sends a termination message detailing the stop reason.\n  - It ignores other types of messages not relevant to the user session.\n\n## Conclusion\nIn conclusion, this code offers a structured approach to creating a chat interface with multiple roles. It relies on unique agents to enrich user interaction and provides a flexible way to receive input, respond, and manage termination through specific conditions. By integrating YAML for configuration and Chainlit for UI, the application enhances both functionality and user experience in conversational AI development.",
    "filename": "python/samples/agentchat_chainlit/app_team_user_proxy.py"
  },
  {
    "content": "# AgentChat Chess Game\n\nThis is a simple chess game that you can play with an AI agent.\n\n## Setup\n\nInstall the `chess` package with the following command:\n\n```bash\npip install \"chess\"\n```\n\nTo use OpenAI models or models hosted on OpenAI-compatible API endpoints,\nyou need to install the `autogen-ext[openai]` package. You can install it with the following command:\n\n```bash\npip install \"autogen-ext[openai]\"\n# pip install \"autogen-ext[openai,azure]\" for Azure OpenAI models\n```\n\nTo run this sample, you will need to install the following packages:\n\n```shell\npip install -U autogen-agentchat pyyaml\n```\n\nCreate a new file named `model_config.yaml` in the the same directory as the script\nto configure the model you want to use.\n\nFor example, to use `gpt-4o` model from OpenAI, you can use the following configuration:\n\n```yaml\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: gpt-4o\n  api_key: replace with your API key or skip it if you have environment variable OPENAI_API_KEY set\n```\n\nTo use `o3-mini-2025-01-31` model from OpenAI, you can use the following configuration:\n\n```yaml\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: o3-mini-2025-01-31\n  api_key: replace with your API key or skip it if you have environment variable OPENAI_API_KEY set\n```\n\nTo use a locally hosted DeepSeek-R1:8b model using Ollama throught its compatibility endpoint,\nyou can use the following configuration:\n\n```yaml\nprovider: autogen_ext.models.openai.OpenAIChatCompletionClient\nconfig:\n  model: deepseek-r1:8b\n  base_url: http://localhost:11434/v1\n  api_key: ollama\n  model_info:\n    function_calling: false\n    json_output: false\n    vision: false\n    family: r1\n```\n\nFor more information on how to configure the model and use other providers,\nplease refer to the [Models documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n## Run\n\nRun the following command to start the game:\n\n```bash\npython main.py\n```\n\nBy default, the game will use a random agent to play against the AI agent.\nYou can enable human vs AI mode by setting the `--human` flag:\n\n```bash\npython main.py --human\n```",
    "filename": "python/samples/agentchat_chess_game/README.md"
  },
  {
    "code": false,
    "content": "# Chess AI Player Documentation\n\nThis Python script implements a command-line-based chess game featuring an AI player that utilizes a ChatCompletion model to make moves. It allows for both human vs. AI gameplay and an automatic AI vs. AI mode. The chess board is represented using the `chess` library, and interactions with the AI are managed through an asynchronous setup using `asyncio`.\n\n## Imports and Dependencies\n\nThe script begins by importing the necessary modules:\n\n- **argparse**: For command-line argument parsing.\n- **asyncio**: To manage asynchronous programming.\n- **yaml**: For loading the model configuration from a YAML file.\n- **random**: To select random moves when necessary.\n- **chess**: The library used to handle chess game logic and moves.\n- **Custom Modules**: Classes and functions from `autogen_agentchat` and `autogen_core` provide AI capabilities and chat management.\n\n## AI Player Creation\n\n### `create_ai_player`\n\n```python\ndef create_ai_player(model_client: ChatCompletionClient) -> AssistantAgent:\n    ...\n```\n\nThis function initializes an AI player, represented as an `AssistantAgent`, using the provided `model_client`. It configures the agent for streaming responses and creates a buffered chat context limited to the last 10 messages. This encapsulation allows the AI player to receive context from previous messages, enabling better decision-making for chess moves.\n\n## Chess Move Handling\n\n### `get_random_move`\n\n```python\ndef get_random_move(board: chess.Board) -> str:\n    ...\n```\n\nThis function retrieves a random legal move from the current state of the chess board. It converts the move to UCI (Universal Chess Interface) format, which is necessary for communication between the AI and the game.\n\n### `get_ai_prompt`\n\n```python\ndef get_ai_prompt(board: chess.Board) -> str:\n    ...\n```\n\nThis function generates a prompt for the AI player based on the current board state, including the last move made, the current player's color, and the list of legal moves. The generated prompt encourages the AI to respond in a specific format that includes the next move in a mandatory UCI format.\n\n### `get_user_prompt`\n\n```python\ndef get_user_prompt(board: chess.Board) -> str:\n    ...\n```\n\nSimilarly, this function creates a prompt intended for human players, detailing the board state, the last move made by the AI, and the list of allowed moves. It asks the player to input their move explicitly in UCI format.\n\n### `extract_move`\n\n```python\ndef extract_move(response: str) -> str:\n    ...\n```\n\nThis utility function extracts the AI's move from the formatted response string. It searches for the `<move>` tags in the response and returns the move between those tags. If the format is invalid, it raises an error.\n\n## AI Move Logic\n\n### `get_ai_move`\n\n```python\nasync def get_ai_move(board: chess.Board, player: AssistantAgent, max_tries: int) -> str:\n    ...\n```\n\nThis asynchronous function coordinates the process of getting a move from the AI player. It generates prompts, processes AI responses, and checks the validity of the proposed moves against the list of legal moves on the board. If the move is invalid, it retries (up to `max_tries` times) until a valid move is found or defaults to a random move.\n\n## Main Game Loop\n\n### `main`\n\n```python\nasync def main(human_player: bool, max_tries: int) -> None:\n    ...\n```\n\nThe main driver function that orchestrates the chess game. It initializes the chess board and the AI player, loading model configurations from a YAML file. In a loop, it alternates between the AI's moves and the user's moves (if in human mode). It checks for end-game conditions after each move and displays the current board state until the game concludes. Finally, it prints the game result (win/loss/draw) and closes the model client.\n\n## Command-Line Interface\n\n```python\nif __name__ == \"__main__\":\n    ...\n```\n\nThe script can be executed directly from the command line. It uses `argparse` to handle options, allowing the user to choose between human vs. AI mode and to specify the maximum number of retries for getting a valid move from the AI. The `asyncio.run` function is used to initiate the main game loop.\n\n## Conclusion\n\nThis script provides a thorough framework for playing chess against an AI opponent. With the ability to easily configure and switch between human and AI modes, it serves as an interactive tool for chess enthusiasts and a platform for testing AI-driven move logic. The structured integration with modern AI model handling promises to enhance the game experience as models evolve.",
    "filename": "python/samples/agentchat_chess_game/main.py"
  },
  {
    "code": false,
    "content": "It seems that there was no code input provided. Please share the source code you would like me to analyze.",
    "filename": "python/samples/agentchat_dspy/single_agent.py"
  },
  {
    "content": "# AgentChat App with FastAPI\n\nThis sample demonstrates how to create a simple chat application using\n[AgentChat](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html)\nand [FastAPI](https://fastapi.tiangolo.com/).\n\nYou will be using the following features of AgentChat:\n\n1. Agent:\n   - `AssistantAgent`\n   - `UserProxyAgent` with a custom websocket input function\n2. Team: `RoundRobinGroupChat`\n3. State persistence: `save_state` and `load_state` methods of both agent and team.\n\n## Setup\n\nInstall the required packages with OpenAI support:\n\n```bash\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" \"fastapi\" \"uvicorn[standard]\" \"PyYAML\"\n```\n\nTo use models other than OpenAI, see the [Models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html) documentation.\n\nCreate a new file named `model_config.yaml` in the same directory as this README file to configure your model settings.\nSee `model_config_template.yaml` for an example.\n\n## Chat with a single agent\n\nTo start the FastAPI server for single-agent chat, run:\n\n```bash\npython app_agent.py\n```\n\nVisit http://localhost:8001 in your browser to start chatting.\n\n## Chat with a team of agents\n\nTo start the FastAPI server for team chat, run:\n\n```bash\npython app_team.py\n```\n\nVisit http://localhost:8002 in your browser to start chatting.\n\nThe team also includes a `UserProxyAgent` agent with a custom websocket input function\nthat allows the user to send messages to the team from the browser.\n\nThe team follows a round-robin strategy so each agent will take turns to respond.\nWhen it is the user's turn, the input box will be enabled.\nOnce the user sends a message, the input box will be disabled and the agents\nwill take turns to respond.\n\n## State persistence\n\nThe agents and team use the `load_state` and `save_state` methods to load and save\ntheir state from and to files on each turn.\nFor the agent, the state is saved to and loaded from `agent_state.json`.\nFor the team, the state is saved to and loaded from `team_state.json`.\nYou can inspect the state files to see the state of the agents and team\nonce you have chatted with them.\n\nWhen the server restarts, the agents and team will load their state from the state files\nto maintain their state across restarts.\n\nAdditionally, the apps uses separate JSON files,\n`agent_history.json` and `team_history.json`, to store the conversation history\nfor display in the browser.",
    "filename": "python/samples/agentchat_fastapi/README.md"
  },
  {
    "code": false,
    "content": "# Documentation for Chat Assistant Application\n\n## Overview\nThis FastAPI application serves as a chat assistant interface, enabling users to interact with a conversational agent powered by a configurable machine learning model. The core functionality includes serving a static HTML interface for users, processing incoming chat messages, maintaining chat history, and preserving the state of the conversational agent across interactions.\n\n## Dependencies\nThe application utilizes several key libraries:\n- **FastAPI**: To create the API endpoints.\n- **aiofiles**: For asynchronous file handling.\n- **YAML**: To read model configuration.\n- **JSON**: To handle data serialization.\n- **CORS Middleware**: To handle cross-origin requests.\n- **Custom Imports**: Includes specific classes and functions from `autogen_agentchat` and `autogen_core`, which appear to facilitate agent management and response generation.\n\n## Application Setup\nThe application is initiated using FastAPI and is configured to allow cross-origin requests (CORS) from all origins, which is particularly useful when the frontend interface is hosted on a different domain. \n\n### CORS Configuration\n```python\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\nThis configuration enables the application to accept requests from any origin, which is useful for development but should be limited in production to only trusted origins.\n\n## Static Files\nThe app serves static files from the current directory through a dedicated endpoint (/static). This functionality is crucial for delivering the front-end HTML (and potentially other assets) required for user interaction.\n\n## Endpoints\n### Root Endpoint\n```python\n@app.get(\"/\")\nasync def root():\n    \"\"\"Serve the chat interface HTML file.\"\"\"\n    return FileResponse(\"app_agent.html\")\n```\nThe root endpoint serves the HTML file for the chat interface, allowing users to initiate conversations with the assistant agent.\n\n### Chat History Retrieval\n```python\n@app.get(\"/history\")\nasync def history() -> list[dict[str, Any]]:\n```\nThis endpoint retrieves the chat history from a JSON file. If the history file does not exist, it returns an empty list. It includes error handling, returning a 500 HTTP status code if any issues arise during the retrieval process.\n\n### Chat Message Handling\n```python\n@app.post(\"/chat\", response_model=TextMessage)\nasync def chat(request: TextMessage) -> TextMessage:\n```\nUsers can send messages to the assistant through this endpoint. The process includes:\n1. Retrieving the assistant agent instance.\n2. Processing the user's message using the agent.\n3. Saving the updated agent state and chat history back to their respective files.\n4. Returning the agent's response.\n\nThis endpoint also implements comprehensive error handling that returns a structured error message to the user if something goes wrong during the chat processing.\n\n## Agent Management\n### Loading the Assistant Agent\n```python\nasync def get_agent() -> AssistantAgent:\n```\nThis function initializes and prepares the assistant agent by loading its configuration and state from files. If the state file does not exist, it simply returns a new agent instance.\n\n### Chat History Management\n```python\nasync def get_history() -> list[dict[str, Any]]:\n```\nThis function retrieves the chat history from a JSON file. If it doesn\u2019t exist, it returns an empty list. This history is crucial for maintaining context in conversation.\n\n## State Persistence\nThe application saves the agent's state and chat history after each interaction. Key paths used for file handling include:\n- `model_config_path`: Path to the YAML file containing model configuration.\n- `state_path`: Path to the JSON file for saving the agent's internal state.\n- `history_path`: Path to the JSON file for saving chat history.\n\nThe combination of these file interactions ensures that the assistant retains context and memory across user sessions, enhancing the conversational experience.\n\n## Running the Application\nThe application includes a simple block to start the FastAPI server using Uvicorn when this script is executed as the main program:\n```python\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n```\nThis command will launch the server listening on all network interfaces at port 8001, and it will be ready to handle incoming requests immediately.\n\n## Conclusion\nThe provided FastAPI application establishes a robust foundation for creating an interactive chat assistant interface. By leveraging asynchronous file operations and providing structured APIs for communication, the application allows for seamless interaction with an AI-driven conversational agent. The well-organized structure and responsive design make it suitable for further enhancements or integration into larger systems.",
    "filename": "python/samples/agentchat_fastapi/app_agent.py"
  },
  {
    "code": false,
    "content": "# Chat Application Backend Documentation\n\nThis documentation outlines the functionality, components, and flow of a FastAPI-based chat application. The chat system supports asynchronous operations and uses WebSocket for real-time communication.\n\n## Overview\n\nThe main component of this application is a chat server that facilitates communication between users and various AI agents. The server manages user inputs, interactions with AI models, and the persistence of chat state and history. The application is designed to serve both static files (such as an HTML interface) and provide WebSocket support for real-time messaging.\n\n## Libraries and Imports\n\nThe application imports several libraries and modules:\n\n- **Standard Libraries**: `json`, `logging`, `os`\n- **Type Hinting**: `Any`, `Awaitable`, `Callable`, `Optional` from `typing`\n- **File Handling**: `aiofiles` for asynchronous file operations\n- **YAML Handling**: `yaml` for configuration loading\n- **FastAPI Modules**: For building the web server and handling requests\n- **Custom Modules**: Imports from `autogen_agentchat` and `autogen_core` for agent management and chat functionalities\n\n## FastAPI Application Setup\n\nThe application is initialized as a FastAPI instance:\n\n```python\napp = FastAPI()\n```\n\nCORS middleware is added to allow cross-origin requests, which enables the client-side application to communicate with the server from different origins.\n\n```python\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\n## Static File Serving\n\nThe application serves static files, specifically an HTML file for the chat interface, through:\n\n```python\napp.mount(\"/static\", StaticFiles(directory=\".\"), name=\"static\")\n```\n\nThis allows users to access the main chat interface via the root endpoint.\n\n## Root Endpoint\n\nThe root endpoint serves the HTML chat interface:\n\n```python\n@app.get(\"/\")\nasync def root():\n    return FileResponse(\"app_team.html\")\n```\n\nThis marks the entry point for users, directing them to the web interface of the chat application.\n\n## Team Management Functions\n\n### Team Creation\n\nThe function `get_team` constructs a chat team consisting of various agents:\n\n```python\nasync def get_team(user_input_func: Callable[[str, Optional[CancellationToken]], Awaitable[str]]) -> RoundRobinGroupChat:\n```\n\n- It reads model configurations from a YAML file and initializes different agent classes (`AssistantAgent` and `UserProxyAgent`).\n- The team is organized in a round-robin format, allowing multiple agents to interact with the user.\n\n### State Handling\n\nThe function also loads the current state of the chat from a JSON file if it exists. This ensures that interactions can continue from the last save point:\n\n```python\nif not os.path.exists(state_path):\n    return team\nasync with aiofiles.open(state_path, \"r\") as file:\n    state = json.loads(await file.read())\nawait team.load_state(state)\n```\n\n## Chat History Management\n\n### Retrieving Chat History\n\nThe function `get_history` retrieves stored chat history:\n\n```python\nasync def get_history() -> list[dict[str, Any]]:\n```\n\nIf the history file does not exist, it returns an empty list. Otherwise, it reads and loads the existing chat history from the JSON file.\n\n### History Endpoint\n\nAn API endpoint is provided for clients to access the chat history:\n\n```python\n@app.get(\"/history\")\nasync def history() -> list[dict[str, Any]]:\n```\n\nIf an error occurs during history retrieval, a 500 HTTP exception is raised.\n\n## WebSocket Communication\n\n### WebSocket Endpoint\n\nThe WebSocket endpoint enables real-time chat:\n\n```python\n@app.websocket(\"/ws/chat\")\nasync def chat(websocket: WebSocket):\n```\n\nUsers can send messages asynchronously through WebSocket connections. This setup allows for continuous interaction without HTTP request limits.\n\n### User Input Handling\n\nInside the `chat` function, a nested asynchronous function is defined to handle user input, receiving messages from the WebSocket:\n\n```python\nasync def _user_input(prompt: str, cancellation_token: CancellationToken | None) -> str:\n```\n\nUpon receiving user input, it processes the data into `TextMessage` format.\n\n### Message Processing Loop\n\nThe main loop listens for incoming messages, executes the team\u2019s responses, and broadcasts them back:\n\n```python\nwhile True:\n    data = await websocket.receive_json()\n    request = TextMessage.model_validate(data)\n```\n\nIt manages team interactions and ensures state and history are saved after each user interaction. Error management is embedded into the loop to handle issues gracefully.\n\n## Error Handling and Cleanup\n\nThroughout the WebSocket handling, various error-catching mechanisms are in place to log errors and send appropriate feedback to clients:\n\n```python\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {str(e)}\")\n    ...\n```\n\nIf an error arises while processing, the server sends an error message to the client and attempts to re-enable user input.\n\n## Running the Application\n\nFinally, the application can be executed using Uvicorn as the ASGI server:\n\n```python\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8002)\n```\n\nThis command runs the server on port 8002, listening on all network interfaces, and making it available for client connections. \n\n## Conclusion\n\nThis FastAPI chat application serves as an asynchronous framework for interactive user-agent communication with robust history management, error handling, and a user-friendly web interface. It leverages WebSocket for real-time messaging, making it an efficient solution for interactive chat applications.",
    "filename": "python/samples/agentchat_fastapi/app_team.py"
  },
  {
    "content": "# Building an AI Assistant Application with AutoGen and GraphRAG\n\nIn this sample, we will build a chat interface that interacts with an intelligent agent built using the [AutoGen AgentChat](https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/index.html) API and the GraphRAG framework.\n\n## High-Level Description\n\nThe `app.py` script sets up a chat interface that communicates with an AutoGen assistant agent. When a chat starts, it:\n\n- Initializes an AssistantAgent equipped with both local and global search tools from GraphRAG.\n- The agent automatically selects the appropriate search tool based on the user's query.\n- The selected tool queries the GraphRAG-indexed dataset and returns relevant information.\n- The agent's responses are streamed back to the chat interface.\n\n## What is GraphRAG?\n\nGraphRAG (Graph-based Retrieval-Augmented Generation) is a framework designed to enhance AI systems by providing robust tools for information retrieval and reasoning. It leverages graph structures to organize and query data efficiently, enabling both global and local search capabilities.\n\nGlobal Search: Global search involves querying the entire indexed dataset to retrieve relevant information. It is ideal for broad queries where the required information might be scattered across multiple documents or nodes in the graph.\n\nLocal Search: Local search focuses on a specific subset of the data, such as a particular node or neighborhood in the graph. This approach is used for queries that are contextually tied to a specific segment of the data.\n\nBy combining these search strategies, GraphRAG ensures comprehensive and context-sensitive responses from the AI assistant.\n\n## Setup\n\nTo set up the project, follow these steps:\n\n1. Install the required Python packages by running:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Navigate to this directory and run `graphrag init` to initialize the GraphRAG configuration. This command will create a `settings.yaml` file in the current directory.\n\n3. _(Optional)_ Download the plain text version of \"The Adventures of Sherlock Holmes\" from [Project Gutenberg](https://www.gutenberg.org/ebooks/1661) and save it to `input/sherlock_book.txt`.\n\n   **Note**: The app will automatically download this file if it doesn't exist when you run it, so this step is optional.\n\n4. Set the `OPENAI_API_KEY` environment variable with your OpenAI API key:\n\n```bash\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\nAlternatively, you can update the `.env` file with the API Key that will be used by GraphRAG:\n\n```bash\nGRAPHRAG_API_KEY=your_openai_api_key_here\n```\n\n5. Adjust your [GraphRAG configuration](https://microsoft.github.io/graphrag/config/yaml/) in the `settings.yaml` file with your LLM and embedding configuration. Ensure that the API keys and other necessary details are correctly set.\n\n6. Create a `model_config.yaml` file with the Assistant model configuration. Use the `model_config_template.yaml` file as a reference. Make sure to remove the comments in the template file.\n\n7. Run the `graphrag prompt-tune` command to tune the prompts. This step adjusts the prompts to better fit the context of the downloaded text.\n\n8. After tuning, run the `graphrag index` command to index the data. This process will create the necessary data structures for performing searches. The indexing may take some time, at least 10 minutes on most machines, depending on the connection to the model API.\n\nThe outputs will be located in the `output/` directory.\n\n## Running the Sample\n\nRun the sample by executing the following command:\n\n```bash\npython app.py\n```\n\nThe application will:\n\n1. Check for the required `OPENAI_API_KEY` environment variable\n2. Automatically download the Sherlock Holmes book if it doesn't exist in the `input/` directory\n3. Initialize both global and local search tools from your GraphRAG configuration\n4. Create an assistant agent equipped with both search tools\n5. Run a demonstration query: \"What does the station-master say about Dr. Becher?\"\n\nThe agent will automatically select the appropriate search tool (in this case, local search for specific entity information) and provide a detailed response based on the indexed data.\n\nYou can modify the hardcoded query in `app.py` line 79 to test different types of questions:\n\n- **Global search examples**: \"What are the main themes in the stories?\" or \"What is the overall sentiment?\"\n- **Local search examples**: \"What does character X say about Y?\" or \"What happened at location Z?\"",
    "filename": "python/samples/agentchat_graphrag/README.md"
  },
  {
    "code": false,
    "content": "# Documentation of the Script\n\nThis script implements a command-line tool that utilizes AI and search functionalities to interpret and answer queries about a text. It specifically downloads a book from project Gutenberg, initializes an AI-based search assistant, and retrieves information based on user queries using specialized search tools.\n\n## Required Libraries\n\nThe script imports several libraries and modules that are crucial for its functionality:\n\n- **argparse**: Handles command-line arguments for flexibility in execution.\n- **asyncio**: Manages asynchronous operations, allowing the program to perform non-blocking tasks.\n- **logging**: Facilitates logging for debugging and error tracking.\n- **os**: Interacts with the operating system, particularly for file and directory handling.\n- **requests**: Handles HTTP requests for downloading the sample data.\n- **pathlib**: Provides an object-oriented interface for handling filesystem paths.\n- **custom libraries**: Imports specific classes and modules that provide tools and capabilities related to the AI and search functionalities of the application.\n\n## Function: `download_sample_data`\n\nThe `download_sample_data` function is responsible for downloading a sample text file from the internet. Its steps include:\n\n1. **URL Definition**: It defines the URL of the Gutenberg book \"Sherlock Holmes\".\n2. **File Path Creation**: It constructs the file path where the downloaded text will be saved within the specified input directory.\n3. **HTTP Request**: It makes a GET request to download the text file with error handling to catch any exceptions that might occur during the download or file-writing operations.\n4. **File Handling**: If the download is successful, it saves the file to the specified location and prints confirmation; otherwise, it prints an error message.\n\n## Asynchronous Function: `main`\n\nThe `main` function orchestrates the main workflow of the script and is called when the script is run directly. Its responsibilities include:\n\n1. **API Key Check**: It checks for the `OPENAI_API_KEY` environment variable, essential for accessing the OpenAI services. If not set, it prompts the user to set it.\n2. **Input Directory Management**: It checks for the existence of an `input` directory, creates it if it does not exist, and downloads the sample data if the file is not already present in that directory.\n3. **Model Client Initialization**: It initializes an instance of `OpenAIChatCompletionClient` to communicate with the OpenAI model, using the key supplied through the environment variable.\n4. **Tool Initialization**: It sets up both global and local search tools from the GraphRAG framework, configured by a settings file.\n5. **Assistant Agent Creation**: It creates an `AssistantAgent` using the model client and search tools. The agent is defined to primarily select the correct search tool based on the user's query.\n6. **Sample Query Execution**: A hardcoded query about \"Dr. Becher\" is executed, demonstrating the query handling capabilities of the assistant agent. The results are printed to the console using an asynchronous output mechanism.\n\n## Command-Line Interface Setup\n\nAt the bottom of the script, a command-line interface is set up using `argparse`:\n\n- **Argument Parsing**: It defines an optional `--verbose` argument, which, when provided, configures the logging level for detailed output.\n- **Logging Configuration**: If verbose mode is enabled, logging is set to the DEBUG level for the `autogen_core` logger, and logs are directed to a file named `graphrag_search.log`.\n\n## Execution and Async Handling\n\nFinally, the execution of the `main` function is wrapped in an `asyncio.run()` call. This ensures that the asynchronous code runs properly and allows for non-blocking execution of tasks, such as waiting for user inputs or server responses.\n\n## Summary\n\nIn summary, this script is an efficient tool for conducting AI-assisted searches within a specific text. It encapsulates the following features:\n\n- Downloads sample data from an external source if it is not already available.\n- Initializes essential tools and services using correctly configured APIs.\n- Provides a command-line interface with optional verbose logging for troubleshooting.\n- Implements an asynchronous workflow to handle multiple operations fluidly.\n\nThis makes the tool adaptable for various querying needs, leveraging the capabilities of the OpenAI model and custom search tools.",
    "filename": "python/samples/agentchat_graphrag/app.py"
  },
  {
    "content": "# Streamlit AgentChat Sample Application\n\nThis is a sample AI chat assistant built with [Streamlit](https://streamlit.io/)\n\n## Setup\n\nInstall the `streamlit` package with the following command:\n\n```bash\npip install streamlit\n```\n\nTo use Azure OpenAI models or models hosted on OpenAI-compatible API endpoints,\nyou need to install the `autogen-ext[openai,azure]` package. You can install it with the following command:\n\n```bash\npip install \"autogen-ext[openai,azure]\"\n# pip install \"autogen-ext[openai]\" for OpenAI models\n```\n\nCreate a new file named `model_config.yml` in the the same directory as the script\nto configure the model you want to use.\n\nFor example, to use `gpt-4o-mini` model from Azure OpenAI, you can use the following configuration:\n\n```yml\nprovider: autogen_ext.models.openai.AzureOpenAIChatCompletionClient\nconfig:\n  azure_deployment: \"gpt-4o-mini\"\n  model: gpt-4o-mini\n  api_version: REPLACE_WITH_MODEL_API_VERSION\n  azure_endpoint: REPLACE_WITH_MODEL_ENDPOINT\n  api_key: REPLACE_WITH_MODEL_API_KEY\n```\n\nFor more information on how to configure the model and use other providers,\nplease refer to the [Models documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n## Run\n\nRun the following command to start the web application:\n\n```bash\nstreamlit run main.py\n```",
    "filename": "python/samples/agentchat_streamlit/README.md"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis Python script defines a class `Agent` that facilitates chat interactions using an AI model. It utilizes the `yaml` library to load configuration settings for the AI model from a YAML file, and the `autogen_agentchat` library to handle chat functionalities. The main purpose of this script is to create an AI-powered conversational assistant that can respond to user inputs in a structured manner.\n\n## Class Definition\n\n### `Agent` Class\n\nThe `Agent` class encapsulates the logic required to initialize and interact with the AI model. It comprises an `__init__` method for setup and an asynchronous method called `chat`.\n\n#### `__init__` Method\n\n- **Initialization**: The `__init__` method is invoked when an instance of the `Agent` class is created.\n- **Configuration Loading**: It reads a configuration file named `model_config.yml` using the `yaml.safe_load()` method to safely parse the YAML content into Python data structures.\n- **Model Client Creation**: It utilizes the `ChatCompletionClient.load_component()` method to instantiate a model client based on the loaded configuration.\n- **Assistant Agent Instantiation**: The method then creates an instance of `AssistantAgent`, which represents the AI assistant. It sets the agent's name to \"assistant\" and provides a system message to guide its behavior, stating that it is a \"helpful AI assistant.\"\n\n```python\nwith open(\"model_config.yml\", \"r\") as f:\n    model_config = yaml.safe_load(f)\nmodel_client = ChatCompletionClient.load_component(model_config)\nself.agent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n```\n\n## Asynchronous Chat Method\n\n### `chat(prompt: str) -> str`\n\nThe `chat` method is designed to facilitate interaction with the assistant. It accepts a single parameter:\n\n- **`prompt`**: A string representing the user's input question or statement.\n\n#### Method Flow\n\n1. **Message Creation**: The method begins by preparing a `TextMessage` object, wrapping the user's input (`prompt`) in a structured format. This object is flagged with a `source` of \"user\".\n  \n2. **Message Handling**: It sends the created message to the `on_messages` method of the `AssistantAgent` instance. This is done asynchronously using the `await` keyword, which allows for concurrent execution without blocking.\n\n3. **Cancellation Support**: The `CancellationToken()` is passed to enable the ability to cancel the operation if required. This feature ensures that the chat operation can be halted mid-process if needed, which can be crucial in responsive applications.\n\n4. **Response Validation**: Once the model processes the message and generates a response, the output is checked to confirm it is indeed a `TextMessage`. An assertion is used for this validation.\n  \n5. **Response Return**: Finally, the method extracts the content of the response and returns it as a string to the caller.\n\n```python\nasync def chat(self, prompt: str) -> str:\n    response = await self.agent.on_messages(\n        [TextMessage(content=prompt, source=\"user\")],\n        CancellationToken(),\n    )\n    assert isinstance(response.chat_message, TextMessage)\n    return response.chat_message.content\n```\n\n## Conclusion\n\nThis script serves as a foundational element for building an interactive AI assistant capable of engaging in chat-based conversations. It efficiently manages configuration loading, model instantiation, and message handling with appropriate asynchronous programming practices. Users can leverage this `Agent` class to create applications that require responsive, intelligent dialogue capabilities. The design is modular, allowing for easy integration and modification in larger systems tailored for specific conversational needs.",
    "filename": "python/samples/agentchat_streamlit/agent.py"
  },
  {
    "code": false,
    "content": "# AI Chat Assistant Documentation\n\n## Overview\nThis script implements an interactive AI chat assistant using Streamlit, a web application framework for Python. The application allows users to send messages and receive responses from an AI agent asynchronously. The chat interface maintains a history of the conversation, preserving context across interactions with the user.\n\n## Import Statements\nThe script begins with import statements for the necessary libraries:\n\n- `asyncio`: This library is used to run asynchronous code, enabling efficient handling of I/O operations.\n- `streamlit as st`: This imports the Streamlit library, which is used to create the web interface of the chat application.\n- `from agent import Agent`: This imports an `Agent` class from a separate file named `agent.py`, which presumably contains the logic for handling user inputs and generating AI responses.\n\n## Main Function\nThe core functionality of the application is encapsulated within the `main()` function. Below is a step-by-step breakdown of its workflow.\n\n### Setting Page Configuration\nThe first task inside the `main` function is to configure the Streamlit page settings, including the title and icon:\n\n```python\nst.set_page_config(page_title=\"AI Chat Assistant\", page_icon=\"\ud83e\udd16\")\nst.title(\"AI Chat Assistant \ud83e\udd16\")\n```\n\n### Session State Initialization\nThe script uses Streamlit's `session_state` to maintain the state across user interactions. It checks for the existence of an `agent` object and a `messages` list, initializing them if they do not exist:\n\n```python\nif \"agent\" not in st.session_state:\n    st.session_state[\"agent\"] = Agent()\n\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []\n```\n\n- **`st.session_state[\"agent\"]`**: This holds an instance of the `Agent` class, which likely contains methods for processing user input and generating AI responses.\n- **`st.session_state[\"messages\"]`**: This is a list that stores the chat history, including both user and assistant messages.\n\n### Displaying the Chat History\nThe script then iterates over the stored messages and displays them in the chat interface:\n\n```python\nfor message in st.session_state[\"messages\"]:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n```\n\n- Each message is displayed with the appropriate role (either user or assistant) using Streamlit's `chat_message` component.\n\n### User Input\nNext, the script provides an input field for the user to type their message:\n\n```python\nprompt = st.chat_input(\"Type a message...\")\n```\n\nIf the user enters a message, it is appended to the `messages` list and displayed in the chat:\n\n```python\nif prompt:\n    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n```\n\n### Processing the Chat Prompt\nOnce the user message is displayed, the script calls the chat method from the `Agent` instance to generate a response asynchronously:\n\n```python\nresponse = asyncio.run(st.session_state[\"agent\"].chat(prompt))\n```\n\nThis line uses `asyncio.run` to call the asynchronous `chat` method, which is expected to handle the AI's response logic.\n\n### Displaying the AI's Response\nThe AI's response is then stored in the `messages` list and displayed in the chat interface:\n\n```python\nst.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\nwith st.chat_message(\"assistant\"):\n    st.markdown(response)\n```\n\nThis step ensures that both user input and AI responses are visually represented in the conversation flow.\n\n## Entry Point\nThe script concludes with a check to see if it is being run as the main module:\n\n```python\nif __name__ == \"__main__\":\n    main()\n```\n\nIf so, it calls the `main()` function to start the application.\n\n## Conclusion\nThis script effectively combines the functionalities of a chat interface and an AI agent using Streamlit and asynchronous programming. It manages chat history, allows user interaction, and updates dynamically, creating a seamless conversational experience. The modular approach, utilizing a separate `Agent` class, hints at a clean separation of concerns, facilitating maintainability and extensibility for the AI capabilities.",
    "filename": "python/samples/agentchat_streamlit/main.py"
  },
  {
    "content": "# Async Human-in-the-Loop Example\n\nAn example showing human-in-the-loop which waits for human input before making the tool call.\n\n## Prerequisites\n\nFirst, you need a shell with AutoGen core and required dependencies installed.\n\n```bash\npip install \"autogen-ext[openai,azure]\" \"pyyaml\"\n```\n\n## Model Configuration\n\nThe model configuration should defined in a `model_config.yml` file.\nUse `model_config_template.yml` as a template.\n\n## Running the example\n\n```bash\npython main.py\n```",
    "filename": "python/samples/core_async_human_in_the_loop/README.md"
  },
  {
    "code": false,
    "content": "# Async Human in the Loop System Documentation\n\n## Overview\nThis code demonstrates an asynchronous human-in-the-loop system designed to facilitate meeting scheduling using two primary agents: an assistant and a user proxy. The assistant utilizes a tool to schedule meetings while the user proxy simulates a slow human response. The system allows for interaction between an AI assistant and a human user, ensuring efficient handling of user input and state management.\n\n## Components\n\n### Agents\n1. **Scheduling Assistant Agent**:\n   - This agent interacts with the user to gather meeting details, invokes scheduling tools, and sends messages back to the user.\n   - It maintains a buffered context to manage conversation history and uses a chat completion client to generate responses.\n\n2. **Slow User Proxy Agent**:\n   - Acts as a proxy to replicate the latency of a real human user.\n   - Forwards messages from the assistant to a human counterpart, while managing its own conversation state.\n\n### Message Structures\nThe system uses multiple message classes for structured communication:\n- **TextMessage**: A base structure for text messages.\n- **UserTextMessage** & **AssistantTextMessage**: Specialized structures derived from `TextMessage` for distinguishing the source of messages.\n- **GetSlowUserMessage**: Represents a query sent to the slow user asking for input.\n- **TerminateMessage**: Used to communicate the completion of meeting scheduling.\n\n### State Management\nA `MockPersistence` class is defined to manage the storage and retrieval of application state, allowing the system to save its current state whenever user input is awaited, or when termination occurs. This persistence mechanism is essential for rehydrating the system state in asynchronous workflows.\n\n## Tool Implementation\n### Scheduling Meeting Tool\nThe `ScheduleMeetingTool` class defines a tool that the assistant can use to schedule meetings. It is parameterized with inputs for recipient details, date, and time, and outputs a simple acknowledgment message once the meeting is scheduled.\n\n```python\nclass ScheduleMeetingTool(BaseTool[ScheduleMeetingInput, ScheduleMeetingOutput]):\n    ...\n```\nHere, the tool's functionality is encapsulated in the `run` method that simulates scheduling by logging the operation.\n\n## Intervention Handlers\nTwo intervention handlers manage interactions with the system:\n1. **NeedsUserInputHandler**: Monitors when user input is required by checking for messages that prompt user responses.\n2. **TerminationHandler**: Monitors for termination messages indicating the conclusion of the task, enabling graceful shutdown.\n\nThese handlers work in conjunction with the executed agents to determine when to pause execution or finalize the process.\n\n## Main Asynchronous Function\nThe `main` function serves as the entry point of the program. Its responsibilities include:\n- Initializing necessary components.\n- Registering user and scheduling agents.\n- Managing runtime states and responding to user input.\n- The core workflow runs until the user input is handled or the operation is terminated.\n\nParameters:\n- `model_config`: Configuration for loading the chat model.\n- `latest_user_input`: Optional argument for providing an initial user input.\n\nHere, the operational logic is extensively evaluated to handle different scenarios and yield user responses, if necessary.\n\n## User Input Handling\nAn `ainput` function is provided to facilitate asynchronous user input gathering. It leverages `ThreadPoolExecutor` to allow input collection without blocking the event loop.\n\n```python\nasync def ainput(prompt: str = \"\") -> str:\n    ...\n```\nThis implementation ensures that user interactions do not freeze the system, maintaining responsiveness.\n\n## Execution Flow\n1. The program starts by loading the model configuration from a YAML file.\n2. It registers the agents and initializes conversation flows.\n3. Depending on whether there is already user input available or not, the system loads the existing state or begins a new conversation.\n4. The runtime is executed until it is either terminated or requires user input; at which point the state is saved for future reference.\n\n### Looping User Interactions\nIf the system requires further input from the user, it prompts for it in a loop via the `run_main` function, capturing the interactions dynamically.\n\n## Conclusion\nThis example provides a flexible framework for creating human-in-the-loop systems that can seamlessly integrate AI feedback with human input, accommodating the inherent delays in real-world interactions. This robust implementation can be expanded for more intricate applications, ensuring that the core principles of responsiveness and state management are upheld.",
    "filename": "python/samples/core_async_human_in_the_loop/main.py"
  },
  {
    "content": "# Core ChainLit Integration Sample\n\nIn this sample, we will demonstrate how to build simple chat interface that\ninteracts with a [Core](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)\nagent or a team, using [Chainlit](https://github.com/Chainlit/chainlit),\nand support streaming messages.\n\n## Overview\n\nThe `core_chainlit` sample is designed to illustrate a simple use case of ChainLit integrated with a single-threaded agent runtime. It includes the following components:\n\n- **Single Agent**: A single agent that operates within the ChainLit environment.\n- **Group Chat**: A group chat setup featuring two agents:\n  - **Assistant Agent**: This agent responds to user inputs.\n  - **Critic Agent**: This agent reflects on and critiques the responses from the Assistant Agent.\n- **Closure Agent**: Utilizes a closure agent to aggregate output messages into an output queue.\n- **Token Streaming**: Demonstrates how to stream tokens to the user interface.\n- **Session Management**: Manages the runtime and output queue within the ChainLit user session.\n\n## Requirements\n\nTo run this sample, you will need:\n- Python 3.8 or higher\n- Installation of necessary Python packages as listed in `requirements.txt`\n\n## Installation\n\nTo run this sample, you will need to install the following packages:\n\n```shell \npip install -U chainlit autogen-core autogen-ext[openai] pyyaml\n```\n\nTo use other model providers, you will need to install a different extra\nfor the `autogen-ext` package.\nSee the [Models documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html) for more information.\n\n## Model Configuration\n\nCreate a configuration file named `model_config.yaml` to configure the model\nyou want to use. Use `model_config_template.yaml` as a template.\n\n\n## Running the Agent Sample\n\nThe first sample demonstrate how to interact with a single AssistantAgent\nfrom the chat interface.\nNote: cd to the sample directory.\n\n```shell\nchainlit run app_agent.py\n```\n\n## Running the Team Sample\n\nThe second sample demonstrate how to interact with a team of agents from the\nchat interface.\n\n```shell\nchainlit run app_team.py -h\n```\n\nThere are two agents in the team: one is instructed to be generally helpful\nand the other one is instructed to be a critic and provide feedback.",
    "filename": "python/samples/core_chainlit/README.md"
  },
  {
    "code": false,
    "content": "# Simple Assistant Agent Documentation\n\n## Overview\n\nThe `SimpleAssistantAgent` class is designed for creating an interactive assistant capable of handling messages in a chat-like environment. It utilizes a chat completion model combined with various tools for function execution. The agent processes user inputs, retrieves responses from the model, and performs requested operations based on defined tools. It supports streaming responses, allowing real-time interaction and message handling in both individual and group chat scenarios.\n\n## Key Components and Imports\n\nThe code imports several essential components, including:\n\n- **Type Annotations**: Types such as `AsyncGenerator`, `List`, and `Optional` for better code readability and structure.\n- **Asyncio**: For asynchronous programming, facilitating non-blocking operations.\n- **Data Classes**: The `dataclass` decorator simplifies the creation of classes intended primarily for storing data.\n- **Autogen Core Models and Tools**: Various components for message handling, function calling, and tool execution.\n- **Pydantic**: Used for data validation and settings management, ensuring that defined models adhere to specified types.\n\n## Data Classes\n\n### Message\n\n```python\n@dataclass\nclass Message:\n    content: str\n```\nThe `Message` class is a simple data structure to encapsulate a string content, representing messages exchanged in the chat.\n\n### StreamResult\n\n```python\nclass StreamResult(BaseModel):\n    content: str | CreateResult | AssistantMessage\n    source: str\n```\nThe `StreamResult` class is defined using Pydantic. It holds the content of the streamed message, which can be of different types (string, `CreateResult`, or `AssistantMessage`), along with a source identifier.\n\n### GroupChatMessage\n\n```python\nclass GroupChatMessage(BaseModel):\n    body: UserMessage\n```\nThis class wraps a `UserMessage`, allowing for structured representation of messages originating from group chats.\n\n### RequestToSpeak\n\n```python\nclass RequestToSpeak(BaseModel):\n    pass\n```\nA placeholder class for handling requests to speak, currently without attributes.\n\n## Main Class: SimpleAssistantAgent\n\n### Initialization\n\n```python\nclass SimpleAssistantAgent(RoutedAgent):\n    def __init__(self, name: str, system_message: str, model_client: ChatCompletionClient, tool_schema: List[Tool] = [], ...):\n```\nThe `SimpleAssistantAgent` extends `RoutedAgent`, initiating with parameters such as agent name, a system message, and various configurations including tools and model client. It sets up necessary attributes, including a chat history list that tracks interactions.\n\n### Model Client Interaction\n\n#### _call_model_client\n\n```python\nasync def _call_model_client(self, cancellation_token: CancellationToken) -> AsyncGenerator[str | CreateResult, None]:\n```\nThis asynchronous method interacts with the chat model, sending the system message and chat history. It yields streamed responses, which could be either streaming text or final results encapsulated in `CreateResult`.\n\n#### _execute_tool_call\n\n```python\nasync def _execute_tool_call(self, call: FunctionCall, cancellation_token: CancellationToken) -> FunctionExecutionResult:\n```\nThis method executes specified tools based on user requests. It validates the tool's existence, attempts to run it with provided arguments, and returns a structured result object containing execution details.\n\n### Message Handling\n\n#### handle_user_message\n\n```python\n@message_handler\nasync def handle_user_message(self, message: UserMessage, ctx: MessageContext) -> Message:\n```\nThis core message handler is responsible for processing user messages. It appends the incoming message to the history, retrieves responses from the model, and checks if any tool calls are required. If tool calls are present, it handles executing them and updating the chat history with the results.\n\n#### handle_message\n\n```python\n@message_handler\nasync def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:\n```\nThis handler processes group chat messages by appending them to the chat history, but does not execute any immediate action until a `RequestToSpeak` is received.\n\n#### handle_request_to_speak\n\n```python\n@message_handler\nasync def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:\n```\nThis handler deals with requests for the assistant to \"speak.\" It processes history reflection and retrieves chat responses again, ensuring the assistant can adopt the persona contextually.\n\n## Error Handling\n\nThe design incorporates several error handling mechanisms:\n- It checks for the type validity of incoming chunks from the model client.\n- Raising `RuntimeError` if the chunk type is invalid or if no final model result is returned during streaming.\n\n## Summary\n\nThe `SimpleAssistantAgent` facilitates a robust framework for managing chat interactions, capable of processing both individual user messages and group communications. Its integration with learning models and tools allows it to execute complex requests while maintaining a conversational history, ensuring a fluid user experience. This class structure is extendable, enabling developers to enhance functionality as needed.",
    "filename": "python/samples/core_chainlit/SimpleAssistantAgent.py"
  },
  {
    "code": false,
    "content": "# Overview of the Source Code\n\nThis Python script, built using the `chainlit` framework, creates an interactive chatbot application focused on providing weather information. Through a combination of asynchronous functions and agent-based architecture, it allows users to interact with a weather chatbot that fetches real-time weather details based on user input.\n\n## Key Components and Libraries\n\nThe script employs several libraries and components:\n- **chainlit**: A library for building and deploying chat applications.\n- **yaml**: Used for parsing configuration files.\n- **asyncio**: Manages asynchronous operations, crucial for real-time processing.\n- **autogen_core**: Contains various classes and functions that facilitate agent-driven design in the chatbot.\n\n## Global Constants\n\nTwo constants are defined at the beginning:\n- **TASK_RESULTS_TOPIC_TYPE**: A string constant representing the type for task results.\n- **task_results_topic_id**: An instance of `TopicId` initialized with `TASK_RESULTS_TOPIC_TYPE` and a default source, used for message routing.\n\n## Chat Startup processes\n\n### Setting Up Starters\n\nThe `set_starts()` asynchronous function initializes the chat with predefined starter messages. These messages prompt users for interaction:\n- A greeting message asking how the bot can assist.\n- A prompt for fetching weather information in New York City.\n\n### Starting the Chat\n\nThe function `start_chat()` is triggered when a user initiates the chat. It performs several crucial tasks:\n1. **Loading Model Configuration**: Reads the model configuration from a YAML file for chatbot behaviors and settings.\n2. **Creating Model Client**: Instantiates a `ChatCompletionClient` based on the loaded configuration.\n3. **Starting Agent Runtime**: Initializes a `SingleThreadedAgentRuntime` and stores it in the user session.\n4. **Creating Tool**: Registers a `FunctionTool`, which in this case allows retrieving weather data.\n5. **Setting Up Output Queue**: Constructs an asynchronous queue to hold the output stream data.\n\n## Agent Registration\n\nOnce the configuration and tools are set, the script:\n1. Registers a `SimpleAssistantAgent` named `weather_agent`, which encompasses the weather tool and model client.\n2. Defines a `ClosureAgent` that processes the streaming responses from the agent. The `output_result` function is specifically registered to handle these messages and place them into the output queue.\n\n## Function Definitions\n\n### Output Result Function\n\nThe `output_result` function is invoked whenever the `ClosureAgent` receives messages. Its role is to retrieve a queue stored in the session and place incoming messages into that queue for further processing.\n\n### Weather Fetch Function\n\nThe `get_weather()` function is defined as a tool to obtain weather information for a specified city. As a placeholder, it returns a static response indicating sunny weather at 73 degrees, highlighting the function\u2019s illustrative purpose.\n\n## Handling User Messages\n\n### Chat Functionality\n\nThe `chat()` function acts as the main handler for incoming messages from users. It performs the following:\n1. Retrieves the runtime and queue from the user session.\n2. Sends the user's message to the `weather_agent`.\n3. Begins consuming items from the queue until a message indicating the completion (through `CreateResult`) is received, streaming each response back to the user in real time.\n\n### Asynchronous Operations\n\nBy utilizing the `asyncio` library, the script is capable of handling multiple I/O-bound operations concurrently, ensuring that users receive timely updates while the bot processes their requests effectively.\n\n## Summary \n\nIn summary, this script demonstrates an advanced chatbot implementation that leverages asynchronous programming and agent-based design to deliver weather-related responses interactively. The architecture allows for extensibility and real-time interaction, providing a solid foundation for developers interested in building conversational agents. It exemplifies how different components can work in harmony to facilitate user engagement while maintaining an efficient backend processing system.",
    "filename": "python/samples/core_chainlit/app_agent.py"
  },
  {
    "code": false,
    "content": "# High-Level Overview of the Script\n\nThis script is designed to facilitate a multi-agent chat system that includes an assistant agent and a critic agent, allowing for interactive conversations with systematic feedback. The system operates asynchronously, employing a message-driven architecture to handle and respond to various inputs from users and the agents involved in the chat.\n\n## Key Components and Imports\n\nThe script imports essential libraries and modules:\n- **`typing`**: For type hinting, especially the `List` type.\n- **`chainlit`**: A framework for building applications using chat interactions.\n- **`yaml`**: For reading configuration files in the YAML format.\n- **`uuid`** and **`string`**: For generating unique identifiers and string manipulations.\n- **`asyncio`**: To enable asynchronous programming, allowing concurrent execution of tasks.\n- **`autogen_core`**: A custom module that provides various classes and functions necessary for handling agent behavior, message context, and topic subscriptions.\n\n## Agent and Context Classes\n\n### 1. `GroupChatManager`\n\nThis class inherits from `RoutedAgent`, implementing the logic for managing a group chat:\n- **Initialization**: It takes a list of participant topic types (for assistant and critic) and a model client to facilitate chat model operations. It maintains a chat history of messages.\n  \n- **`handle_message` Method**: \n  - It processes incoming messages, appending them to the chat history.\n  - Approval messages can stop the chat if received from either the User or Critic.\n  - Implements a round-robin mechanism to determine which participant should speak next and sends the \"RequestToSpeak\" message accordingly.\n\n### 2. `output_result`\n\nAn asynchronous function designed to handle results when a closure agent receives messages. It takes the resulting messages and places them into an output queue to be handled later.\n\n## Chat Initialization and Setup\n\n### 3. `start_chat`\n\nThis function initializes the chat environment when the chat starts:\n- **Model Client Setup**: Loads model configurations from a YAML file and initializes a `ChatCompletionClient`.\n  \n- **Runtime Creation**: Sets up a `SingleThreadedAgentRuntime`, which manages the lifecycle of the agents and message processing.\n  \n- **Assistant and Critic Agent Creation**: Registers both the assistant and critic agents to listen to specific topics relevant to their roles in the chat.\n  \n- **Group Chat Manager Registration**: It creates and registers an instance of `GroupChatManager` to coordinate interactions between the assistant and critic.\n  \n- **Closure Agent Registration**: Allows streamed responses to be sent into the output queue.\n\n### 4. Setting Initial Messages\n\nThe `set_starts` function provides predefined starter messages that a user can select to initiate conversations, such as writing poems or stories.\n\n## User Interaction and Message Handling\n\n### 5. `pass_msg_to_ui`\n\nThis function continuously listens to the output queue and streams messages to the UI:\n- It checks the type of stream message and constructs appropriate responses based on the message content.\n- It facilitates the dynamic display of messages, making interactive chat possible.\n\n### 6. `chat`\n\nThe `chat` function is triggered when a user sends a message:\n- It retrieves the current runtime and output queue from the user session.\n- A user message is published to the group chat via `GroupChatMessage`.\n- A task is created to handle messages meant for the UI through the `pass_msg_to_ui` function.\n\n## Conclusion\n\nIn summary, the script combines multiple asynchronously operated agents to create a conversational system where one agent provides assistance and the other critiques the provided assistance. Messages are structured and sent amongst the agents based on predetermined rules (such as approval for stopping conversations), creating a seamless interaction that enhances user experience through structured feedback. The use of an output queue ensures that responses are orderly and timely displayed in the user interface.",
    "filename": "python/samples/core_chainlit/app_team.py"
  },
  {
    "content": "# Chess Game Example\n\nAn example with two chess player agents that executes its own tools to demonstrate tool use and reflection on tool use.\n\n## Prerequisites\n\nFirst, you need a shell with AutoGen core and required dependencies installed.\n\n```bash\npip install \"autogen-ext[openai,azure]\" \"chess\" \"pyyaml\"\n```\n\n## Model Configuration\n\nThe model configuration should defined in a `model_config.yml` file.\nUse `model_config_template.yml` as a template.\n\n## Running the example\n\n```bash\npython main.py\n```",
    "filename": "python/samples/core_chess_game/README.md"
  },
  {
    "code": false,
    "content": "# Chess Game Simulation Documentation\n\n## Overview\nThis Python script simulates a chess game between two artificial intelligence (AI) agents. The agents communicate through a message-passing system, making moves based on game state reasoning. The game is managed using tools that encapsulate different functionalities related to chess gameplay, including fetching legal moves, making moves, and retrieving the current state of the chessboard.\n\n## Necessary Imports and Dependencies\nThe script begins with several import statements, which include essential libraries for functionality:\n- **argparse**: For handling command-line arguments.\n- **asyncio**: For managing asynchronous operations and concurrency.\n- **logging**: For logging events and debugging information.\n- **yaml**: For loading configuration files.\n- **chess**: A library for managing chess logic.\n- **pydantic**: For creating data models.\n- **autogen_core**: Custom library for managing agents and interactions.\n\n## Data Models\n### `TextMessage`\nThis class is a Pydantic model that represents a message exchanged between agents. It contains:\n- `source`: A string indicating the origin of the message.\n- `content`: A string representing the message text.\n\n## Player Agent Class\n### `PlayerAgent`\nThis class extends `RoutedAgent`, allowing agents to handle messages within a specified framework. Key elements of the `PlayerAgent` class include:\n\n- **Constructor**: Initializes the agent with a description, specific instructions, a model client for AI communication, tool schemas, and a tool agent type.\n- **Message Handler**: Implements the `handle_message` function, which:\n  1. Adds received messages to the model context.\n  2. Processes the message using a tool agent caller loop, facilitating interactions with tools.\n  3. Publishes the assistant message response back to the message topic.\n\n## Chess Game Logic\n### Turn Validation\n#### `validate_turn`\nThis utility function ensures that it is the correct player's turn to move. It checks the last move in the move stack and raises a `ValueError` if the player attempts to move out of turn.\n\n### Legal Moves Retrieval\n#### `get_legal_moves`\nThis function retrieves all legal moves for a specified player (either \"white\" or \"black\") while validating that it is indeed that player's turn. It outputs a formatted string listing the possible moves.\n\n### Board Representation\n#### `get_board`\nThis function returns a string representation of the current chessboard state.\n\n### Move Execution\n#### `make_move`\nThis function facilitates executing a move on the chessboard. It:\n1. Validates it is the correct player's turn.\n2. Converts a move in UCI format to a chess move object.\n3. Updates the board and logs details of the performed move, including the moving piece and its new position.\n4. Returns a description of the move made.\n\n## Game Setup and Execution\n### `chess_game`\nThis asynchronous function initializes the chess game by:\n1. Creating a new chess board.\n2. Defining tools for both players that allow them to retrieve legal moves, make moves, and get the current board state.\n3. Registering tool agents for both black and white players.\n\n### Main Functionality\n#### `main`\nThe `main` function serves as the entry point to the script. It:\n1. Initializes the agent runtime and loads AI model configurations from a specified file.\n2. Calls the `chess_game` function to start the game.\n3. Sends an initial message to notify player white to make the first move.\n\n## Command-Line Interface\nThe script includes a command-line interface that allows the user to specify:\n- Verbose logging mode for debugging output.\n- A path to the model configuration file, defaulting to `model_config.yml`.\n\nAfter processing command-line arguments, the script synchronously executes the `main` function using `asyncio.run`.\n\n## Conclusion\nThis script effectively simulates a chess game between two AI agents managed by an agent runtime. The well-defined structure facilitates easy adjustments to agent behavior, tool capabilities, and game rules through encapsulation and modular programming practices. The uniform handling of messages and state transitions provides a robust framework for future enhancements and potential expansions to the chess game simulation.",
    "filename": "python/samples/core_chess_game/main.py"
  },
  {
    "content": "# Distributed Group Chat\n\nThis example runs a gRPC server using [GrpcWorkerAgentRuntimeHost](../../src/autogen_core/application/_worker_runtime_host.py) and instantiates three distributed runtimes using [GrpcWorkerAgentRuntime](../../src/autogen_core/application/_worker_runtime.py). These runtimes connect to the gRPC server as hosts and facilitate a round-robin distributed group chat. This example leverages the [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service) to implement writer and editor LLM agents. Agents are instructed to provide concise answers, as the primary goal of this example is to showcase the distributed runtime rather than the quality of agent responses.\n\n## Setup\n\n### Setup Python Environment\n\n1. Create a virtual environment and activate it. (e.g. `python3.12 -m venv .venv && source .venv/bin/activate`)\n2. Install dependencies.\n\n```bash\npip install \"autogen-ext[openai,azure,chainlit,rich]\" \"pyyaml\"\n```\n\n### General Configuration\n\nIn the `config.yaml` file, you can configure the `client_config` section to connect the code to the Azure OpenAI Service.\n\n### Authentication\n\nThe recommended method for authentication is through Azure Active Directory (AAD), as explained in [Model Clients - Azure AI](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/framework/model-clients.html#azure-openai). This example works with both the AAD approach (recommended) and by providing the `api_key` in the `config.yaml` file.\n\n## Run\n\n### Run Through Scripts\n\nThe [run.sh](./run.sh) file provides commands to run the host and agents using [tmux](https://github.com/tmux/tmux/wiki). The steps for this approach are:\n\n1. Install tmux.\n2. Activate the Python environment: `source .venv/bin/activate`.\n3. Run the bash script: `./run.sh`.\n\nHere is a screen recording of the execution:\n\n[![Distributed Group Chat Demo with Simple UI Integration](https://img.youtube.com/vi/503QJ1onV8I/0.jpg)](https://youtu.be/503QJ1onV8I?feature=shared)\n\n**Note**: Some `asyncio.sleep` commands have been added to the example code to make the `./run.sh` execution look sequential and visually easy to follow. In practice, these lines are not necessary.\n\n### Run Individual Files\n\nIf you prefer to run Python files individually, follow these steps. Note that each step must be run in a different terminal process, and the virtual environment should be activated using `source .venv/bin/activate`.\n\n1. `python run_host.py`: Starts the host and listens for agent connections.\n2. `chainlit run run_ui.py  --port 8001`: Starts the Chainlit app and UI agent and listens on UI topic to display messages. We're using port 8001 as the default port 8000 is used to run host (assuming using same machine to run all of the agents)\n3. `python run_editor_agent.py`: Starts the <img src=\"./public/avatars/editor.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> editor agent and connects it to the host.\n4. `python run_writer_agent.py`: Starts the <img src=\"./public/avatars/writer.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> writer agent and connects it to the host.\n5. `python run_group_chat_manager.py`: Run chainlit app which starts <img src=\"./public/avatars/group_chat_manager.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> group chat manager agent and sends the initial message to start the conversation.\n\n## What's Going On?\n\nThe general flow of this example is as follows:\n\n0. The UI Agent runs starts the UI App, listens for stream of messages in the UI topic and displays them in the UI.\n1. The <img src=\"./public/avatars/group_chat_manager.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> Group Chat Manager, on behalf of <img src=\"./public/avatars/user.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `User`, sends a `RequestToSpeak` request to the <img src=\"./public/avatars/writer.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `writer_agent`.\n2. The <img src=\"./public/avatars/writer.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `writer_agent` writes a short sentence into the group chat topic.\n3. The <img src=\"./public/avatars/editor.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `editor_agent` receives the message in the group chat topic and updates its memory.\n4. The <img src=\"./public/avatars/group_chat_manager.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> Group Chat Manager receives the message sent by the writer into the group chat simultaneously and sends the next participant, the <img src=\"./public/avatars/editor.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `editor_agent`, a `RequestToSpeak` message.\n5. The <img src=\"./public/avatars/editor.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `editor_agent` sends its feedback to the group chat topic.\n6. The <img src=\"./public/avatars/writer.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> `writer_agent` receives the feedback and updates its memory.\n7. The <img src=\"./public/avatars/group_chat_manager.png\" width=\"20\" height=\"20\" style=\"vertical-align:middle\"> Group Chat Manager receives the message simultaneously and repeats the loop from step 1.\n\nHere is an illustration of the system developed in this example:\n\n```mermaid\ngraph TD;\n    subgraph Host\n        A1[GRPC Server]\n        wt[Writer Topic]\n        et[Editor Topic]\n        ut[UI Topic]\n        gct[Group Chat Topic]\n    end\n    all_agents[All Agents -  Simplified Arrows!] --> A1\n\n    subgraph Distributed Writer Runtime\n        wt -.->|2 - Subscription| writer_agent\n        gct -.->|4 - Subscription| writer_agent\n        writer_agent -.->|3.1 - Publish: UI Message| ut\n        writer_agent -.->|3.2 - Publish: Group Chat Message| gct\n    end\n\n    subgraph Distributed Editor Runtime\n        et -.->|6 - Subscription| editor_agent\n        gct -.->|4 - Subscription| editor_agent\n        editor_agent -.->|7.1 - Publish: UI Message| ut\n        editor_agent -.->|7.2 - Publish: Group Chat Message| gct\n    end\n\n    subgraph Distributed Group Chat Manager Runtime\n        gct -.->|4 - Subscription| group_chat_manager\n        group_chat_manager -.->|1 - Request To Speak| wt\n        group_chat_manager -.->|5 - Request To Speak| et\n        group_chat_manager -.->|\\* - Publish Some of to UI Message| ut\n    end\n\n    subgraph Distributed UI Runtime\n        ut -.->|\\* - Subscription| ui_agent\n    end\n\n\n    style wt fill:#beb2c3,color:#000\n    style et fill:#beb2c3,color:#000\n    style gct fill:#beb2c3,color:#000\n    style ut fill:#beb2c3,color:#000\n    style writer_agent fill:#b7c4d7,color:#000\n    style editor_agent fill:#b7c4d7,color:#000\n    style group_chat_manager fill:#b7c4d7,color:#000\n    style ui_agent fill:#b7c4d7,color:#000\n\n```\n\n## TODO:\n\n- [ ] Properly handle chat restarts. It complains about group chat manager being already registered\n- [ ] Add streaming to the UI like [this example](https://docs.chainlit.io/advanced-features/streaming) when [this bug](https://github.com/microsoft/autogen/issues/4213) is resolved",
    "filename": "python/samples/core_distributed-group-chat/README.md"
  },
  {
    "code": false,
    "content": "# Documentation for Group Chat System\n\n## Overview\n\nThis script implements a group chat system using a Large Language Model (LLM). The architecture includes multiple agents that interact in a chat environment, handling user messages, managing conversation context, and publishing responses to a user interface (UI). The primary components of this system are `BaseGroupChatAgent`, `GroupChatManager`, and `UIAgent`. Each of these classes has specific responsibilities related to message handling and conversation management.\n\n## BaseGroupChatAgent Class\n\n### Purpose\nThe `BaseGroupChatAgent` is designed to act as a participant in a group chat, leveraging an LLM to generate responses based on the conversation history and predefined system messages.\n\n### Initialization\n- **Attributes**:\n  - `description`: Characterizes the agent.\n  - `group_chat_topic_type`: Represents the topic this agent is focused on.\n  - `model_client`: An instance of `ChatCompletionClient` for interacting with the LLM.\n  - `system_message`: A `SystemMessage` that provides context.\n  - `chat_history`: Maintains a history of the conversation.\n  - `ui_config`: Configuration details for UI.\n- **Console**: An instance of `Console` from the `rich` library to handle formatted output in the console.\n\n### Message Handling\n1. **handle_message** - This method processes incoming messages, updating the chat history and appending the source of the message. It allows the agent to recognize when it has received a message directed to it.\n   \n2. **handle_request_to_speak** - On receiving a request to speak, this method generates a response using the LLM based on the chat history and system message. It logs the response to the console and publishes the message to both the UI and backend.\n\n## GroupChatManager Class\n\n### Purpose\nThe `GroupChatManager` oversees the entire group conversation, coordinating between various participants and determining which one should respond next based on the conversation's context.\n\n### Initialization\n- **Attributes**:\n  - `model_client`: Used to interact with the LLM.\n  - `participant_topic_types`: List of topics for different participants.\n  - `participant_descriptions`: Descriptions of the respective participants.\n  - `max_rounds`: The maximum number of conversation rounds allowed.\n  - `chat_history`: Stores all messages exchanged in the conversation.\n- **Console**: For formatted console output.\n\n### Message Handling\n1. **handle_message** - This method handles incoming user messages, appending them to the chat history and formatting the message context. It builds a prompt for the LLM to select the next participant's role and determines if the conversation should finish or continue with a new role.\n\n### Role Selection Mechanism\n- Constructs a system message for the LLM based on the chat history and participant roles.\n- If the LLM determines that the conversation has reached completion, it sends a final message to the UI; otherwise, it identifies the next participant to \"speak.\"\n\n## UIAgent Class\n\n### Purpose\nThe `UIAgent` serves as the interface between the chat system and its users, processing message chunks that are displayed in the user interface.\n\n### Initialization\n- **Attributes**:\n  - `on_message_chunk_func`: A callback function that processes message chunks asynchronously upon receipt.\n\n### Message Processing\n- **handle_message_chunk** - This method receives a chunk of a message and invokes the specified callback function to handle it, allowing for flexibility in how message data is processed by the UI.\n\n## Message Publishing Functions\n\n### publish_message_to_ui\nThis asynchronous function publishes user messages to the UI in a chunked format, allowing for more dynamic display. It:\n1. Generates unique message IDs.\n2. Streams the message word by word with specified delays to simulate typing.\n3. Sends a final chunk signaling the completion of the message.\n\n### publish_message_to_ui_and_backend\nThis function combines the publishing of messages to both the UI and the backend:\n1. Calls `publish_message_to_ui` to handle the UI display.\n2. Immediately publishes a `GroupChatMessage` to the backend, ensuring all messages are logged and retrievable for history or analysis.\n\n## Summary\n\nThe implemented system efficiently integrates multiple roles in a dynamic chat environment, leveraging the capabilities of LLMs to facilitate conversation. Each component is distinct yet interconnected, ensuring seamless interactions and message processing. The use of asynchronous programming enhances the responsiveness of the chat system, making it adaptable to various user inputs and chat dynamics.",
    "filename": "python/samples/core_distributed-group-chat/_agents.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis Python script utilizes data classes and models from various libraries to define a structure for a chat application involving agents, configuration settings, and message handling. The code primarily focuses on the representation of chat messages, agent configurations, and application settings.\n\n## Imports\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nfrom autogen_core.models import LLMMessage\nfrom autogen_ext.models.openai.config import AzureOpenAIClientConfiguration\nfrom pydantic import BaseModel\n```\n\nThis section imports necessary libraries:\n- `dataclass` from the `dataclasses` module for creating classes with minimal boilerplate.\n- `Dict` from `typing` to specify dictionary types.\n- `LLMMessage` from `autogen_core.models`, assumed to represent messages from a large language model (LLM).\n- `AzureOpenAIClientConfiguration` from `autogen_ext.models.openai.config`, likely for configuring connections to Azure OpenAI services.\n- `BaseModel` from `pydantic`, which is a base class for creating data models with validation.\n\n## Message Handling\n\n### GroupChatMessage\n\n```python\nclass GroupChatMessage(BaseModel):\n    \"\"\"Implements a sample message sent by an LLM agent\"\"\"\n    body: LLMMessage\n```\n\nThe `GroupChatMessage` class represents a message intended for a group chat, specifically illustrated as being generated by an LLM agent. It has a single attribute called `body`, which is typed to be of `LLMMessage`. This structure likely signifies that the content of a message will be an LLM-generated piece of text.\n\n### RequestToSpeak\n\n```python\nclass RequestToSpeak(BaseModel):\n    \"\"\"Message type for agents to speak\"\"\"\n    pass\n```\n\nThe `RequestToSpeak` class is a placeholder that serves as a message type for agents wishing to initiate speaking. It currently has no attributes or methods but could be extended for specific features later on.\n\n## Chunking Messages\n\n### MessageChunk\n\n```python\n@dataclass\nclass MessageChunk:\n    message_id: str\n    text: str\n    author: str\n    finished: bool\n\n    def __str__(self) -> str:\n        return f\"{self.author}({self.message_id}): {self.text}\"\n```\n\nThe `MessageChunk` class annotates a chunk of a message that includes an identifier, the message text, the author of the message, and a boolean indicating if the message has been fully sent (finished). The `__str__` method provides a formatted string representation of the message chunk, encapsulating the author, message ID, and content.\n\n## Configuration Models\n\n### HostConfig\n\n```python\nclass HostConfig(BaseModel):\n    hostname: str\n    port: int\n\n    @property\n    def address(self) -> str:\n        return f\"{self.hostname}:{self.port}\"\n```\n\nThe `HostConfig` class is a model for network configuration, encapsulating the hostname and port of the server that hosts the chat application. It includes a property method `address`, which formats these attributes into a uniform address string.\n\n### GroupChatManagerConfig\n\n```python\nclass GroupChatManagerConfig(BaseModel):\n    topic_type: str\n    max_rounds: int\n```\n\n`GroupChatManagerConfig` defines configuration settings for managing group chats, including the type of topics retained and the maximum number of discussion rounds allowed.\n\n### ChatAgentConfig\n\n```python\nclass ChatAgentConfig(BaseModel):\n    topic_type: str\n    description: str\n    system_message: str\n```\n\nThe `ChatAgentConfig` class encapsulates configuration settings for chat agents, including a `topic_type`, a description of the agent's purpose, and a system message that may guide its behavior or interactions.\n\n### UIAgentConfig\n\n```python\nclass UIAgentConfig(BaseModel):\n    topic_type: str\n    artificial_stream_delay_seconds: Dict[str, float]\n\n    @property\n    def min_delay(self) -> float:\n        return self.artificial_stream_delay_seconds.get(\"min\", 0.0)\n\n    @property\n    def max_delay(self) -> float:\n        return self.artificial_stream_delay_seconds.get(\"max\", 0.0)\n```\n\nThe `UIAgentConfig` class manages the settings for a UI agent, described with `topic_type`, and parameters to simulate artificial streaming delays, allowing for smooth user experience customization. It includes properties for accessing minimum and maximum delay values.\n\n## Overall Application Configuration\n\n### AppConfig\n\n```python\nclass AppConfig(BaseModel):\n    host: HostConfig\n    group_chat_manager: GroupChatManagerConfig\n    writer_agent: ChatAgentConfig\n    editor_agent: ChatAgentConfig\n    ui_agent: UIAgentConfig\n    client_config: AzureOpenAIClientConfiguration = None  # type: ignore[assignment]\n```\n\nThe `AppConfig` class acts as a comprehensive model aggregating all configuration components required for the chat application. It includes instances of `HostConfig`, `GroupChatManagerConfig`, multiple `ChatAgentConfig` instances (for writer and editor agents), and `UIAgentConfig`. It also optionally holds an `AzureOpenAIClientConfiguration` for advanced functionalities. \n\nThis structured approach allows for a clear separation of concerns, encapsulating configuration settings specific to different functional areas within a single app-wide context.",
    "filename": "python/samples/core_distributed-group-chat/_types.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script is designed to manage configurations and serializers for an application interacting with Azure's OpenAI service. It includes functions to load configurations from a YAML file, retrieve serializers for specific types, and set logging levels across the application.\n\n## Imports\n\nThe following libraries and modules are imported for use in the script:\n\n- **logging**: Provides logging functionalities.\n- **os**: Used for file path operations.\n- **typing**: Supports type hints for better code clarity.\n- **yaml**: Facilitates the reading of YAML configuration files.\n- **_types**: Custom module presumably containing type definitions, such as `AppConfig`.\n- **autogen_core**: Presumed to assist in message serialization.\n- **autogen_ext.models.openai.config**: Contains the `AzureOpenAIClientConfiguration` class for Azure usage.\n- **azure.identity**: Offers Azure identity management utilities, including credential handling and token provisioning.\n\n## Function: load_config\n\n### Purpose\n\nThe `load_config` function reads a configuration file in YAML format, constructs an application configuration object, and sets up any necessary client configurations for Azure OpenAI.\n\n### Parameters\n\n- **file_path**: A string that indicates the path to the YAML configuration file. It defaults to \"config.yaml\" located in the script directory.\n\n### Description\n\n1. Initializes an empty dictionary `model_client`.\n2. Opens the specified YAML configuration file and loads its contents.\n3. Extracts `client_config` from the loaded configuration data and removes it to avoid inclusion in the main configuration.\n4. Constructs an `AppConfig` object with the remaining configuration data.\n5. Checks if the `api_key` in the client configuration is provided. If not, it sets up an Azure Active Directory token provider using the default credential.\n6. Assigns an instance of `AzureOpenAIClientConfiguration` to the client configuration which comprises the extracted client configuration and any generated AAD parameters.\n7. Returns the constructed `AppConfig` object.\n\n## Function: get_serializers\n\n### Purpose\n\nThe `get_serializers` function retrieves a list of message serializers associated with a given list of type definitions.\n\n### Parameters\n\n- **types**: An iterable collection of types for which serializers are to be fetched.\n\n### Description\n\n1. Initializes an empty list named `serializers`.\n2. Iterates over the provided `types`, attempting to get known serializers for each type by calling `try_get_known_serializers_for_type`.\n3. Each returned serializer is added to the `serializers` list.\n4. Finally, it returns the complete list of serializers.\n\n## Function: set_all_log_levels\n\n### Purpose\n\nThe `set_all_log_levels` function standardizes the logging level across all loggers within the application's logging hierarchy.\n\n### Parameters\n\n- **log_leve**: An integer representing the log level to be set (e.g., `DEBUG`, `INFO`, `WARNING`, etc.).\n\n### Description\n\n1. Iterates through all existing loggers maintained by the root logger's manager.\n2. Checks if the logger is an instance of `logging.Logger` to ensure it can have its level modified.\n3. Sets the specified log level for each valid logger object, allowing for a uniform logging configuration across the application.\n4. This function may be useful for silencing logs during development or when initializing the application.\n\n## Summary\n\nThis script automates the configuration loading and management for an application utilizing the Azure OpenAI API. By building on the capabilities provided by YAML for configuration and Azure for identity management, the script ensures that the application is properly configured for use with Azure services. Additionally, the serializer retrieval mechanism and logging level configuration aid in maintaining clarity and control over logging output during application execution. The functions provided are modular, allowing for easy integration and testing within larger systems.",
    "filename": "python/samples/core_distributed-group-chat/_utils.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis Python script functions as a part of a chat management system that utilizes asynchronous programming via the `asyncio` library. The primary components include the setup of an editor agent that interacts with a group chat framework, managing message types, and integrating with an AI model client for processing chat data. The script incorporates logging and configuration loading to set parameters for the operation.\n\n## Imports and Dependencies\n\nThe script imports several modules and classes:\n\n- `asyncio`: For asynchronous programming, allowing for non-blocking operations.\n- `logging`: For logging error messages.\n- `warnings`: To handle warning messages, specifically filtering specific user warnings.\n- Custom imports from local modules like `_agents`, `_types`, and `_utils`, indicating integration with a broader application framework.\n- Classes from `autogen_core` and `autogen_ext` are utilized to handle agent operations and AI API interactions.\n- Rich library components (`Console`, `Markdown`) are used for styled console output.\n\n## Main Functionality\n\n### `main` Function\n\nThe core function of the script is defined within the `main` asynchronous function. Here\u2019s a breakdown of its operations:\n\n1. **Logging Configuration**: It sets all log levels to error to reduce verbosity during execution, facilitating focus on critical issues.\n  \n2. **Runtime Initialization**: A `GrpcWorkerAgentRuntime` object is created, initialized with the host address specified in the provided configuration.\n\n3. **Message Serialization Setup**: The function adds message serializers for handling different message types (`RequestToSpeak`, `GroupChatMessage`, `MessageChunk`). This is crucial for ensuring messages are serialized and deserialized correctly during communication.\n\n4. **Startup Delay**: A brief asynchronous sleep (4 seconds) is introduced to allow for system readiness before proceeding.\n\n5. **Console Output**: It makes use of the `Console` class from the Rich library to print a styled Markdown message indicating that the Editor Agent is starting. \n\n6. **Agent Registration**: The function registers an instance of the `BaseGroupChatAgent`:\n   - It describes the agent through provided configuration settings.\n   - The agent will manage group chat interactions and responds to user input.\n   - The agent is associated with a specific topic type from the configuration and tied to the AI model client.\n\n7. **Subscription Management**: The function subscribes the editor agent to two topic types:\n   - One for handling editor commands.\n   - One for managing general group chat interactions.\n\n8. **Signal Handling**: It sets the runtime to stop when a specific signal is received, allowing for a graceful shutdown.\n\n9. **Model Client Closure**: Finally, the Azure OpenAI client connection is closed to free up resources, maintaining a clean shutdown procedure.\n\n### Conditional Execution\n\nThe segment at the bottom (`if __name__ == \"__main__\":`) ensures that the script executes the `main` function when run as a standalone program:\n\n- It configures logging settings similarly to the main function.\n- It filters out specific user warnings and then invokes the `load_config()` function to load necessary configurations.\n- The `asyncio.run()` method is employed to run the main asynchronous function, allowing for clean management of the event loop.\n\n## Summary and Purpose\n\nOverall, this script is designed to initiate an AI-driven editor agent that integrates into a group chat system. It sets up necessary configurations, manages message formats, and subscribes the agent to relevant message types. The combination of logging, warning management, and proper resource handling ensures that the script operates effectively within the larger application context. \n\nBy utilizing the Azure OpenAI model, the script enables intelligent interactions within a group chat, making it suitable for applications that require advanced conversational capabilities.",
    "filename": "python/samples/core_distributed-group-chat/run_editor_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for Group Chat Manager Script\n\nThis document provides an overview of a Python script designed to manage a group chat using an asynchronous framework. The script integrates several components, including message serialization, a chat manager, and a client to generate responses based on the input.\n\n## Overview\n\nThe script utilizes Python's `asyncio` module for asynchronous execution, alongside various imported modules that facilitate chat functionality, logging, configuration management, and messaging. The main feature of this script is to run a chat manager that can interact with users and generate responses using a pre-defined AI model. \n\n## Initialization and Logging Setup\n\nThe script initiates with the importation of several libraries that provide essential functionalities. This includes logging management, configuration loading, message serialization, and client interactions. Following the imports, `set_all_log_levels(logging.ERROR)` is executed to configure the logger to only display error messages, reducing log noise.\n\n### Key Components:\n- **Log Level Configuration**: Minimizes logging output to error messages only.\n- **Imported Libraries**: Functions for handling chat, messaging, and AI interactions.\n\n## Main Function Execution\n\nThe main function, `main`, takes an `AppConfig` object as its parameter. This object holds configuration settings necessary for executing the group chat manager. The function is marked with `async`, indicating that it will use asynchronous calls for operations like initializing the chat manager and handling messages.\n\n```python\nasync def main(config: AppConfig):\n    ...\n```\n\n### Steps Inside the Main Function\n\n1. **Runtime Initialization**: A `GrpcWorkerAgentRuntime` instance is created, using the host address from the passed configuration.\n  \n2. **Message Serializer Addition**: The runtime is configured with necessary message serializers for various message types.\n\n3. **Chat Manager Startup**: The script prints a Markdown message indicating the start of the group chat manager and calls its `start` method.\n\n4. **AI Client Creation**: An instance of `AzureOpenAIChatCompletionClient` is initialized to manage AI responses based on user messages.\n\n## Group Chat Manager Registration\n\nThe script registers the `GroupChatManager` within the runtime environment, passing it a lambda function that initializes it with pertinent parameters like model client, participant types, descriptions, and maximum rounds acceptable for chat interactions.\n\n```python\ngroup_chat_manager_type = await GroupChatManager.register(\n    ...\n)\n```\n\n### Parameters of Registration:\n- **Model Client**: The AI model used to handle user interactions.\n- **Participant Topics**: The types of participants in the chat.\n- **UI Configuration**: Configuration settings pertaining to the user interface.\n\n## Subscription and User Interaction\n\nOnce the chat manager is registered, the script establishes a subscription to the group chat topic utilizing the `TypeSubscription` class. This allows the group chat manager to receive messages related to that particular topic.\n\n```python\nawait group_chat_manager_runtime.add_subscription(\n   ...\n)\n```\n\n### Initiating Conversation\nA delay is implemented to allow the system to stabilize before sending an initial message to the UI, informing users about the responsible AI considerations. Following this, the script simulates a user input message requesting a short story.\n\n## Message Publishing\n\nThe script includes functionality to publish messages both to the UI and the backend. This occurs in two steps:\n1. **System Message**: An automated message is published by the group chat manager to inform users about the bot's role during the session.\n2. **User Message**: The request for a story is published using the `publish_message_to_ui_and_backend` function, sending it to both the user interface and backend systems.\n\n### Informative Console Output\nDuring the process, console outputs are used to provide feedback on actions being performed, such as simulating user input, which helps with visual tracking of the activity.\n\n## Cleanup and Termination\n\nAt the end of the script, both the group chat manager runtime and the AI client are instructed to close cleanly. The `stop_when_signal` method ensures that the manager finishes receiving messages before shutting down.\n\n```python\nawait group_chat_manager_runtime.stop_when_signal()\nawait model_client.close()\n```\n\n### Final Console Message\nThe script concludes by printing a message indicating that the manager has exited the chat, providing an indication of successful completion.\n\n## Script Entry Point\n\nThe script checks if it is being run as the main program and initializes the logging settings again. It filters out specific warnings related to model mismatches and directly invokes the `main` function within an `asyncio.run()` context using loaded configuration settings.\n\n```python\nif __name__ == \"__main__\":\n   ...\n   asyncio.run(main(load_config()))\n```\n\n## Conclusion\n\nThis script represents a structured approach to handling a group chat using asynchronous programming in Python. It leverages various modules for message handling, serialization, and AI interaction, delivering a user-friendly chat manager that can facilitate conversations and generate responses. The careful setup of logging and configuration ensures that it operates with minimal disruption and is easily debuggable when issues arise.",
    "filename": "python/samples/core_distributed-group-chat/run_group_chat_manager.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis documentation provides a high-level overview of the provided Python code, detailing its functional components, structure, and behavior.\n\n## Overview\n\nThe code snippet is an asynchronous Python script that initializes a gRPC server for a distributed host. It sets up a host to listen for incoming connections based on the defined host configuration. The script uses several libraries and utilities to handle asynchronous operations and enhance console output.\n\n## Imports and Dependencies\n\nThe script begins with the following import statements:\n\n- `asyncio`: This is the standard library for writing concurrent code using the async/await syntax.\n- `_types`: This module is expected to define a `HostConfig` type that outlines the configuration needed for the host.\n- `_utils`: This module includes the `load_config` function, which retrieves the necessary configuration settings.\n- `autogen_ext.runtimes.grpc`: This likely contains the implementation of `GrpcWorkerAgentRuntimeHost`, a class that manages the gRPC server.\n- `rich.console`: Used for advanced console output formatting.\n- `rich.markdown`: Provides the capability to render Markdown formatted strings in the console.\n\n## Function: `main`\n\nThe primary function defined in the script is `main`. This function is an asynchronous coroutine that operates on an instance of `HostConfig`. Its role is to set up and run a gRPC worker host. Below is the step-by-step breakdown of what the `main` function does:\n\n### Host Initialization\n\n1. **Initialization of the Host**:\n   - The `GrpcWorkerAgentRuntimeHost` object, named `host`, is created using the address specified in the `host_config`. This setup prepares the server to accept incoming client connections.\n\n2. **Starting the Host**:\n   - The `start` method of the `host` instance is called, which begins the process of listening for network traffic on the specified address.\n\n### Console Output\n\n3. **Creating a Console Instance**:\n   - A `Console` object from the `rich.console` module is instantiated. This object enhances the logging and output experience in the terminal.\n\n4. **Printing a Status Message**:\n   - Using the `console.print` method, a Markdown-formatted message is displayed indicating that the distributed host is operational and specifies the address at which it can be accessed. The use of Markdown formatting enhances the readability of the output.\n\n### Signal Handling\n\n5. **Awaiting Shutdown Signal**:\n   - The `await` statement is used to call the `stop_when_signal` method on the `host` instance, which pauses the execution of `main` until a termination signal is received (e.g., SIGINT). This allows for graceful shutdown of the server.\n\n## Entry Point of the Script\n\nAt the bottom of the script, a typical Python entry point check is performed:\n\n```python\nif __name__ == \"__main__\":\n    asyncio.run(main(load_config().host))\n```\n\n### Configuration Loading and Execution\n\n6. **Loading Configuration**:\n   - The `load_config()` function is called to retrieve configuration settings. This function is expected to return an object containing various settings, including the `host` configuration.\n\n7. **Running the Asynchronous Event Loop**:\n   - The `asyncio.run` function is used to execute the `main` coroutine with the loaded host configuration. This handles the necessary event loop creation and execution, allowing the asynchronous code defined in `main` to run correctly.\n\n## Summary\n\nIn summary, this script is designed to set up a gRPC server that listens for incoming connections. It accomplishes this by:\n\n1. Loading the necessary host configuration.\n2. Initializing and starting a gRPC service.\n3. Displaying a confirmation message on the console.\n4. Awaiting a shutdown signal to terminate the service gracefully.\n\nThe implementation demonstrates effective use of asynchronous programming in Python, enhancing both performance and responsiveness in networked applications.",
    "filename": "python/samples/core_distributed-group-chat/run_host.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code represents a simple asynchronous application developed using the `chainlit` framework, along with several utility and agent modules. The primary focus of this application is to facilitate real-time chat functionality between users and a UI agent, utilizing message streaming capabilities.\n\n## Dependencies and Initialization\n\n### Import Statements\n\nThe script begins by importing various libraries and modules necessary for its operation:\n\n- **Standard Libraries**: `asyncio`, `logging`, and `warnings` are imported for asynchronous programming, logging errors, and managing warnings respectively.\n- **Chainlit**: This library, likely the primary framework for user interaction, is imported under the alias `cl`.\n- **Custom Modules**: Several components from custom modules are also imported, such as `MessageChunk`, `UIAgent`, and various data types from `_types` and `_utils`.\n\n### Setting Log Levels\n\nLogging is configured to capture only error messages across the application:\n\n```python\nset_all_log_levels(logging.ERROR)\n```\n\nThis ensures that unnecessary log output does not clutter the console during execution.\n\n## Message Streaming Functionality\n\n### `send_cl_stream` Function\n\nThe function `send_cl_stream` is designed to handle message chunks asynchronously. Its purpose is to either update an existing message or send a new message depending on whether the message chunk has finished streaming.\n\n- **Message Storage**: A dictionary, `message_chunks`, is used to store messages by their `message_id`. This allows for efficient updating of messages as they arrive in chunks.\n- **Streaming Logic**: When a message chunk is received:\n  - If it is not finished, it updates the content of the message.\n  - If it is finished, it finalizes the message, waits for 3 seconds, and then sends the complete message to the UI.\n\n```python\nasync def send_cl_stream(msg: MessageChunk) -> None:\n```\n\n### Message Management\n\nMessages are handled using the `Message` class from the `chainlit` library, allowing for real-time user interaction and response collection.\n\n## Main Application Logic\n\n### `main` Function\n\nThe core logic of the application resides in the asynchronous `main` function. It takes `config` (application configurations) as a parameter and orchestrates the setup and execution of the UI agent.\n\n- **Runtime Initialization**: Initializes a `GrpcWorkerAgentRuntime` instance using the host address specified in the config.\n  \n- **Message Serializer**: Registers message serializers for handling incoming message types like `RequestToSpeak`, `GroupChatMessage`, and `MessageChunk`. This makes it easy to encode and decode messages as they are communicated.\n\n- **Starting the UI Agent**: The application registers an instance of `UIAgent` which provides the message chunking functionality defined in `send_cl_stream`. This allows the agent to handle messages as they are received.\n\n- **Subscription**: A topic subscription is added for the UI agent, allowing it to listen to specific messages defined in the configuration's `topic_type`.\n\n- **Shutdown Logic**: The application stops the agent runtime gracefully when a signal is received, and prints a confirmation message.\n\n```python\nasync def main(config: AppConfig):\n```\n\n## Chat Start Handling\n\n### `start_chat` Function\n\nThe entry point for the chat, this function is decorated with `@cl.on_chat_start`, signifying that it will run when a chat session begins in the `chainlit` interface.\n\n- **Warning Filtering**: It suppresses user warnings, specifically regarding model mismatches that may not be relevant for the user experience.\n- **Event Loop Execution**: Utilizes `asyncio.run` to execute the main asynchronous function, which handles all associated tasks asynchronously.\n  \n```python\n@cl.on_chat_start\nasync def start_chat():\n```\n\n## Summary of Functionality\n\nOverall, this script is structured to create a chat interface where messages can be sent in chunks. It sets up a real-time interaction loop between users and a UI agent, handles effectively managing messages, and gracefully shuts down when required. The use of asynchronous programming allows for smooth and responsive performance, making this an effective prototype for engaging users in a chat application.",
    "filename": "python/samples/core_distributed-group-chat/run_ui.py"
  },
  {
    "code": false,
    "content": "# Documentation for Group Chat Agent Script\n\n## Overview\nThis script sets up an asynchronous context for a group chat agent using a gRPC worker runtime. It leverages an Azure OpenAI model for chat completion and integrates with various components for message handling and logging. The main functionality revolves around registering a chat agent, configuring message serialization, and managing agent subscriptions for handling group chat messages.\n\n## Import Statements\nThe script begins by importing several modules necessary for its operations:\n- **asyncio**: For handling asynchronous tasks.\n- **logging**: For logging messages and handling log levels.\n- **warnings**: To manage warning messages that may arise during execution.\n- Various custom modules for agents, types, utilities, and external clients.\n\n## Main Functionality\nThe primary function of the script is encapsulated in the `main` function. This function takes a configuration object of type `AppConfig` and performs the following tasks:\n\n### 1. Logging Configuration\nThe log level is set to `ERROR`, which means only error messages will be logged. This is handled by the `set_all_log_levels(logging.ERROR)` call at the start of the `main` function.\n\n### 2. Initialize the gRPC Worker Agent Runtime\nAn instance of `GrpcWorkerAgentRuntime` is created using the host address specified in the configuration. This instance is responsible for handling communications and interactions with the group chat agent.\n\n### 3. Message Serialization\nThe script adds serializers for various message types, including `RequestToSpeak`, `GroupChatMessage`, and `MessageChunk`. This enables the proper formatting of messages sent and received by the agent runtime.\n\n### 4. Initial Delay and User Feedback\nAfter a brief pause of 3 seconds (to allow subsystems to initialize), the script prints a message to the console indicating that the \"Writer Agent\" is starting. This is formatted with Markdown for enhanced readability.\n\n### 5. Model Client Initialization\nThe script initializes an `AzureOpenAIChatCompletionClient` instance using client configuration parameters provided in the `AppConfig`. This client will facilitate interactions with the Azure OpenAI model for generating chat responses.\n\n### 6. Registering the Writer Agent\nThe script registers a new instance of `BaseGroupChatAgent` with the runtime, passing a lambda function that initializes this agent with specific configurations like description, system message, and the model client.\n\n### 7. Agent Subscriptions\nThe registered agent subscribes to two topics:\n- The writer agent's dedicated topic type.\n- The group chat manager's topic type.\n\nThese subscriptions are established using the `TypeSubscription` class, allowing the agent to receive relevant messages for processing.\n\n### 8. Signal Handling\nThe runtime is configured to stop when a termination signal is received (e.g., CTRL+C), ensuring that graceful shutdown processes can be executed.\n\n### 9. Closing Resources\nBefore exiting, the script ensures that the Azure model client is closed properly to release any resources that were allocated during its use.\n\n## Script Execution\nThe script checks if it's being run as the main module. If so, it sets log levels to error and filters user warnings related to model mismatch issues in the execution context. Finally, it executes the `main` function within an asynchronous event loop, passing in the application configuration loaded from a configuration file.\n\n## Conclusion\nThis script is designed to set up and run a group chat agent, utilizing asynchronous programming techniques to enhance responsiveness and performance. It integrates various components necessary for message handling and leverages Azure's OpenAI services for generating dynamic chat interactions. The clear structure of initialization, configuration, registration, and resource management ensures a robust solution for the intended functionality of the group chat agent.",
    "filename": "python/samples/core_distributed-group-chat/run_writer_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation of Cascading Messaging System\n\nThis documentation provides a high-level overview of a code implementation for a cascading messaging system, focusing on two agents: `CascadingAgent` and `ObserverAgent`. These agents are designed to handle and communicate messages in a defined manner, enabling a message-passing interaction across rounds.\n\n## Core Components\n\n### CascadingMessage Dataclass\n\n```python\n@dataclass\nclass CascadingMessage:\n    round: int\n```\n\n- **Purpose**: The `CascadingMessage` class is a simple data structure that holds a single piece of information: the `round` number. This round number indicates the specific iteration of a message within the cascading process.\n\n### ReceiveMessageEvent Dataclass\n\n```python\n@dataclass\nclass ReceiveMessageEvent:\n    round: int\n    sender: str\n    recipient: str\n```\n\n- **Purpose**: This class captures details of a message that has been received. It includes:\n    - `round`: The round of the message.\n    - `sender`: The identifier of the message sender.\n    - `recipient`: The identifier of the message recipient.\n\n## Cascading Agent\n\n### Class Definition\n\n```python\n@default_subscription\nclass CascadingAgent(RoutedAgent):\n```\n\n- **Purpose**: The `CascadingAgent` class is derived from the `RoutedAgent` class. It represents an agent that sends out messages in a cascading manner up to a specified maximum number of rounds.\n\n### Initialization\n\n```python\ndef __init__(self, max_rounds: int) -> None:\n    super().__init__(\"A cascading agent.\")\n    self.max_rounds = max_rounds\n```\n\n- **Details**: Upon instantiation, the `CascadingAgent` takes a parameter `max_rounds`, which determines how many rounds of messages will be sent. The agent also initializes its description.\n\n### Message Handler\n\n```python\n@message_handler\nasync def on_new_message(self, message: CascadingMessage, ctx: MessageContext) -> None:\n```\n\n- **Functionality**: This asynchronous method processes incoming messages of type `CascadingMessage`. The method performs the following tasks:\n    1. Publishes a `ReceiveMessageEvent` indicating that a message was received, including details of the message's round, sender, and recipient.\n    2. Checks if the current round number matches the `max_rounds`. If it does, the method returns, halting further message propagation.\n    3. If more rounds are allowed, it sends a new `CascadingMessage` for the next round (incrementing the round number by one).\n\n## Observer Agent\n\n### Class Definition\n\n```python\n@default_subscription\nclass ObserverAgent(RoutedAgent):\n```\n\n- **Purpose**: The `ObserverAgent` is also a derivative of the `RoutedAgent` class. Its primary role is to observe and log messages as they are received from the `CascadingAgent`.\n\n### Initialization\n\n```python\ndef __init__(self) -> None:\n    super().__init__(\"An observer agent.\")\n```\n\n- **Details**: This agent does not take any parameters upon initialization, simply establishing its description.\n\n### Message Handler\n\n```python\n@message_handler\nasync def on_receive_message(self, message: ReceiveMessageEvent, ctx: MessageContext) -> None:\n```\n\n- **Functionality**: This asynchronous method is triggered when the `ObserverAgent` receives a `ReceiveMessageEvent`. It carries out the following:\n    - It outputs to the console a formatted string detailing the round number, the sender of the message, and the recipient.\n    - This method serves as a logging mechanism to track the communication flow between agents.\n\n## Summary of Interaction Flow\n\n1. An instance of `CascadingAgent` is created with a specified maximum number of rounds.\n2. The `CascadingAgent` listens for incoming `CascadingMessage` instances.\n3. Upon receiving a `CascadingMessage`, it publishes a `ReceiveMessageEvent`, documenting the message's transmission.\n4. If the current round is less than the maximum, it triggers another `CascadingMessage` for the next round, resulting in a cascading effect.\n5. An instance of `ObserverAgent` listens for `ReceiveMessageEvent` instances and logs the communication, allowing for monitoring of the overall messaging process.\n\nThis framework establishes a clear delineation between message generation and observation, making it suitable for various applications where tracking multi-round communications is essential.",
    "filename": "python/samples/core_grpc_worker_runtime/agents.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script is designed to set up and run a gRPC-based agent system utilizing asynchronous programming in Python. It primarily leverages the functionalities provided by the `autogen_core` and `autogen_ext` libraries to facilitate message handling and agent registration within a distributed system architecture. The main components include initializing runtime for communication, registering an observer agent, and publishing cascading messages.\n\n## Dependencies\nThe script imports several classes and functions from different modules, indicating its reliance on a specific framework. Key imports include:\n\n- `CascadingMessage`: A message type likely associated with the cascading behavior of data or operations in a system.\n- `ObserverAgent`: Represents an agent that presumably observes or monitors messages or events within the system.\n- `DefaultTopicId`: A constant representing a default identifier for messaging topics.\n- `try_get_known_serializers_for_type`: A utility function to retrieve appropriate serializers for the specified message type.\n- `GrpcWorkerAgentRuntime`: Responsible for managing runtime instances that utilize gRPC communication.\n\n## Main Function\nThe `main` function encapsulates the script's primary workflow, allowing it to operate asynchronously. It performs several key tasks:\n\n1. **Runtime Initialization**:\n   - A `GrpcWorkerAgentRuntime` instance is created, which listens on `localhost:50051`. This sets up a gRPC server capable of handling incoming requests for the agent system.\n\n2. **Message Serializer Registration**:\n   - The script retrieves a serializer for the `CascadingMessage` type using `try_get_known_serializers_for_type`. This serializer is then added to the runtime, enabling the correct format for message serialization during communication.\n\n3. **Starting the Runtime**:\n   - The runtime is started with `await runtime.start()`, initializing the gRPC server and preparing it for interactions.\n\n4. **Agent Registration**:\n   - The `ObserverAgent` is registered with the runtime by calling `ObserverAgent.register`. The lambda function initializes a new instance of `ObserverAgent` upon registration, indicating that it will monitor or listen for messages sent through the system.\n\n5. **Message Publishing**:\n   - A `CascadingMessage` is created and published to the default topic with `await runtime.publish_message(CascadingMessage(round=1), topic_id=DefaultTopicId())`. This demonstrates the mechanism for sending data through the system upon which the observer agent can act.\n\n6. **Runtime Shutdown**:\n   - The script concludes with `await runtime.stop_when_signal()`, which keeps the runtime active until it receives a termination signal, allowing graceful shutdown and ensuring that all messaging processes are completed.\n\n## Asynchronous Execution\nThe script uses the `asyncio` module to handle asynchronous tasks. The `asyncio.run(main())` call ensures that the `main` function is executed in an event loop, enabling the non-blocking execution of operations such as starting the runtime, registering agents, and publishing messages.\n\n## Entry Point\nThe entry point of the script is encapsulated in the `if __name__ == \"__main__\":` block. This ensures that the `main()` function runs only when the script is executed directly, making it suitable for both module importation and standalone execution.\n\n## Logging (Commented)\nThere is a section of commented code that, if uncommented, would configure logging for the `autogen_core` module. This could aid in debugging or understanding the flow of operations when the script is executed, providing insight into message handling and agent behaviors.\n\n## Conclusion\nOverall, the script serves as an essential component of a distributed agent-based architecture, enabling dynamic message communication and flexibility through the use of gRPC. By integrating a runtime, agent registration, and message handling, it establishes a foundation for building more sophisticated systems capable of processing asynchronous events and messages in real-time.",
    "filename": "python/samples/core_grpc_worker_runtime/run_cascading_publisher.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThis script is an asynchronous Python program that sets up and runs a gRPC worker agent for handling messages in a cascading manner. It utilizes a UUID to create a unique agent type identifier and integrates message serialization to process incoming messages effectively.\n\n## Imports\nThe script begins by importing necessary modules:\n- `uuid`: This module is used to generate a unique identifier for the agent type.\n- Agent-related classes and functions:\n  - `CascadingAgent`: Likely represents an agent that can handle messages in a cascading fashion.\n  - `ReceiveMessageEvent`: Presumably a class that encapsulates incoming messages that the agent will process.\n- `try_get_known_serializers_for_type`: A function that retrieves message serializers for specific types.\n- `GrpcWorkerAgentRuntime`: A class that manages the runtime environment for the gRPC worker agent.\n\n## Main Function\nThe `main()` function is the primary entry point of the script. This function contains the essential logic for initializing and managing the gRPC agent runtime.\n\n### Setting Up the gRPC Runtime\nInside the `main` function:\n1. **Instantiate the GrpcWorkerAgentRuntime**:\n   - A runtime is created with the host address set to `localhost` on port `50051`. This indicates that the gRPC server is expected to run on the local machine.\n\n2. **Message Serialization**:\n   - The method `try_get_known_serializers_for_type` is called with `ReceiveMessageEvent` to add a serializer specifically designed for handling incoming messages. This is crucial for ensuring that messages are correctly formatted and can be processed by the agent.\n\n3. **Start Runtime**:\n   - The runtime is started asynchronously, allowing it to begin listening for incoming messages.\n\n### Registering the Cascading Agent\nNext, the script creates a unique agent type:\n- **UUID Generation**: A UUID is generated, and its hyphens are replaced with underscores to form a valid agent type string.\n- **CascadingAgent Registration**: The `CascadingAgent` is registered with the runtime using the `agent_type`. A lambda function is provided to instantiate the `CascadingAgent` with a maximum of 3 processing rounds. This means the agent can perform its operation up to three times per message, which could be useful for scenarios requiring multiple processing phases.\n\n### Handling Program Termination\nThe `stop_when_signal()` method is called on the runtime:\n- This method allows the runtime to stop gracefully when a termination signal is received, ensuring that any ongoing processing can conclude properly.\n\n## Logging Configuration\nAt the bottom of the script, logging is set up:\n- **Logging Configuration**: The logging is configured to display messages of level DEBUG or higher. This can help in tracing the execution flow and understanding operational issues during runtime.\n- **Logger Setup**: A logger is created specifically for the \"autogen_core\" namespace, facilitating targeted logging of messages from this part of the application.\n\n## Executing the Main Function\nFinally, the script checks if it is run as the main program:\n- **Asynchronous Execution**: The `asyncio.run(main())` statement is used to launch the `main()` coroutine. This enables the script to run the asynchronous operations defined within the `main()` function.\n\n## Conclusion\nIn summary, this script initializes a gRPC worker runtime for handling cascading agents that process incoming messages with serialization support. It uses asynchronous programming paradigms to ensure responsiveness and efficiency, making it suitable for scenarios requiring robust message handling. The incorporation of logging adds a layer of observability, aiding in monitoring and debugging the agent's behavior during runtime.",
    "filename": "python/samples/core_grpc_worker_runtime/run_cascading_worker.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Async gRPC Service Code\n\n## Overview\n\nThis Python script initializes a gRPC service that runs asynchronously using the `asyncio` library. The primary function is to create and manage a server that listens for incoming requests on a specified address and port. It also gracefully handles shutdown signals and provides logging capabilities to aid in debugging and monitoring.\n\n## Imports\n\nThe script begins by importing necessary modules:\n- `asyncio`: A standard Python library used for writing single-threaded concurrent code using coroutines.\n- `os`: A module that provides a way of using operating system-dependent functionality.\n- `GrpcWorkerAgentRuntimeHost`: A class presumably defined in the `autogen_ext` package, which is responsible for managing the gRPC service runtime.\n\n## Main Coroutine\n\nThe `main()` function is defined as an asynchronous coroutine:\n- **Service Initialization**: It creates an instance of `GrpcWorkerAgentRuntimeHost`, specifying the address (`localhost:50051`) on which the service will listen.\n- **Service Start**: It calls the `start()` method on the service instance, which likely initializes the necessary components for the gRPC server to begin accepting connections.\n\n### Service Management\n\nThe service operates within a `try` block:\n- **Platform-Specific Signal Handling**:\n    - **Windows**: Uses an `asyncio.Event` to block the coroutine. Since Windows does not handle termination signals like UNIX systems do (via Ctrl+C), it waits indefinitely until the event is set.\n    - **Other Operating Systems**: Invokes the `stop_when_signal()` method to await a termination signal, allowing the service to shut down gracefully when signaled (e.g., via Ctrl+C).\n\n### Exception Handling\n\nAn exception handler is in place to manage `KeyboardInterrupt`, which allows the service to respond to manual interruptions (e.g., user pressing Ctrl+C). When interrupted, the message \"Stopping service...\" is printed to console output.\n\n### Cleanup on Exit\n\nThe `finally` block is executed irrespective of whether an exception occurred or not:\n- Calls the `stop()` method on the service instance to ensure that any cleanup processes are performed before the script exits.\n\n## Main Entry Point\n\nThe script includes a conditional `if __name__ == \"__main__\":`, which checks if the script is being run as the main program:\n- **Logging Configuration**: Initializes the logging system. It sets the logging level for the entire application to `WARNING`, while specifically lowering the logging level of the `autogen_core` module to `DEBUG`. This distinction allows for more verbose output from that module, which is useful for diagnosing issues.\n- **Execute Async Main**: The `asyncio.run(main())` statement is invoked, which starts the execution of the `main()` coroutine, handling the event loop and ensuring that all asynchronous tasks are run until completion.\n\n## Functional Overview\n\nThe script can be summarized in several steps:\n1. **Import necessary modules** for async handling, OS utility functions, and service class.\n2. **Define an asynchronous `main` function** to manage the lifecycle of the gRPC service.\n3. **Initialize the gRPC service** on `localhost` at port `50051`.\n4. **Start the service** to begin listening for incoming requests.\n5. **Handle graceful shutdown signals** with platform-specific logic.\n6. **Log status messages** at different verbosity levels for debugging.\n7. **Clean up resources** by stopping the service on exit.\n8. **Run the `main` function** within an event loop if the script is executed directly. \n\nThis design effectively encapsulates the gRPC service's lifecycle while providing robust logging and error handling mechanisms.",
    "filename": "python/samples/core_grpc_worker_runtime/run_host.py"
  },
  {
    "code": false,
    "content": "# Documentation of the Async Greeting and Feedback System\n\nThis document outlines the functionality of the provided source code, which implements a simple greeting and feedback mechanism using asynchronous programming in Python. The code defines two main agent classes, `ReceiveAgent` and `GreeterAgent`, which communicate through messages and a gRPC worker runtime.\n\n## Overview\n\nThe code utilizes the `asyncio` library to facilitate asynchronous operations, promoting efficient handling of multiple concurrent tasks. It leverages message serialization and gRPC for inter-agent communication. The primary purpose of this system is to enable an interaction protocol where one agent can send greeting requests and receive feedback.\n\n### Data Classes\n\nSeveral data classes are defined to encapsulate the messages that agents will process:\n\n- **AskToGreet**: Represents a request to greet a user, containing a string `content`.\n- **Greeting**: Represents a greeting message, containing a string `content`.\n- **ReturnedGreeting**: Represents a response to a greeting, also containing a string `content`.\n- **Feedback**: Represents feedback for a greeting, encapsulating a string.\n- **ReturnedFeedback**: Represents the response to feedback given, containing a string.\n\nThese classes define the message structure and facilitate the serialization process.\n\n## ReceiveAgent Class\n\n### Purpose\n\nThe `ReceiveAgent` class inherits from `RoutedAgent` and is responsible for handling incoming messages related to greetings and feedback. \n\n### Methods\n\n- **`__init__`**: Initializes the agent with the name \"Receive Agent\".\n  \n- **`on_greet`**: Asynchronously handles `Greeting` messages. When a greeting is received, it publishes a `ReturnedGreeting` message back to the system acknowledging the received greeting.\n\n- **`on_feedback`**: Asynchronously handles `Feedback` messages. It publishes a `ReturnedFeedback` message in response to feedback received from the `GreeterAgent`.\n\n- **`on_unhandled_message`**: This method is triggered for any messages that do not match the registered handlers. It prints the unhandled messages to the console, aiding in debugging.\n\n## GreeterAgent Class\n\n### Purpose\n\nSimilar to `ReceiveAgent`, the `GreeterAgent` class, also inheriting from `RoutedAgent`, is designed to manage incoming greetings and provide feedback.\n\n### Methods\n\n- **`__init__`**: Initializes the agent with the name \"Greeter Agent\".\n\n- **`on_ask`**: This method handles messages of type `AskToGreet`. It responds by publishing a `Greeting` message that includes a personalized greeting based on the content provided.\n\n- **`on_returned_greet`**: Handles `ReturnedGreeting` messages from the `ReceiveAgent`. Upon receiving a returned greeting, it publishes a `Feedback` message expressing the agent's acknowledgment.\n\n- **`on_unhandled_message`**: Similar to the `ReceiveAgent`, this method handles messages that do not correspond to any defined handlers, logging them for review.\n\n## Main Functionality\n\n### Execution Flow\n\n1. **Runtime Initialization**: The `main` function starts a gRPC worker runtime. It specifies the address of the service (`localhost:50051`) to establish a communication endpoint.\n\n2. **Serializer Registration**: For each of the message types defined (e.g., `AskToGreet`, `Greeting`, etc.), the system adds serialization methods needed for processing messages.\n\n3. **Agent Registration**: The `ReceiveAgent` and `GreeterAgent` are registered with the runtime, both being associated with a default subscription identified by their respective agent types.\n\n4. **Initial Message Publication**: The `main` function publishes an initial `AskToGreet` message, prompting the greeting interactions to commence.\n\n5. **Signal Handling**: The runtime will cleanly stop when signaled, ensuring that any ongoing process is terminated gracefully.\n\n### Logging Configuration\n\nAt the entry point of the application (`if __name__ == \"__main__\"`), logging is configured to print debug-level messages. This aids developers in tracking the application's behavior in real-time and is crucial for debugging and optimization.\n\n## Conclusion\n\nThe provided Python code establishes a simple but efficient framework for asynchronous communication between agents using greetings and feedback. By structuring the application around agents and messages, it emphasizes the modular design principles common in modern software systems. The use of gRPC allows for the potential scalability of this design in distributed applications.",
    "filename": "python/samples/core_grpc_worker_runtime/run_worker_pub_sub.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis Python script defines a distributed agent-based system utilizing asynchronous messaging. It comprises two main agents, `ReceiveAgent` and `GreeterAgent`, which communicate with each other through a gRPC framework. The agents utilize message handlers to process incoming messages and respond accordingly, facilitating a simple greeting mechanism paired with feedback handling.\n\n## Key Data Structures\n\n### Message Definitions\n\n1. **AskToGreet**: A dataclass that holds the content of a greeting request. It represents a structured message indicating a desire to greet someone.\n   ```python\n   @dataclass\n   class AskToGreet:\n       content: str\n   ```\n\n2. **Greeting**: A dataclass that encapsulates a greeting message. It is designed to respond to `AskToGreet` messages.\n   ```python\n   @dataclass\n   class Greeting:\n       content: str\n   ```\n\n3. **Feedback**: A dataclass for feedback messages that the `GreeterAgent` sends back after greeting.\n   ```python\n   @dataclass\n   class Feedback:\n       content: str\n   ```\n\n## Agent Class Descriptions\n\n### ReceiveAgent Class\n\nThe `ReceiveAgent` is responsible for receiving greeting and feedback messages. \n\n- **Initialization**: It calls the superclass constructor with a name identifier.\n  ```python\n  class ReceiveAgent(RoutedAgent):\n      def __init__(self) -> None:\n          super().__init__(\"Receive Agent\")\n  ```\n\n- **Message Handlers**:\n  - `on_greet`: Asynchronously handles `Greeting` messages. It constructs a response by acknowledging receipt of the message.\n    ```python\n    @message_handler\n    async def on_greet(self, message: Greeting, ctx: MessageContext) -> Greeting:\n        return Greeting(content=f\"Received: {message.content}\")\n    ```\n\n  - `on_feedback`: Asynchronously processes `Feedback` messages and prints the content of the feedback to the console.\n    ```python\n    @message_handler\n    async def on_feedback(self, message: Feedback, ctx: MessageContext) -> None:\n        print(f\"Feedback received: {message.content}\")\n    ```\n\n### GreeterAgent Class\n\nThe `GreeterAgent` is tasked with initiating the greeting process when asked. \n\n- **Initialization**: Similar to `ReceiveAgent`, it initializes with an identifier and prepares to send messages to the receiver agent.\n  ```python\n  class GreeterAgent(RoutedAgent):\n      def __init__(self, receive_agent_type: str) -> None:\n          super().__init__(\"Greeter Agent\")\n          self._receive_agent_id = AgentId(receive_agent_type, self.id.key)\n  ```\n\n- **Message Handlers**:\n  - `on_ask`: This method is invoked when it receives an `AskToGreet` message. It sends a `Greeting` message to the `ReceiveAgent` and also publishes feedback.\n    ```python\n    @message_handler\n    async def on_ask(self, message: AskToGreet, ctx: MessageContext) -> None:\n        response = await self.send_message(Greeting(f\"Hello, {message.content}!\"), recipient=self._receive_agent_id)\n        await self.publish_message(Feedback(f\"Feedback: {response.content}\"), topic_id=DefaultTopicId())\n    ```\n\n## Main Execution Flow\n\n### Main Function\n\nThe `main` function orchestrates the execution of the script.\n\n- **Runtime Initialization**: It initializes the gRPC runtime, specifying the host address.\n  ```python\n  runtime = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n  await runtime.start()\n  ```\n\n- **Agent Registration**: It registers both `ReceiveAgent` and `GreeterAgent` with the runtime and sets up their subscriptions.\n  ```python\n  await ReceiveAgent.register(runtime, \"receiver\", lambda: ReceiveAgent())\n  await runtime.add_subscription(DefaultSubscription(agent_type=\"receiver\"))\n  await GreeterAgent.register(runtime, \"greeter\", lambda: GreeterAgent(\"receiver\"))\n  await runtime.add_subscription(DefaultSubscription(agent_type=\"greeter\"))\n  ```\n\n- **Publishing the Initial Message**: The `main` function sends an initial `AskToGreet` message to the system to trigger the greeting process.\n  ```python\n  await runtime.publish_message(AskToGreet(\"Hello World!\"), topic_id=DefaultTopicId())\n  ```\n\n- **Signal Handling**: The runtime is set to stop when it receives a termination signal.\n  ```python\n  await runtime.stop_when_signal()\n  ```\n\n## Execution Entry Point\n\nThe script has a standard Python entry point, configuring logging for debugging and running the asynchronous `main` function. \n\n```python\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger(\"autogen_core\")\n    logger.setLevel(logging.DEBUG)\n    asyncio.run(main())\n```\n\n### Logging Configuration\n\nThe logging is set at the DEBUG level, allowing for detailed output, which is helpful for troubleshooting and understanding the flow of messages between agents. \n\nOverall, this code serves as a foundation for building a simple agent-based interaction system where agents can communicate using structured messages over a gRPC interface.",
    "filename": "python/samples/core_grpc_worker_runtime/run_worker_rpc.py"
  },
  {
    "content": "# Multi Agent Orchestration, Distributed Agent Runtime Example\n\nThis repository is an example of how to run a distributed agent runtime. The system is composed of three main components:\n\n1. The agent host runtime, which is responsible for managing the eventing engine, and the pub/sub message system.\n2. The worker runtime, which is responsible for the lifecycle of the distributed agents, including the \"semantic router\".\n3. The user proxy, which is responsible for managing the user interface and the user interactions with the agents.\n\n\n## Example Scenario\n\nIn this example, we have a simple scenario where we have a set of distributed agents (an \"HR\", and a \"Finance\" agent) which an enterprise may use to manage their HR and Finance operations. Each of these agents are independent, and can be running on different machines. While many multi-agent systems are built to have the agents collaborate to solve a difficult task - the goal of this example is to show how an enterprise may manage a large set of agents that are suited to individual tasks, and how to route a user to the most relevant agent for the task at hand.\n\nThe way this system is designed, when a user initiates a session, the semantic router agent will identify the intent of the user (currently using the overly simple method of string matching), identify the most relevant agent, and then route the user to that agent. The agent will then manage the conversation with the user, and the user will be able to interact with the agent in a conversational manner.\n\nWhile the logic of the agents is simple in this example, the goal is to show how the distributed runtime capabilities of autogen supports this scenario independantly of the capabilities of the agents themselves.\n\n## Getting Started\n\n1. Install `autogen-core` and its dependencies\n\n## To run\n\nSince this example is meant to demonstrate a distributed runtime, the components of this example are meant to run in different processes - i.e. different terminals.\n\nIn 2 separate terminals, run:\n\n```bash\n# Terminal 1, to run the Agent Host Runtime\npython run_host.py\n```\n\n```bash\n# Terminal 2, to run the Worker Runtime\npython run_semantic_router.py\n```\n\nThe first terminal should log a series of events where the vrious agents are registered\nagainst the runtime.\n\nIn the second terminal, you may enter a request related to finance or hr scenarios.\nIn our simple example here, this means using one of the following keywords in your request:\n\n- For the finance agent: \"finance\", \"money\", \"budget\"\n- For the hr agent: \"hr\", \"human resources\", \"employee\"   \n\nYou will then see the host and worker runtimes send messages back and forth, routing to the correct\nagent, before the final response is printed.\n\nThe conversation can then continue with the selected agent until the user sends a message containing \"END\",at which point the agent will be disconnected from the user and a new conversation can start.\n\n## Message Flow\n\nUsing the \"Topic\" feature of the agent host runtime, the message flow of the system is as follows:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Closure_Agent\n    participant User_Proxy_Agent\n    participant Semantic_Router\n    participant Worker_Agent\n\n    User->>User_Proxy_Agent: Send initial message\n    Semantic_Router->>Worker_Agent: Route message to appropriate agent\n    Worker_Agent->>User_Proxy_Agent: Respond to user message\n    User_Proxy_Agent->>Closure_Agent: Forward message to externally facing Closure Agent\n    Closure_Agent->>User: Expose the response to the User\n    User->>Worker_Agent: Directly send follow up message\n    Worker_Agent->>User_Proxy_Agent: Respond to user message\n    User_Proxy_Agent->>Closure_Agent: Forward message to externally facing Closure Agent\n    Closure_Agent->>User: Return response\n    User->>Worker_Agent: Send \"END\" message\n    Worker_Agent->>User_Proxy_Agent: Confirm session end\n    User_Proxy_Agent->>Closure_Agent: Confirm session end\n    Closure_Agent->>User: Display session end message\n```\n### Contributors\n\n- Diana Iftimie (@diftimieMSFT)\n- Oscar Fimbres (@ofimbres)\n- Taylor Rockey (@tarockey)",
    "filename": "python/samples/core_semantic_router/README.md"
  },
  {
    "code": false,
    "content": "# Documentation of the Asynchronous Messaging System\n\nThis documentation provides a high-level overview of an asynchronous messaging framework represented by the classes `WorkerAgent` and `UserProxyAgent`. These classes orchestrate the handling of messages in response to user inputs and facilitate inter-agent communication. Implementing the decorator `@message_handler`, each class operates within an event-driven environment using Python's `asyncio`.\n\n## Overview\n\nThe code primarily establishes a messaging architecture where two agents\u2014`WorkerAgent` and `UserProxyAgent`\u2014interact with user inputs, responding appropriately to commands and terminating conversations per user requests. Communication is mediated through message passing, where messages are published to designated topics.\n\n## Dependencies and Logging\n\nThe code begins by importing several modules, including:\n\n- `asyncio`: Enables asynchronous programming by allowing non-blocking execution of tasks.\n- `logging`: Provides a flexible logging framework to track the execution of the application.\n- Custom components from `_semantic_router_components` and `autogen_core`, defining message types and contexts used in communication.\n\nA logger is configured to output debug-level logs, which assists in tracking received messages and internal operations.\n\n## WorkerAgent Class\n\n### Initialization\n\nThe `WorkerAgent` class inherits from `RoutedAgent` and customizes its initialization by assigning a name to the agent. This name distinguishes different agent instances in logs and messages.\n\n### Message Handling\n\nThe `my_message_handler` method is a coroutine designed to handle messages received from a `UserProxyMessage`. Key functionalities include:\n\n- **Message Reception**: It logs the message received from the user source.\n- **Termination Handling**: If the message content contains \"END\", it publishes a `TerminationMessage` back to the user, indicating that the conversation has ended.\n- **Response Generation**: For other messages, the agent constructs a friendly response incorporating the original user input and publishes it as a `WorkerAgentMessage`.\n\n## UserProxyAgent Class\n\n### Initialization\n\nLike `WorkerAgent`, `UserProxyAgent` inherits from `RoutedAgent`, allowing it to define its behavior in message management. Its constructor takes a description parameter, which helps identify it in the logging system.\n\n### Termination Handling\n\nThe `on_terminate` method responds to termination messages. It logs the reason for conversation termination and publishes a `FinalResult` message indicating the end state of the conversation. This effectively signals other parts of the system that no further interaction will occur.\n\n### Agent Message Handling\n\nThe `on_agent_message` method processes messages from `WorkerAgent`. It performs the following functions:\n\n- **Log Reception**: Logs the message from the Worker Agent.\n- **Message Transmission**: Publishes the received message to a `Closure Agent`, thus promoting interaction within a broader system context. \n\n### User Input Retrieval\n\nThe `get_user_input` method is an asynchronous coroutine responsible for prompting the user for input. It uses the event loop to run input retrieval in a non-blocking manner. Customizations could be made on this method for different input mechanisms beyond the console.\n\n## Discussion\n\nThis setup exemplifies an event-driven architecture where agents are designed to listen, respond, and maintain context based on conversations. Each agent acts autonomously but collaborates through the publish-subscribe mechanism effectively. \n\n### Flexibility and Extensibility\n\nThe architecture facilitates future extensions where new types of agents can be added without significant changes to existing code. Custom properties and behaviors of each agent can be managed through polymorphism and overriding their methods.\n\n### Logging and Debugging\n\nThe comprehensive logging aids in monitoring and debugging the application. By observing logs, developers can track message flow, identify issues, and analyze agent interactions and terminations.\n\n### Use Cases\n\nThis framework is well-suited for applications that require conversational interfaces, such as chatbots, virtual assistants, or any scenario where multiple intelligent agents interact based on user inputs, providing context-aware responses.\n\n## Conclusion\n\nOverall, the `WorkerAgent` and `UserProxyAgent` provide a robust framework for building responsive and interactive agents. By leveraging asynchronous programming and clean message handling patterns, the code facilitates fluid conversations with users while maintaining the potential for scalability and customization in future developments.",
    "filename": "python/samples/core_semantic_router/_agents.py"
  },
  {
    "code": false,
    "content": "# Semantic Router Agent Documentation\n\n## Overview\nThe provided code defines the `SemanticRouterAgent`, an asynchronous agent designed to route user messages to the appropriate intents using intent classification and agent registry. It listens for incoming user messages, identifies their intents, and forwards them to the relevant agent, or sends a termination message if no suitable agent is found.\n\n## Logging Configuration\nThe code begins by setting up a logging configuration using the standard logging module. It establishes a logger specifically for the `semantic_router` component, allowing visibility into the various operations occurring within the `SemanticRouterAgent`. The debugging level is set to `DEBUG`, meaning that all log messages at this level and above will be captured.\n\n```python\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(f\"{TRACE_LOGGER_NAME}.semantic_router\")\nlogger.setLevel(logging.DEBUG)\n```\n\n## Class Definition: SemanticRouterAgent\nThe `SemanticRouterAgent` class inherits from `RoutedAgent` and is decorated with `@default_subscription`, indicating that instances of this agent will subscribe to default messaging behavior. This class is initialized with three parameters: `name`, `agent_registry`, and `intent_classifier`, where:\n- `name`: A string representing the name of the agent.\n- `agent_registry`: An instance of `AgentRegistryBase`, which manages available agents.\n- `intent_classifier`: An instance of `IntentClassifierBase`, which classifies user intents.\n\n```python\n@default_subscription\nclass SemanticRouterAgent(RoutedAgent):\n    def __init__(self, name: str, agent_registry: AgentRegistryBase, intent_classifier: IntentClassifierBase) -> None:\n        super().__init__(\"Semantic Router Agent\")\n        self._name = name\n        self._registry = agent_registry\n        self._classifier = intent_classifier\n```\n\n## Message Handling: route_to_agent\nThe `route_to_agent` method is an asynchronous message handler triggered when a user sends a message. This method takes two parameters:\n- `message`: An instance of `UserProxyMessage` containing the user\u2019s content,\n- `ctx`: A `MessageContext` object providing context such as `topic_id`.\n\nWithin this method, the agent verifies that a topic ID is present, logs the incoming message, identifies the intent, and then tries to find the corresponding agent to handle the intent.\n\n```python\n@message_handler\nasync def route_to_agent(self, message: UserProxyMessage, ctx: MessageContext) -> None:\n    assert ctx.topic_id is not None\n    logger.debug(f\"Received message from {message.source}: {message.content}\")\n    session_id = ctx.topic_id.source\n    intent = await self._identify_intent(message)\n    agent = await self._find_agent(intent)\n    await self.contact_agent(agent, message, session_id)\n```\n\n## Identifying User Intent: _identify_intent\nThe `_identify_intent` method is an asynchronous helper function that utilizes the `intent_classifier` to classify the intent from the user's message content. It takes a singular parameter, `message`, and returns the identified intent as a string.\n\n```python\nasync def _identify_intent(self, message: UserProxyMessage) -> str:\n    return await self._classifier.classify_intent(message.content)\n```\n\n## Finding Relevant Agent: _find_agent\nThe `_find_agent` method is responsible for locating the agent associated with the identified intent. It tries to retrieve the agent from the agent registry using the provided intent. If no agent is found for the intent, it defaults to returning the string \"termination\", indicating that no relevant agent is available.\n\n```python\nasync def _find_agent(self, intent: str) -> str:\n    logger.debug(f\"Identified intent: {intent}\")\n    try:\n        agent = await self._registry.get_agent(intent)\n        return agent\n    except KeyError:\n        logger.debug(\"No relevant agent found for intent: \" + intent)\n        return \"termination\"\n```\n\n## Contacting the Relevant Agent: contact_agent\nThe `contact_agent` method forwards the user message to the identified agent or sends a termination message if no appropriate agent was found. This method takes three parameters: `agent` (the name of the identified agent), `message` (the original user message), and `session_id` (to maintain session context). It checks if the agent is \"termination\" and publishes an appropriate message to the relevant topic.\n\n```python\nasync def contact_agent(self, agent: str, message: UserProxyMessage, session_id: str) -> None:\n    if agent == \"termination\":\n        logger.debug(\"No relevant agent found\")\n        await self.publish_message(\n            TerminationMessage(reason=\"No relevant agent found\", content=message.content, source=self.type),\n            DefaultTopicId(type=\"user_proxy\", source=session_id),\n        )\n    else:\n        logger.debug(\"Routing to agent: \" + agent)\n        await self.publish_message(\n            UserProxyMessage(content=message.content, source=message.source),\n            DefaultTopicId(type=agent, source=session_id),\n        )\n```\n\n## Conclusion\nThe `SemanticRouterAgent` serves as a core component in routing user messages within a conversational AI framework. Its modular design allows for easy integration with various intent classification and agent registry implementations. The agent's ability to identify intents and route messages plays a critical role in fostering effective human-agent interactions.",
    "filename": "python/samples/core_semantic_router/_semantic_router_agent.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code defines an abstract framework for handling message classification and processing within an agent-based architecture. The structure includes abstract classes for intent classification and agent retrieval, as well as various message types used in the conversation flow between users and agents. The intent of this code is to provide a flexible foundation for building chat or messaging systems that can classify user messages, route them to appropriate agents, and manage the conversation lifecycle.\n\n## Intent Classification\n\n### `IntentClassifierBase`\n\nThe `IntentClassifierBase` class is an abstract base class that requires subclasses to implement the `classify_intent` method. This method is intended to take a message (string) as input and asynchronously return a string representing the identified intent of the message. By enforcing this abstraction, the code ensures that any intent classification logic can be implemented in a standardized manner, allowing for various models or techniques to be employed without altering the overall system.\n\n### Role and Purpose\n\n- **Purpose**: Serves as a foundational interface for intent classification.\n- **Role**: This class is meant to be inherited by specific intent classification implementations, which will handle the logic of determining what a user intends based on their input.\n\n## Agent Registry\n\n### `AgentRegistryBase`\n\nSimilar to the `IntentClassifierBase`, the `AgentRegistryBase` is another abstract class that defines an essential interface for retrieving agents based on intents. The `get_agent` method is expected to be implemented by subclasses to provide an agent string corresponding to a given intent string.\n\n### Role and Purpose\n\n- **Purpose**: Functions as a mechanism to retrieve suitable agents to handle user requests based on classified intents.\n- **Role**: It allows for any registry of agents to be implemented that can fulfill the role of processing user messages according to their intents.\n\n## Message Classes\n\nThe following classes define different message types within the communication flow. These classes utilize Python\u2019s `dataclass` feature for simplicity and to ensure clear data encapsulation.\n\n### `BaseMessage`\n\n`BaseMessage` is a basic message structure that contains a string attribute named `source`. This could represent the origin of the message (e.g., user, system, agent).\n\n- **Purpose**: Acts as a foundational message from which other message types can inherit.\n- **Role**: Ensures that all message types have a source associated with them.\n\n### `TextMessage`\n\nExtending `BaseMessage`, `TextMessage` includes an additional attribute `content`, which is a string representing the actual content of the message. It also implements a method to return the length of the content, allowing for easy content analysis.\n\n- **Purpose**: Represents a textual message and facilitates measuring content length.\n- **Role**: Serves as a more specific message type that can hold user or system text.\n\n### `UserProxyMessage`\n\n`UserProxyMessage` inherits from `TextMessage` and is intended to be utilized for messages originating from users. This class highlights its purpose in the context of the system where user messages must be routed appropriately to agents based on their intent.\n\n- **Purpose**: Represents messages sent from users to the system.\n- **Role**: Provides a specialized structure that may include additional attributes or methods specific to user interactions in the future.\n\n### `TerminationMessage`\n\n`TerminationMessage` is another subclass of `TextMessage` that indicates the end of a conversation. In addition to `content`, it includes a `reason` attribute to describe why the conversation is terminating (e.g., successful resolution, user request, timeout).\n\n- **Purpose**: Signals to users that the conversation has ended and provides context for termination.\n- **Role**: Enhances the messaging system by managing conversation closure effectively.\n\n### `WorkerAgentMessage`\n\n`WorkerAgentMessage` is a subclass of `TextMessage` that represents messages sent from worker agents back to users. This class is expected to eventually include attributes or functionality specific to agent responses.\n\n- **Purpose**: Encapsulates messages produced by worker agents.\n- **Role**: Facilitates communication flow in scenarios where agents provide feedback or results to users.\n\n### `FinalResult`\n\n`FinalResult` also extends `TextMessage`, intended to signify the final response sent from the agent to the user, indicating that the conversation is concluded.\n\n- **Purpose**: Represents the last output in the conversation cycle from the agent.\n- **Role**: Aids in signaling the end of dialogue, maintaining clarity in message flow.\n\n## Summary\n\nOverall, the code provides a structured approach to managing conversations in an agent-based system using abstract intents and messages that represent communication entities. It encourages extensibility: new intent classification strategies and agents can be easily plugged into this framework. Additionally, the distinct message classes facilitate clear communication pathways and expandability for future features.",
    "filename": "python/samples/core_semantic_router/_semantic_router_components.py"
  },
  {
    "code": false,
    "content": "# Documentation for Async GRPC Host Script\n\n## Overview\nThis script is designed to create and run an asynchronous gRPC server host using the `GrpcWorkerAgentRuntimeHost` class from an external module named `autogen_ext`. It sets up a service on a specified address and incorporates graceful shutdown features.\n\n## Imports\nThe script imports the following modules:\n- `asyncio`: This module is used for writing concurrent code using the async/await syntax.\n- `logging`: This module provides a way to configure and use a logging system for tracking events within the script.\n- `platform`: This module is used to access underlying platform information, which helps in handling different operating system behaviors.\n\nThe script also imports components from:\n- `autogen_core`: Specifically, it imports `TRACE_LOGGER_NAME`, which is used in logger configuration.\n- `autogen_ext.runtimes.grpc`: Here, it imports `GrpcWorkerAgentRuntimeHost`, which is likely responsible for the functionality of hosting the gRPC service.\n\n## Async Function: `run_host`\nThe script defines an asynchronous function named `run_host`. The purpose of this function is to set up and manage a gRPC hosting service.\n\n### Detailed Functionality\n1. **Initialization**: \n   - It creates an instance of the `GrpcWorkerAgentRuntimeHost` class, specifying the address \"localhost:50051\". This instance will serve as the host for the gRPC service.\n\n2. **Starting the Host**: \n   - The `start()` method of the `host` instance is called, which likely initiates the background service. This means the gRPC service will be listening for incoming requests on the specified address.\n\n3. **Platform Handling**:\n   - The function checks the operating system using `platform.system()`. If the system is Windows, it enters a `while` loop that essentially keeps the service running, checking for user interruptions via `KeyboardInterrupt`. This allows the service to remain responsive until a manual stop signal is given.\n   - For non-Windows platforms, it calls the method `stop_when_signal()`, which presumably handles different termination signals (such as SIGINT or SIGTERM) to gracefully shut down the host.\n\n## Main Entry Point\nThe script contains a standard `if __name__ == \"__main__\"` block, which serves as the entry point of the program. Here's how this section works:\n\n1. **Logging Configuration**:\n   - It configures the logging system by setting the level to `DEBUG`. This means all log messages at this level and above will be captured, which is useful for detailed monitoring and diagnostics.\n\n2. **Logger Initialization**:\n   - A logger is created using `getLogger()` with `TRACE_LOGGER_NAME` as a prefix. This sets up a specific logger for tracking the host's activities.\n\n3. **Event Loop Execution**:\n   - `asyncio.run(run_host())` is called to execute the asynchronous `run_host` function. This starts the event loop, which manages the running of the asynchronous task and handles incoming requests to the gRPC service.\n\n## Summary\nThis Python script effectively creates a gRPC server that runs asynchronously, capable of managing incoming requests. It features graceful shutdown capabilities tailored for both Windows and other operating systems. By leveraging the `asyncio` library and implementing structured logging, it provides a robust infrastructure for handling remote procedure calls in an efficient manner.",
    "filename": "python/samples/core_semantic_router/run_host.py"
  },
  {
    "code": false,
    "content": "# Semantic Router Example Documentation\n\nThis document provides a high-level overview of a Python script designed to handle user messages through a Semantic Router mechanism. The router dynamically routes user inquiries to the appropriate agent based on the intent identified from the message. It utilizes various design components to manage communication between agents.\n\n## Overview\n\nThe script introduces a framework where user messages are processed by a Semantic Router Agent. The primary role of this agent is to:\n- Receive messages from users.\n- Determine the intent behind these messages.\n- Route them to the relevant agent for further handling.\n- Conclude conversations if the intent is unrecognized.\n\nThe architecture follows a pub-sub model and incorporates an \"Agent Registry,\" which matches the identified intent to specific agent names, facilitating efficient routing.\n\n## Components Description\n\n### Intent Classification\n\nThe script includes a `MockIntentClassifier` class derived from `IntentClassifierBase`. This class is tasked with classifying user messages based on predefined keywords associated with distinct intents. It contains:\n- A dictionary mapping intents (e.g., \"finance_intent,\" \"hr_intent\") to a list of keywords that help determine the intent from the user's input.\n- An asynchronous method, `classify_intent`, which scans the message for these keywords and returns the corresponding intent. If no intent is discovered from the keywords, it defaults to \"general.\"\n\n### Agent Registry\n\nThe `MockAgentRegistry` class, a subclass of `AgentRegistryBase`, maintains a mapping between identified intents and corresponding agents. It provides the following functionality:\n- Initializes a simple dictionary that pairs each intent with its respective agent name (e.g., \"finance_intent\" maps to \"finance\").\n- Implements the `get_agent` method, which retrieves the appropriate agent name for a given intent.\n\n### Output Result Handling\n\nThe `output_result` asynchronous function handles the output of agent messages and user interaction:\n- It checks if the received message is from a `WorkerAgentMessage` or `FinalResult`.\n- For `WorkerAgentMessage`, it prints the content and prompts for user input, subsequently publishing the response to the topic identified by the source agent.\n- For `FinalResult`, it signifies the end of a conversation and prompts for a new conversation starter.\n\n### Agent Execution\n\nIn the `run_workers` function, the script initializes different agents and manages their lifecycle:\n- It begins by creating a `GrpcWorkerAgentRuntime` instance to handle inter-agent communication.\n- Multiple agents, including \"finance\" and \"hr\" agents, are registered alongside a user proxy agent.\n- A closure agent is registered to facilitate the output of results to external systems.\n\n### Semantic Router Integration\n\nThe primary innovation of this script lies in the integration of the `SemanticRouterAgent`. After creating the necessary agents, it:\n- Instantiates a `MockAgentRegistry` and a `MockIntentClassifier`.\n- Registers the router agent to manage the messaging flow based on identified intents.\n\n### Starting a Conversation\n\nAfter registration, the script prompts the user to enter a message, which is published to the system as a `UserProxyMessage`. The conversation is then active, waiting for user input and routes messages based on the registered agents according to identified intents.\n\n## Execution Flow\n\n1. **Agent Initialization**: The script begins by setting up the agent runtime environment.\n2. **Component Registration**: Various agents (finance, HR, user proxy, and the Semantic Router) are registered in the agent runtime.\n3. **User Prompt**: The conversation initiates with a prompt to the user for input.\n4. **Message Handling**: The user's message is processed:\n   - The system identifies intent using the `MockIntentClassifier`.\n   - It retrieves the appropriate agent from `MockAgentRegistry`.\n   - Messages are routed accordingly.\n5. **User Interaction**: The system handles responses from both agents and the user, allowing for continued interaction or termination of the session.\n6. **Lifecycle Management**: The script gracefully shuts down based on user input or system signals.\n\n## Conclusion\n\nThis example effectively illustrates a simplistic yet powerful way of routing conversation messages to specific agents based on their content, using a semantic analysis approach. Each component works together to provide a streamlined user experience, illustrating fundamental principles of agent-based communication and intent classification. This scenario can be extended to more complex systems employing advanced intent classifiers and agent registries.",
    "filename": "python/samples/core_semantic_router/run_semantic_router.py"
  },
  {
    "content": "# AutoGen-Core Streaming Chat with Multi-Agent Handoffs via FastAPI\n\nThis sample demonstrates how to build a streaming chat API featuring multi-agent handoffs and persistent conversation history using `autogen-core` and FastAPI. For more details on the handoff pattern, see the [AutoGen documentation](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/handoffs.html).\n\nInspired by `@ToryPan`'s example for streaming with Core API.\n\n## Key Features\n\n1.  **Streaming Response**: Implements real-time streaming of agent responses using FastAPI's `StreamingResponse`, `autogen-core`'s asynchronous features, and an `asyncio.Queue` to manage the data stream.\n2.  **Multi-Agent Handoffs**: Showcases a system where different agents (Triage, Sales, Issues & Repairs) handle specific parts of a conversation, using tools (`delegate_tools`) to transfer the conversation between agents based on the context.\n3.  **Persistent Multi-Turn Conversation**: Agents receive and process conversation history, enabling context-aware interactions. History is saved per conversation ID in JSON files within the `chat_history` directory, allowing conversations to resume across sessions.\n4.  **Simple Web UI**: Includes a basic web interface (served via FastAPI's static files) for easy interaction with the chat system directly from a browser.\n\n## File Structure\n\n*   `app.py`: Main FastAPI application code, including API endpoints, agent definitions, runtime setup, handoff logic, and streaming.\n*   `agent_user.py`: Defines the `UserAgent` responsible for interacting with the human user and saving chat history.\n*   `agent_base.py`: Defines the base `AIAgent` class used by specialized agents.\n*   `models.py`: Contains data models used for communication (e.g., `UserTask`, `AgentResponse`).\n*   `topics.py`: Defines topic types used for routing messages between agents.\n*   `tools.py`: Defines tools that agents can execute (e.g., `execute_order_tool`).\n*   `tools_delegate.py`: Defines tools specifically for delegating/transferring the conversation to other agents.\n*   `README.md`: (This document) Project introduction and usage instructions.\n*   `static/`: Contains static files for the web UI (e.g., `index.html`).\n*   `model_config_template.yaml`: Template for the model configuration file.\n\n## Installation\n\nFirst, ensure you have Python installed (recommended 3.8 or higher). Then, install the necessary libraries:\n\n```bash\npip install \"fastapi\" \"uvicorn[standard]\" \"autogen-core\" \"autogen-ext[openai]\" \"PyYAML\"\n```\n\n## Configuration\n\nCreate a new file named `model_config.yaml` in the same directory as this README file to configure your language model settings (e.g., Azure OpenAI details). Use `model_config_template.yaml` as a starting point.\n\n**Note**: For production, manage API keys securely using environment variables or other secrets management tools instead of hardcoding them in the configuration file.\n\n## Running the Application\n\nIn the directory containing `app.py`, run the following command to start the FastAPI application:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8501 --reload\n```\n\nThe application includes a simple web interface. After starting the server, navigate to `http://localhost:8501` in your browser.\n\nThe API endpoint for chat completions will be available at `http://localhost:8501/chat/completions`.\n\n## Using the API\n\nYou can interact with the agent system by sending a POST request to the `/chat/completions` endpoint. The request body must be in JSON format and contain a `message` field (the user's input) and a `conversation_id` field to track the chat session.\n\n**Request Body Format**:\n\n```json\n{\n  \"message\": \"I need refund for a product.\",\n  \"conversation_id\": \"user123_session456\"\n}\n```\n\n**Example (using curl)**:\n\n```bash\ncurl -N -X POST http://localhost:8501/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"message\": \"Hi, I bought a rocket-powered unicycle and it exploded.\",\n  \"conversation_id\": \"wile_e_coyote_1\"\n}'\n```\n\n**Example (using Python requests)**:\n\n```python\nimport requests\nimport json\nimport uuid\n\nurl = \"http://localhost:8501/chat/completions\"\nconversation_id = f\"conv-id\" # Generate a unique conversation ID for a different session.\n\ndef send_message(message_text):\n    data = {\n        'message': message_text,\n        'conversation_id': conversation_id\n    }\n    headers = {'Content-Type': 'application/json'}\n    try:\n        print(f\"\\n>>> User: {message_text}\")\n        print(\"<<< Assistant: \", end=\"\", flush=True)\n        response = requests.post(url, json=data, headers=headers, stream=True)\n        response.raise_for_status()\n        full_response = \"\"\n        for chunk in response.iter_content(chunk_size=None):\n            if chunk:\n                try:\n                    # Decode the chunk\n                    chunk_str = chunk.decode('utf-8')\n                    # Handle potential multiple JSON objects in a single chunk\n                    for line in chunk_str.strip().split('\\n'):\n                        if line:\n                            data = json.loads(line)\n                            # Check the new structure\n                            if 'content' in data and isinstance(data['content'], dict) and 'message' in data['content']:\n                                message_content = data['content']['message']\n                                message_type = data['content'].get('type', 'string') # Default to string if type is missing\n\n                                # Print based on type (optional, could just print message_content)\n                                if message_type == 'function':\n                                    print(f\"[{message_type.upper()}] {message_content}\", end='\\n', flush=True) # Print function calls on new lines for clarity\n                                    print(\"<<< Assistant: \", end=\"\", flush=True) # Reprint prefix for next string part\n                                else:\n                                    print(message_content, end='', flush=True)\n\n                                full_response += message_content # Append only the message part\n                            else:\n                                print(f\"\\nUnexpected chunk format: {line}\")\n\n                except json.JSONDecodeError:\n                    print(f\"\\nError decoding chunk/line: '{line if 'line' in locals() else chunk_str}'\")\n\n        print(\"\\n--- End of Response ---\")\n        return full_response\n\n    except requests.exceptions.RequestException as e:\n        print(f\"\\nError: {e}\")\n    except Exception as e:\n        print(f\"\\nAn unexpected error occurred: {e}\")\n\n# Start conversation\nsend_message(\"I want refund\")\n# Continue conversation (example)\n# send_message(\"I want the rocket my friend Amith bought.\")\n# send_message(\"They are the SpaceX 3000s\")\n# send_message(\"That sounds great, I'll take it!\")\n# send_message(\"Yes, I agree to the price and the caveat.\")\n\n\n```",
    "filename": "python/samples/core_streaming_handoffs_fastapi/README.md"
  },
  {
    "code": false,
    "content": "# AIAgent Documentation\n\n## Overview\nThe `AIAgent` class is designed as an advanced conversational agent implemented in Python. It interacts with a language model through a client interface and utilizes various tools to process user tasks effectively. The agent can manage tasks involving both standard responses and delegated actions, making it suitable for complex conversation flows and multi-agent systems.\n\n## Dependencies\nThe class relies on several modules and classes:\n- **JSON**: For processing arguments and responses.\n- **Typing**: To provide type hints for better code clarity.\n- **Asyncio**: For handling asynchronous operations, specifically for managing task execution and streaming responses.\n- **autogen_core**: A custom library which includes essential components such as `FunctionCall`, `MessageContext`, `RoutedAgent`, and associated models.\n- **models**: Custom classes for user tasks and agent responses.\n\n## Class Definition: AIAgent\nThe `AIAgent` class inherits from `RoutedAgent`, giving it capabilities to receive messages and respond accordingly. It initializes with parameters to define its behavior, tools, and messaging context. The class can handle streaming language model responses and execute calls to designated tools.\n\n### Constructor `__init__`\nThe constructor takes multiple parameters to set up the agent:\n- **description**: A string describing the agent.\n- **system_message**: A predefined system message for guidance.\n- **model_client**: An instance of `ChatCompletionClient` to connect to the language model.\n- **tools**: A list of tools the agent can utilize.\n- **delegate_tools**: Tools for delegating tasks to other agents.\n- **agent_topic_type**: A string representing the agent's topic type.\n- **user_topic_type**: A string representing the user's topic type.\n- **response_queue**: An asynchronous queue for managing responses.\n\n## Message Handling: `handle_task`\n`handle_task` is a coroutine method responsible for processing incoming user tasks and managing conversation state. \n\n### Step-by-step Breakdown\n1. **Streaming Initialization**: The method begins by creating a stream from the language model using the system message and the context from the incoming user task.\n   \n2. **Asynchronous Response Loop**: It enters an asynchronous loop, processing response chunks from the model. Responses can either be strings (immediate messages) or structured data (potential functions to call).\n\n3. **Final Response Handling**: It checks if the model response includes function calls. If so, the agent processes each call by either executing a tool directly or delegating to another agent.\n\n4. **Tool Execution**: For tools that are part of the agent, the agent executes the function call, collects the results, and formats them accordingly.\n\n5. **Delegation Logic**: If the function call targets a delegate tool, the agent formulates a new user task context and prepares it for the relevant topic.\n\n6. **Publishing Messages**: If there are results from tool executions or if the task needs delegation, it sends messages back to the appropriate topics through the response queue or by publishing messages.\n\n7. **Additional LLM Calls**: After processing any function calls, it may initiate additional calls to the language model based on gathered results to continue the conversation flow.\n\n8. **Final Response Publishing**: Once all tasks are completed, including final results from the language model, the agent publishes the outcome back to the user's topic.\n\n## Error Handling\nThe method includes assertions and exception handling:\n- It asserts that there is always a response from the model.\n- If an unknown tool is encountered during function execution, it raises a `ValueError`.\n\n## Conclusion\nThe `AIAgent` class encapsulates complex interactions with a language model while facilitating a structured response mechanism via tools and delegated agents. This class functions effectively in environments where multi-agent interaction is essential, and it ensures a seamless user experience through its asynchronous handling of tasks and messages.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/agent_base.py"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThe provided code defines a user agent class named `UserAgent`, which is designed to manage the processing of messages in an asynchronous environment. This class is part of a larger framework referenced as `autogen_core`. The primary functionality of the `UserAgent` revolves around handling task results and saving chat history in a structured format.\n\n## Key Imports\n\nThe code begins with necessary imports from various modules:\n\n- **autogen_core**: This provides functionalities such as `MessageContext`, `RoutedAgent`, and `message_handler`, which are essential for message routing and context management.\n- **BufferedChatCompletionContext**: A specific context manager used to handle chat completion states.\n- **AgentResponse**: Presumably defines the structure of the responses this agent is expected to work with.\n- **asyncio**: A Python library for writing concurrent code using the async/await syntax.\n- **json & os**: Libraries used for file handling and JSON serialization.\n\n## UserAgent Class\n\n### Constructor\n\nThe `UserAgent` class inherits from `RoutedAgent` and is initialized with several parameters: \n\n- **description**: A string that describes the purpose of the agent.\n- **user_topic_type**: A string indicating the type of topics related to the user.\n- **agent_topic_type**: A string representing the type of topics the agent will handle.\n- **response_queue**: An `asyncio.Queue` used to manage asynchronous communication of responses.\n- **stream_done**: An object indicating the completion of a data stream.\n\nThe constructor calls the parent class's initializer and sets the passed parameters on the instance.\n\n### Message Handling\n\nThe core functionality of the `UserAgent` is encapsulated in the `handle_task_result` asynchronous method, which is decorated with `@message_handler`. This indicates that it is designed to process incoming messages.\n\n#### Parameters\n\n- **message**: An instance of `AgentResponse`, which contains information about the task result to be handled.\n- **ctx**: An instance of `MessageContext`, which provides the context for the message being processed.\n\n### Save Chat History\n\nWithin `handle_task_result`, the following steps are executed:\n\n1. **Chat Context Management**: A new instance of `BufferedChatCompletionContext` is created, which is initialized with a buffer size of 10 and the initial messages contained in `message.context`. This context allows for tracking chat history effectively.\n   \n2. **State Saving**: The context's state is saved asynchronously through `save_state()`, providing a snapshot of the current chat history.\n\n3. **File Handling**: \n   - A directory named `chat_history` is targeted for storing the chat history files.\n   - A verification is performed to ensure that `ctx.topic_id` is not `None`. If it is, a `ValueError` is raised, indicating that chat history cannot be stored without a valid topic ID.\n   - A file path is constructed using the topic ID to store the chat history as a JSON file. The naming convention used is `history-{topic_id.source}.json`.\n\n4. **Writing to JSON**: The function attempts to open the file at the constructed path in write mode and serializes the `save_context` data into JSON format, saving it neatly with an indentation of four spaces for readability.\n\n### Stream Completion Notification\n\nAfter saving the chat history, the method concludes by placing a signal (`self._STREAM_DONE`) into the `response_queue`. This indicates that the processing of the current message/task is complete and allows any waiting coroutines to proceed accordingly.\n\n## Conclusion\n\nIn summary, the `UserAgent` class is responsible for message handling, particularly focusing on storing chat histories in JSON format after processing task results. It operates asynchronously, leveraging Python\u2019s `asyncio` for concurrency, and encapsulates task result management within a structured class that adheres to a specific design pattern seen in agent-based architectures. The class showcases robust mechanisms for maintaining chat context and ensures that chat histories are persistently stored for future access or analysis.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/agent_user.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Chatbot Agent Application\n\n## Overview\nThis application implements a customer service chatbot for ACME Inc. using FastAPI. It employs multiple agent types, including triage, sales, and issues and repairs agents, each designed to handle specific tasks in a conversational manner. The agents use a single-threaded runtime to process user queries and respond accordingly. The setup includes asynchronous capabilities and streaming responses to efficiently manage real-time interactions.\n\n## Runtime Initialization\nThe application starts by importing necessary libraries and defining the runtime for the agents using `SingleThreadedAgentRuntime`. It initializes a response queue to collect messages from the agents and signal the end of the stream with a sentinel object.\n\n```python\nruntime = SingleThreadedAgentRuntime()\nresponse_queue: asyncio.Queue[str | object] = asyncio.Queue()\nSTREAM_DONE = object()\n```\n\nAn asynchronous context manager `lifespan` is defined to manage the lifecycle of the FastAPI application. Within this context, it ensures the creation of a directory for chat history, loads the model configuration from a YAML file, and registers multiple agents (triage, sales, issues and repairs, and user agents) with their specific functionalities and tools.\n\n## Agent Registration\nThe application registers four different agents:\n\n1. **Triage Agent**: Designed to gather initial information from the user and direct them to the appropriate department. It uses a subtle approach to question the user about their issue.\n\n2. **Sales Agent**: Focuses on upselling ACME products by engaging the user about their problems and proposing fictitious solutions while incorporating a humorous tone.\n\n3. **Issues and Repairs Agent**: Responsible for troubleshooting user problems and proposing fixes, alongside provisions for refunds if the user is dissatisfied.\n\n4. **User Agent**: Acts as an intermediary that manages the conversation flow between the user and the appropriate service agent.\n\n```python\ntriage_agent_type = await AIAgent.register(\n    ...\n)\n\nsales_agent_type = await AIAgent.register(\n    ...\n)\n\nissues_and_repairs_agent_type = await AIAgent.register(\n    ...\n)\n\nuser_agent_type = await UserAgent.register(\n    ...\n)\n```\n\nEach agent is defined with its operational logic via an associated system message, reinforcement of tools, and subscription to specific topics to process incoming messages relevant to them.\n\n## FastAPI Application Setup\nThe application, defined as `app = FastAPI(lifespan=lifespan)`, mounts a static files directory to serve HTML files and other assets.\n\n```python\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n```\n\nIt has an endpoint for the root HTML file:\n\n```python\n@app.get(\"/\")\nasync def read_index():\n    return FileResponse('static/index.html')\n```\n\n## Chat Completions Handling\nThe `/chat/completions` endpoint is designed for streaming chat interactions. When a user sends a request, it processes the incoming message along with its conversation ID. Input validation checks are performed to ensure the integrity and security of the input data.\n\n1. **Validation**: It checks that `message` and `conversation_id` are strings and matches the expected formatting to prevent potential security issues such as path traversal attacks.\n\n2. **Loading Chat History**: The application attempts to load previous chat history from a JSON file, handling errors gracefully to reset to default values when necessary.\n\n```python\nif os.path.exists(chat_history_file):\n    ...\n```\n\n3. **Creating a Response Stream**: An asynchronous generator function is defined to manage the response stream. It publishes the user message to the runtime and listens for messages from the agents through the response queue.\n\n```python\nasync def response_stream() -> AsyncGenerator[str, None]:\n    ...\n```\n\nThe function yields each item from the response queue back to the user, ensuring that each response is properly formatted as JSON.\n\n4. **Streaming Response**: Finally, the stream is returned as a `StreamingResponse`, allowing real-time interaction with the user.\n\n```python\nreturn StreamingResponse(response_stream(), media_type=\"text/plain\")\n```\n\n## Main Execution\nThe application can be run directly using Uvicorn, a lightning-fast ASGI server, which allows it to host on a specified address and port.\n\n```python\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8501)\n```\n\nThis enables the application to be accessible from any host.\n\n## Tools Integration\nVarious tools and utility functions are imported to aid in specific operations, such as:\n- Executing orders or refunds.\n- Looking up items for users based on their requests.\n\nThese tools are assigned to the respective agents and facilitate their operational functionality.\n\n## Conclusion\nIn summary, this FastAPI application serves as a robust framework for implementing a conversational agent system, leveraging multiple specialized agents to provide tailored user interactions. It handles input validation, chat history management, and streaming responses efficiently, supporting real-time customer service functionalities. The modular architecture allows for ease of extension and enhancement as needed.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/app.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code snippet primarily defines three data models for user interactions with a system, likely related to a chatbot or generative AI application. It utilizes Python's type hinting and the Pydantic library to enforce data validation and structure.\n\n### Imports\n\nThe program imports the following:\n\n1. **List**: A type hint from the built-in `typing` module, indicating that a variable is expected to be a list.\n2. **LLMMessage**: A class from the module `autogen_core.models`, presumably representing a message structure in the context of a language model.\n3. **BaseModel**: A class from the Pydantic library that serves as the base for data models and ensures data validation.\n\n## UserLogin Class\n\n### Purpose\n\nThe `UserLogin` class is defined as a subclass of `BaseModel`, but currently does not contain any fields or methods. Its intended use is likely to be extended in future implementations to carry attributes relevant to a user\u2019s login information, such as username and password, though they are not defined here.\n\n### Role\n\nAs a placeholder model, `UserLogin` sets the stage for any forthcoming enhancements related to user authentication processes. It inherits from `BaseModel`, granting it validation capabilities once specific fields are added.\n\n## UserTask Class\n\n### Purpose\n\nThe `UserTask` class is designed to model a user task, encapsulating a context made up of a list of `LLMMessage` instances. This suggests that each task performed by the user will involve interpreting some contextual information in terms of messages.\n\n### Role\n\nBy utilizing the Pydantic functionality, `UserTask` allows the structured representation of a context that can be validated when instantiated. When integrated into a broader application, this could facilitate task management, user interaction, or conversation flows with a language model. \n\n### Structure\n\n- **context**: A list of `LLMMessage` objects, indicating that this model expects to hold multiple structured messages that can inform the task being performed.\n\n## AgentResponse Class\n\n### Purpose\n\nThe `AgentResponse` class is another Pydantic model that represents the output or reply generated by an agent, likely in response to a user task. This model is more fully defined than the `UserLogin` class and provides clear expectations for incoming data.\n\n### Role\n\nThis class is crucial for managing and validating the structure of responses generated by the AI agent. It can play a significant role in ensuring that responses are consistently formatted and contain all necessary information for user-system interactions.\n\n### Structure\n\n- **reply_to_topic_type**: A string that presumably categorizes or describes the nature of the topic to which the agent's response relates. This might be useful in determining how to handle the response in a broader conversation.\n- **context**: Similar to the `UserTask`, this field is also a list of `LLMMessage` objects, indicating that an agent's response may also involve contextual messages that can aid in understanding or processing the response.\n\n## Conclusion\n\nThis snippet sets up foundational data structures for managing user interactions with a potential AI model. While the code is minimal, it establishes a clear pathway toward developing robust features related to user authentication and task processing. The use of Pydantic ensures that future extensions maintain data integrity, making it easier to build sophisticated logic around user tasks and agent responses.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/models.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\nThis code is a Python script that defines several functions related to e-commerce-like operations: executing an order, looking up an item, and processing refunds. Additionally, it uses a tool called `FunctionTool` from the `autogen_core` library to wrap these functions, preparing them for further use, possibly in a larger framework or application.\n\n## Function Definitions\n\n### `execute_order`\n\n```python\ndef execute_order(product: str, price: int) -> Dict[str, Union[str, int]]:\n```\n\nThis function is responsible for processing an order. It takes the product name and the price as input parameters. Upon execution, it prints an order summary to the console, displaying the product and its corresponding price. The function then returns a dictionary containing the product name and price.\n\n- **Parameters:**\n  - `product`: A string representing the name of the product being ordered.\n  - `price`: An integer representing the price of the product in USD.\n\n- **Returns:**\n  - A dictionary with keys `product` and `price`, encapsulating the order details.\n\n### `look_up_item`\n\n```python\ndef look_up_item(search_query: str) -> Dict[str, str]:\n```\n\nThis function allows users to find an item based on a search query. The implementation in this case appears to be static, returning a hardcoded item ID. It returns a dictionary with the `item_id` and a status indicating that the item has been found.\n\n- **Parameters:**\n  - `search_query`: A string intended for the search term (though it is not actually used in this function's implementation).\n\n- **Returns:**\n  - A dictionary with keys `item_id` and `status`, indicating the result of the item lookup.\n\n### `execute_refund`\n\n```python\ndef execute_refund(item_id: str, reason: str = \"not provided\") -> Dict[str, str]:\n```\n\nThis function handles the refund process for an item. It takes the item ID and an optional reason for the refund. The function prints a summary of the refund details to the console and confirms that the refund execution is successful. It returns a dictionary with the item ID, the reason given for the refund, and the status of the refund operation.\n\n- **Parameters:**\n  - `item_id`: A string representing the ID of the item that is being refunded.\n  - `reason`: A string explaining the reason for the refund, defaulting to \"not provided\".\n\n- **Returns:**\n  - A dictionary with keys `item_id`, `reason`, and `refund_status`, encapsulating the details of the refund.\n\n## Tool Wrapping with `FunctionTool`\n\nEach of the previously defined functions is wrapped with the `FunctionTool` class, which appears to serve a purpose of enhancing or documenting their functionality.\n\n### `execute_order_tool`\n\n```python\nexecute_order_tool = FunctionTool(execute_order, description=\"Price should be in USD.\")\n```\n\nThis instance wraps the `execute_order` function. The description provided outlines the criteria for the price parameter, indicating it should be in USD. This could serve as an important detail for users or developers interacting with the tool.\n\n### `look_up_item_tool`\n\n```python\nlook_up_item_tool = FunctionTool(\n    look_up_item, description=\"Use to find item ID.\\nSearch query can be a description or keywords.\"\n)\n```\n\nSimilarly, this instance wraps the `look_up_item` function with a description that explains its usage. This informs users that the function can be used to find an item ID based on the provided search query, which may contain a description or relevant keywords.\n\n### `execute_refund_tool`\n\n```python\nexecute_refund_tool = FunctionTool(execute_refund, description=\"\")\n```\n\nThe `execute_refund_tool` wraps the `execute_refund` function but does not provide a description. This could imply that either the function's purpose is considered self-evident or that additional documentation is necessary for clarity.\n\n## Overall Functionality\n\nIn summary, this script is for managing a basic e-commerce process that involves order execution, item lookup, and refund processing. It prints relevant information to the console for user clarity and wraps function calls in a tool to enable enhanced usability and documentation.\n\n- **Order Management**: The functions help manage user interactions during the ordering and refunding processes.\n- **Console Output**: The script provides a user-friendly summary of operations through print statements.\n- **Extensibility**: By wrapping functions with `FunctionTool`, the code is prepared for potential integration into a larger system or framework, benefiting from added metadata and functionality.\n\nThis structured approach simplifies the development process while maintaining clear specifications about various operations.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/tools.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\nThe provided code snippet defines a set of functions and tools related to transferring user interactions to specific types of agents in a conversational AI or automated support system. Each function corresponds to a particular area of expertise (sales, issues and repairs, and triage), and the tools provide a structured way to interface with these functions.\n\n## Function Definitions\n\n### `transfer_to_sales_agent()`\n```python\ndef transfer_to_sales_agent() -> str:\n    return sales_agent_topic_type\n```\nThis function is designed to handle user queries related to sales or purchasing. It returns a predefined type indicator (`sales_agent_topic_type`) that likely helps route the user to a sales-specific agent or subsystem. The function explicitly states its role in the support flow, allowing for clear routing of sales-related inquiries.\n\n### `transfer_to_issues_and_repairs()`\n```python\ndef transfer_to_issues_and_repairs() -> str:\n    return issues_and_repairs_agent_topic_type\n```\nSimilar to the previous function, `transfer_to_issues_and_repairs` directs user interactions related to issues, repairs, or refunds to the appropriate support channel. It returns `issues_and_repairs_agent_topic_type`, effectively guiding the system to escalate the user's concern to the relevant agent for assistance in these areas.\n\n### `transfer_back_to_triage()`\n```python\ndef transfer_back_to_triage() -> str:\n    return triage_agent_topic_type\n```\nThis function is specifically intended for situations where a user's inquiry extends beyond the scope of the other specified areas. By returning `triage_agent_topic_type`, it indicates that the conversation should return to a general triage agent, possibly to assess the next best step in addressing the user's needs. This function can also be helpful for situations requiring escalation to a human operator.\n\n## Tool Definitions\n\n### `transfer_to_sales_agent_tool`\n```python\ntransfer_to_sales_agent_tool = FunctionTool(\n    transfer_to_sales_agent, description=\"Use for anything sales or buying related.\"\n)\n```\nThis segment creates a `FunctionTool` instance for the `transfer_to_sales_agent` function. It associates the function with a description, providing context for its use case: sales or buying-related inquiries. The `FunctionTool` acts as a structured wrapper, allowing the system to efficiently engage with this specific functionality.\n\n### `transfer_to_issues_and_repairs_tool`\n```python\ntransfer_to_issues_and_repairs_tool = FunctionTool(\n    transfer_to_issues_and_repairs, description=\"Use for issues, repairs, or refunds.\"\n)\n```\nHere, a similar `FunctionTool` is created for managing issues and repairs. The description clarifies its purpose, indicating that it should be invoked for requests related to problem-solving or refunds. This structured approach supports clear communication and expectation management between users and the system.\n\n### `transfer_back_to_triage_tool`\n```python\ntransfer_back_to_triage_tool = FunctionTool(\n    transfer_back_to_triage,\n    description=\"Call this if the user brings up a topic outside of your purview,\\nincluding escalating to human.\"\n)\n```\nThis tool packages the `transfer_back_to_triage` function, with a detailed description highlighting its necessity when inquiries diverge from specific topics. The inclusion of the escalation note to human support emphasizes the tool\u2019s role in maintaining user satisfaction and effective problem resolution.\n\n## Summary of Functionality\nIn summary, this code snippet establishes a simple yet effective mechanism for routing user inquiries to the appropriate agents based on their needs. By defining specific functions for different categories and wrapping them in `FunctionTool` instances with descriptive context, the design ensures that users receive relevant assistance. This framework serves as a foundational component for a more extensive automated support system, providing clarity and efficiency in communication paths.\n\nThe system's modular design allows for easier updates or extensions should new types of agents or functionalities be introduced in the future. Each function's clear mapping to specific user needs enhances the overall usability of the automated tool, providing a seamless experience for users seeking support across various topics.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/tools_delegate.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThe provided code snippet is a simple declaration of string variables that represent different topic types or categories related to agents and users. These variables could be utilized in a broader context, such as in a messaging system, chat application, or an automated workflow that distinguishes different agent types and user roles.\n\n## Variable Declarations\n\n### SalesAgent\n\n```python\nsales_agent_topic_type = \"SalesAgent\"\n```\n\nThis variable `sales_agent_topic_type` is defined to hold the string \"SalesAgent\". It likely represents an agent that deals with sales-related inquiries and tasks. The use of a dedicated variable makes it easier to refer to or modify this topic type throughout the application, ensuring consistency.\n\n### IssuesAndRepairsAgent\n\n```python\nissues_and_repairs_agent_topic_type = \"IssuesAndRepairsAgent\"\n```\n\nThe `issues_and_repairs_agent_topic_type` variable assigns the string \"IssuesAndRepairsAgent\". This implies that the agent's role focuses on handling issues and repairs, possibly in a customer support context. Its designation as a variable enhances readability, allowing developers to reference this role in conditionals or functions related to issue handling.\n\n### TriageAgent\n\n```python\ntriage_agent_topic_type = \"TriageAgent\"\n```\n\nThe variable `triage_agent_topic_type` is set to \"TriageAgent\". This indicates that this type of agent might be responsible for assessing and prioritizing requests or issues that come into a system. The use of this variable allows the system to correctly route or handle incoming communications based on agent specialization.\n\n### User\n\n```python\nuser_topic_type = \"User\"\n```\n\nThe `user_topic_type` variable is declared with the value \"User\". This general designation represents the end-user or customer interacting with the agents. It can be used in the context of user identification, routing messages to human agents, or other interactions in a customer service workflow. \n\n## Purpose and Role of the Variables\n\nEach of these variables plays a critical role in categorizing different types of interactions within a system. By having explicitly defined variables:\n\n1. **Clarity**: The code conveys its intentions clearly. Each variable indicates what kind of agent or user is being dealt with, which is vital for maintaining readability and understanding throughout the codebase.\n  \n2. **Modularity**: Should any changes to these topic types be necessary (for example, renaming them or updating their values), they can be handled in one place, avoiding potential errors from hard-coded strings scattered throughout the code.\n\n3. **Scalability**: As features or agent types evolve, new variables can be added alongside the existing ones. This arrangement helps in maintaining organized and scalable code.\n\n4. **Maintainability**: Using variables instead of raw strings improves maintainability. Developers can quickly adjust the functionality without the risk of missing instances of a string throughout the code when modifications are required.\n\n## Suggested Usage Context\n\nIn a broader application context, these variables could be part of a messaging framework, chatbot architecture, or customer support workflow where various agents handle differentiated tasks. \n\nFor example, if a user submits a request pertaining to a sales inquiry, the system may route the inquiry to an agent of type `SalesAgent`. In contrast, issues requiring attention for repairs would trigger a flow to an `IssuesAndRepairsAgent`. \n\nSimilarly, a `TriageAgent` could assess incoming requests, directing them appropriately based on their nature before final handling. The `User` variable denotes which entity is initiating an interaction, crucial for any response or service to be addressed back to the correct user.\n\n## Conclusion\n\nThe code snippet presents a fundamental design principle in software development where distinct roles are captured clearly through aptly named variables. This design not only promotes better organization but also aids in scalability and maintainability, which are crucial attributes for evolving codebases.",
    "filename": "python/samples/core_streaming_handoffs_fastapi/topics.py"
  },
  {
    "content": "# AutoGen-Core Streaming Chat API with FastAPI\n\nThis sample demonstrates how to build a streaming chat API with multi-turn conversation history using `autogen-core` and FastAPI.\n\n## Key Features\n\n1.  **Streaming Response**: Implements real-time streaming of LLM responses by utilizing FastAPI's `StreamingResponse`, `autogen-core`'s asynchronous features, and a global queue created with `asyncio.Queue()` to manage the data stream, thereby providing faster user-perceived response times.\n2.  **Multi-Turn Conversation**: The Agent (`MyAgent`) can receive and process chat history records (`ChatHistory`) containing multiple turns of interaction, enabling context-aware continuous conversations.\n\n## File Structure\n\n*   `app.py`: FastAPI application code, including API endpoints, Agent definitions, runtime settings, and streaming logic.\n*   `README.md`: (This document) Project introduction and usage instructions.\n\n## Installation\n\nFirst, make sure you have Python installed (recommended 3.8 or higher). Then, in your project directory, install the necessary libraries via pip:\n\n```bash\npip install \"fastapi\" \"uvicorn[standard]\" \"autogen-core\" \"autogen-ext[openai]\"\n```\n\n## Configuration\n\nCreate a new file named `model_config.yaml` in the same directory as this README file to configure your model settings.\nSee `model_config_template.yaml` for an example.\n\n**Note**: Hardcoding API keys directly in the code is only suitable for local testing. For production environments, it is strongly recommended to use environment variables or other secure methods to manage keys.\n\n## Running the Application\n\nIn the directory containing `app.py`, run the following command to start the FastAPI application:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8501 --reload\n```\n\nAfter the service starts, the API endpoint will be available at `http://<your-server-ip>:8501/chat/completions`.\n\n## Using the API\n\nYou can interact with the Agent by sending a POST request to the `/chat/completions` endpoint. The request body must be in JSON format and contain a `messages` field, the value of which is a list, where each element represents a turn of conversation.\n\n**Request Body Format**:\n\n```json\n{\n  \"messages\": [\n    {\"source\": \"user\", \"content\": \"Hello!\"},\n    {\"source\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n    {\"source\": \"user\", \"content\": \"Introduce yourself.\"}\n  ]\n}\n```\n\n**Example (using curl)**:\n\n```bash\ncurl -N -X POST http://localhost:8501/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"messages\": [\n    {\"source\": \"user\", \"content\": \"Hello, I'\\''m Tory.\"},\n    {\"source\": \"assistant\", \"content\": \"Hello Tory, nice to meet you!\"},\n    {\"source\": \"user\", \"content\": \"Say hello by my name and introduce yourself.\"}\n  ]\n}'\n```\n\n**Example (using Python requests)**:\n\n```python\nimport requests\nimport json\nurl = \"http://localhost:8501/chat/completions\"\ndata = {\n    'stream': True,\n    'messages': [\n            {'source': 'user', 'content': \"Hello,I'm tory.\"},\n            {'source': 'assistant', 'content':\"hello Tory, nice to meet you!\"},\n            {'source': 'user', 'content': \"Say hello by my name and introduce yourself.\"}\n        ]\n    }\nheaders = {'Content-Type': 'application/json'}\ntry:\n    response = requests.post(url, json=data, headers=headers, stream=True)\n    response.raise_for_status()\n    for chunk in response.iter_content(chunk_size=None):\n        if chunk:\n            print(json.loads(chunk)[\"content\"], end='', flush=True)\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error: {e}\")\nexcept json.JSONDecodeError as e:\n    print(f\"JSON Decode Error: {e}\")\n```",
    "filename": "python/samples/core_streaming_response_fastapi/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis code implements an asynchronous FastAPI application that serves as an interface to a language model (LLM) agent. The main purpose of the application is to receive user messages, process them through the LLM, and stream the generated responses back to the client. The agent uses a queue to handle streaming results, with error handling and clean termination processes.\n\n## Key Components\n\n### Data Classes\n\n1. **AgentResponse**\n   - Represents the final accumulated response from the LLM agent.\n   - Contains one attribute:\n     - `content`: A string holding the agent's final response content.\n  \n2. **UserRequest**\n   - Represents the user's chat history.\n   - Contains one attribute:\n     - `messages`: A list of messages in dictionary format, where each message has 'source' (indicating the speaker: user or assistant) and 'content' keys.\n\n### Agent Implementation\n\n- **MyAgent Class**\n  - Inherits from `RoutedAgent`.\n  - Responsible for handling user messages and generating responses.\n  - Contains methods to process incoming messages and communicate with the LLM model client.\n\n#### Method: `handle_user_message`\n   - Decorated with `@message_handler`, this method processes user requests.\n   - It accumulates the responses from assistant messages.\n   - Converts incoming messages to either `UserMessage` or `AssistantMessage` objects based on their source.\n   - Streams the responses from the LLM client, sending them to `response_queue`.\n   - Handles exceptions gracefully by sending error messages to the queue.\n\n### Application Lifespan Management\n\n- **lifespan Async Context Manager**\n  - Manages the lifecycle of the FastAPI application. \n  - Loads the model configuration from a YAML file and initializes the `ChatCompletionClient`.\n  - Registers the `MyAgent` with the application\u2019s runtime (`SingleThreadedAgentRuntime`).\n  - Starts the agent runtime and ensures proper shutdown.\n\n### FastAPI Application\n\n- **FastAPI Instance (`app`)**\n  - Created with a custom lifespan context manager.\n  \n- **Endpoint: `/chat/completions`**\n  - This POST endpoint accepts JSON payloads containing user messages.\n  - It validates the incoming request and initializes a `UserRequest` object.\n  - An asynchronous response stream function is defined within this endpoint to handle the streaming of responses.\n\n#### Inner Function: `response_stream`\n  - Creates a task to send a message to the `runtime`.\n  - Continuously consumes items from the `response_queue`, yielding them as responses while checking for errors or termination signals.\n  - Ends the stream when signaled by the `STREAM_DONE` object.\n\n### Running the Application\n\n- The application is designed to be run using Uvicorn, an ASGI server. The entry point is wrapped in the `if __name__ == \"__main__\"` block, where it specifies the host and port for running the FastAPI application.\n\n## Error Handling\n\nThe application features basic error handling:\n- If the incoming request structure is invalid (i.e., `messages` is not a list), it raises a `HTTPException` with a 400 status code.\n- Any exceptions that occur during the message handling process in `handle_user_message` are caught and a corresponding error message is sent to the `response_queue`.\n\n## Conclusion \n\nThis FastAPI application effectively establishes a real-time chat interface with an LLM agent. It integrates asynchronous programming practices, streamlines response handling, and manages the lifecycle of both the application and the agent runtime efficiently. Furthermore, it is designed for scalability and ensures a seamless user experience during interactions with the chat model.",
    "filename": "python/samples/core_streaming_response_fastapi/app.py"
  },
  {
    "content": "# Python and dotnet agents interoperability sample\n\nThis sample demonstrates how to create a Python agent that interacts with a .NET agent.\nTo run the sample, check out the autogen repository.\nThen do the following:\n\n1. Navigate to autogen/dotnet/samples/Hello/Hello.AppHost\n2. Run `dotnet run` to start the .NET Aspire app host, which runs three projects:\n    - Backend (the .NET Agent Runtime)\n    - HelloAgent (the .NET Agent)\n    - this Python agent - hello_python_agent.py\n3. The AppHost will start the Aspire dashboard on [https://localhost:15887](https://localhost:15887).\n\nThe Python agent will interact with the .NET agent by sending a message to the .NET runtime, which will relay the message to the .NET agent.",
    "filename": "python/samples/core_xlang_hello_python_agent/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script is an asynchronous Python application that sets up a worker agent utilizing gRPC for communication. It loads environment variables, initializes a runtime for handling messages, registers the agent, subscribes to various message types, and publishes messages, demonstrating a simple interaction using protocol buffers.\n\n## Key Modules and Imports\n\nThe script imports various libraries, including:\n- `asyncio`: For asynchronous programming.\n- `logging`: To manage log messages.\n- `os` and `sys`: For environment and system path operations.\n- `dotenv`: To load environment variables from a `.env` file.\n- Protocol buffers (`NewMessageReceived` and `Output`): To define the message structures for gRPC communication.\n- `UserProxy`: A utility for handling user input and interactions.\n\nThe code integrates a runtime class (`GrpcWorkerAgentRuntime`) designed to facilitate gRPC communication.\n\n## Main Function\n\n### Initialization\n\nThe `main()` function serves as the entry point of the script. It begins by loading environment variables using `load_dotenv()`. The script retrieves the `AGENT_HOST` variable, defaulting to `http://localhost:50673` if the variable is not present. It also ensures that the host does not include protocol prefixes (HTTP or HTTPS).\n\n### Setting Up gRPC Runtime\n\nAfter logging the obtained host address, the function initializes an instance of `GrpcWorkerAgentRuntime`, passing the cleaned host address and a data serialization format. This instance will handle communication with the specified agent host.\n\n### Registering User Proxy\n\nNext, the function registers a user proxy with the runtime using a specific agent name (\"HelloAgent\"). The lambda function creates a new instance of `UserProxy`, which is likely responsible for managing interaction with the user.\n\n### Subscribing to Messages\n\nThe script adds multiple subscriptions to the runtime for different message types. Subscriptions are crucial for defining which events or messages the agent should listen for:\n- **Default Subscription for agent type \"HelloAgent\".**\n- **Type Subscriptions for various topics** like \"HelloTopic\", \"agents.NewMessageReceived\", \"agents.ConversationClosed\", and \"agents.Output\" specifically for the agent type \"HelloAgent\".\n\n## Message Publishing\n\n### Creating and Sending Messages\n\nAfter setting up the subscriptions, the function constructs two message instances:\n1. **`NewMessageReceived`**: Represents an incoming message with a string payload (\"Hello from Python!\").\n2. **`Output`**: Represents a response or output message (\"^v^v^v---Wild Hello from Python!---^v^v^v\").\n\nThe function then publishes both messages to the same topic (\"HelloTopic\") using the `publish_message` method of the runtime. Each message specifies the topic ID and a sender ID for identification.\n\n## Runtime Cleanup and Stopping\n\nAfter the messages are sent, the function waits for a shutdown signal using `stop_when_signal()`, which handles graceful termination of the runtime when a signal is received. This ensures that the agent can shut down cleanly, allowing any ongoing processes to finalize before exiting.\n\n## Logger Configuration and Execution\n\n### Logger Setup\n\nIn the `__main__` block, the logging configuration is set to DEBUG level, allowing for verbose output during execution. The logger instance is configured to provide insights into the worker's operation.\n\n### Running the Main Loop\n\nFinally, the `asyncio.run(main())` method is invoked to start the program's main loop, executing the asynchronous `main()` function. This coroutine-driven execution enables the agent to handle messages and user interactions efficiently.\n\n## Summary\n\nIn summary, this script establishes a gRPC-enabled worker agent capable of message exchange through defined protocols. By employing asynchronous programming, it gathers user input, registers message types, and manages communication effectively. The robust logging approach helps facilitate troubleshooting and monitoring of the agent's activities.",
    "filename": "python/samples/core_xlang_hello_python_agent/hello_python_agent.py"
  },
  {
    "code": false,
    "content": "# Documentation for `autogen_core.worker.protos` Module\n\n## Overview\n\nThe `autogen_core.worker.protos` module is designed to facilitate communication between agents and workers using Google Protocol Buffers (protobuf). This module acts as a foundational component for enabling efficient serialization and deserialization of structured data exchanged between different parts of a system that utilizes agents and worker processes.\n\n## Purpose\n\nThe primary purpose of this module is to define the data structures that agents and workers will use to communicate effectively. Protocol Buffers is a language-neutral, platform-neutral extensible mechanism for serializing structured data, making it ideal for defining APIs and for communication between services in a distributed system.\n\n## Imports and Setup\n\n```python\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n```\n\n### Module Imports\n\n1. **os**: This module provides a way to interact with the operating system, allowing for path manipulations and environment variable access.\n2. **sys**: This module is essential for managing Python's runtime environment, including manipulating the Python path.\n\n### Path Configuration\n\nThe script includes a configuration step where it modifies the `sys.path`. This allows the module to import files or submodules that are located in the same directory as the module itself or deeper. By inserting the absolute path to the current directory at the beginning of the `sys.path`, it ensures that any imports are resolved against the intended directory first, reducing potential import errors from similarly named modules located elsewhere.\n\n## Communication Protocol\n\nWhile the specific protobuf classes are not detailed in the provided snippet, generally:\n\n- **Agent-Worker Communication**: The defined protobuf classes would encapsulate the messages passed between agents (which typically perform tasks) and workers (which execute those tasks), allowing for smooth operation and interaction.\n- **Data Types**: The protobuf format allows for various data types (e.g., integers, strings, lists) to be serialized into a compact binary format, enhancing efficiency in communication.\n\n## Future Implementation\n\nThis module is likely designed to be extended with specific protobuf message definitions that detail various communication scenarios. These might include:\n\n- **Message types**: Different types of messages such as requests, responses, and notifications.\n- **Data fields**: Specific fields required for each message type, which may encapsulate parameters relevant to tasks or status updates from the agent and the worker.\n\n## Conclusion\n\nIn summary, the `autogen_core.worker.protos` module serves as an essential building block for the communication framework between agents and workers. By employing Google Protobuf, it enables structured and efficient data exchange crucial for the modular functionality of the system. Future development will likely involve adding specific protobuf message definitions to define the interaction patterns clearly and robustly.\n\nThis module exemplifies the typical preprocessing and setup necessary for a well-architected service-oriented architecture where different components need to work seamlessly together.",
    "filename": "python/samples/core_xlang_hello_python_agent/protos/__init__.py"
  },
  {
    "code": false,
    "content": "# Agent Events Protocol Buffer Documentation\n\n## Overview\n\nThe provided code is a protocol buffer (protobuf) definition for various event messages related to an agent system. Protocol Buffers is a language-neutral, platform-neutral extensible mechanism for serializing structured data. This specific code is generated from a `.proto` file, `agent_events.proto`, and serves as the Python implementation of these definitions.\n\nThe various message types defined in the protobuf file allow for structured communication between systems or components that use this event definition. These event messages may represent activities such as messages being received, processed, and stored, or operational states like closures and shutdowns.\n\n## Message Definitions\n\n### TextMessage\n\n```python\nclass TextMessage:\n    textMessage: str\n    source: str\n```\n\nThe `TextMessage` class captures a textual message from an agent system. It includes:\n- `textMessage`: The content of the text message.\n- `source`: The origin of the message, allowing readers to track where the message came from.\n\n### Input\n\n```python\nclass Input:\n    message: str\n```\n\nThe `Input` class encapsulates incoming messages to the agent. It contains:\n- `message`: The actual message for processing by the agent.\n\n### InputProcessed\n\n```python\nclass InputProcessed:\n    route: str\n```\n\nThis class signifies that the input has been processed. It includes:\n- `route`: Information on the route the message took through the processing pipeline.\n\n### Output\n\n```python\nclass Output:\n    message: str\n```\n\nThe `Output` class represents a message that the agent generates as a response or result of input processing. It contains:\n- `message`: The response message generated by the agent.\n\n### OutputWritten\n\n```python\nclass OutputWritten:\n    route: str\n```\n\nThis class confirms that the output has been successfully recorded or delivered. It includes:\n- `route`: Details the trajectory of the message after output generation.\n\n### IOError\n\n```python\nclass IOError:\n    message: str\n```\n\nThe `IOError` class signifies an error related to input/output operations. This class has:\n- `message`: Describes the nature of the I/O error encountered.\n\n### NewMessageReceived\n\n```python\nclass NewMessageReceived:\n    message: str\n```\n\nThis class signals the arrival of a new message to the agent. It comprises:\n- `message`: The newly received message awaiting processing.\n\n### ResponseGenerated\n\n```python\nclass ResponseGenerated:\n    response: str\n```\n\nThe `ResponseGenerated` class indicates that the agent has produced a response to processed input. It features:\n- `response`: The content of the generated response.\n\n### GoodBye\n\n```python\nclass GoodBye:\n    message: str\n```\n\nThe `GoodBye` class is a termination signal from the agent or its component. It includes:\n- `message`: A message indicating goodbye or termination context.\n\n### MessageStored\n\n```python\nclass MessageStored:\n    message: str\n```\n\nThis class confirms that a message has been stored in the system. It contains:\n- `message`: The stored message indicating successful storage.\n\n### ConversationClosed\n\n```python\nclass ConversationClosed:\n    user_id: str\n    user_message: str\n```\n\nThe `ConversationClosed` class signals the end of an interaction. It provides:\n- `user_id`: Identifier for the user engaged in the conversation.\n- `user_message`: The last message sent by the user in that conversation.\n\n### Shutdown\n\n```python\nclass Shutdown:\n    message: str\n```\n\nThe `Shutdown` class indicates that the agent system is shutting down. It includes:\n- `message`: Specifies the context of the shutdown.\n\n## Conclusion\n\nThis protobuf definition provides a structured way for different components within an agent system to communicate various events related to input, response generation, storage, and operational signals. Each message type serves a specific function within the larger framework of agent operations, enabling developers to seamlessly integrate and manage agent behavior across multiple interactions.",
    "filename": "python/samples/core_xlang_hello_python_agent/protos/agent_events_pb2.py"
  },
  {
    "code": false,
    "content": "# gRPC Python Protocol Compiler Generated Module\n\nThis module is generated by the gRPC Python protocol compiler plugin and contains client and server classes corresponding to services defined in Protocol Buffers (protobuf). The primary purpose of this code is to ensure that the gRPC version being used is compatible with the generated code, alerting developers if there is a version mismatch.\n\n## Version Definitions\n\nThe constants `GRPC_GENERATED_VERSION` and `GRPC_VERSION` are defined to hold the versions of the gRPC library against which the generated code was built and the version currently installed, respectively:\n\n```python\nGRPC_GENERATED_VERSION = '1.70.0'\nGRPC_VERSION = grpc.__version__\n```\n\nThis section establishes the context for potential compatibility issues, which is critical when using generated code that relies on specific features of the gRPC library.\n\n## Compatibility Check\n\nThe module attempts to verify whether the installed gRPC version is compatible with the generated code:\n\n```python\ntry:\n    from grpc._utilities import first_version_is_lower\n    _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n    _version_not_supported = True\n```\n\nHere, it imports a utility function designed to compare versions. If the utility import fails, an assumption is made that the version is not supported.\n\n## Exception Handling\n\nIf the verification step determines that the installed gRPC version is lower than the required version, the module raises a `RuntimeError`. This exception is descriptive, guiding the user on how to resolve the version conflict:\n\n```python\nif _version_not_supported:\n    raise RuntimeError(\n        f'The grpc package installed is at version {GRPC_VERSION},'\n        + f' but the generated code in agent_events_pb2_grpc.py depends on'\n        + f' grpcio>={GRPC_GENERATED_VERSION}.'\n        + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n        + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n    )\n```\n\nThe structured error message provides clear instructions, enhancing the user experience by eliminating ambiguity during troubleshooting.\n\n## Summary and Purpose\n\nIn summary, this module serves the crucial role of ensuring that the gRPC client and server classes generated from Protocol Buffers are compatible with the version of the gRPC library installed in the environment. It prevents runtime errors that could arise from incompatibilities by enforcing version requirements right at the module's import stage.\n\nBy actively checking versions and raising appropriate errors, this module improves the robustness of applications built using gRPC, facilitating smoother development workflows. This is especially important in environments where the gRPC library may be updated or modified, ensuring that developers are always aware of the compatibility of their generated code.",
    "filename": "python/samples/core_xlang_hello_python_agent/protos/agent_events_pb2_grpc.py"
  },
  {
    "code": false,
    "content": "# UserProxy Agent Documentation\n\nThe provided code defines a UserProxy class within an asynchronous framework intended for handling messages in a conversational environment. This documentation describes the functionality of the class and its methods, along with potential interactions with the system.\n\n## Overview\n\nThe `UserProxy` class inherits from the `RoutedAgent` and is designed to simulate a human user participating in a conversation. It interacts with incoming messages of various types and processes user input accordingly. \n\n## Imports and Dependencies\n\nThe code imports several libraries and modules necessary for its operation:\n\n1. **asyncio**: This standard library module is used for writing asynchronous code.\n2. **logging**: Used for logging information at various stages of execution, helping in debugging and monitoring.\n3. **typing**: Specifically, `Union` is imported to type hint that a variable can accept more than one type of messages.\n\nAdditionally, it imports several message types from `autogen_core` and protocol buffers, which are used to communicate different events in the conversation.\n\n## Class Definition: UserProxy\n\n### Purpose\n\nThe `UserProxy` class serves the specific purpose of allowing a human user to interact in a conversational setting by handling messages that arrive in various formats. It can interpret and act on different types of messages sent to it.\n\n### Initialization\n\nUpon initialization, the `UserProxy` class sets a default description for the agent. This is done through the constructor (`__init__`), which allows for an optional custom description at instantiation. The constructor calls the parent class (`RoutedAgent`) to initialize its base functionality.\n\n```python\ndef __init__(\n    self,\n    description: str = DEFAULT_DESCRIPTION,\n) -> None:\n    super().__init__(description)\n```\n\n## Message Handling with @message_handler\n\nThe `handle_user_chat_input` method is decorated with `@message_handler`, which signifies that it's designed to respond to incoming messages.\n\n### Method: handle_user_chat_input\n\n#### Parameters\n- `message`: Accepts any type from the defined `input_types`, which could be `ConversationClosed`, `Input`, or `Output`.\n- `ctx`: An instance of `MessageContext`, providing contextual information about the message.\n\n#### Functionality\n\n- **Input Handling**: If the message is an instance of `Input`, it prompts the user to provide input via the `ainput` method. The user\u2019s response is stripped of whitespace and logged for monitoring. The response is then published as a new message of type `NewMessageReceived` to a default topic identified by `DefaultTopicId()`.\n\n- **Output Handling**: If the message is an instance of `Output`, it simply logs the output message to the logger.\n\n- **Fallback**: If the message does not match any expected type, the method does nothing.\n\n```python\n@message_handler\nasync def handle_user_chat_input(self, message: input_types, ctx: MessageContext) -> None:\n    logger = logging.getLogger(\"autogen_core\")\n    # Processing logic based on message type...\n```\n\n## User Input: ainput Method\n\n### Purpose\n\nThe `ainput` method is defined to facilitate asynchronous user input. This method wraps the standard input function within an asyncio thread, allowing it to operate without blocking other asynchronous tasks.\n\n#### Parameters\n- `prompt`: A string that specifies the prompt to display to the user.\n\n#### Return Value\n\nIt returns the user\u2019s input as a string after awaiting the result.\n\n```python\nasync def ainput(self, prompt: str) -> str:\n    return await asyncio.to_thread(input, f\"{prompt} \")\n```\n\n## Logging\n\nThroughout the class, the logging module provides a mechanism to record events. The logger is initialized with the name \"autogen_core\", ensuring that messages are categorized correctly for later retrieval and analysis.\n\n## Summary\n\nThe `UserProxy` class effectively serves as an interactive agent in a conversation setting, allowing for human-like interactions. It can process user messages, respond accordingly, and handle output messages seamlessly. The use of asynchronous programming provides efficiency, ensuring that the agent can maintain responsiveness while awaiting user input. \n\nOverall, this class encapsulates important functionalities for facilitating a dialogue between a human user and an automated system, proving essential for applications that require conversational interfaces.",
    "filename": "python/samples/core_xlang_hello_python_agent/user_input.py"
  },
  {
    "content": "# gitty (Warning: WIP)\n\nThis is an AutoGen powered CLI that generates draft replies for issues and pull requests\nto reduce maintenance overhead for open source projects.\n\nSimple installation and CLI:\n\n   ```bash\n   gitty --repo microsoft/autogen issue 5212\n   ```\n\n*Important*: Install the dependencies and set OpenAI API key:\n\n   ```bash\n   uv sync --all-extras\n   source .venv/bin/activate\n   export OPENAI_API_KEY=sk-....\n   ```",
    "filename": "python/samples/gitty/README.md"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis snippet of code is a module that imports the `main` function from the `__main__` module and specifies that `main` is a publicly accessible object of this module. The `__all__` list determines what is exported when the module is imported using `from module import *`. \n\n## Import Statement\n\n### `from .__main__ import main`\n\n- This line imports the `main` function from the `__main__` module in the same package. \n- The dot (`.`) before `__main__` indicates a relative import, which suggests this code is structured as part of a larger package or module hierarchy.\n- The `main` function is likely the entry point for executing the program logic or features defined within the `__main__` module.\n\n## Public API Definition\n\n### `__all__ = [\"main\"]`\n\n- The `__all__` variable defines a public API for the module. When a user imports this module using the statement `from module_name import *`, only the objects listed in `__all__` will be imported.\n- By including `\"main\"` in this list, the module signals that the `main` function is intended for external use while potentially hiding other internal functionalities or components that are not included.\n\n## Purpose and Usage\n\nThis code snippet serves several important functions:\n\n1. **Module Initialization**: It organizes how the module will expose its functionalities, specifically emphasizing the `main` function, which is presumably crucial for the execution of the module.\n\n2. **Code Maintainability**: By explicitly listing the public interface in `__all__`, the code provides clarity to other developers regarding what functions or classes are intended to be used from this module, promoting better maintainability and reducing confusion.\n\n3. **Encapsulation**: This structure allows developers to encapsulate internal functionalities that should not be used or accessed directly by other modules, encouraging clean usage patterns and reducing the risk of interference.\n\n## Conclusion\n\nIn summary, this code is a straightforward yet effective organization of components within a Python module. By focusing on a single public function (`main`) via a controlled import mechanism, it lays the groundwork for a structured and maintainable codebase that can be built upon by developers. This modular approach enhances readability, usability, and overall quality of the software.",
    "filename": "python/samples/gitty/src/gitty/__init__.py"
  },
  {
    "code": false,
    "content": "# Gitty: GitHub Issue/PR Assistant Documentation\n\n## Overview\nThis script, named `gitty`, serves as an assistant for managing GitHub issues and pull requests (PRs) using an AI backend. It facilitates interaction with GitHub repositories, allowing users to retrieve, respond to, and manage issues or PRs efficiently. The tool supports various commands to either process issue or PR details, or to modify configuration files related to the tool's operation.\n\n## Dependencies\nThe script relies on several external tools and libraries:\n- **argparse**: For parsing command-line arguments.\n- **asyncio**: For running asynchronous tasks.\n- **os and subprocess**: For interacting with the operating system and executing shell commands.\n- **rich.console**: For enhanced console output.\n- Internal modules: `_gitty` for core functionalities related to GitHub interactions, and `_db` for managing issues in a database.\n\n## Environment Checks\n### OpenAI API Key\nThe `check_openai_key` function verifies if the OpenAI API key is available in the environment variables. If not, it prompts the user with instructions on how to set it up and exits the program.\n\n### GitHub CLI Check\nThe `check_gh_cli` function checks whether the GitHub Command Line Interface (CLI) is installed and accessible. It attempts to run `gh --version` and, if it fails, informs the user to install the CLI and exits.\n\n## Command-Line Interface\n### Argument Parsing\nThe `main` function utilizes the `argparse` library to define the command-line interface of the script. Users can execute different commands (`issue`, `pr`, `fetch`, `local`, or `global`) along with an optional issue or PR number. The help text provides guidance on usage and expected parameters.\n\n### Command Handling\n- **Issue and PR Commands**: Before executing the `issue` or `pr` commands, the script checks for both the `gh` CLI installation and the OpenAI API key. It then auto-detects the repository in the current directory using `gh repo view`.\n- **Fetch Command**: This retrieves issues from the GitHub repository and updates a local database with the fetched details.\n- **Local and Global Config Commands**: These commands allow users to edit Gitty configuration files. Local configurations are stored relative to the Gitty directory, while global configurations are stored in the user's home directory.\n\n## Configuration File Editing\nThe function `edit_config_file` is responsible for creating (if it doesn't exist) and opening a configuration file for editing using the user's preferred text editor, which defaults to `vi`. The function checks for the existence of the specified file and initializes it with instructions if necessary.\n\n## Execution Flow\n1. The script begins execution in the `if __name__ == \"__main__\":` block, calling the `main` function.\n2. It establishes a command-line interface for users to interact with via `argparse`.\n3. Upon receiving a command, it performs necessary checks for the GitHub CLI and OpenAI API key if the command involves issues or pull requests.\n4. The appropriate command is executed based on the user's input, utilizing subprocess calls to interface with the GitHub CLI.\n5. Output is presented in a user-friendly manner thanks to the Rich library for console interactions.\n\n## Conclusion\nThe `gitty` script encapsulates functionalities for managing GitHub issues and pull requests while integrating AI for enhanced responses. By checking essential dependencies and facilitating command-line interactions, it provides a comprehensive solution for developers looking to streamline their workflow with GitHub.",
    "filename": "python/samples/gitty/src/gitty/__main__.py"
  },
  {
    "code": false,
    "content": "# Documentation for the Script\n\nThis script is designed to interact with a Git repository by identifying the repository's root directory and managing a custom directory within that root. It utilizes subprocesses to communicate with Git and handles filesystem operations. The script also incorporates a custom theme for output formatting, enhancing readability using the Rich library.\n\n## Environment Configuration\n\nAt the beginning of the script, there is an environment variable configuration that disables parallelism for the tokenizers, which prevents warnings in relation to the use of parallel tasks. This is set with:\n\n```python\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # disable parallelism to avoid warning\n```\n\nDisabling this feature helps provide a smoother experience while executing the script, particularly when working with environments where parallel execution may cause issues, such as certain types of deployments or testing environments.\n\n## Custom Theme Definition\n\nThe script defines a custom theme for displaying output, utilizing the Rich library to enhance terminal text visualization. The theme includes various styles that can be applied:\n\n```python\ncustom_theme = Theme(\n    {\n        \"header\": \"bold\",\n        \"thinking\": \"italic yellow\",\n        \"acting\": \"italic red\",\n        \"prompt\": \"italic\",\n        \"observe\": \"italic\",\n        \"success\": \"bold green\",\n    }\n)\n```\n\nThis theme helps convey different types of information in a visually distinctive manner, improving user experience through formatted console output.\n\n## Fetching the Repository Root\n\nThe function `get_repo_root` is defined to determine the root directory of the current Git repository:\n\n```python\ndef get_repo_root() -> str:\n```\n\n### Purpose and Mechanism\n\n- **Git Command Execution**: The function utilizes the `subprocess.run()` method to execute a Git command (`git rev-parse --show-toplevel`) that retrieves the root directory of the repository.\n- **Error Handling**: If the command fails (for instance, if the current directory is not within a Git repository), it raises a `CalledProcessError`, triggering an error message and exits the script gracefully using `sys.exit(1)`.\n  \nThe function returns the path to the repository's root directory as a string, ensuring that subsequent operations can reliably locate where Git-related actions should take place.\n\n## Managing the Gitty Directory\n\nThe following function, `get_gitty_dir`, is responsible for creating or retrieving a specific directory within the Git repository:\n\n```python\ndef get_gitty_dir() -> str:\n```\n\n### Overview and Logic\n\n- **Directory Retrieval**: The function first calls `get_repo_root()` to determine the root of the Git repository.\n- **Path Construction**: It constructs the path to a subdirectory named `.gitty` within the repository root using `os.path.join`.\n- **Directory Creation**: If the `.gitty` directory does not already exist, it creates the directory using `os.makedirs`, ensuring that all necessary parent directories are also created if they do not exist.\n\nThis function ensures that the script has a designated place to store or manage files related to its operations, maintaining an organized structure within the Git repository.\n\n## Summary of Functionality\n\nIn summary, this script serves as an interactive utility designed to streamline operations within a Git repository. Key functionalities include:\n\n1. **Environment Setup**: It disables tokenizer parallelism for cleaner execution.\n2. **Custom Theme Setup**: Outputs can be formatted with visually distinctive styles using the Rich library.\n3. **Repository Navigation**: Retrieves the root of the Git repository, with error handling to manage non-Git directories.\n4. **Directory Management**: Creates or accesses a `.gitty` directory, ensuring the necessary structure for storing data or configurations pertinent to the script's operations.\n\nThese combined functionalities provide a robust foundation for further development, enabling users to build on top of this repository utility with additional features or commands as needed.",
    "filename": "python/samples/gitty/src/gitty/_config.py"
  },
  {
    "code": false,
    "content": "# Documentation for GitHub Issues Management Script\n\nThis document provides a high-level overview of the source code that manages GitHub issues by fetching, storing, and updating them in a local SQLite database, while also updating an embedding store (Chroma DB). \n\n## Dependencies\n\nThe script relies on several external libraries and frameworks:\n\n- `os`: For file and directory operations.\n- `json`: To parse JSON data received from the GitHub API.\n- `subprocess`: To execute shell commands efficiently.\n- `asyncio`: For asynchronous I/O operations.\n- `tqdm`: For displaying progress bars in the console.\n- `sqlite3`: For managing the local SQLite database.\n- `chromadb`: For working with a persistent client for embeddings.\n- Local imports related to configuration and GitHub content retrieval.\n\n## Database Initialization\n\n### `init_db(db_path: str) -> None`\n\nThis function initializes a SQLite database at the specified `db_path`. \n\n- **Purpose**: Sets up an `issues` table with columns for issue number, title, updated timestamp, and content.\n- **Table Declaration**: Ensures that the table exists for storing issues based on their attributes.\n\n## Updating the Database\n\n### `update_issue(db_path: str, number: int, title: str, updatedAt: str, content: str) -> None`\n\nThis function updates or inserts an issue into the predefined `issues` table in the SQLite database.\n\n- **Purpose**: Allows for the replacement of an existing issue entry or adding a new one, based on the issue number.\n- **Parameters**: Takes the `db_path`, `issue number`, `title`, `updatedAt` timestamp, and the `content` of the issue.\n\n## Chroma Database Update\n\n### `update_chroma(gitty_dir: str, db_path: str) -> None`\n\nThis function interacts with Chroma DB to update the embeddings of issues.\n\n- **Purpose**: Synchronizes the embeddings of issues stored in the SQLite database with Chroma's persistent collection.\n- **Embedding Creation**: Uses the `DefaultEmbeddingFunction` from `chromadb.utils` to convert issue content into embeddings.\n- **Operations**:\n  - Checks if a collection named `issues` exists; if not, creates one.\n  - Fetches all issues from the SQLite database, computes their embeddings, and upserts the data into Chroma DB.\n\n## Fetching and Updating Issues from GitHub\n\n### `fetch_and_update_issues(owner: str, repo: str, db_path: Optional[str] = None) -> None`\n\nThis function serves as the primary method to fetch all GitHub issues from a specified repository and update the local SQLite database accordingly. \n\n- **Parameters**: Takes `owner` (repository owner), `repo` (repository name), and an optional `db_path` (if not provided, defaults to `<repo_root>/.gitty.db`).\n- **Process Overview**:\n  1. **Fetches All Issues**: Executes a shell command using `gh` CLI to list issues in JSON format (up to a limit of 1000 issues).\n  2. **Database Connection**: Connects to the SQLite database to either create it or update it.\n  3. **Updating Logic**: \n     - For each issue, it checks if it already exists in the database:\n       - If yes, it only updates if the fetched issue\u2019s `updatedAt` timestamp is more recent than the existing entry.\n       - If no, it inserts the issue into the database.\n  4. **Async Content Retrieval**: Uses an asynchronous function `get_github_issue_content` to fetch the complete content of each issue.\n\n## Final Chroma Database Update\n\nAfter all issues are fetched and stored in the SQLite database, the function proceeds to update Chroma DB just like in the `update_chroma` function. This step involves fetching the latest data and creating embeddings again for each issue.\n\n- **Overall Purpose**: Ensures that both the SQLite and Chroma databases are synchronized with the latest issue data for efficient retrieval and embedding analysis.\n\n## Conclusion\n\nThis script efficiently handles the fetching, storage, and updating of GitHub issues using SQLite, while also maintaining a Chroma DB for embedding representations. It integrates subprocess calls to external GitHub CLI commands, uses asynchronous operations for content fetching, and ensures that the local database is updated based on the most recent data from GitHub. \n\nThe code utilizes strong database design principles, proper error handling, and efficient embedding management, making it a robust solution for managing GitHub issues in a developer's local environment.",
    "filename": "python/samples/gitty/src/gitty/_db.py"
  },
  {
    "code": false,
    "content": "# Documentation for Issue Management Script\n\nThis script interacts with GitHub issues via the GitHub CLI, fetching issue details, generating summaries, and identifying related issues. It employs asynchronous programming to improve efficiency in fetching data and utilizes the `chromadb` library for storing and querying related issues.\n\n## Imports and Dependencies\n\nThe script begins with the following imports:\n- `os`, `re`, `asyncio`, `json`, `subprocess`, and `sys`: Standard libraries for file management, regular expressions, asynchronous operations, JSON handling, subprocess execution, and system functionalities.\n- `Dict`, `List`, `Any`: Type hints from the `typing` module for improved code clarity.\n- `PersistentClient`: A class from the `chromadb` library, used to manage a persistent database of issues.\n\n## Function: `generate_issue_tdlr`\n\n```python\nasync def generate_issue_tdlr(issue_number: str, tldr: str) -> str:\n```\n\n### Purpose\nThis asynchronous function creates a concise summary (TLDR) for a specified issue. \n\n### Parameters\n- `issue_number`: The identifier for the GitHub issue.\n- `tldr`: The brief summary text of the issue.\n\n### Returns\nA formatted string indicating the issue\u2019s TLDR.\n\n## Function: `get_mentioned_issues`\n\n```python\ndef get_mentioned_issues(issue_number: int, issue_content: str) -> List[int]:\n```\n\n### Purpose\nThis function identifies other issues mentioned in the content of a specific issue based on the presence of issue numbers prefixed with `#`.\n\n### Parameters\n- `issue_number`: The ID of the current issue being analyzed.\n- `issue_content`: The content of the current issue which may contain references to other issues.\n\n### Returns\nA list of integers representing the issue numbers that were mentioned, excluding the current issue.\n\n### Logic\nThe function uses regular expressions to find mentions in the format `#<issue_number>` and excludes the current issue number from the results.\n\n## Function: `get_related_issues`\n\n```python\ndef get_related_issues(issue_number: int, issue_content: str, gitty_dir: str, n_results: int = 2) -> List[int]:\n```\n\n### Purpose\nThis function queries a persistent database to find related issues based on the content of the current issue.\n\n### Parameters\n- `issue_number`: The ID of the current issue.\n- `issue_content`: The text content of the current issue.\n- `gitty_dir`: Directory where the persistent database is stored.\n- `n_results`: The maximum number of related issues to retrieve (default is 2).\n\n### Returns\nA list of related issue numbers, ensuring that the current issue is not included.\n\n### Logic\nThe function accesses a `chroma` collection to execute a query with the provided issue content. Results are processed to exclude the current issue.\n\n## Function: `get_github_issue_content`\n\n```python\nasync def get_github_issue_content(owner: str, repo: str, issue_number: int) -> str:\n```\n\n### Purpose\nThis asynchronous function fetches detailed information about a specific GitHub issue using the GitHub CLI.\n\n### Parameters\n- `owner`: The username of the repository owner.\n- `repo`: The name of the repository.\n- `issue_number`: The number of the issue to retrieve.\n\n### Returns\nA formatted string containing the issue's content, author, and comments.\n\n### Logic\nThe function constructs and executes a command-line call to the GitHub CLI. It handles potential errors and formats the output into a comprehensive string detailing the issue and associated comments.\n\n## Function: `fetch_issue_summaries`\n\n```python\ndef fetch_issue_summaries(owner: str, repo: str) -> List[Dict[Any, Any]]:\n```\n\n### Purpose\nThis function retrieves a list of issues from a specified GitHub repository, including their numbers, titles, and last updated timestamps.\n\n### Parameters\n- `owner`: The GitHub username of the repository owner.\n- `repo`: The name of the repository.\n\n### Returns\nA list of dictionaries, each containing metadata about an issue.\n\n### Logic\nThe function executes a command to list issues with defined properties. Any potential errors in execution or JSON parsing are managed with appropriate error reporting.\n\n## Overall Workflow\n\nThe script\u2019s overall purpose is to facilitate the management and understanding of GitHub issues by offering tools to retrieve and summarize information efficiently. It integrates multiple functionalities:\n\n1. **Fetching Issue Content**: Developers can pull issue descriptions and comments asynchronously.\n2. **Extracting Related Issues**: The script can identify and return issues that are contextually linked.\n3. **Handling JSON Data**: Both fetching and processing of issues and their metadata are managed using JSON for ease and flexibility.\n4. **Error Management**: Throughout the script, clear error messages guide users when fetching data fails, improving usability.\n\nThis combination of features enables effective issue tracking and collaboration for development teams using GitHub.",
    "filename": "python/samples/gitty/src/gitty/_github.py"
  },
  {
    "code": false,
    "content": "# Gitty - GitHub Issue/PR Assistant Documentation\n\n## Overview\nThis document outlines the functionality of a Python script designed to assist in responding to GitHub issues and pull requests (PRs). Utilizing the OpenAI GPT-4 model, the script fetches, processes, and generates responses to GitHub issues based on user-specified repositories and issue numbers. The assistant is intended to provide concise, technical replies to aid in issue resolution while maintaining a user-friendly interface.\n\n## Dependencies\nThe script utilizes several libraries and modules to facilitate its operation:\n\n- **os, sys**: For file management and system interaction.\n- **autogen_agentchat**: Contains the core functionality for chat-based interactions with a pre-defined agent.\n- **rich**: For enhanced console output, providing formatted and colorful print capabilities.\n- **_github**: Includes helper functions for retrieving issue content and related data from GitHub.\n- **_config**: Manages configuration settings, such as obtaining directories for local configuration files.\n\n## Console Setup\nA `Console` object is instantiated with a custom theme defined in the `_config` module. This console is used throughout the script to provide styled output to the user, enhancing readability and engagement.\n\n## Core Functions\n\n### 1. `_run`\nThe `_run` function is an asynchronous function responsible for managing the communication with the `AssistantAgent`. It performs the following tasks:\n\n- Takes a task description and streams responses from the agent.\n- Processes different types of messages from the agent including tool call requests, execution events, and replies.\n- Formats and displays output in the console.\n- Logs responses if required.\n\n### 2. `_get_user_input`\nThis asynchronous function prompts the user for input. It checks if the user wants to exit the program (via input \"exit\"). If the user provides any other response, that input is returned for further interpretation by the script.\n\n### 3. `run_gitty`\nThe `run_gitty` function serves as the main entry point for operating the GitHub assistant. Its functionality can be broken down as follows:\n\n- **Initialization**: The function begins by displaying a header for the GitHub assistant and gathering configurations. It attempts to read global and local instructions from configuration files that may customize the agent's behavior.\n  \n- **Agent Preparation**: Constructs a base message to initialize the `AssistantAgent` with system instructions outlining its expected behavior, which includes responding to GitHub issues accurately and concisely.\n\n- **Fetch Issue Content**: It queries GitHub for specific issue content and prints the response.\n\n- **Check for Mentioned Issues**: The function identifies any issues mentioned within the content of the specified issue and fetches summaries for those.\n\n- **Related Issues**: It gathers any related issues that may assist in addressing the current issue being examined.\n\n- **Contextual Updates**: An updated prompt is constructed with all gathered data to further guide the assistant in composing a refined response.\n\n- **Final Responses**: The script ends with steps that involve summarizing the issue's status and generating a suggested response based on the assistant\u2019s analysis.\n\n## User Interaction\nThe main interaction loop allows users to:\n\n- View suggested responses generated by the assistant.\n- Provide feedback to potentially refine the suggested response.\n- Exit the application cleanly when needed.\n\nThis loop continues until the user opts to exit, allowing for iterative enhancement of the responses based on real-time feedback.\n\n## Error Handling\nThe script contains basic error handling during configuration file reading, notifying the user when files cannot be accessed. It gracefully exits when the user inputs \"exit\".\n\n## Summary\nIn summary, this GitHub Issue/PR Assistant is structured to optimize technical communication in handling GitHub scenarios. Through fetching, analyzing, and responding, it aims to streamline workflows for developers managing issues or PRs while offering a user-friendly command-line interface. The use of advanced natural language processing via the OpenAI API allows for powerful, context-aware responses tailored to the specific needs of GitHub project maintainers.",
    "filename": "python/samples/gitty/src/gitty/_gitty.py"
  },
  {
    "content": "# Task-Centric Memory Code Samples\n_(EXPERIMENTAL, RESEARCH IN PROGRESS)_\n\n<p align=\"right\">\n  <img src=\"../../packages/autogen-ext/imgs/task_centric_memory.png\" alt=\"Description\" width=\"300\" align=\"right\" style=\"margin-left: 10px;\">\n</p>\n\nThis directory contains code samples that illustrate the following forms of fast, memory-based learning:\n* Direct memory storage and retrieval\n* Learning from user advice and corrections\n* Learning from user demonstrations\n* Learning from the agent's own experience\n\nEach sample connects task-centric memory to a selectable agent with no changes to that agent's code.\nSee the block diagram to the right for an overview of the components and their interactions.\n\nEach sample is contained in a separate python script, using data and configs stored in yaml files for easy modification.\nNote that since agent behavior is non-deterministic, results will vary between runs.\n\nTo watch operations live in a browser and see how task-centric memory works,\nopen the HTML page at the location specified at the top of the config file,\nsuch as: `./pagelogs/teachability/0  Call Tree.html`\nTo turn off logging entirely, set logging level to NONE in the config file.\n\nThe config files specify an _AssistantAgent_ by default, which uses a fixed, multi-step system prompt.\nTo use _MagenticOneGroupChat_ instead, specify that in the yaml file where indicated.\n\n\n## Installation\n\nInstall AutoGen and its extension package as follows:\n\n```bash\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\" \"autogen-ext[task-centric-memory]\"\n```\n\nAssign your OpenAI key to the environment variable OPENAI_API_KEY,\nor else modify `utils/client.py` as appropriate for the model you choose.\n\n\n## Running the Samples\n\nThe following samples are listed in order of increasing complexity.\nExecute the corresponding commands from the `python/samples/task_centric_memory` directory.\n\n\n### Making AssistantAgent Teachable\n\nThis short, interactive code sample shows how to make the AssistantAgent teachable.\nThe following steps show the agent learning a user teaching from one chat session to the next,\nstarting with an empty memory bank.\nThe memory bank can be cleared manually by deleting the memory_bank directory (if it exists from a prior run), as shown below.\n    \n```bash\nrm -r memory_bank\npython chat_with_teachable_agent.py\nNow chatting with a teachable agent. Please enter your first message. Type 'exit' or 'quit' to quit.\n\nYou: How many items should be put in research summaries?\n---------- user ----------\nHow many items should be put in research summaries?\n---------- teachable_agent ----------\n<generates a long discussion>\n\nYou: Whenever asked to prepare a research summary, try to cover just the 5 top items.\n---------- user ----------\nWhenever asked to prepare a research summary, try to cover just the 5 top items.\n---------- teachable_agent ----------\n<discusses the advice>\n\nYou: quit\n\npython chat_with_teachable_agent.py`\nNow chatting with a teachable agent. Please enter your first message. Type 'exit' or 'quit' to quit.\n\nYou: How many items should be put in research summaries?\n---------- user ----------\nHow many items should be put in research summaries?\n---------- teachable_agent ----------\n[MemoryContent(content='Whenever asked to prepare a research summary, try to cover just the 5 top items.', mime_type='MemoryMimeType.TEXT', metadata={})]\n---------- teachable_agent ----------\n<generates a more appropriate answer> \n```\n\n\n### Direct Memory Storage and Retrieval\n\nThis sample shows how an app can access the `MemoryController` directly\nto retrieve previously stored task-insight pairs as potentially useful examplars when solving some new task.\nA task is any text instruction that the app may give to an agent.\nAn insight is any text (like a hint, advice, a demonstration or plan) that might help the agent perform such tasks.\n\nA typical app will perform the following steps in some interleaved order:\n1. Call the `MemoryController` repeatedly to store a set of memories (task-insight pairs).\n2. Call the `MemoryController` repeatedly to retrieve any memories related to a new task.\n3. Use the retrieved insights, typically by adding them to the agent's context window. (This step is not illustrated by this code sample.)\n\nThis sample code adds several task-insight pairs to memory, retrieves memories for a set of new tasks,\nlogs the full retrieval results, and reports the retrieval precision and recall.\n\n`python eval_retrieval.py configs/retrieval.yaml`\n\nPrecision and recall for this sample are usually near 100%.\n\n\n### Agent Learning from User Advice and Corrections\n\nThis sample first tests the agent (once) for knowledge it currently lacks.\nThen the agent is given advice to help it solve the task, and the context window is cleared.\nFinally the agent is once tested again to see if it can retrieve and use the advice successfully.\n\n`python eval_teachability.py configs/teachability.yaml`\n\nWith the benefit of memory, the agent usually succeeds on this sample.\n\n\n### Agent Learning from User Demonstrations\n\nThis sample asks the agent to perform a reasoning task (ten times) on which it usually fails.\nThe agent is then given one demonstration of how to solve a similar but different task, and the context window is cleared.\nFinally the agent is tested 10 more times to see if it can retrieve and apply the demonstration to the original task.\n\n`python eval_learning_from_demonstration.py configs/demonstration.yaml`\n\nThe agent's success rate tends to be measurably higher after the demonstration has been stored in memory.\n\n\n### Agent Learning from Its Own Experience\n\nThis sample asks the agent to perform a reasoning task on which it usually fails.\nThen using automatic success or failure feedback (for a verifiable task with no side-effects on the environment), \nthe agent iterates through a background learning loop to find a solution, which it then stores as an insight in memory.\nFinally the agent is tested again to see if it can retrieve and apply its insight to the original task,\nas well as to a similar but different task as a test of generalization.\n\n`python eval_self_teaching.py configs/self_teaching.yaml`\n\nUsing memory, the agent usually completes both tasks successfully in the second set of trials.",
    "filename": "python/samples/task_centric_memory/README.md"
  },
  {
    "code": false,
    "content": "# Documentation for the Teachable Chatbot Script\n\nThis document provides a high-level description of the Python script designed for creating a chat interface with a customizable AI assistant capable of remembering user interactions.\n\n## Overview\n\nThe script sets up an interactive chat session with an AI assistant leveraging OpenAI's GPT-4 technology. The assistant, referred to as `teachable_agent`, is designed to learn from conversations through a \"Task-Centric Memory\" model, which allows it to retain information provided by the user over the course of multiple sessions.\n\n## Key Components\n\n### Imports\n\n1. **AssistantAgent**: A class responsible for managing user interactions with the AI assistant.\n2. **Console**: A utility for handling console input and output, which facilitates interaction with the user.\n3. **OpenAIChatCompletionClient**: A client interface for OpenAI's GPT-4 model, enabling the assistant to generate responses.\n4. **MemoryController**: Manages task-centric memory, responsible for retaining user-provided information.\n5. **Teachability**: A utility that wraps the MemoryController, enabling the assistant to \"learn\" from conversations.\n\n### Main Function\n\nThe `main()` function orchestrates the setup and execution of the chat session. Here\u2019s a step-by-step breakdown of its functionality:\n\n1. **Client Initialization**: A new instance of `OpenAIChatCompletionClient` is created, specifying the model `gpt-4o-2024-08-06` for generating responses. This model serves as the foundation for the AI assistant's capabilities.\n\n2. **Memory Controller Setup**: An instance of `MemoryController` is initialized with the `reset` parameter set to `False`, signaling that existing memory (if any) should not be erased. This instance enables the AI to remember context and conversations with the user.\n\n3. **Teachability Integration**: The `Teachability` class is instantiated with the `memory_controller` as an argument. This prepares the assistant to dynamically learn from user interactions.\n\n4. **Assistant Agent Creation**: An `AssistantAgent` instance named `teachable_agent` is created. It is configured with a system message guiding its interaction style, using the `client` for responses and attaching the previously created `teachability` for memory retention.\n\n5. **Chat Loop**: The script enters an infinite loop, prompting the user for input. It accepts text from the console, which allows the user to chat with the AI. If the user types \"exit\" or \"quit,\" the loop breaks and the session concludes.\n\n6. **Integration with Console**: The `Console` utility is employed to handle the asynchronous execution of the assistant's response generation, ensuring smooth interaction.\n\n7. **Client Closure**: After the user exits the loop, the `client.close()` method is called, terminating the connection to the OpenAI service cleanly.\n\n### Execution\n\nThe script is designed to run asynchronously by wrapping the `main()` function within `asyncio.run()`. This encourages efficient handling of I/O operations, particularly useful in chat applications where there is a need for timely responses.\n\n### User Experience\n\nUpon running the script, the user is greeted with a message prompting them to start a conversation. The assistant is designed to provide helpful responses while incorporating learned information into future interactions, enhancing the overall experience over time.\n\n### Conclusion\n\nThe script offers a straightforward yet powerful framework for implementing a teachable AI assistant using OpenAI's GPT-4 model. By integrating memory capabilities, it allows for a more personalized and context-aware user experience. This foundational setup can be further expanded or customized for more advanced applications in conversational AI.",
    "filename": "python/samples/task_centric_memory/chat_with_teachable_agent.py"
  },
  {
    "code": false,
    "content": "# Evaluation of Learning from Demonstration Code Documentation\n\n## Overview\n\nThis Python code sample is designed for evaluating the ability of an agent (referred to as \"Apprentice\") to learn from demonstrations of similar tasks. It employs an asynchronous architecture with components defined in various imported modules. The execution flow connects task-centric memory to an agent capable of problem-solving and incorporates logging and grading mechanisms to assess performance improvements after demonstrations.\n\nThe code allows users to execute it with a specified configuration file via a command line interface. The provided configuration settings are crucial as they govern the behavior and parameters of the learning evaluation.\n\n## Invocation Instructions\n\nTo run this script, execute the following command in your terminal:\n\n```bash\npython eval_learning_from_demonstration.py configs/demonstration.yaml\n```\n\nThis command reads from a YAML configuration file, which outlines the necessary tasks, their expected answers, and any demonstration solutions provided to the agent.\n\n## Main Components\n\n### 1. Function `eval_learning_from_demonstration`\n\nThe core function of this code sample is `eval_learning_from_demonstration`, which performs the following steps to assess learning capabilities:\n\n- **Input Parameters:**\n  - `apprentice`: An instance of the `Apprentice` class, responsible for handling task memory and responses.\n  - `client`: An instance of `ChatCompletionClient`, which facilitates communication for the task evaluations.\n  - `logger`: An instance of `PageLogger`, used for logging events and results during execution.\n  - `config`: A dictionary containing configuration data.\n\n- **Execution Details:**\n  - Clears previous memory before executing baseline tests.\n  - Loads the necessary main and demonstration task data from the configuration.\n  - Conducts an initial test to benchmark the agent's performance on the main task without memory utilization.\n  - Records and logs the success rate before demonstrating a solution.\n  - Provides a demonstration for a similar task by adding it to the agent's memory.\n  - Retests the agent to see if it can apply the learned solution from memory to improve its performance.\n  - Logs and returns success rates before and after the demonstration.\n\n### 2. Function `run_example`\n\nThe `run_example` function orchestrates the execution of the overall evaluation process:\n\n- **Input Parameter:**\n  - `config_filepath`: The path to the YAML configuration file containing task descriptions and settings.\n\n- **Process Overview:**\n  - Loads the configuration settings from the supplied YAML file.\n  - Initializes logging, client, and apprentice components based on the loaded configuration.\n  - Calls the `eval_learning_from_demonstration` function to evaluate the agent.\n  - Outputs the results of the evaluation.\n\n## Execution Flow\n\n1. **Main Script Execution:**\n   - The script starts by checking command-line arguments. If the arguments are lacking or misformatted, a usage message is printed.\n   - If valid arguments are provided, the `run_example` function is executed within an asynchronous event loop using `asyncio.run()`.\n\n2. **Configuration Loading:**\n   - The YAML configuration file defines various parameters essential for the agent's behavior and the tasks to be evaluated.\n   - This file must include task descriptions, expected answers, and any information related to the demonstration tasks and solutions.\n\n3. **Memory Management:**\n   - The agent's memory is cleared to establish a baseline and ensure that success in subsequent tests can be directly attributed to the newly added demonstration.\n\n4. **Task Evaluation:**\n   - The agent undergoes a series of tests (specified by `num_trials` in the configuration) with and without memory utilization.\n   - The `Grader` class handles the evaluation logic, returning the count of successful task completions.\n\n5. **Demonstration Process:**\n   - After establishing a baseline, a demonstration is provided.\n   - The agent's memory is updated with the demonstration solution, allowing it to utilize this knowledge in future evaluations.\n\n6. **Results Compilation:**\n   - The outcomes of both pre- and post-demonstration tests are logged and formatted into a string for easy inspection.\n\n## Class and Utility Descriptions\n\n- **Apprentice Class:** Represents the agent that learns from task demonstrations. It is responsible for resetting memory and storing task-solution pairs to enhance its problem-solving capabilities.\n\n- **Grader Class:** Facilitates the evaluation process by testing the apprentice against provided tasks and comparing its results against expected answers.\n\n- **PageLogger:** A utility for logging informational messages and results, providing insights into the evaluation process.\n\n- **create_oai_client Function:** Responsible for creating a client connection utilized for communication during the task evaluations.\n\n- **load_yaml_file Function:** A utility for loading data from YAML configuration files, facilitating dynamic configuration adjustments.\n\n## Conclusion\n\nThis code sample serves as a practical demonstration of how to assess and enhance an agent's learning capabilities through previous task demonstrations. By leveraging structured configurations and asynchronous operations, the framework supports a methodical evaluation of learning in agents without modifying their code directly. This modular design allows adaptations for various tasks and components, making it a valuable resource in experimental machine learning applications.",
    "filename": "python/samples/task_centric_memory/eval_learning_from_demonstration.py"
  },
  {
    "code": false,
    "content": "# Memory Retrieval Evaluation Script Documentation\n\nThis script is designed to evaluate the precision and recall of a memory retrieval system using a task-centric approach. Specifically, it measures how effectively insights related to tasks can be fetched from a memory store. The evaluation is conducted using configuration settings specified in a YAML file. \n\n## Overview of the Script\n\nThe script imports essential modules and components, including `asyncio` for asynchronous operations, system modules for command line interaction, and specific libraries for memory management and YAML file handling. The primary objective is to assess the relationship between tasks and insights and determine how accurately relevant insights can be retrieved based on provided tasks.\n\n### Key Components\n\n- **MemoryController**: This is the central component responsible for managing task-insight pairs in memory. It allows for adding, resetting, and retrieving insights related to tasks.\n- **PageLogger**: This component is used for logging information during the evaluation process, helping in keeping track of the operations performed and the results obtained.\n- **ChatCompletionClient**: This is likely a client used to facilitate communication with an AI model for task completion.\n\n## Configuration\n\nThe script requires a configuration file, specified via command line arguments, which outlines the structure and settings for the evaluation:\n\n- **Tasks**: A list of tasks to be evaluated.\n- **Insights**: A collection of insights related to the tasks.\n- **Task-Insight Relevance**: A matrix that defines the relevance of each insight to corresponding tasks.\n\nThe config file is expected to be in YAML format, and its structure is fundamental for the operations of the script.\n\n## Evaluation Function: `eval_retrieval`\n\n### Purpose\n\nThe `eval_retrieval` function is the core of the script, performing several key tasks to evaluate the memory\u2019s performance in retrieving relevant insights based on defined tasks.\n\n#### Steps Performed\n\n1. **Function Entry**: It starts by invoking `logger.enter_function()` to log the entry into the evaluation function.\n   \n2. **Data Loading**: The function loads tasks and insights from specified files defined in the configuration. For each task, it extracts the description, and for each insight, it gathers the insight text.\n\n3. **Memory Reset**: It resets the memory to ensure a clean slate before adding new task-insight pairs.\n\n4. **Storing Insights**: The relevant insights (based on the relevance matrix) are added to the memory for each task. Specifically, if an insight is deemed highly relevant (value 2), it is stored.\n\n5. **Memory Retrieval Testing**: The function iterates over each task to:\n   - Retrieve insights from memory.\n   - Compare retrieved insights against the ground truth to determine relevance.\n\n6. **Count Accumulation**: It counts the total retrieved insights, the number of relevant insights, and the number of insights that were both relevant and retrieved.\n\n7. **Logging Results**: After the evaluation, it logs the total counts, precision, and recall metrics based on the evaluation results.\n\n8. **Return Results**: Finally returns a formatted string containing evaluation results.\n\n### Precision and Recall Calculation\n\nPrecision is calculated as the ratio of relevant insights retrieved to the total number of insights retrieved. Recall is calculated as the ratio of relevant insights retrieved to the total number of relevant insights present. Both metrics are logged in percentage format.\n\n## Execution Function: `run_example`\n\n### Purpose\n\nThe `run_example` function orchestrates the overall execution of the script by setting up components and calling the evaluation function.\n\n#### Steps Performed\n\n1. **Reading Configuration**: It loads the configuration file specified by the user.\n\n2. **Component Creation**: Initializes necessary components:\n   - Logger for operational tracking.\n   - ChatCompletionClient for task completion.\n   - MemoryController for managing memory operations.\n\n3. **Calling Evaluation**: Calls the `eval_retrieval` function with the initialized components.\n\n4. **Results Display**: Outputs the results of the evaluation to the console.\n\n## Main Script Execution\n\nThe script starts executing in the `if __name__ == \"__main__\":` block. It checks for command line arguments to specify the configuration file. If the argument count is incorrect, it provides usage instructions. If the argument is valid, it invokes the `run_example` function asynchronously using `asyncio.run`.\n\n## Summary\n\nThis script provides a structured way to test and evaluate the memory retrieval capabilities of a system centered around task-insight relationships. Through the defined precision and recall metrics, it offers a quantitative assessment of how well the system can leverage past insights to assist in task performance. The framework implemented allows for easy modifications and adaptations based on changing configurations, thus making it versatile for different scenarios and datasets.",
    "filename": "python/samples/task_centric_memory/eval_retrieval.py"
  },
  {
    "code": false,
    "content": "# Documentation for Self-Teaching Agent Evaluation Script\n\n## Overview\n\nThis script is designed to evaluate a self-teaching agent's capability to learn from its own trial and error based on a defined reasoning task. The agent iteratively improves its performance through a series of tasks, utilizes automatic feedback for learning, and stores insights gained during the process. It can then apply these insights both to the original task and to a similar one, measuring its ability to generalize knowledge.\n\nThe components of the script include classes and functions dealing with task management, learning insights, and overall performance measurement, encapsulated in an asynchronous execution model.\n\n## Prerequisites\n\nBefore executing the script, ensure you have the following:\n\n- Proper dependencies installed, including asyncio and relevant libraries for creating chat completions and managing logging.\n- A configuration file in YAML format that outlines the agent's settings and specific tasks. The example config file is located at `configs/self_teaching.yaml`.\n\n## Main Components and Workflow\n\n### 1. Configuration and Task Loading\n\nThe script begins with importing necessary modules and defining the `eval_self_teaching` asynchronous function. This function takes an instance of `Apprentice`, `ChatCompletionClient`, and `PageLogger`, along with some configuration parameters encapsulated in a dictionary.\n\nUpon execution, it loads tasks and expected answers from the provided YAML configuration file using the `load_yaml_file` function. It retrieves definitions for two tasks to assess:\n\n- **Task 1**: A learning task for which the agent will initially train and subsequently test.\n- **Task 2**: A similar but different task aimed to evaluate the agent's ability to generalize its learned knowledge.\n\n### 2. Agent Memory Management\n\nThe `Apprentice` class instance's memory is reset to ensure no prior insights influence the learning process. This enables a clean evaluation of the agent's learning capabilities from the ground up.\n\n### 3. Training and Testing Loop\n\nThe core of the evaluation is a loop that runs a specified number of iterations (`num_loops`). Within each iteration:\n\n- The agent trains on **Task 1** using the `train_on_task` method from the `Apprentice` class, attempting to derive the correct answers based on the provided descriptions.\n- After training, the agent undergoes a test phase for both tasks:\n  - For **Task 1**, its performance is assessed through the `test_apprentice` method, logging the success rate.\n  - Similarly, it is tested on **Task 2**, with results logged for further analysis.\n  \nThroughout the iterations, the script accumulates success counts and logs relevant information using the `PageLogger`.\n\n### 4. Performance Summary\n\nAfter completing all iterations, the script calculates the overall success rates for both tasks by comparing the number of successful trials to the total number of trials executed. This summary provides insights into how well the agent adapted to both tasks.\n\n### 5. Example Execution\n\nThe `run_example` asynchronous function coordinates the evaluation process by:\n\n- Loading the configuration settings from the specified YAML file.\n- Instantiating the necessary components: `PageLogger`, `ChatCompletionClient`, and `Apprentice`.\n- Calling the `eval_self_teaching` function to execute the learning and testing processes.\n- Finally, it prints out the results gathered during the evaluation.\n\n### 6. Command-Line Execution\n\nThe script is executable from the command line. If run directly, it expects a single argument specifying the path to the YAML configuration file. If the argument count is incorrect, it provides appropriate usage information.\n\n### Summary of Execution Flow\n\n1. The script loads configuration settings and tasks from a YAML file.\n2. It initializes logging, client, and apprentice components.\n3. It conducts training and testing for both tasks across several loops.\n4. It logs success rates and overall performance evaluations.\n5. Finally, it outputs the results after all test iterations are completed.\n\n## Conclusion\n\nThis script exemplifies an automated system to test an agent's self-teaching capabilities. By learning through iterative trial-and-error processes and evaluating its performance against tasks, it serves as a foundational tool for developing advanced AI agents capable of self-improvement without explicit user guidance.",
    "filename": "python/samples/task_centric_memory/eval_self_teaching.py"
  },
  {
    "code": false,
    "content": "# Overview\n\nThis Python script provides an asynchronous framework to evaluate the \"teachability\" of a selectable agent. It uses a task-centric memory model to enable an agent to learn quickly from user instructions, insights, and advice. This document explains the code structure and functionality step by step, detailing its methods and classes.\n\n## Components and Libraries\n\nThe script relies on several external modules, including:\n- **asyncio**: A standard Python library for writing concurrent code using asynchronous IO.\n- **sys**: Provides access to system-specific parameters and functions.\n- **typing**: Used to specify variable types for better clarity and static analysis.\n- **autogen_core**: Contains the `ChatCompletionClient` that likely facilitates interactions with a language model API.\n- **autogen_ext**: Provides utility classes such as `Apprentice`, `Grader`, and `PageLogger`, which support teaching, grading, and logging within the context of agent interactions.\n- **utils**: A module that provides functions to create an OpenAI API client and load configurations from YAML files.\n\nThe script can be executed from the command line, with the user specifying a configuration file path, such as `configs/eval_teachability.yaml`.\n\n## Main Functionality\n\nThe core functionality is encapsulated in two asynchronous functions: `eval_teachability` and `run_example`. \n\n### `eval_teachability`\n\nThe `eval_teachability` function evaluates the agent's performance in learning from user-provided instructions. Its parameters include an instance of `Apprentice`, a `ChatCompletionClient`, a `PageLogger`, and a configuration dictionary, `config`.\n\n1. **Initialization**: It logs entry into the function using the `PageLogger`.\n  \n2. **Data Loading**: Task-related data (task description and expected answer) and insights are loaded from specified files using the `load_yaml_file` function.\n\n3. **First Interaction**: The agent's memory is reset to simulate the lack of prior knowledge. It attempts to answer a question based on the task description. The response is evaluated against the expected answer using a `Grader` instance. The correctness of the response is logged.\n\n4. **Providing Insight**: An advice message, loaded earlier, is provided to the agent to inform it about the expected knowledge.\n\n5. **Second Interaction**: The agent again attempts to answer the same question after receiving advice. This response is also evaluated and logged.\n\n6. **Completion**: The function exits, returning a summary of the results.\n\n### `run_example`\n\nThe `run_example` function manages the setup and execution of the evaluation process.\n\n1. **Configuration Loading**: It reads the configuration from the provided YAML file.\n\n2. **Component Creation**: Instances of `PageLogger`, `ChatCompletionClient`, and `Apprentice` are created based on the loaded configurations.\n\n3. **Execution of Evaluation**: The `eval_teachability` function is invoked to carry out the test.\n\n4. **Output**: The results of the evaluation are printed to the console.\n\n## Main Script Execution\n\nThe script's entry point checks for command-line arguments to ensure a YAML file path is provided. If the user does not supply a valid argument, it displays usage instructions. If provided, it launches the asynchronous evaluation process by calling the `run_example` function, passing the YAML file path as an argument.\n\n## Class Descriptions\n\n### `Apprentice`\n\nThe `Apprentice` class represents the agent that learns from user teachings and is designed to process user messages. It offers functionality for resetting memory and handling conversations.\n\n### `Grader`\n\nThe `Grader` class is responsible for evaluating the correctness of the agent's responses. It compares the agent's outputs to expected answers and extracts useful information for logging.\n\n### `PageLogger`\n\nThe `PageLogger` class facilitates structured logging throughout the execution flow. It is used to track function entries, exits, and important information at various stages in the process.\n\n## Conclusion\n\nThis script is a demonstration of using a task-centric memory model to enhance the learning capabilities of an agent. By conducting evaluations over multiple interactions, it assesses improvements following user input. The modular design allows for adaptability by substituting different implementations for the `Apprentice` class or others as necessary. The structured logging aids in debugging and telemetry, providing insights into the agent's performance.",
    "filename": "python/samples/task_centric_memory/eval_teachability.py"
  },
  {
    "code": false,
    "content": "# Documentation\n\n## Overview\n\nThis code snippet defines functions for creating a chat completion client using OpenAI's API, as well as for loading configuration settings from a YAML file. The primary purpose of this code is to facilitate the integration of OpenAI's natural language processing capabilities into applications by managing the necessary configurations and client initialization.\n\n## Imports\n\n1. **Modules**:\n   - `Any, Dict` from `typing`: Used for type hinting to indicate that the function can accept any type of data and specifically a dictionary, respectively.\n   - `yaml`: A library for parsing YAML files, which are commonly used for configuration.\n   - `ChatCompletionClient` from `autogen_core.models`: This is a base model class likely used to define behaviors for chat completion clients.\n   - `OpenAIChatCompletionClient` from `autogen_ext.models.openai`: This is a specific implementation of `ChatCompletionClient` intended for use with OpenAI's chat completion API.\n\n## Function: `create_oai_client`\n\n### Purpose\n\nThe `create_oai_client` function is responsible for instantiating a new OpenAI chat completion client based on the provided configuration dictionary. This client allows the application to interface with OpenAI\u2019s chatbot API.\n\n### Parameters\n\n- `config` (Dict[str, Any]): A dictionary containing various parameters needed to configure the chat client. Expected keys within this dictionary include:\n  - `model`: Specifies the model to be used by the client.\n  - `max_completion_tokens`: Determines the maximum number of tokens for the response.\n  - `max_retries`: Number of times to retry the request in case of failures.\n  - `temperature`: Controls the randomness of the output.\n  - `presence_penalty`: A penalty for using new tokens in the response.\n  - `frequency_penalty`: A penalty for using common tokens in the response.\n  - `top_p`: A sampling strategy for narrowing down the token choices.\n\n### Return Value\n\n- Returns an instance of `OpenAIChatCompletionClient`, fully configured with the settings provided in the `config` dictionary.\n\n## Function: `load_yaml_file`\n\n### Purpose\n\nThe `load_yaml_file` function handles loading a YAML file and converting its content into a Python dictionary or other data structure. This is typically used to retrieve configuration settings required for setting up the client or other components in the application.\n\n### Parameters\n\n- `file_path` (str): The file path of the YAML file that the function attempts to open and read.\n\n### Return Value\n\n- Returns the contents of the YAML file, potentially as a dictionary, allowing for easy access to configuration parameters.\n\n## Usage Flow\n\n1. **Configuration Management**:\n   - The YAML file, which contains the necessary configuration parameters, is loaded into the application using `load_yaml_file`.\n   - The parameters extracted from the YAML file are then structured as a dictionary.\n\n2. **Client Creation**:\n   - The dictionary containing the configuration parameters is passed to `create_oai_client`. This results in an instance of `OpenAIChatCompletionClient`, which is ready to be used to send requests to OpenAI\u2019s API.\n   \n3. **Integration with Application**:\n   - The created client can now be utilized within the application to generate chat completions, benefiting from OpenAI's machine learning models according to the specified configurations.\n\n### Example Flow\n\n1. Load the configuration settings from a YAML file:\n   ```python\n   config = load_yaml_file(\"config.yaml\")\n   ```\n\n2. Create an OpenAI client using the loaded configuration:\n   ```python\n   openai_client = create_oai_client(config)\n   ```\n\n3. Use the client in the application for chat completions.",
    "filename": "python/samples/task_centric_memory/utils.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis Python script automates the process of moving a source directory to a specified target directory and modifying a Python project configuration file (`pyproject.toml`) to include a new package source definition. It utilizes standard libraries like `os`, `shutil`, and `pathlib`, along with third-party libraries `tomllib` and `tomli_w` for handling TOML configuration files.\n\n## Importing Required Libraries\n\n```python\nimport os\nimport shutil\nfrom pathlib import Path\nimport tomli_w\nimport tomllib\n```\n\nThe script begins by importing several libraries:\n\n- **os**: Provides a way to interact with the operating system, mainly for file and directory manipulation.\n- **shutil**: Offers higher-level file operations, including copying and moving files and directories.\n- **pathlib**: A modern way to handle filesystem paths.\n- **tomli_w** and **tomllib**: Allow reading and writing TOML files, a configuration file format.\n\n## Setting Source and Target Directories\n\n```python\nsource_dir = os.getcwd()\ntarget_dir = \"{{ cookiecutter.__final_destination }}\"\n```\n\n- `source_dir` is set to the current working directory using `os.getcwd()`.\n- `target_dir` is defined using a placeholder (likely intended for a project's template system, such as Cookiecutter) that specifies where the source directory should be moved.\n\n## Moving the Source Directory\n\n```python\nshutil.move(source_dir, target_dir)\n```\n\nThis line of code moves the `source_dir` (current working directory) to the `target_dir`. If the target directory already exists, this operation will effectively rename or move the files there, handling any existing contents according to the filesystem's behavior.\n\n## Defining the File Directory of the Script\n\n```python\nTHIS_FILE_DIR = Path(__file__).parent\n```\n\nThis line determines the directory where the current script file resides using `Path(__file__).parent`. This is important for referencing other files relative to the script's location.\n\n## Accessing the Workspace Definition\n\n```python\nworkspace_def_path = THIS_FILE_DIR / \"..\" / \"..\" / \"..\" / \"pyproject.toml\"\n```\n\nHere, the script constructs the path to the `pyproject.toml` file, which exists three directory levels up from the script's directory. This file is typically used in Python projects managed by tools like Poetry or Flit for configuration metadata.\n\n## Modifying the Project Configuration\n\n```python\nwith workspace_def_path.open(\"rb\") as f:\n    config = tomllib.load(f)\n```\n\nThe script opens `pyproject.toml` in binary read mode and loads its existing configuration into the `config` variable using `tomllib.load`. This step reads the current settings defined in the TOML configuration file.\n\n## Updating the Configuration\n\n```python\nconfig[\"tool\"][\"uv\"][\"sources\"][\"{{ cookiecutter.package_name }}\"] = {\"workspace\": True}\n```\n\nNext, the script modifies the loaded configuration dictionary. It adds a new entry under `tool.uv.sources` with the key set to a package name specified by another placeholder (`{{ cookiecutter.package_name }}`). This entry informs the tool that this package should be treated as a workspace during development.\n\n## Writing Back the Updated Configuration\n\n```python\nwith workspace_def_path.open(\"wb\") as f:\n    tomli_w.dump(config, f)\n```\n\nFinally, the updated configuration is written back to `pyproject.toml` in binary write mode using `tomli_w.dump`. This action saves all the modifications made to the configuration, effectively updating it with the newly added package source.\n\n## Conclusion\n\nThis script serves a dual purpose: it moves the current working directory to a specified destination and updates the `pyproject.toml` file to include a new package configuration. It is particularly useful in project initialization scenarios where directory structure and metadata need to be set up automatically. By leveraging placeholders, it integrates seamlessly into template-based workflows, making it adaptable for various project configurations.",
    "filename": "python/templates/new-package/hooks/post_gen_project.py"
  },
  {
    "code": false,
    "content": "# Code Documentation\n\n## Overview\n\nThis script is designed to validate a package name and packaging version that are typically provided by a template engine like Cookiecutter during project initialization. It ensures that the package name adheres to a specific naming convention (\"kebab case\") and verifies that the version string follows the proper semantic versioning format.\n\n## Imports\n\n```python\nimport re\nimport sys\nfrom packaging import version\n```\n\n1. **re**: This module provides regular expression matching operations, which are essential for validating the package name and version string.\n2. **sys**: This module provides access to some variables used or maintained by the interpreter; it is used here primarily to exit the program if validation fails.\n3. **version** from **packaging**: This module is utilized to access predefined patterns that validate version strings according to semantic versioning rules.\n\n## Constants\n\n### MODULE_REGEX\n\n```python\nMODULE_REGEX = r\"^[a-zA-Z][\\-a-zA-Z0-9]+$\"\n```\n\n- This regular expression defines the allowable format for the package name.\n- The name must start with an alphabet character, followed by any combination of hyphens, lowercase letters, and numbers.\n- The naming structure ensures that the package name complies with conventions that are often required for Python packages.\n\n## Input Validation\n\n### Package Name Validation\n\n```python\npackage_name = \"{{ cookiecutter.package_name }}\"\n\nat_least_one_error = False\nif not re.match(MODULE_REGEX, package_name):\n    print(f'ERROR: \"{package_name}\" must use kebab case')\n    at_least_one_error = True\n```\n\n- The script fetches the package name from a placeholder intended to be filled by Cookiecutter.\n- It initializes a boolean flag `at_least_one_error` to track validation errors.\n- It checks whether the package name matches the `MODULE_REGEX`. If it does not, an error message is printed to the console, and the error flag is set to `True`.\n\n### Version String Validation\n\n```python\npackaging_version = \"{{ cookiecutter.version }}\"\n\n# Check version format using version.VERSION_PATTERN\nif not re.match(version.VERSION_PATTERN, packaging_version, re.VERBOSE | re.IGNORECASE):\n    print(f'ERROR: \"{packaging_version}\" is not a valid version string')\n    at_least_one_error = True\n```\n\n- Similarly, the script retrieves the packaging version from another placeholder.\n- The version string is then validated against the pattern defined in the `packaging.version` module, which supports various semantic versioning formats.\n- An error is printed, and the error flag is updated if validation fails.\n\n## Exit Procedure\n\n```python\nif at_least_one_error:\n    sys.exit(1)\n```\n\n- At the end of the script, if any validation error was encountered (as indicated by the `at_least_one_error` flag), the script exits with a status code of `1`. This typically indicates to any calling process (like a CI/CD pipeline) that an error occurred during initialization.\n\n## Summary\n\nThis script serves a crucial role in initializing Python packages by ensuring that foundational elements, like the package name and version, follow established conventions. By validating these inputs, the script prevents potential issues during the package's later usage and distribution, promoting good practices in Python package management.",
    "filename": "python/templates/new-package/hooks/pre_gen_project.py"
  },
  {
    "content": "# {{cookiecutter.package_name}}",
    "filename": "python/templates/new-package/{{cookiecutter.package_name}}/README.md"
  },
  {
    "code": false,
    "content": "# Code Analysis Documentation\n\n## Overview\n\nThis document provides a high-level overview of the given source code. It describes the key functionalities and structures of the code, helping users to understand its purpose and logic. The code may include functions, methods, or classes that encapsulate various behaviors or processes.\n\n## Main Functionality\n\nThe code serves to perform specific tasks based on defined logic. Depending on its nature, it may handle data processing, manipulation, input/output operations, or other computations. The primary functionality would be focused on a single task or a series of related tasks.\n\n## Code Structure\n\n### Functions and Methods\n\nIf the code includes defined functions, each function will generally encapsulate a distinct operation. \n\n- **Function 1**: Typically, the first function would initialize critical variables and set the stage for subsequent computations. It might also contain logic to validate input data or perform initial calculations.\n\n- **Function 2**: The second function might handle a primary computation or processing step. This function could take inputs, manipulate them, and return outputs based on defined parameters.\n\n- **Additional Functions**: Other functions present in the code would usually support the main operations by providing utility methods, error handling, or data transformation. They could be responsible for tasks like formatting data, logging information, or managing resources.\n\n### Classes\n\nIf the code defines classes, each class would group related functions and data attributes that represent an entity or concept within the application.\n\n- **Class 1**: This class might encapsulate the core data structure relevant to the task. It would likely include attributes to store important variables and methods to operate on these attributes.\n\n- **Class Methods**: Methods within the classes would provide functionalities such as initializing objects, updating attributes, or performing computed properties based on the object's state.\n\n### Control Flow\n\nThe code likely employs standard control flow mechanisms such as loops, conditionals, and exceptions to manage the flow of execution. \n\n- **Control Structures**: The presence of conditional statements (like `if` or `switch`) would determine the execution path based on defined rules, while loops (like `for` or `while`) would repeat operations until a specified condition is met.\n\n- **Error Handling**: If there are catch blocks or try-except constructs, these would manage errors gracefully and provide informative feedback, ensuring robustness in various scenarios.\n\n## Data Handling\n\nThe script may involve data inputs, processing, and outputs.\n\n- **Input**: Data could be sourced from user input, external files, or APIs. The initial part of the script might handle parsing this input to an appropriate format for processing.\n\n- **Processing Logic**: Following input handling, the code would move on to processing the data based on the functions or methods defined. This processing would include calculations, filtering, or aggregating data as necessary.\n\n- **Output**: Finally, the script would produce output, which could be displayed to the user, written to files, or sent to other services. This output section would encapsulate the final result of the operations performed.\n\n## Conclusion\n\nThe provided code consists of critical functionality structured into functions and possibly classes. This document outlines the potential roles these components play in achieving the code's objectives, covering aspects like initialization, data processing, control flow, and output management. Understanding these elements aids in maintaining and enhancing the code in future endeavors.",
    "filename": "python/templates/new-package/{{cookiecutter.package_name}}/src/{{cookiecutter.__project_slug}}/__init__.py"
  },
  {
    "code": false,
    "content": "# Overview of the Code\n\nThis code snippet defines an asynchronous function in Python. The function is specifically designed for testing purposes, indicated by its naming convention and structure. The use of `async` suggests that it deals with asynchronous operations, which is common in modern Python programming, especially when handling I/O-bound tasks.\n\n## Function Definition\n\n### `test_example`\n\n- **Type**: Asynchronous Function\n- **Return Type**: `None`\n- **Purpose**: The function is intended to verify a condition or functionality within a test scenario.\n\n### Function Details\n\n- **Assertion**: The function contains a single assertion statement: `assert True`. \n  - This statement checks that the expression evaluates to `True`. If it does, the test passes successfully.\n  - If the expression evaluates to `False`, an `AssertionError` will be raised, indicating that the test has failed.\n\n## Asynchronous Context\n\n### Asynchronous Programming\n\n- Asynchronous functions are generally used to perform tasks that may take time to complete, such as network requests or file operations, without blocking the execution of other code.\n- In this case, the function does not utilize any asynchronous operations or await expressions. Thus, its asynchronous declaration may not be essential unless expanded in the future to include time-consuming operations.\n\n## Testing Framework Compatibility\n\n### Testing Conventions\n\n- The function\u2019s name follows a common naming pattern for test functions (typically starting with `test_`), indicating that it can be integrated into a testing framework. \n- It may be used with frameworks like `pytest` or `unittest`, which automatically identify and run functions that are designed to test code.\n\n## Practical Usage\n\n### Intended Use Case\n\n- This function serves as a placeholder or a very basic test case within a larger testing suite.\n- It can be expanded to include more complex assertions or checks as needed.\n\n## Summary\n\nIn summary, `test_example` is a minimalistic asynchronous test function that asserts a condition (which always passes in this case). While it does not perform any operations that leverage its `async` nature, it sets a foundation for future tests. This function demonstrates the structure and style employed in writing test functions, accommodating growth and complexity as the codebase evolves.",
    "filename": "python/templates/new-package/{{cookiecutter.package_name}}/tests/test_example.py"
  }
]