{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown and code files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\" \n",
    "    url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/main.zip'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc2c3b1-5bb7-4dc1-8524-8474d67432f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 749 files from microsoft/autogen.\n"
     ]
    }
   ],
   "source": [
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "# This will print the number of files you successfully processed\n",
    "print(f'Processed {len(autogen_data)} files from microsoft/autogen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for record in autogen_data:\n",
    "    index[record['filename']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {
    "_sphinx_cell_id": "d4ce051b-aaae-4725-9eef-869cf50e26fb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "exporter = MarkdownExporter()\n",
    "exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "def format_notebook_as_md(raw_notebook: str) -> str:\n",
    "    nb_parsed = nbformat.reads(\n",
    "        raw_notebook,\n",
    "        as_version=nbformat.NO_CONVERT,\n",
    "    )\n",
    "    md_body, _ = exporter.from_notebook_node(nb_parsed)\n",
    "    return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8d3523-688c-4724-8b6c-47247388b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    if not text.startswith(\"```\"):\n",
    "        return text\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This line loads the variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now the OpenAI client will be able to find the key in your environment\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# You can also explicitly pass the key if you prefer\n",
    "# import os\n",
    "# openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_editing_instructions = \"\"\"\n",
    "You're a professional coding editor.\n",
    "\n",
    "You are given a Markdown file that was converted from a Jupyter notebook.  \n",
    "The file already contains code blocks and inline comments.  \n",
    "\n",
    "Your task:\n",
    "\n",
    "- Turn it into clear, well-structured documentation.  \n",
    "- Add section headers (##) where appropriate. Keep sections relatively large (8-10 paragraphs and code blocks)\n",
    "- Add concise, high-level explanations for each code block.  \n",
    "- Summarize what the code is doing without being overly verbose.  \n",
    "- Keep the formatting in Markdown.\n",
    "- Aim for a balance: clear enough to guide someone new, but not overloaded with detail. \n",
    "\n",
    "Output the improved Markdown file with the new documentation.\n",
    "\"\"\".strip()\n",
    "\n",
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, open the file and read its content\n",
    "# with open('data-processing-code.ipynb', 'r', encoding='utf-8') as f:\n",
    "#     raw_notebook_content = f.read()\n",
    "\n",
    "# result = llm(notebook_editing_instructions, md_body)\n",
    "# print(result)\n",
    "# #result = llm(system_prompt, md_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a456b4aa-40f2-428c-973c-4d40f0005213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0f975c-f603-4a3b-9a32-c261a44d1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 49 jupyter notebooks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38326f6876044754b80642b625da26a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipynb_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') == True and record['filename'].endswith('.ipynb'):\n",
    "        ipynb_data.append(record)\n",
    "\n",
    "\n",
    "print(f'processing {len(ipynb_data)} jupyter notebooks...')\n",
    "\n",
    "for record in tqdm(ipynb_data):\n",
    "    md_body = format_notebook_as_md(record['content'])\n",
    "    new_content = llm(notebook_editing_instructions, md_body)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d791ab0-a49b-4439-a143-bd3e8c38067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941e67d0-9260-48ca-bcdf-8abb6401c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 539 code files...\n"
     ]
    }
   ],
   "source": [
    "code_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') != True:\n",
    "        continue\n",
    "\n",
    "    path = record['filename']\n",
    "    ext = path.split('.')[-1]\n",
    "\n",
    "    if ext not in code_extensions:\n",
    "        continue\n",
    "\n",
    "    if ext == 'ipynb':\n",
    "        continue\n",
    "\n",
    "    # print(path)\n",
    "    code_data.append(record)\n",
    "\n",
    "print(f'processing {len(code_data)} code files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5755e9-1f70-4510-b75c-6b952ad303fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb84bc7c21b4d61b42c54699dd38536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(code_data):\n",
    "    code = record['content']\n",
    "\n",
    "    new_content = llm(code_doc_instructions, code)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d50b9af-207b-421c-a567-3aacc0bfacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5aeaab-dc9c-4b1d-a00b-ce29bbf9b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da555e70-19ba-4996-9f69-9cfd18a477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/autogen_data_processed.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(autogen_data, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8984a6-78ac-4c05-b065-76f7dc910382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->\\n\\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\\n\\n## Why are these changes needed?\\n\\n<!-- Please give a short summary of the change and the problem this solves. -->\\n\\n## Related issue number\\n\\n<!-- For example: \\\"Closes #1234\\\" -->\\n\\n## Checks\\n\\n- [ ] I've included any doc changes needed for <https://microsoft.github.io/autogen/>. See <https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md> to build and test documentation locally.\\n- [ ] I've added tests (if relevant) corresponding to the changes introduced in this PR.\\n- [ ] I've made sure all auto checks have passed.\",\n",
      "    \"filename\": \".github/PULL_REQUEST_TEMPLATE.md\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"# AutoGen Multi-Agent AI Framework\\n\\nAutoGen is a multi-language framework for creating AI agents that can act autonomously or work alongside humans. The project has separate Python and .NET implementations with their own development workflows.\\n\\nAlways reference these instructions first and fallback to search or bash commands only when you encounter unexpected information that does not match the info here.\\n\\n## Working Effectively\\n\\n### Prerequisites and Environment Setup\\n\\n**CRITICAL**: Install both .NET 8.0 and 9.0 for full compatibility:\\n- Install uv package manager: `python3 -m pip install uv` \\n- Install .NET 9.0 SDK: `wget https://dot.net/v1/dotnet-install.sh && chmod +x dotnet-install.sh && ./dotnet-install.sh --channel 9.0`\\n- Install .NET 8.0 runtime: `./dotnet-install.sh --channel 8.0 --runtime dotnet && ./dotnet-install.sh --channel 8.0 --runtime aspnetcore`\\n- Update PATH: `export PATH=\\\"$HOME/.dotnet:$PATH\\\"`\\n\\n### Python Development Workflow\\n\\n**Bootstrap and build Python environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nuv sync --all-extras  # NEVER CANCEL: Takes 2 minutes. Set timeout to 300+ seconds.\\nsource .venv/bin/activate\\n```\\n\\n**Validate Python development:**\\n```bash\\n# Quick validation (under 1 second each)\\npoe format  # Code formatting\\npoe lint    # Linting with ruff\\n\\n# Type checking - NEVER CANCEL these commands\\npoe mypy     # Takes 6 minutes. Set timeout to 600+ seconds.\\npoe pyright  # Takes 41 seconds. Set timeout to 120+ seconds.\\n\\n# Individual package testing (core package example)\\npoe --directory ./packages/autogen-core test  # Takes 10 seconds. Set timeout to 60+ seconds.\\n\\n# Documentation - NEVER CANCEL\\npoe docs-build  # Takes 1 minute 16 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: Python environment setup takes 2 minutes minimum\\n- **NEVER CANCEL**: mypy type checking takes 6 minutes \\n- **NEVER CANCEL**: Documentation build takes 1+ minutes\\n- Format/lint tasks complete in under 1 second\\n- Individual package tests typically complete in 10-60 seconds\\n\\n### .NET Development Workflow\\n\\n**Bootstrap and build .NET environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/dotnet\\nexport PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet restore  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\ndotnet build --configuration Release  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**Validate .NET development:**\\n```bash\\n# Unit tests - NEVER CANCEL\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Takes 25 seconds. Set timeout to 120+ seconds.\\n\\n# Format check (if build fails) \\ndotnet format --verify-no-changes\\n\\n# Run samples\\ncd samples/Hello\\ndotnet run\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: .NET restore takes 53 seconds minimum\\n- **NEVER CANCEL**: .NET build takes 53 seconds minimum  \\n- **NEVER CANCEL**: .NET unit tests take 25 seconds minimum\\n- All build and test commands require appropriate timeouts\\n\\n### Complete Validation Workflow\\n\\n**Run full check suite (Python):**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nsource .venv/bin/activate\\npoe check  # NEVER CANCEL: Runs all checks. Takes 7+ minutes total. Set timeout to 900+ seconds.\\n```\\n\\n## Validation Scenarios\\n\\n### Manual Validation Requirements\\nAlways manually validate changes by running complete user scenarios after making modifications:\\n\\n**Python validation scenarios:**\\n1. **Import test**: Verify core imports work:\\n   ```python\\n   from autogen_agentchat.agents import AssistantAgent\\n   from autogen_core import AgentRuntime\\n   from autogen_ext.models.openai import OpenAIChatCompletionClient\\n   ```\\n\\n2. **AutoGen Studio test**: Verify web interface can start:\\n   ```bash\\n   autogenstudio ui --help  # Should show help without errors\\n   ```\\n\\n3. **Documentation test**: Build and verify docs generate without errors:\\n   ```bash\\n   poe docs-build && ls docs/build/index.html\\n   ```\\n\\n**.NET validation scenarios:**\\n1. **Sample execution**: Run Hello sample to verify runtime works:\\n   ```bash\\n   cd dotnet/samples/Hello && dotnet run --help\\n   ```\\n\\n2. **Build validation**: Ensure all projects compile:\\n   ```bash\\n   dotnet build --configuration Release --no-restore\\n   ```\\n\\n3. **Test execution**: Run unit tests to verify functionality:\\n   ```bash\\n   dotnet test --filter \\\"Category=UnitV2\\\" --configuration Release --no-build\\n   ```\\n\\n## Common Issues and Workarounds\\n\\n### Network-Related Issues\\n- **Python tests may fail** with network errors (tiktoken downloads, Playwright browser downloads) in sandboxed environments - this is expected\\n- **Documentation intersphinx warnings** due to inability to reach external documentation sites - this is expected\\n- **Individual package tests work better** than full test suite in network-restricted environments\\n\\n### .NET Runtime Issues  \\n- **Requires both .NET 8.0 and 9.0**: Build uses 9.0 SDK but tests need 8.0 runtime\\n- **Global.json specifies 9.0.100**: Must install exact .NET 9.0 version or later\\n- **Path configuration critical**: Ensure `$HOME/.dotnet` is in PATH before system .NET\\n\\n### Python Package Issues\\n- **Use uv exclusively**: Do not use pip/conda for dependency management\\n- **Virtual environment required**: Always activate `.venv` before running commands\\n- **Package workspace structure**: Project uses uv workspace with multiple packages\\n\\n## Timing Reference\\n\\n### Python Commands\\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `uv sync --all-extras` | 2 minutes | 300+ seconds | NEVER CANCEL |\\n| `poe mypy` | 6 minutes | 600+ seconds | NEVER CANCEL |\\n| `poe pyright` | 41 seconds | 120+ seconds | NEVER CANCEL |\\n| `poe docs-build` | 1 min 16 sec | 300+ seconds | NEVER CANCEL |\\n| `poe format` | <1 second | 30 seconds | Quick |\\n| `poe lint` | <1 second | 30 seconds | Quick |\\n| Individual package test | 10 seconds | 60+ seconds | May have network failures |\\n\\n### .NET Commands  \\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `dotnet restore` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet build --configuration Release` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet test --filter \\\"Category=UnitV2\\\"` | 25 seconds | 120+ seconds | NEVER CANCEL |\\n| `dotnet format --verify-no-changes` | 5-10 seconds | 60 seconds | Quick validation |\\n\\n## Repository Structure\\n\\n### Python Packages (`python/packages/`)\\n- `autogen-core`: Core agent runtime, model interfaces, and base components\\n- `autogen-agentchat`: High-level multi-agent conversation APIs  \\n- `autogen-ext`: Extensions for specific model providers and tools\\n- `autogen-studio`: Web-based IDE for agent workflows\\n- `agbench`: Benchmarking suite for agent performance\\n- `magentic-one-cli`: Multi-agent team CLI application\\n\\n### .NET Projects (`dotnet/src/`)\\n- `AutoGen`: Legacy 0.2-style .NET packages (being deprecated)\\n- `Microsoft.AutoGen.*`: New event-driven .NET packages\\n- `AutoGen.Core`: Core .NET agent functionality\\n- Multiple provider packages: OpenAI, Anthropic, Ollama, etc.\\n\\n### Key Configuration Files\\n- `python/pyproject.toml`: Python workspace and tool configuration\\n- `dotnet/global.json`: .NET SDK version requirements  \\n- `dotnet/AutoGen.sln`: .NET solution file\\n- `python/uv.lock`: Locked Python dependencies\\n\\n## Development Best Practices\\n\\n### Before Committing Changes\\n**ALWAYS run these validation steps:**\\n\\n**Python:**\\n```bash\\ncd python && source .venv/bin/activate\\npoe format    # Fix formatting\\npoe lint      # Check code quality  \\npoe mypy      # Type checking (6 minutes)\\npoe docs-build # Verify docs build (1+ minutes)\\n```\\n\\n**.NET:**\\n```bash  \\ncd dotnet && export PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet format --verify-no-changes  # Check formatting\\ndotnet build --configuration Release --no-restore  # Build (53 seconds)\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Test (25 seconds)\\n```\\n\\n### Key Directories Reference\\n```\\nautogen/\\n\\u251c\\u2500\\u2500 python/                    # Python implementation\\n\\u2502   \\u251c\\u2500\\u2500 packages/             # Individual Python packages\\n\\u2502   \\u251c\\u2500\\u2500 docs/                 # Sphinx documentation\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Example code\\n\\u2502   \\u2514\\u2500\\u2500 pyproject.toml        # Workspace configuration\\n\\u251c\\u2500\\u2500 dotnet/                   # .NET implementation  \\n\\u2502   \\u251c\\u2500\\u2500 src/                  # Source projects\\n\\u2502   \\u251c\\u2500\\u2500 test/                 # Test projects\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Sample applications\\n\\u2502   \\u2514\\u2500\\u2500 AutoGen.sln          # Solution file\\n\\u251c\\u2500\\u2500 .github/workflows/        # CI/CD pipelines\\n\\u2514\\u2500\\u2500 docs/                     # Additional documentation\\n```\\n\\nThis framework supports creating both simple single-agent applications and complex multi-agent workflows with support for various LLM providers, tools, and deployment patterns.\",\n",
      "    \"filename\": \".github/copilot-instructions.md\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head data/autogen_data_processed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "966c577e-28da-401a-84ff-9c69ddabb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d46bee-ed92-4f7c-bac3-3168560d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen_data_chunks = []\n",
    "\n",
    "for doc in autogen_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    autogen_data_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655c97c2-d5cc-4b53-a31a-fccffb8424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autogen_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb350c9-ffb1-4d2a-be36-d961e5bd450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'chunk': \"## Prerequisites\\n\\n- Access to gpt3.5-turbo or preferably gpt4 - [Get access here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai)\\n- [Setup a Github app](#how-do-i-setup-the-github-app)\\n- [Install the Github app](https://docs.github.com/en/apps/using-github-apps/installing-your-own-github-app)\\n- [Provision the azure resources](#how-do-I-deploy-the-azure-bits)\\n- [Create labels for the dev team skills](#which-labels-should-i-create)\\n\\n### How do I setup the Github app?\\n\\n- [Register a Github app](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app), with the options listed below:\\n    - Give your App a name and add a description\\n    - Homepage URL: Can be anything (Example: repository URL)\\n    - Add a dummy value for the webhook url, we'll come back to this setting\\n    - Enter a webhook secret, which you'll need later on when filling in the `WebhookSecret` property in the `appsettings.json` file\\n    - Setup the following permissions\\n        - Repository \\n            - Contents - read and write\\n            - Issues - read and write\\n            - Metadata - read only\\n            - Pull requests - read and write\\n    - Subscribe to the following events:\\n        - Issues\\n        - Issue comment\\n    - Allow this app to be installed by any user or organization\\n    \\n- After the app is created, generate a private key, we'll use it later for authentication to Github from the app\\n\\n### Which labels should I create?\\n\\nIn order for us to know which skill and persona we need to talk with, we are using Labels in Github Issues.\\n\\nThe default bunch of skills and personnas are as follow:\\n- PM.Readme\\n- Do.It\\n- DevLead.Plan\\n- Developer.Implement\\n\\nAdd them to your repository (They are not there by default).\\n\\nOnce you start adding your own skills, just remember to add the corresponding label to your repository.\\n\\n## How do I run this locally?\\n\\nCodespaces are preset for this repo. For codespa\",\n",
       " 'filename': 'dotnet/samples/dev-team/docs/github-flow-getting-started.md'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autogen_data_chunks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b39ee7-b0c1-41cf-8954-74fcfd61d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
