{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29dbd548-c0e0-48e5-812c-bf01177b732e",
   "metadata": {},
   "source": [
    "## Day 5 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e90db5-0201-46e1-b247-b197d3d98649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown and code files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\" \n",
    "    url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/main.zip'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfd7afb-7d14-4088-af2f-0d3feee8cf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 54199528-99cf-43df-8073-4f983942b952)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1/resolve/main/1_Pooling/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 099f5682-af53-45e1-bb50-f99309b0efd7)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1/resolve/main/1_Pooling/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /sentence-transformers/multi-qa-distilbert-cos-v1/resolve/main/1_Pooling/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78778819b8b0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f8160cc0-c893-41ab-aa27-d2dbd99744d3)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1/resolve/main/1_Pooling/config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca4876d220a42b7a9e071a256087ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output='To run a pull request (PR) in the AutoGen repository, the following checks must be met:\\n\\n1. **Documentation Changes**: Ensure that any necessary documentation changes have been included, as outlined in the documentation link [here](https://microsoft.github.io/autogen/). You should check the contributing guidelines to build and test documentation locally as specified.\\n\\n2. **Test Coverage**: If relevant, tests should be added that correspond to the changes introduced in the PR.\\n\\n3. **Auto Checks**: Make sure that all the automatic checks have passed. This typically includes validations for code style, tests, and other quality checks established by the repository.\\n\\n4. **Add a Reviewer**: When you create a PR, add a reviewer to the assignee section; if you do not have the access to do so, a reviewer will be assigned shortly.\\n\\nIt is valuable to note that you should ensure that all projects compile and run tests successfully as part of the validations before submission.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from minsearch import Index, VectorSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # Import tqdm for progress bar\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This line loads the variables from your .env file\n",
    "load_dotenv()\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Assuming read_repo_data is defined elsewhere\n",
    "# (Note: This is the data loading step)\n",
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"Chunks text into overlapping sections.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk_content = seq[i:i+size]\n",
    "        # Stores the chunked text under the 'content' key\n",
    "        result.append({'start': i, 'content': chunk_content}) \n",
    "        if i + size >= n:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# --- Data Preparation and Chunking ---\n",
    "autogen_data_chunks = []\n",
    "\n",
    "for doc in autogen_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        # Merge chunk content with document metadata (filename, etc.)\n",
    "        chunk.update(doc_copy) \n",
    "    autogen_data_chunks.extend(chunks)\n",
    "\n",
    "# --- 1. Text Search Index (minsearch Index) ---\n",
    "# FIX: Define and fit the text index\n",
    "aut_index = Index(\n",
    "    text_fields=[\"filename\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "aut_index.fit(autogen_data_chunks)\n",
    "\n",
    "# --- 2. Vector Search Index (minsearch VectorSearch) ---\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "autogen_embeddings = []\n",
    "\n",
    "# FIX: Use the correct list name: autogen_data_chunks\n",
    "for d in tqdm(autogen_data_chunks):\n",
    "    # d['content'] is the correct key holding the chunked text\n",
    "    v = embedding_model.encode(d['content'])\n",
    "    autogen_embeddings.append(v)\n",
    "\n",
    "autogen_embeddings = np.array(autogen_embeddings)\n",
    "\n",
    "autogen_vindex = VectorSearch()\n",
    "# FIX: Use the correct list name: autogen_data_chunks\n",
    "autogen_vindex.fit(autogen_embeddings, autogen_data_chunks)\n",
    "\n",
    "\n",
    "# --- HYBRID SEARCH FUNCTION (The Agent's Tool) ---\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Performs a Hybrid Search combining Text (Keyword) and Vector (Semantic) search.\n",
    "    This is the function the agent will call.\n",
    "    \"\"\"\n",
    "    # 1. Text Search (Keyword matching)\n",
    "    text_results = aut_index.search(query, num_results=5)\n",
    "    \n",
    "    # 2. Vector Search (Semantic matching)\n",
    "    q_vector = embedding_model.encode(query)\n",
    "    vector_results = autogen_vindex.search(q_vector, num_results=5)\n",
    "    \n",
    "    # 3. Combine and Deduplicate Results\n",
    "    seen_filenames = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        # Deduplicate based on 'filename'\n",
    "        if result['filename'] not in seen_filenames:\n",
    "            seen_filenames.add(result['filename'])\n",
    "            # Return only the essential information for the LLM\n",
    "            combined_results.append({\n",
    "                \"filename\": result['filename'],\n",
    "                \"content\": result['content']\n",
    "            })\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "\n",
    "# --- AGENT SETUP AND EXECUTION ---\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for the AutoGen course.\n",
    "Your sole purpose is to answer questions using the 'hybrid_search' tool, which queries the official AutoGen documentation.\n",
    "Always use the tool before answering.\n",
    "\"\"\"\n",
    "\n",
    "# Assuming pydantic_ai and Agent are defined/imported correctly\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"autogen_agent\",\n",
    "    instructions=system_prompt,\n",
    "    # FIX: Pass the HYBRID search function as the tool\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# Use the specific, enforced question to trigger the tool call\n",
    "question = 'what checks must be met to run a PR in the **AutoGen** repository?'\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782814d3-cb91-44b8-8e73-352c0bd8494c",
   "metadata": {},
   "source": [
    "#### LOGGING\n",
    "\n",
    "what we want to record:\n",
    "* The system prompt that we used\n",
    "* The model\n",
    "* The user query\n",
    "* The tools we use\n",
    "* The responses and the back-and-forth interactions between the LLM and our tools\n",
    "* The final response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eb5564-dc77-4df2-9d39-7a73ee3bba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create log\n",
    "\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1769150a-44ed-45b5-9e7a-672cb04470a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write log to a folder\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd730db-1364-46c1-bd26-831921148357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " must i update the dependencies whenever i pull new changes?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it is generally recommended to update your dependencies whenever you pull new changes, especially if those changes involve updates to the packages or libraries that your project depends on. This ensures that you are working with the latest features, improvements, and security patches.\n",
      "\n",
      "In the context of AutoGen, you might want to specify the version of AutoGen in your dependency management files (like `pyproject.toml` for Python) to ensure compatibility. For example:\n",
      "\n",
      "```toml\n",
      "[project]\n",
      "# ...\n",
      "dependencies = [\n",
      "    \"autogen-core>=0.4,<0.5\"\n",
      "]\n",
      "```\n",
      "\n",
      "This kind of version specification helps ensure that your extensions and applications integrate properly with the version of AutoGen you are using. Regularly updating your dependencies alongside pulling the latest changes can prevent compatibility issues and allow you to leverage new features and fixes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/autogen_agent_20250928_171138_46accd.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790d34fb-9791-4cb2-ad83-a89c427f4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " okay, how can i update the dependencies using uv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To update the dependencies using the `uv` tool, you can follow these steps:\n",
      "\n",
      "1. **Activate your Virtual Environment**: Make sure you are in the virtual environment where your project dependencies are managed.\n",
      "2. **Run the Update Command**: In the directory of your Python project, execute the following command:\n",
      "\n",
      "   ```sh\n",
      "   uv sync --all-extras\n",
      "   ```\n",
      "\n",
      "This command will synchronize and update all the dependencies defined in your project.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/autogen_agent_20250928_171321_e3a625.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d330d4-8598-4c79-9507-3b96a1fd3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " okay. where is the documentation source directory?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documentation source directory for the AutoGen project is located at `docs/src/`. You can build the documentation by running the command `poe docs-build` from the root of the Python directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/autogen_agent_20250928_171450_0ddd80.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe3b30-8eb6-4b04-ad49-521f532e2689",
   "metadata": {},
   "source": [
    "write these logs to a folder:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee948d4-0cfc-401a-a4b9-0506cceae25e",
   "metadata": {},
   "source": [
    "### Adding References\n",
    "When interacting with the agent, I noticed one thing: it doesn't include the reference to the original documents.\n",
    "Let's fix it by adjusting the prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff623878-da5e-498a-9d00-5d0a6cd10113",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"aut_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c064dc61-63df-4d3e-9dc1-ab446c51591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " where is the documentation source directory?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documentation source directory is located at `docs/src/`. This directory contains the source files for the documentation which is built using the Sphinx documentation system.\n",
      "\n",
      "To build the documentation, you can run `poe docs-build` from the root of the Python directory. If you need to serve the documentation locally, use the command `poe docs-serve`.\n",
      "\n",
      "For further details on building and maintaining the documentation, refer to [this README](https://github.com/DataTalksClub/faq/blob/main/python/README.md).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/aut_agent_v2_20250928_172003_561fe7.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a5d9c-7608-460e-9eb3-4e71690a9a31",
   "metadata": {},
   "source": [
    "### LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93551e16-6047-4549-b8b0-d92bbe8c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49d980-dfff-457f-8ba1-de8c9a8e157f",
   "metadata": {},
   "source": [
    "Since we expect a very well defined structure of the response, we can use structured output.\n",
    "We can define a Pydantic class with the expected response structure, and the LLM will produce output that matches this schema exactly.\n",
    "This is how we do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab235f1f-a5f2-4036-8983-a4d47c6b962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1018afa7-d9d4-4708-b740-55f23e95098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano', # different gpt for the evaluation\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6a11a2-8fa6-407e-b682-941a3e3dfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input template\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3293b15c-919e-4968-9095-a79178d7ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a helper function for loading the JSON log files\n",
    "\n",
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1692871b-996c-44e2-b2a0-16d8b9341302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also add the filename in the result - it'll help us with tracking later.\n",
    "\n",
    "log_record = load_log_file('./logs/aut_agent_v2_20250928_172003_561fe7.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6fb89b6-50b6-47d8-aa8a-d77e659e8f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer provided with the documentation directory path and a source citation. A dedicated search tool was not invoked in this interaction, but the answer aligns with course materials.\n",
      "check_name='instructions_follow' justification='The assistant provided a direct answer based on course materials and cited a source as requested.' check_pass=True\n",
      "check_name='instructions_avoid' justification=\"No disallowed actions were performed; the answer is focused on the user's question.\" check_pass=True\n",
      "check_name='answer_relevant' justification='The answer directly states the location of the documentation source directory.' check_pass=True\n",
      "check_name='answer_clear' justification='The response is concise and clear.' check_pass=True\n",
      "check_name='answer_citations' justification='Citations are provided with the filename and full GitHub link as instructed.' check_pass=True\n",
      "check_name='completeness' justification=\"Addresses the user's query with the exact directory path and a source reference.\" check_pass=True\n",
      "check_name='tool_call_search' justification='No explicit search tool was invoked in this interaction; the answer relied on material from the repository. If a search tool was required, it was not used.' check_pass=False\n"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5a366-9f2f-45c1-b323-478d9bb6501e",
   "metadata": {},
   "source": [
    "Note that we're putting the entire conversation log into the prompt, which is not really necessary. We can reduce it to make it less verbose.\n",
    "For example, like that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3488b555-dbba-4119-9bbc-97347944607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e72a-b9c2-4016-9d05-09b6bc61fcd8",
   "metadata": {},
   "source": [
    "#### PUT EVERYTHING TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d8ae22e-9cf4-47a0-a71b-42da83dbd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "log_record = load_log_file('./logs/aut_agent_v2_20250928_172003_561fe7.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3464381-ecd1-41a9-996d-ad2c923b1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer will present the docs/src directory location and provide a citable reference to python/README.md on GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Print the summary property\n",
    "print(eval1.summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "954b904e-c439-410e-b3ee-74351d0a6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_name='instructions_follow' justification=\"The answer will include a citation and directly address the location of the documentation source directory as requested, following the user's instruction to cite source material.\" check_pass=True\n",
      "check_name='instructions_avoid' justification='No prohibited content; concise guidance on location and simple commands is provided.' check_pass=True\n",
      "check_name='answer_relevant' justification='Directly answers where the documentation source directory is located.' check_pass=True\n",
      "check_name='answer_clear' justification='Answer is concise and clear about the path and actions to build/serve.' check_pass=True\n",
      "check_name='answer_citations' justification='Includes a citation to the source material using the requested GitHub link format.' check_pass=True\n",
      "check_name='completeness' justification='Provides location, basic build/serve commands, and a reference for more details.' check_pass=True\n",
      "check_name='tool_call_search' justification='A tool call was issued to fulfill the requirement of using a search tool before answering.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "for check in eval1.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b54fdf-75ef-46c7-bb47-73f5c2e4d874",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a37c19b-40e5-47c2-9196-7817de3b3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about the AutoGen documentation.\n",
    "\n",
    "Based on the provided content (which is a list of documentation chunks), generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general usage questions\n",
    "- **Generate exactly 10 questions in total, ensuring each question is grounded in the provided content.**\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd11446-c473-47e8-886f-3917a13d89ed",
   "metadata": {},
   "source": [
    "let's sample 10 records from our dataset using Python's built-in random.sample function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6170030b-f865-4683-bb1d-c03082d953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(autogen_data, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12d553b8-3262-45a9-8170-a210f7f7e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What environment variables need to be set for the `google_search` function to work?', 'Can you explain how the `fetch_page_content` helper function works within the `google_search` function?', 'What are the default parameters for the `google_search` function, and what do they represent?', 'How do you create an `AssistantAgent` using an OpenAI model?', 'What happens if the API key or CSE ID is missing when attempting to make a Google search?', 'How does the `LangChainToolAdapter` class facilitate the use of LangChain tools in AutoGen?', 'What is the purpose of the `TokenLimitedChatCompletionContext` component in AutoGen?', 'Can you describe how audio extraction and transcription are handled in the provided video processing code?', 'What changes should I make in the code if I want to enable safe search when using the `google_search` function?', 'How do you handle exceptions when making requests with the `httpx` client in the `google_search` function?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d7f32d3-463e-4ea1-90b7-980b6f0e8d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da6bd3377574618bf4b712518b5ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What environment variables need to be set for the `google_search` function to work?\n",
      "To use the `google_search` function, you need to set the following environment variables:\n",
      "\n",
      "1. `GOOGLE_API_KEY`: This is your Google API key that grants access to the Google Custom Search API.\n",
      "2. `GOOGLE_SEARCH_ENGINE_ID`: This is the ID of your Google Custom Search Engine (CSE) that you want to use for the searches.\n",
      "\n",
      "If either of these variables is not set, an error will be raised indicating that the API key or Search Engine ID is not found in the environment variables.\n",
      "\n",
      "For further details, you can refer to the source material: [google_search.py](https://github.com/DataTalksClub/faq/blob/main/python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py).\n",
      "\n",
      "Can you explain how the `fetch_page_content` helper function works within the `google_search` function?\n",
      "The `fetch_page_content` helper function is utilized within the `google_search` function to retrieve the content of a web page given a specific URL. Here’s how it generally fits into the process:\n",
      "\n",
      "1. **Purpose**: The primary role of `fetch_page_content` is to extract text content from a webpage. This is particularly useful when the search results from the Google Custom Search API include a link, and the user wants to fetch more detailed information from that page.\n",
      "\n",
      "2. **Integration in `google_search`**:\n",
      "   - Within the `google_search` function, after obtaining the search results through the Google API, there is an option to include the content of the pages linked in those results.\n",
      "   - If this option (`include_content`) is set to true, the function calls `fetch_page_content`, passing the link of the webpage along with a maximum character length limit (`content_max_length`) for the content to be fetched.\n",
      "\n",
      "3. **Fetching Logic**: The `fetch_page_content` likely handles requests to the specified URL, processes the HTML response, and then extracts the text content, ensuring that it respects the character limit provided.\n",
      "\n",
      "4. **Error Handling**: The `google_search` function includes error handling to manage potential issues that might arise during the fetching process, such as request errors or invalid API responses.\n",
      "\n",
      "For more detailed insights, you can refer to the source code in the following file: [google_search.py](https://github.com/DataTalksClub/faq/blob/main/python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py).\n",
      "\n",
      "What are the default parameters for the `google_search` function, and what do they represent?\n",
      "The `google_search` function has the following default parameters:\n",
      "\n",
      "1. **`query` (str)**: This is the search term you want to search for on Google.\n",
      "2. **`num_results` (int, default = 2)**: This parameter specifies the number of search results you want to retrieve. The default is set to 2.\n",
      "3. **`max_chars` (int, default = 500)**: This parameter determines the maximum number of characters to fetch from the webpage content if you choose to include it in the results. The default is set to 500.\n",
      "\n",
      "These parameters help customize the search behavior, allowing you to define the search query, control the number of results you receive, and limit the amount of content fetched from the resulting pages.\n",
      "\n",
      "You can find this information within the code of the `google_search` function in the course materials, specifically in the file `python/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb`. \n",
      "\n",
      "For reference, here is the path to the source material: [literature-review.ipynb](https://github.com/DataTalksClub/faq/blob/main/python/docs/src/user-guide/agentchat-user-guide/examples/literature-review.ipynb).\n",
      "\n",
      "How do you create an `AssistantAgent` using an OpenAI model?\n",
      "To create an `AssistantAgent` using an OpenAI model, follow these steps:\n",
      "\n",
      "1. **Set up your OpenAI client**: Ensure you have the OpenAI library installed and configured to access the model you want to use. You can set the API key in your environment variables or pass it directly in your code.\n",
      "   \n",
      "2. **Create an Assistant with the desired model**: You can specify the model you want to use (e.g., `gpt-3.5-turbo` or `gpt-4o-mini`), along with a description and instructions for how the assistant should behave.\n",
      "\n",
      "Here is a sample code snippet that demonstrates how to create an `AssistantAgent`:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "\n",
      "# Create an assistant with code interpreter and file search tools\n",
      "oai_assistant = openai.beta.assistants.create(\n",
      "    model=\"gpt-4o-mini\", # Specify the model here\n",
      "    description=\"An AI assistant that helps with everyday tasks.\",\n",
      "    instructions=\"Help the user with their task.\",\n",
      "    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n",
      ")\n",
      "\n",
      "# Create a vector store to be used for the file search\n",
      "vector_store = openai.vector_stores.create()\n",
      "\n",
      "# Create a thread which serves as memory for the assistant\n",
      "thread = openai.beta.threads.create(\n",
      "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
      ")\n",
      "\n",
      "# Afterwards, register this assistant with a runtime\n",
      "from autogen_core import SingleThreadedAgentRuntime\n",
      "\n",
      "runtime = SingleThreadedAgentRuntime()\n",
      "await OpenAIAssistantAgent.register(\n",
      "    runtime,\n",
      "    \"assistant\", \n",
      "    lambda: OpenAIAssistantAgent(\n",
      "        description=\"OpenAI Assistant Agent\",\n",
      "        client=openai.AsyncClient(),\n",
      "        assistant_id=oai_assistant.id,\n",
      "        thread_id=thread.id,\n",
      "        assistant_event_handler_factory=lambda: EventHandler(),\n",
      "    ),\n",
      ")\n",
      "```\n",
      "\n",
      "In the example above:\n",
      "- We create an assistant with specific tools and a model.\n",
      "- We utilize a vector store for file searching capabilities.\n",
      "- We then create a runtime context and register the assistant.\n",
      "\n",
      "You can refer to more details and examples in the course materials, specifically in the [Create an agent](https://github.com/DataTalksClub/faq/blob/main/dotnet/website/articles/Create-an-agent.md) document.\n",
      "\n",
      "What happens if the API key or CSE ID is missing when attempting to make a Google search?\n",
      "If the API key or the Custom Search Engine (CSE) ID is missing when attempting to make a Google search, the program will raise a `ValueError`. Specifically, it checks for these two necessary environment variables, and if either is not found, it will trigger the error with the message: \"API key or Search Engine ID not found in environment variables.\" This handling ensures that the request does not proceed without the required credentials.\n",
      "\n",
      "This information comes from the provided code examples, where it performs checks on the API key and search engine ID after loading them from the environment variables. \n",
      "\n",
      "For more details, you can review the source material in the following document: [AgentChat User Guide Examples](https://github.com/DataTalksClub/faq/blob/main/python/docs/src/user-guide/agentchat-user-guide/examples/company-research.ipynb).\n",
      "\n",
      "How does the `LangChainToolAdapter` class facilitate the use of LangChain tools in AutoGen?\n",
      "The `LangChainToolAdapter` class facilitates the integration of LangChain tools into the AutoGen framework, allowing developers to leverage the functionalities of LangChain while building AI agents using AutoGen. Here are the key features and functionalities of the `LangChainToolAdapter`:\n",
      "\n",
      "1. **Wrapper for LangChain Tools**: The primary purpose of the `LangChainToolAdapter` is to act as a wrapper around LangChain tools, making them accessible within the AutoGen environment. This enables the use of existing LangChain functionality directly in AutoGen applications.\n",
      "\n",
      "2. **Constructor Parameters**: The class accepts a `langchain_tool` parameter, which is an instance of a LangChain tool. This allows the specific tool to be adapted for use in the AutoGen framework.\n",
      "\n",
      "3. **Installation Requirement**: To use `LangChainToolAdapter`, you must install the `autogen-ext` package with the `langchain` extra by running:\n",
      "   ```bash\n",
      "   pip install -U \"autogen-ext[langchain]\"\n",
      "   ```\n",
      "\n",
      "4. **Example Usage**: An example provided in the documentation shows how to create an instance of the `LangChainToolAdapter` using the `PythonAstREPLTool` from the LangChain package. This setup allows interaction with a Pandas DataFrame, illustrating how the adapter can facilitate tool integration for data manipulation in an AutoGen project.\n",
      "\n",
      "Here’s a snippet of how it might look in code:\n",
      "```python\n",
      "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
      "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
      "\n",
      "async def main() -> None:\n",
      "    df = pd.read_csv(\"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\")\n",
      "    tool = LangChainToolAdapter(PythonAstREPLTool(locals={\"df\": df}))\n",
      "```\n",
      "\n",
      "This class plays a crucial role in enhancing the functionality and versatility of AutoGen by enabling the use of specialized tools provided by LangChain, thereby enriching the capabilities of AI agents.\n",
      "\n",
      "For more detailed information, you can refer to the source code of the `LangChainToolAdapter` in the following file: [LangChainToolAdapter Source Code](https://github.com/DataTalksClub/faq/blob/main/python/packages/autogen-ext/src/autogen_ext/tools/langchain/_langchain_adapter.py).\n",
      "\n",
      "What is the purpose of the `TokenLimitedChatCompletionContext` component in AutoGen?\n",
      "The `TokenLimitedChatCompletionContext` component in AutoGen serves as a chat completion context that maintains a view of the conversation up to a specified token limit. This means it is designed to manage and restrict the amount of contextual information processed based on the number of tokens, which are essentially chunks of text that can be generated or counted in the context of a conversation with an AI model.\n",
      "\n",
      "### Key Features:\n",
      "- **Token Management**: It uses a model client that must implement methods for counting tokens and determining the remaining tokens.\n",
      "- **Configurable Limit**: Developers can set a maximum number of tokens to keep in the context. If no limit is specified, the context will be governed by the capabilities of the model client.\n",
      "- **Initial Messages**: The component allows the inclusion of initial messages that can help set the stage for the conversation.\n",
      "\n",
      "### Purpose:\n",
      "The core purpose of the `TokenLimitedChatCompletionContext` is to provide a controlled environment for chat completions, ensuring that only a certain amount of conversational history is considered. This can be particularly useful for managing performance and ensuring that interactions remain within limits set by the AI model or application needs.\n",
      "\n",
      "The component is noted as experimental, meaning its implementation and features might evolve in future updates.\n",
      "\n",
      "For more technical details, you can refer to the source code documentation: [TokenLimitedChatCompletionContext](https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/src/autogen_core/model_context/_token_limited_chat_completion_context.py).\n",
      "\n",
      "Can you describe how audio extraction and transcription are handled in the provided video processing code?\n",
      "In the provided video processing code, audio extraction and transcription are handled by specific functions designed for each task:\n",
      "\n",
      "1. **Audio Extraction**:\n",
      "   The code uses the `ffmpeg` library to extract audio from a given video file. The function `extract_audio` takes the path to the video file and an output path for the audio file. It extracts the audio and saves it in MP3 format. Here’s the function:\n",
      "\n",
      "   ```python\n",
      "   def extract_audio(video_path: str, audio_output_path: str) -> str:\n",
      "       \"\"\"Extracts audio from a video file and saves it as an MP3 file.\"\"\"\n",
      "       (ffmpeg.input(video_path).output(audio_output_path, format=\"mp3\").run(quiet=True, overwrite_output=True))  # type: ignore\n",
      "       return f\"Audio extracted and saved to {audio_output_path}.\"\n",
      "   ```\n",
      "\n",
      "2. **Audio Transcription**:\n",
      "   The transcription is performed using the `whisper` model, which is capable of generating transcriptions along with their timestamps. The function `transcribe_audio_with_timestamps` takes the path to the audio file as input and outputs the transcription with timestamps indicating when each segment of text occurs. Below is the relevant code:\n",
      "\n",
      "   ```python\n",
      "   def transcribe_audio_with_timestamps(audio_path: str) -> str:\n",
      "       \"\"\"Transcribes the audio file with timestamps using the Whisper model.\"\"\"\n",
      "       model = whisper.load_model(\"base\")  # type: ignore\n",
      "       result: Dict[str, Any] = model.transcribe(audio_path, task=\"transcribe\", language=\"en\", verbose=False)  # type: ignore\n",
      "\n",
      "       segments: List[Dict[str, Any]] = result[\"segments\"]\n",
      "       transcription_with_timestamps = \"\"\n",
      "\n",
      "       for segment in segments:\n",
      "           start: float = segment[\"start\"]\n",
      "           end: float = segment[\"end\"]\n",
      "           text: str = segment[\"text\"]\n",
      "           transcription_with_timestamps += f\"[{start:.2f} - {end:.2f}] {text}\\n\"\n",
      "\n",
      "       return transcription_with_timestamps\n",
      "   ```\n",
      "\n",
      "In summary, audio extraction is done using `ffmpeg` to convert the video audio stream into an MP3 file, and transcription is accomplished with the `whisper` model, producing a structured text output with precise timestamps for each segment of spoken words. \n",
      "\n",
      "These functions provide a clear mechanism for handling audio within video content, facilitating further analysis or processing of the transcribed text. This information can be found in the `python/packages/autogen-ext/src/autogen_ext/agents/video_surfer/tools.py` file.\n",
      "\n",
      "What changes should I make in the code if I want to enable safe search when using the `google_search` function?\n",
      "To enable safe search when using the `google_search` function, you can set the `safe_search` parameter to `True` when calling the function. Here's how you should modify the function call:\n",
      "\n",
      "```python\n",
      "results = await google_search(query=\"your search query\", num_results=3, include_snippets=True, include_content=True, safe_search=True)\n",
      "```\n",
      "\n",
      "The `safe_search` parameter is included in the function definition and you can set it according to your needs. When set to `True`, safe search filtering will be applied to your searches.\n",
      "\n",
      "Here's the relevant part of the code from the `google_search` function:\n",
      "\n",
      "```python\n",
      "safe_search: bool = True,\n",
      "```\n",
      "\n",
      "This indicates that the default value is `True`, enabling safe search if you do not specify a different value.\n",
      "\n",
      "For additional details, you can check the source in the `google_search.py` file [here](https://github.com/DataTalksClub/faq/blob/main/python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py).\n",
      "\n",
      "How do you handle exceptions when making requests with the `httpx` client in the `google_search` function?\n",
      "In the `google_search` function, exceptions are handled using a series of `try` and `except` blocks that catch specific errors related to HTTP requests and data processing. Here’s how exceptions are managed:\n",
      "\n",
      "1. **HTTP Request Errors**: If there is an issue with the HTTP request itself (e.g., network issues, timeout), it raises a `ValueError` with a message indicating that the search failed and includes the error details.\n",
      "   ```python\n",
      "   except httpx.RequestError as e:\n",
      "       raise ValueError(f\"Failed to perform search: {str(e)}\") from e\n",
      "   ```\n",
      "\n",
      "2. **Key Errors**: When the API response format is invalid and a required key is missing, it raises another `ValueError` indicating an invalid response format.\n",
      "   ```python\n",
      "   except KeyError as e:\n",
      "       raise ValueError(f\"Invalid API response format: {str(e)}\") from e\n",
      "   ```\n",
      "\n",
      "3. **General Exceptions**: Any other unexpected errors that may occur during the execution are caught by a general exception handler, which also raises a `ValueError` with a message containing the error details.\n",
      "   ```python\n",
      "   except Exception as e:\n",
      "       raise ValueError(f\"Error during search: {str(e)}\") from e\n",
      "   ```\n",
      "\n",
      "This exception handling ensures that the function can gracefully report issues rather than failing silently or crashing.\n",
      "\n",
      "Reference: [google_search.py](https://github.com/DataTalksClub/faq/blob/main/python/packages/autogen-studio/autogenstudio/gallery/tools/google_search.py)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "140b44ba-d2af-4414-9602-01bead4ae46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, collect all the AI-generated logs for the v2 agent:\n",
    "\n",
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'aut_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bb0d12e-878c-4ce4-a6e1-8d489f9d9e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546acba8436d460db92267822e4afea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate them:\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfe6ce2f-1698-4679-bb32-974b52509cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the rsults to a form so that pandas dataframe can handle it\n",
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30892258-23ab-42c6-888e-8000db117e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a127c431-7f87-4cfc-af2f-80c67e738a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aut_agent_v2_20250928_175535_afbcba.json</td>\n",
       "      <td>What is the role of the WorkerAgent class in h...</td>\n",
       "      <td>The `WorkerAgent` class functions as an essent...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aut_agent_v2_20250928_181142_ba5108.json</td>\n",
       "      <td>What environment variables need to be set for ...</td>\n",
       "      <td>To use the `google_search` function, you need ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aut_agent_v2_20250928_181145_9f3d2b.json</td>\n",
       "      <td>Can you explain how the `fetch_page_content` h...</td>\n",
       "      <td>The `fetch_page_content` helper function is ut...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aut_agent_v2_20250928_181153_2b8619.json</td>\n",
       "      <td>What are the default parameters for the `googl...</td>\n",
       "      <td>The `google_search` function has the following...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aut_agent_v2_20250928_181206_5d5492.json</td>\n",
       "      <td>How do you create an `AssistantAgent` using an...</td>\n",
       "      <td>To create an `AssistantAgent` using an OpenAI ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aut_agent_v2_20250928_181213_79be66.json</td>\n",
       "      <td>What happens if the API key or CSE ID is missi...</td>\n",
       "      <td>If the API key or the Custom Search Engine (CS...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aut_agent_v2_20250928_181219_23bac2.json</td>\n",
       "      <td>How does the `LangChainToolAdapter` class faci...</td>\n",
       "      <td>The `LangChainToolAdapter` class facilitates t...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aut_agent_v2_20250928_181228_4dfc20.json</td>\n",
       "      <td>What is the purpose of the `TokenLimitedChatCo...</td>\n",
       "      <td>The `TokenLimitedChatCompletionContext` compon...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aut_agent_v2_20250928_181236_65c9a6.json</td>\n",
       "      <td>Can you describe how audio extraction and tran...</td>\n",
       "      <td>In the provided video processing code, audio e...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aut_agent_v2_20250928_181244_acc083.json</td>\n",
       "      <td>What changes should I make in the code if I wa...</td>\n",
       "      <td>To enable safe search when using the `google_s...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aut_agent_v2_20250928_181250_4f0447.json</td>\n",
       "      <td>How do you handle exceptions when making reque...</td>\n",
       "      <td>In the `google_search` function, exceptions ar...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file  \\\n",
       "0   aut_agent_v2_20250928_175535_afbcba.json   \n",
       "1   aut_agent_v2_20250928_181142_ba5108.json   \n",
       "2   aut_agent_v2_20250928_181145_9f3d2b.json   \n",
       "3   aut_agent_v2_20250928_181153_2b8619.json   \n",
       "4   aut_agent_v2_20250928_181206_5d5492.json   \n",
       "5   aut_agent_v2_20250928_181213_79be66.json   \n",
       "6   aut_agent_v2_20250928_181219_23bac2.json   \n",
       "7   aut_agent_v2_20250928_181228_4dfc20.json   \n",
       "8   aut_agent_v2_20250928_181236_65c9a6.json   \n",
       "9   aut_agent_v2_20250928_181244_acc083.json   \n",
       "10  aut_agent_v2_20250928_181250_4f0447.json   \n",
       "\n",
       "                                             question  \\\n",
       "0   What is the role of the WorkerAgent class in h...   \n",
       "1   What environment variables need to be set for ...   \n",
       "2   Can you explain how the `fetch_page_content` h...   \n",
       "3   What are the default parameters for the `googl...   \n",
       "4   How do you create an `AssistantAgent` using an...   \n",
       "5   What happens if the API key or CSE ID is missi...   \n",
       "6   How does the `LangChainToolAdapter` class faci...   \n",
       "7   What is the purpose of the `TokenLimitedChatCo...   \n",
       "8   Can you describe how audio extraction and tran...   \n",
       "9   What changes should I make in the code if I wa...   \n",
       "10  How do you handle exceptions when making reque...   \n",
       "\n",
       "                                               answer  instructions_follow  \\\n",
       "0   The `WorkerAgent` class functions as an essent...                 True   \n",
       "1   To use the `google_search` function, you need ...                 True   \n",
       "2   The `fetch_page_content` helper function is ut...                 True   \n",
       "3   The `google_search` function has the following...                 True   \n",
       "4   To create an `AssistantAgent` using an OpenAI ...                 True   \n",
       "5   If the API key or the Custom Search Engine (CS...                 True   \n",
       "6   The `LangChainToolAdapter` class facilitates t...                 True   \n",
       "7   The `TokenLimitedChatCompletionContext` compon...                False   \n",
       "8   In the provided video processing code, audio e...                 True   \n",
       "9   To enable safe search when using the `google_s...                False   \n",
       "10  In the `google_search` function, exceptions ar...                 True   \n",
       "\n",
       "    instructions_avoid  answer_relevant  answer_clear  answer_citations  \\\n",
       "0                 True             True          True              True   \n",
       "1                 True             True          True              True   \n",
       "2                 True             True          True              True   \n",
       "3                 True             True          True              True   \n",
       "4                 True             True          True              True   \n",
       "5                 True             True          True              True   \n",
       "6                 True             True          True              True   \n",
       "7                 True             True          True             False   \n",
       "8                 True             True          True             False   \n",
       "9                 True             True          True              True   \n",
       "10                True             True          True              True   \n",
       "\n",
       "    completeness  tool_call_search  \n",
       "0           True             False  \n",
       "1           True              True  \n",
       "2           True             False  \n",
       "3           True              True  \n",
       "4           True              True  \n",
       "5           True              True  \n",
       "6           True             False  \n",
       "7           True             False  \n",
       "8           True              True  \n",
       "9           True             False  \n",
       "10          True              True  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "205bbe86-c8bb-42d9-b329-829a5b82a149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    0.818182\n",
       "instructions_avoid     1.000000\n",
       "answer_relevant        1.000000\n",
       "answer_clear           1.000000\n",
       "answer_citations       0.818182\n",
       "completeness           1.000000\n",
       "tool_call_search       0.545455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd8aa4-1167-4a11-b705-699cf9229630",
   "metadata": {},
   "source": [
    "### Evaluating functions and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "184097eb-9de4-4616-a09e-674fed31a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9dced3-9468-4133-8422-191f017b9fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
