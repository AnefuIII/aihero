{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown and code files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\" \n",
    "    url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/main.zip'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc2c3b1-5bb7-4dc1-8524-8474d67432f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 749 files from microsoft/autogen.\n"
     ]
    }
   ],
   "source": [
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "# This will print the number of files you successfully processed\n",
    "print(f'Processed {len(autogen_data)} files from microsoft/autogen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for record in autogen_data:\n",
    "    index[record['filename']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {
    "_sphinx_cell_id": "d4ce051b-aaae-4725-9eef-869cf50e26fb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "exporter = MarkdownExporter()\n",
    "exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "def format_notebook_as_md(raw_notebook: str) -> str:\n",
    "    nb_parsed = nbformat.reads(\n",
    "        raw_notebook,\n",
    "        as_version=nbformat.NO_CONVERT,\n",
    "    )\n",
    "    md_body, _ = exporter.from_notebook_node(nb_parsed)\n",
    "    return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8d3523-688c-4724-8b6c-47247388b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    if not text.startswith(\"```\"):\n",
    "        return text\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This line loads the variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now the OpenAI client will be able to find the key in your environment\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# You can also explicitly pass the key if you prefer\n",
    "# import os\n",
    "# openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_editing_instructions = \"\"\"\n",
    "You're a professional coding editor.\n",
    "\n",
    "You are given a Markdown file that was converted from a Jupyter notebook.  \n",
    "The file already contains code blocks and inline comments.  \n",
    "\n",
    "Your task:\n",
    "\n",
    "- Turn it into clear, well-structured documentation.  \n",
    "- Add section headers (##) where appropriate. Keep sections relatively large (8-10 paragraphs and code blocks)\n",
    "- Add concise, high-level explanations for each code block.  \n",
    "- Summarize what the code is doing without being overly verbose.  \n",
    "- Keep the formatting in Markdown.\n",
    "- Aim for a balance: clear enough to guide someone new, but not overloaded with detail. \n",
    "\n",
    "Output the improved Markdown file with the new documentation.\n",
    "\"\"\".strip()\n",
    "\n",
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, open the file and read its content\n",
    "# with open('data-processing-code.ipynb', 'r', encoding='utf-8') as f:\n",
    "#     raw_notebook_content = f.read()\n",
    "\n",
    "# result = llm(notebook_editing_instructions, md_body)\n",
    "# print(result)\n",
    "# #result = llm(system_prompt, md_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a456b4aa-40f2-428c-973c-4d40f0005213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0f975c-f603-4a3b-9a32-c261a44d1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 49 jupyter notebooks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38326f6876044754b80642b625da26a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipynb_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') == True and record['filename'].endswith('.ipynb'):\n",
    "        ipynb_data.append(record)\n",
    "\n",
    "\n",
    "print(f'processing {len(ipynb_data)} jupyter notebooks...')\n",
    "\n",
    "for record in tqdm(ipynb_data):\n",
    "    md_body = format_notebook_as_md(record['content'])\n",
    "    new_content = llm(notebook_editing_instructions, md_body)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d791ab0-a49b-4439-a143-bd3e8c38067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941e67d0-9260-48ca-bcdf-8abb6401c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 539 code files...\n"
     ]
    }
   ],
   "source": [
    "code_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') != True:\n",
    "        continue\n",
    "\n",
    "    path = record['filename']\n",
    "    ext = path.split('.')[-1]\n",
    "\n",
    "    if ext not in code_extensions:\n",
    "        continue\n",
    "\n",
    "    if ext == 'ipynb':\n",
    "        continue\n",
    "\n",
    "    # print(path)\n",
    "    code_data.append(record)\n",
    "\n",
    "print(f'processing {len(code_data)} code files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5755e9-1f70-4510-b75c-6b952ad303fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb84bc7c21b4d61b42c54699dd38536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(code_data):\n",
    "    code = record['content']\n",
    "\n",
    "    new_content = llm(code_doc_instructions, code)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d50b9af-207b-421c-a567-3aacc0bfacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5aeaab-dc9c-4b1d-a00b-ce29bbf9b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da555e70-19ba-4996-9f69-9cfd18a477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/autogen_data_processed.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(autogen_data, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8984a6-78ac-4c05-b065-76f7dc910382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->\\n\\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\\n\\n## Why are these changes needed?\\n\\n<!-- Please give a short summary of the change and the problem this solves. -->\\n\\n## Related issue number\\n\\n<!-- For example: \\\"Closes #1234\\\" -->\\n\\n## Checks\\n\\n- [ ] I've included any doc changes needed for <https://microsoft.github.io/autogen/>. See <https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md> to build and test documentation locally.\\n- [ ] I've added tests (if relevant) corresponding to the changes introduced in this PR.\\n- [ ] I've made sure all auto checks have passed.\",\n",
      "    \"filename\": \".github/PULL_REQUEST_TEMPLATE.md\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"# AutoGen Multi-Agent AI Framework\\n\\nAutoGen is a multi-language framework for creating AI agents that can act autonomously or work alongside humans. The project has separate Python and .NET implementations with their own development workflows.\\n\\nAlways reference these instructions first and fallback to search or bash commands only when you encounter unexpected information that does not match the info here.\\n\\n## Working Effectively\\n\\n### Prerequisites and Environment Setup\\n\\n**CRITICAL**: Install both .NET 8.0 and 9.0 for full compatibility:\\n- Install uv package manager: `python3 -m pip install uv` \\n- Install .NET 9.0 SDK: `wget https://dot.net/v1/dotnet-install.sh && chmod +x dotnet-install.sh && ./dotnet-install.sh --channel 9.0`\\n- Install .NET 8.0 runtime: `./dotnet-install.sh --channel 8.0 --runtime dotnet && ./dotnet-install.sh --channel 8.0 --runtime aspnetcore`\\n- Update PATH: `export PATH=\\\"$HOME/.dotnet:$PATH\\\"`\\n\\n### Python Development Workflow\\n\\n**Bootstrap and build Python environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nuv sync --all-extras  # NEVER CANCEL: Takes 2 minutes. Set timeout to 300+ seconds.\\nsource .venv/bin/activate\\n```\\n\\n**Validate Python development:**\\n```bash\\n# Quick validation (under 1 second each)\\npoe format  # Code formatting\\npoe lint    # Linting with ruff\\n\\n# Type checking - NEVER CANCEL these commands\\npoe mypy     # Takes 6 minutes. Set timeout to 600+ seconds.\\npoe pyright  # Takes 41 seconds. Set timeout to 120+ seconds.\\n\\n# Individual package testing (core package example)\\npoe --directory ./packages/autogen-core test  # Takes 10 seconds. Set timeout to 60+ seconds.\\n\\n# Documentation - NEVER CANCEL\\npoe docs-build  # Takes 1 minute 16 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: Python environment setup takes 2 minutes minimum\\n- **NEVER CANCEL**: mypy type checking takes 6 minutes \\n- **NEVER CANCEL**: Documentation build takes 1+ minutes\\n- Format/lint tasks complete in under 1 second\\n- Individual package tests typically complete in 10-60 seconds\\n\\n### .NET Development Workflow\\n\\n**Bootstrap and build .NET environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/dotnet\\nexport PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet restore  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\ndotnet build --configuration Release  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**Validate .NET development:**\\n```bash\\n# Unit tests - NEVER CANCEL\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Takes 25 seconds. Set timeout to 120+ seconds.\\n\\n# Format check (if build fails) \\ndotnet format --verify-no-changes\\n\\n# Run samples\\ncd samples/Hello\\ndotnet run\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: .NET restore takes 53 seconds minimum\\n- **NEVER CANCEL**: .NET build takes 53 seconds minimum  \\n- **NEVER CANCEL**: .NET unit tests take 25 seconds minimum\\n- All build and test commands require appropriate timeouts\\n\\n### Complete Validation Workflow\\n\\n**Run full check suite (Python):**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nsource .venv/bin/activate\\npoe check  # NEVER CANCEL: Runs all checks. Takes 7+ minutes total. Set timeout to 900+ seconds.\\n```\\n\\n## Validation Scenarios\\n\\n### Manual Validation Requirements\\nAlways manually validate changes by running complete user scenarios after making modifications:\\n\\n**Python validation scenarios:**\\n1. **Import test**: Verify core imports work:\\n   ```python\\n   from autogen_agentchat.agents import AssistantAgent\\n   from autogen_core import AgentRuntime\\n   from autogen_ext.models.openai import OpenAIChatCompletionClient\\n   ```\\n\\n2. **AutoGen Studio test**: Verify web interface can start:\\n   ```bash\\n   autogenstudio ui --help  # Should show help without errors\\n   ```\\n\\n3. **Documentation test**: Build and verify docs generate without errors:\\n   ```bash\\n   poe docs-build && ls docs/build/index.html\\n   ```\\n\\n**.NET validation scenarios:**\\n1. **Sample execution**: Run Hello sample to verify runtime works:\\n   ```bash\\n   cd dotnet/samples/Hello && dotnet run --help\\n   ```\\n\\n2. **Build validation**: Ensure all projects compile:\\n   ```bash\\n   dotnet build --configuration Release --no-restore\\n   ```\\n\\n3. **Test execution**: Run unit tests to verify functionality:\\n   ```bash\\n   dotnet test --filter \\\"Category=UnitV2\\\" --configuration Release --no-build\\n   ```\\n\\n## Common Issues and Workarounds\\n\\n### Network-Related Issues\\n- **Python tests may fail** with network errors (tiktoken downloads, Playwright browser downloads) in sandboxed environments - this is expected\\n- **Documentation intersphinx warnings** due to inability to reach external documentation sites - this is expected\\n- **Individual package tests work better** than full test suite in network-restricted environments\\n\\n### .NET Runtime Issues  \\n- **Requires both .NET 8.0 and 9.0**: Build uses 9.0 SDK but tests need 8.0 runtime\\n- **Global.json specifies 9.0.100**: Must install exact .NET 9.0 version or later\\n- **Path configuration critical**: Ensure `$HOME/.dotnet` is in PATH before system .NET\\n\\n### Python Package Issues\\n- **Use uv exclusively**: Do not use pip/conda for dependency management\\n- **Virtual environment required**: Always activate `.venv` before running commands\\n- **Package workspace structure**: Project uses uv workspace with multiple packages\\n\\n## Timing Reference\\n\\n### Python Commands\\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `uv sync --all-extras` | 2 minutes | 300+ seconds | NEVER CANCEL |\\n| `poe mypy` | 6 minutes | 600+ seconds | NEVER CANCEL |\\n| `poe pyright` | 41 seconds | 120+ seconds | NEVER CANCEL |\\n| `poe docs-build` | 1 min 16 sec | 300+ seconds | NEVER CANCEL |\\n| `poe format` | <1 second | 30 seconds | Quick |\\n| `poe lint` | <1 second | 30 seconds | Quick |\\n| Individual package test | 10 seconds | 60+ seconds | May have network failures |\\n\\n### .NET Commands  \\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `dotnet restore` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet build --configuration Release` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet test --filter \\\"Category=UnitV2\\\"` | 25 seconds | 120+ seconds | NEVER CANCEL |\\n| `dotnet format --verify-no-changes` | 5-10 seconds | 60 seconds | Quick validation |\\n\\n## Repository Structure\\n\\n### Python Packages (`python/packages/`)\\n- `autogen-core`: Core agent runtime, model interfaces, and base components\\n- `autogen-agentchat`: High-level multi-agent conversation APIs  \\n- `autogen-ext`: Extensions for specific model providers and tools\\n- `autogen-studio`: Web-based IDE for agent workflows\\n- `agbench`: Benchmarking suite for agent performance\\n- `magentic-one-cli`: Multi-agent team CLI application\\n\\n### .NET Projects (`dotnet/src/`)\\n- `AutoGen`: Legacy 0.2-style .NET packages (being deprecated)\\n- `Microsoft.AutoGen.*`: New event-driven .NET packages\\n- `AutoGen.Core`: Core .NET agent functionality\\n- Multiple provider packages: OpenAI, Anthropic, Ollama, etc.\\n\\n### Key Configuration Files\\n- `python/pyproject.toml`: Python workspace and tool configuration\\n- `dotnet/global.json`: .NET SDK version requirements  \\n- `dotnet/AutoGen.sln`: .NET solution file\\n- `python/uv.lock`: Locked Python dependencies\\n\\n## Development Best Practices\\n\\n### Before Committing Changes\\n**ALWAYS run these validation steps:**\\n\\n**Python:**\\n```bash\\ncd python && source .venv/bin/activate\\npoe format    # Fix formatting\\npoe lint      # Check code quality  \\npoe mypy      # Type checking (6 minutes)\\npoe docs-build # Verify docs build (1+ minutes)\\n```\\n\\n**.NET:**\\n```bash  \\ncd dotnet && export PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet format --verify-no-changes  # Check formatting\\ndotnet build --configuration Release --no-restore  # Build (53 seconds)\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Test (25 seconds)\\n```\\n\\n### Key Directories Reference\\n```\\nautogen/\\n\\u251c\\u2500\\u2500 python/                    # Python implementation\\n\\u2502   \\u251c\\u2500\\u2500 packages/             # Individual Python packages\\n\\u2502   \\u251c\\u2500\\u2500 docs/                 # Sphinx documentation\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Example code\\n\\u2502   \\u2514\\u2500\\u2500 pyproject.toml        # Workspace configuration\\n\\u251c\\u2500\\u2500 dotnet/                   # .NET implementation  \\n\\u2502   \\u251c\\u2500\\u2500 src/                  # Source projects\\n\\u2502   \\u251c\\u2500\\u2500 test/                 # Test projects\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Sample applications\\n\\u2502   \\u2514\\u2500\\u2500 AutoGen.sln          # Solution file\\n\\u251c\\u2500\\u2500 .github/workflows/        # CI/CD pipelines\\n\\u2514\\u2500\\u2500 docs/                     # Additional documentation\\n```\\n\\nThis framework supports creating both simple single-agent applications and complex multi-agent workflows with support for various LLM providers, tools, and deployment patterns.\",\n",
      "    \"filename\": \".github/copilot-instructions.md\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head data/autogen_data_processed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "966c577e-28da-401a-84ff-9c69ddabb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d46bee-ed92-4f7c-bac3-3168560d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen_data_chunks = []\n",
    "\n",
    "for doc in autogen_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    autogen_data_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655c97c2-d5cc-4b53-a31a-fccffb8424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autogen_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb350c9-ffb1-4d2a-be36-d961e5bd450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'chunk': \"## Prerequisites\\n\\n- Access to gpt3.5-turbo or preferably gpt4 - [Get access here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai)\\n- [Setup a Github app](#how-do-i-setup-the-github-app)\\n- [Install the Github app](https://docs.github.com/en/apps/using-github-apps/installing-your-own-github-app)\\n- [Provision the azure resources](#how-do-I-deploy-the-azure-bits)\\n- [Create labels for the dev team skills](#which-labels-should-i-create)\\n\\n### How do I setup the Github app?\\n\\n- [Register a Github app](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app), with the options listed below:\\n    - Give your App a name and add a description\\n    - Homepage URL: Can be anything (Example: repository URL)\\n    - Add a dummy value for the webhook url, we'll come back to this setting\\n    - Enter a webhook secret, which you'll need later on when filling in the `WebhookSecret` property in the `appsettings.json` file\\n    - Setup the following permissions\\n        - Repository \\n            - Contents - read and write\\n            - Issues - read and write\\n            - Metadata - read only\\n            - Pull requests - read and write\\n    - Subscribe to the following events:\\n        - Issues\\n        - Issue comment\\n    - Allow this app to be installed by any user or organization\\n    \\n- After the app is created, generate a private key, we'll use it later for authentication to Github from the app\\n\\n### Which labels should I create?\\n\\nIn order for us to know which skill and persona we need to talk with, we are using Labels in Github Issues.\\n\\nThe default bunch of skills and personnas are as follow:\\n- PM.Readme\\n- Do.It\\n- DevLead.Plan\\n- Developer.Implement\\n\\nAdd them to your repository (They are not there by default).\\n\\nOnce you start adding your own skills, just remember to add the corresponding label to your repository.\\n\\n## How do I run this locally?\\n\\nCodespaces are preset for this repo. For codespa\",\n",
       " 'filename': 'dotnet/samples/dev-team/docs/github-flow-getting-started.md'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autogen_data_chunks[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dfd46-106f-4f7d-8ed8-30d9b92389e5",
   "metadata": {},
   "source": [
    "### Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b39ee7-b0c1-41cf-8954-74fcfd61d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from data/autogen_data_processed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'data/autogen_data_processed.json'\n",
    "autogen_data_chunk = None # Initialize the variable\n",
    "\n",
    "try:\n",
    "    # Open the file in 'r' (read) mode\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        # Use json.load() to read the JSON data from the file object (f_in)\n",
    "        autogen_data_chunk = json.load(f_in)\n",
    "        \n",
    "    print(f\"Successfully loaded data from {input_file}\")\n",
    "    # You can now work with autogen_data_chunk\n",
    "    # print(type(autogen_data_chunk)) \n",
    "    # print(autogen_data_chunk)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {input_file} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Failed to decode JSON from {input_file}. The file might be malformed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8a3f9-3103-491a-8af4-88a0d441a4fe",
   "metadata": {},
   "source": [
    "#### TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cfced2-b5ea-41cd-a997-3eda055456b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7fbfbf5e4fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(autogen_data_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05660303-3c4e-4dd4-9435-0a716d0a202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what does interface definitions and reference implementations of agent runtime, model, tool, workbench, memory, tracing'\n",
    "query2 = 'tell me about packages/autogen-core'\n",
    "results = index.search(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bc6927-ead6-4236-84b8-6951b0a6bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '# AutoGen Core\\n\\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)\\n\\nAutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.', 'filename': 'python/packages/autogen-core/README.md'}, {'code': False, 'content': '# Overview of `autogen_core`\\n\\nThe `autogen_core` module serves as the foundation for a system centered around agents, interoperability, and component management. This module facilitates the creation and management of agents, provides tools for data serialization, defines message handling protocols, and manages logging functionalities. By importing various submodules and defining global constants and classes, it offers an organized layout for the internal structure of the agent management system.\\n\\n## Versioning Information\\n\\nThe module starts by retrieving its version using the `importlib.metadata` package. This is a common practice to enable tracking of the module’s version which helps in ensuring compatibility and debugging:\\n\\n```python\\n__version__ = importlib.metadata.version(\"autogen_core\")\\n```\\n\\n## Core Agent Functionality\\n\\nThe module defines several core classes representing different aspects of agent functionality including:\\n\\n- **Agent**: Central class representing an agent that encapsulates core behaviors and properties.\\n- **AgentId**: A class that likely manages unique identifiers for agents, which is essential for tracking and communication.\\n- **AgentInstantiationContext**: Manages context information during the instantiation of agents.\\n- **AgentMetadata**: Stores metadata related to agents such as attributes or configurations.\\n\\nThese components contribute to defining how agents operate within this framework, maintaining unique states and handling interactions.\\n\\n## Agent Management and Runtime\\n\\nThe module includes a range of classes designed to handle the runtime behavior of agents:\\n\\n- **AgentRuntime**: Likely manages the lifecycle of agents, including their execution and state management.\\n- **SingleThreadedAgentRuntime**: A special runtime for executing agents in a single-threaded context, ensuring sequential processing without concurrency issues.\\n\\nAdditionally, **AgentProxy** allows for interfacing with agents from potentially different execution contexts or systems, facilitating remote communication.\\n\\n## Component System\\n\\nA significant portion of the module revolves around component management:\\n\\n- **Component** and its related classes (e.g., **ComponentBase**, **ComponentLoader**) allow the system to dynamically load and configure components at runtime. \\n- This includes defining schemas (**ComponentSchemaType**) and potentially handling configurations through **ComponentFromConfig** and **ComponentToConfig**.\\n\\nThese components likely enable extensibility and modularity, where new features can be integrated without significant modifications to the core system.\\n\\n## Messaging and Interventions\\n\\nThe module also defines mechanisms for handling messages and interventions:\\n\\n- **MessageContext** and **MessageHandlerContext** encapsulate information necessary for message processing and handler states.\\n- **InterventionHandler** and **DefaultInterventionHandler** are utilized to manage disruptions or changes in message flows, which help in error handling or providing alternate routes for message processing.\\n\\nThis aids in maintaining the reliability of communication in agent interactions.\\n\\n## Subscription Mechanism\\n\\nTo support an event-driven architecture, the module provides several classes and functions related to subscriptions:\\n\\n- **Subscription**: Central class for managing subscriptions to events or message types.\\n- **TypeSubscription** and **TypePrefixSubscription** allow for more specific subscription behaviors based on message types, enabling agents to respond to targeted messages.\\n\\nThis mechanism enhances the responsiveness of agents to specific events, promoting efficient processing.\\n\\n## Serialization Handling\\n\\nData serialization is crucial in this context, and the module defines elements for handling various data formats:\\n\\n- **MessageSerializer**: Implements serialization logic, allowing messages to be converted to and from different formats.\\n- Constants like **JSON_DATA_CONTENT_TYPE** and **PROTOBUF_DATA_CONTENT_TYPE** define the expected data types for serialization.\\n\\nBy supporting multiple formats, the system can handle a broader range of data interactions, making it versatile for different use cases.\\n\\n## Logging and Telemetry\\n\\nThe module includes robust logging capabilities:\\n\\n- **EVENT_LOGGER_NAME**, **ROOT_LOGGER_NAME**, and **TRACE_LOGGER_NAME** provide structured logging support, enabling tracking and debugging of agent activities.\\n- Functions like **trace_create_agent_span** and **trace_invoke_agent_span** help in monitoring performance and analyzing agent behavior over time.\\n\\nThis focus on logging ensures that users can gather insights into system operations, which is essential for maintenance and troubleshooting.\\n\\n## Exposed API \\n\\nLastly, the module exports a well-defined set of public interfaces with the `__all__` variable, outlining which classes and functions are accessible for use in other modules. This approach aids in defining a clear API for users of the `autogen_core`, ensuring encapsulation and reducing the risk of interference with internal workings. Each of these exported components plays a distinct role in agent management, message processing, or system integration, enabling a comprehensive ecosystem of functionalities tailored for developing agent-based applications.', 'filename': 'python/packages/autogen-core/src/autogen_core/__init__.py'}, {'code': False, 'content': \"# Module Overview\\n\\nThis module serves as an interface for a toolkit that likely facilitates the handling of various tools, including stream processing and function execution within a broader workflow or workbench context. It imports and organizes several classes and schemas from different submodules, making them accessible for external use. The use of `__all__` allows for the specification of a public API for the module.\\n\\n## Imports\\n\\nThe module starts by importing a set of classes and schemas from different parts of the package. Here's a breakdown of the imports:\\n\\n- **Base Classes**  \\n  The `BaseStreamTool`, `BaseTool`, and `BaseToolWithState` classes are foundational tools meant to be extended for specific purposes within the toolkit. They likely provide common functionality that can be shared among various tool implementations.\\n\\n- **Schemas**  \\n  `ParametersSchema` and `ToolSchema` probably define the structure or validation requirements for parameters and tools, respectively.\\n\\n- **Specific Tools**  \\n  `StreamTool` could represent a specialized tool designed for processing streaming data, while `FunctionTool` likely wraps around functions to provide them as tools within the workbench environment.\\n\\n- **Workbench Components**  \\n  The `Workbench`, `StaticWorkbench`, `StaticStreamWorkbench`, `ToolResult`, `TextResultContent`, and `ImageResultContent` classes are likely designed to manage and present results from the tools executed within a workbench setup. They may handle different data types, encapsulating the results of tool executions along with relevant metadata.\\n\\n## Public API\\n\\nThe `__all__` list explicitly declares the public API of the module, outlining which classes and functions can be accessed when using `from module import *`. This serves several purposes:\\n\\n- **Encapsulation**  \\n  By controlling what is exposed, the module can prevent direct access to internal components that are not intended for use outside of the module, encouraging better practices and reducing the chance of incorrect usage.\\n\\n- **Ease of Access**  \\n  Users of the module can easily see which tools and classes are available for them to use, streamlining their interaction with the toolkit.\\n\\n## Key Classes and Their Roles\\n\\n### Base Classes\\n\\n- **BaseTool**: This class probably provides a framework for creating tools within the module, encapsulating common functionalities needed by all tools. \\n- **BaseToolWithState**: This likely extends `BaseTool` to support maintaining internal state information across invocations, useful for tools that need to remember context or progress.\\n- **BaseStreamTool**: This may provide specialized functionalities needed for streaming tools, such as buffering or continuous processing capabilities.\\n\\n### Tool Classes\\n\\n- **Tool**: This is likely a more generalized utility for representing a tool within the workbench.\\n- **StreamTool**: Designed for tools that work with data streams, it may encapsulate the logic and necessary configurations to handle streaming data effectively.\\n- **FunctionTool**: This acts as a wrapper for standard functions, turning them into tools that can be used within the workbench framework.\\n\\n### Workbench Components\\n\\n- **Workbench**: This may be the main hub for executing and managing tools, providing infrastructure for input/output as well as result handling.\\n- **StaticWorkbench**: A variant of `Workbench`, potentially designed for static data rather than streaming, giving users a different set of capabilities for non-streaming tasks.\\n- **StaticStreamWorkbench**: Presumably combines aspects of both static and streaming workbenches, perhaps designed for scenarios that require both paradigms.\\n\\n## Result Handling Classes\\n\\n- **ToolResult**: Likely encapsulates the results returned from tool executions, including success/failure statuses and relevant output data.\\n- **TextResultContent**: This class is expected to manage the textual output produced by tools, providing a way to access and format the results.\\n- **ImageResultContent**: Similar to `TextResultContent`, but specifically designed for visual data outputs, allowing users to handle and display images generated as a result of tool execution.\\n\\n## Conclusion\\n\\nThis module provides a structured foundation for tool integration and management within a workflow, facilitating both streaming and static data processing. It abstracts common functionalities through base classes and organizes tool results in a way that's accessible and easy to interact with. Overall, the design suggests a robust toolkit aimed at enhancing productivity in contexts where various tools need to be systematically utilized and managed. This modular approach allows for flexibility in extending and customizing functionality according to user requirements.\", 'filename': 'python/packages/autogen-core/src/autogen_core/tools/__init__.py'}, {'code': False, 'content': '# Code Documentation\\n\\n## Overview\\n\\nThe given code is an import statement from a Python module, which outlines how various classes, types, and functions are structured and made available for use in the current module. This code is organized into several logical sections that include both model client functionalities and type definitions. It utilizes an `__all__` list to explicitly define which components should be accessible when the module is imported.\\n\\n## Module Imports\\n\\nThe code begins by importing several components from internal modules (`._model_client` and `._types`):\\n\\n### From `_model_client`\\n\\n- **ChatCompletionClient**: This likely represents a client that manages interactions with a chat model, possibly facilitating the initiation and handling of chat completions.\\n- **ModelCapabilities**: This class or structure may provide information about the capabilities of different models available through the client.\\n- **ModelFamily**: This could categorize different models under broader classes or groups.\\n- **ModelInfo**: Would likely contain metadata or specifics about a particular model, including its characteristics and performance metrics.\\n- **validate_model_info**: A function designed to validate the provided model information to ensure it meets certain specifications.\\n\\n### From `_types`\\n\\n- **AssistantMessage**: This type probably encapsulates messages generated by an AI assistant during a chat session.\\n- **ChatCompletionTokenLogprob**: This type may handle the token probabilities for generated completions, useful for analyzing responses.\\n- **CreateResult**: Presumably represents the outcomes of a create action concerning chat generation.\\n- **FinishReasons**: Likely enumerates various conditions or reasons for the termination of a chat session or message generation.\\n- **FunctionExecutionResult** and **FunctionExecutionResultMessage**: These would represent results and associated messages from executing certain functions.\\n- **LLMMessage**: This type could encapsulate messages sent or received by a large language model (LLM).\\n- **RequestUsage**: Probably tracks the usage metrics or statistics for requests made to the client or model.\\n- **SystemMessage**: Likely categorizes messages intended for system-level interactions or commands.\\n- **UserMessage**: Represents messages provided by the user interacting with the AI model.\\n- **TopLogprob**: This may represent the top log probabilities associated with token outputs from the model.\\n\\n## `__all__` Declaration\\n\\nThe `__all__` list is defined, which controls what is exported when the module is imported using `from module import *`. By including the various components in this list, the developer is promoting a controlled interface. This enforcement of public API helps in maintaining a clean namespace and provides clear expectations to users of the module.\\n\\n### Listed Components\\n\\nThe components included in the `__all__` list include both classes and function references that provide the primary functionality for users. \\n\\n- Model-related classes and functions (`ModelInfo`, `ModelCapabilities`, etc.) are essential for interacting with the models.\\n- Message types such as `UserMessage`, `AssistantMessage`, and `SystemMessage` form the core of the chat interaction protocols.\\n\\n## Conclusion\\n\\nThis module appears to be part of a larger system designed to facilitate interactions with a chat model, particularly an AI-driven system. The imports suggest an organized architecture that separates model management and message type definitions. The `__all__` declaration indicates a commitment to a logical interface design, improving usability and reducing the potential for conflicts in a larger application.\\n\\nUsers of this module would utilize the imported classes and types for tasks related to chat message construction, model interaction, and information handling, contributing to the development of AI-enhanced conversational applications.', 'filename': 'python/packages/autogen-core/src/autogen_core/models/__init__.py'}, {'code': False, 'content': '# Module Documentation\\n\\nThis module serves as an interface for functionalities related to JSON schema and Pydantic models. It imports specific functions from separate modules to facilitate these capabilities and provides them for external access.\\n\\n## Imports\\n\\n### `schema_to_pydantic_model`\\n\\n- **Source**: Imported from the module `._json_to_pydantic`.\\n- **Purpose**: This function is intended to convert a JSON schema into a Pydantic model. Pydantic is a data validation and settings management library for Python, leveraging Python type annotations.\\n- **Usage**: When invoked, it takes a JSON schema as input and outputs a corresponding Pydantic model, which can be used for data validation and serialization.\\n\\n### `extract_json_from_str`\\n\\n- **Source**: Imported from the module `._load_json`.\\n- **Purpose**: This function is designed to extract JSON data from a string format.\\n- **Usage**: It accepts a string that represents JSON data and returns a Python object. This is useful for converting JSON strings into usable data structures, such as dictionaries or lists.\\n\\n## Exposed Interfaces\\n\\n### `__all__`\\n\\n- **Definition**: The `__all__` variable is defined as a list containing the names of the functions that are intended to be publicly accessible from this module.\\n- **Contents**: \\n  - `\"schema_to_pydantic_model\"`\\n  - `\"extract_json_from_str\"`\\n- **Functionality**: By defining `__all__`, the module specifies which elements can be imported when a user employs the `from module import *` syntax. This helps in maintaining a clean namespace and avoiding unintended access to internal functions.\\n\\n## Summary\\n\\nThis module integrates JSON handling and Pydantic model generation capabilities by importing two specific functions from their respective modules. It sets up a clean interface for users, allowing them to:\\n- Convert JSON schemas into Pydantic models for application development.\\n- Extract JSON data from string representations for manipulation and processing in Python.\\n\\nThe overall architecture promotes modularity and reusability of code, providing key functionalities without exposing inner workings, thus aligning with good software design principles.', 'filename': 'python/packages/autogen-core/src/autogen_core/utils/__init__.py'}]\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 results\n",
    "top_results = results[:5]\n",
    "\n",
    "# Print the subset (if the results are already string/dict)\n",
    "print(top_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d1aa6-c151-4413-b977-ee63a4eb6415",
   "metadata": {},
   "source": [
    "#### VECTOR SEARCH\n",
    "uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e004786-e664-46af-8950-073b8425aa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a21ea7b29154c7195557aa085b502e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c1318726e54d5299b352cf5363f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6302492366d04ba8876768982dda0a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b610db9b37647f883f7ae79a8b57e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2c81f6ee2445c5a66df403d721aaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a684ba35ba6944d28ae10e2b33205f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831ead1-9fe7-4c0f-8324-366df601cab9",
   "metadata": {},
   "source": [
    "The multi-qa-distilbert-cos-v1 model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
    "Other popular models include:\n",
    "* all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
    "* all-mpnet-base-v2 - Higher quality, slower\n",
    "* Check Sentence Transformers documentation for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8143bf-7de2-4c76-86f9-532553d4d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen_data = read_repo_data('microsoft', 'autogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31ac67e-e2d7-4977-81fe-e36aa9ebb30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# Check the keys of the failing record\n",
    "print(autogen_data[2].keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394cb5ca-2623-436d-8f85-31ac9beded24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7fbeb3439ff0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "#autogen_data = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "aut_index = Index(\n",
    "    text_fields=[\"filename\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "aut_index.fit(autogen_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c828fb0a-7dff-447f-9dc5-a368e8249d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = autogen_data[2]\n",
    "text = record['filename'] + ' ' + record['content'] \n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77a8db1-f461-4638-9205-a9d8d2dc929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what checks must be met to run a PR?'\n",
    "v_query = embedding_model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf69888-09fb-4c5c-b2de-73e1b8be44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "346ca284-9b2e-4c8c-8272-414aa5aac7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0018b772a25a4bf798e9755df9fdbeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "aut_embeddings = []\n",
    "\n",
    "for d in tqdm(autogen_data):\n",
    "    text = d['filename'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    aut_embeddings.append(v)\n",
    "\n",
    "aut_embeddings = np.array(aut_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d912c-9238-4701-abd4-953e7791752b",
   "metadata": {},
   "source": [
    "###### vector search usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f03e647-50e0-4197-943c-034e46d71fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x7fbe944a0700>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "aut_vindex = VectorSearch()\n",
    "aut_vindex.fit(aut_embeddings, autogen_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdad5300-8d4a-46f0-b78d-d5cf82b0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what checks must be met to run a PR?'\n",
    "q = embedding_model.encode(query)\n",
    "results = aut_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c867fae3-7392-4daf-bb51-3961e43f52b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in the first document:\n",
      "dict_keys(['content', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# Run this line to see what keys are actually available\n",
    "if autogen_data_chunk:\n",
    "    print(\"Available keys in the first document:\")\n",
    "    print(autogen_data_chunk[0].keys())\n",
    "else:\n",
    "    print(\"autogen_data_chunk is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "500de25d-9730-4a0d-bbaf-4a2e703577ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1913b675c13464a99081cda6b68dba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x7fbe303dfdf0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autogen_embeddings = []\n",
    "\n",
    "for d in tqdm(autogen_data_chunk):\n",
    "    v = embedding_model.encode(d['content'])\n",
    "    autogen_embeddings.append(v)\n",
    "\n",
    "autogen_embeddings = np.array(autogen_embeddings)\n",
    "\n",
    "autogen_vindex = VectorSearch()\n",
    "autogen_vindex.fit(autogen_embeddings, autogen_data_chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ddd3a2-bc82-49b5-ad1c-0be3e260f8f6",
   "metadata": {},
   "source": [
    "#### HYBRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76976855-b25c-4130-a01f-e5c7338f5db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search returned 10 results.\n"
     ]
    }
   ],
   "source": [
    "# 1. Text Search (Keyword Matching) - Uses the text-based index\n",
    "query = 'what checks must be met to run a PR?'\n",
    "text_results = aut_index.search(query, num_results=5) \n",
    "\n",
    "# 2. Vector Search (Semantic Matching) - Uses the vector-based index\n",
    "q = embedding_model.encode(query)\n",
    "# --- FIX IS HERE ---\n",
    "vector_results = autogen_vindex.search(q, num_results=5) \n",
    "\n",
    "# 3. Combine results\n",
    "final_results = text_results + vector_results\n",
    "\n",
    "print(f\"Hybrid search returned {len(final_results)} results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6544a45-e976-4956-80ac-ae4e3e692a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->\\n\\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don\\'t have the access to it, we will shortly find a reviewer and assign them to your PR. -->\\n\\n## Why are these changes needed?\\n\\n<!-- Please give a short summary of the change and the problem this solves. -->\\n\\n## Related issue number\\n\\n<!-- For example: \"Closes #1234\" -->\\n\\n## Checks\\n\\n- [ ] I\\'ve included any doc changes needed for <https://microsoft.github.io/autogen/>. See <https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md> to build and test documentation locally.\\n- [ ] I\\'ve added tests (if relevant) corresponding to the changes introduced in this PR.\\n- [ ] I\\'ve made sure all auto checks have passed.', 'filename': '.github/PULL_REQUEST_TEMPLATE.md'}, {'content': '# Contributing\\n\\nThe project welcomes contributions from developers and organizations worldwide. Our goal is to foster a collaborative and inclusive community where diverse perspectives and expertise can drive innovation and enhance the project\\'s capabilities. Whether you are an individual contributor or represent an organization, we invite you to join us in shaping the future of this project. Possible contributions include but not limited to:\\n\\n- Pushing patches.\\n- Code review of pull requests.\\n- Documentation, examples and test cases.\\n- Readability improvement, e.g., improvement on docstr and comments.\\n- Community participation in [issues](https://github.com/microsoft/autogen/issues), [discussions](https://github.com/microsoft/autogen/discussions), [twitter](https://twitter.com/pyautogen), and [Discord](https://aka.ms/autogen-discord).\\n- Tutorials, blog posts, talks that promote the project.\\n- Sharing application scenarios and/or related research.\\n\\nMost contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\\n\\nIf you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.\\n\\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\n\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\\n\\n## Running CI checks locally\\n\\nIt is important to use `uv` when running CI checks locally as it ensures that the correct dependencies and versions are used.\\n\\nPlease follow the instructions [here](./python/README.md#setup) to get set up.\\n\\nFor common tasks that are helpful during development and run in CI, see [here](./python/README.md#common-tasks).\\n\\n## Roadmap\\n\\nWe use GitHub issues and milestones to track our roadmap. You can view the upcoming milestones [here]([Roadmap Issues](https://aka.ms/autogen-roadmap)).\\n\\n## Versioning\\n\\nThe set of `autogen-*` packages are generally all versioned together. When a change is made to one package, all packages are updated to the same version. This is to ensure that all packages are in sync with each other.\\n\\nWe will update verion numbers according to the following rules:\\n\\n- Increase minor version (0.X.0) upon breaking changes\\n- Increase patch version (0.0.X) upon new features or bug fixes\\n\\n## Release process\\n\\n1. Create a PR that updates the version numbers across the codebase ([example](https://github.com/microsoft/autogen/pull/4359))\\n2. The docs CI will fail for the PR, but this is expected and will be resolved in the next step\\n3. After merging the PR, create and push a tag that corresponds to the new verion. For example, for `0.4.0.dev13`:\\n    - `git tag v0.4.0.dev13 && git push origin v0.4.0.dev13`\\n4. Restart the docs CI by finding the failed [job corresponding to the `push` event](https://github.com/microsoft/autogen/actions/workflows/docs.yml) and restarting all jobs\\n5. Run [this](https://github.com/microsoft/autogen/actions/workflows/single-python-package.yml) workflow for each of the packages that need to be released and get an approval for the release for it to run\\n\\n## Triage process\\n\\nTo help ensure the health of the project and community the AutoGen committers have a weekly triage process to ensure that all issues and pull requests are reviewed and addressed in a timely manner. The following documents the responsibilites while on triage duty:\\n\\n- Issues\\n  - Review all new issues - these will be tagged with [`needs-triage`](https://github.com/microsoft/autogen/issues?q=is%3Aissue%20state%3Aopen%20label%3Aneeds-triage).\\n  - Apply appropriate labels:\\n    - One of `proj-*` labels based on the project the issue is related to\\n    - `documentation`: related to documentation\\n    - `x-lang`: related to cross language functionality\\n    - `dotnet`: related to .NET\\n  - Add the issue to a relevant milestone if necessary\\n  - If you can resolve the issue or reply to the OP please do.\\n  - If you cannot resolve the issue, assign it to the appropriate person.\\n  - If awaiting a reply add the tag `awaiting-op-response` (this will be auto removed when the OP replies).\\n  - Bonus: there is a backlog of old issues that need to be reviewed - if you have time, review these as well and close or refresh as many as you can.\\n- PRs\\n  - The UX on GH flags all recently updated PRs. Draft PRs can be ignored, otherwise review all recently updated PRs.\\n  - If a PR is ready for review and you can provide one please go ahead. If you cant, please assign someone. You can quickly spin up a codespace with the PR to test it out.\\n  - If a PR is needing a reply from the op, please tag it `awaiting-op-response`.\\n  - If a PR is approved and passes CI, its ready to merge, please do so.\\n  - If it looks like there is a possibly transient CI failure, re-run failed jobs.\\n- Discussions\\n  - Look for recently updated discussions and reply as needed or find someone on the team to reply.\\n- Security\\n  - Look through any securty alerts and file issues or dismiss as needed.\\n\\n## Becoming a Reviewer\\n\\nThere is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors.\\n\\n## What makes a good docstring?\\n\\n- Concise and to the point\\n- Describe the expected contract/behavior of the function/class\\n- Describe all parameters, return values, and exceptions\\n- Provide an example if possible\\n\\nFor example, this is the docstring for the [TypeSubscription](https://microsoft.github.io/autogen/dev/reference/python/autogen_core.html#autogen_core.TypeSubscription) class:\\n\\n```python\\n\"\"\"This subscription matches on topics based on a prefix of the type and maps to agents using the source of the topic as the agent key.\\n\\nThis subscription causes each source to have its own agent instance.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from autogen_core import TypePrefixSubscription\\n\\n        subscription = TypePrefixSubscription(topic_type_prefix=\"t1\", agent_type=\"a1\")\\n\\n    In this case:\\n\\n    - A topic_id with type `t1` and source `s1` will be handled by an agent of type `a1` with key `s1`\\n    - A topic_id with type `t1` and source `s2` will be handled by an agent of type `a1` with key `s2`.\\n    - A topic_id with type `t1SUFFIX` and source `s2` will be handled by an agent of type `a1` with key `s2`.\\n\\nArgs:\\n    topic_type_prefix (str): Topic type prefix to match against\\n    agent_type (str): Agent type to handle this subscription\\n\"\"\"\\n```\\n\\n## Docs when adding a new API\\n\\nNow that 0.4.0 is out, we should ensure the docs between versions are easy to navigate. To this end, added or changed APIs should have the following added to their docstrings respectively:\\n\\n```rst\\n.. versionadded:: v0.4.1\\n\\n   Here\\'s a version added message.\\n\\n.. versionchanged:: v0.4.1\\n\\n   Here\\'s a version changed message.\\n```\\n\\nSee [here](https://pydata-sphinx-theme.readthedocs.io/en/stable/examples/kitchen-sink/admonitions.html#versionadded) for how they are rendered.', 'filename': 'CONTRIBUTING.md'}, {'code': True, 'content': 'import argparse\\nimport asyncio\\nimport os\\nimport subprocess\\nimport sys\\nfrom rich.console import Console\\n\\nfrom ._gitty import run_gitty, get_gitty_dir\\nfrom ._db import fetch_and_update_issues\\n\\nconsole = Console()\\n\\ndef check_openai_key() -> None:\\n    \"\"\"Check if OpenAI API key is set in environment variables.\"\"\"\\n    if not os.getenv(\"OPENAI_API_KEY\"):\\n        print(\"Error: OPENAI_API_KEY environment variable is not set.\")\\n        print(\"Please set your OpenAI API key using:\")\\n        print(\"  export OPENAI_API_KEY=\\'your-api-key\\'\")\\n        sys.exit(1)\\n\\n\\ndef check_gh_cli() -> bool:\\n    \"\"\"Check if GitHub CLI is installed and accessible.\"\"\"\\n    try:\\n        subprocess.run([\"gh\", \"--version\"], capture_output=True, check=True)\\n        return True\\n    except (subprocess.CalledProcessError, FileNotFoundError):\\n        print(\"[error]Error: GitHub CLI (gh) is not installed or not found in PATH.[/error]\")\\n        print(\"Please install it from: https://cli.github.com\")\\n        sys.exit(1)\\n\\n\\ndef edit_config_file(file_path: str) -> None:\\n    if not os.path.exists(file_path):\\n        with open(file_path, \"w\") as f:\\n            f.write(\"# Instructions for gitty agents\\\\n\")\\n            f.write(\"# Add your configuration below\\\\n\")\\n    editor = os.getenv(\"EDITOR\", \"vi\")\\n    subprocess.run([editor, file_path])\\n\\n\\ndef main() -> None:\\n    parser = argparse.ArgumentParser(\\n        description=\"Gitty: A GitHub Issue/PR Assistant.\\\\n\\\\n\"\\n        \"This tool fetches GitHub issues or pull requests and uses an AI assistant to generate concise,\\\\n\"\\n        \"technical responses to help make progress on your project. You can specify a repository using --repo\\\\n\"\\n        \"or let the tool auto-detect the repository based on the current directory.\",\\n        epilog=\"Subcommands:\\\\n  issue - Process and respond to GitHub issues\\\\n  pr    - Process and respond to GitHub pull requests\\\\n  local - Edit repo-specific gitty config\\\\n  global- Edit global gitty config\\\\n\\\\n\"\\n        \"Usage examples:\\\\n  gitty issue 123\\\\n  gitty pr 456\\\\n  gitty local\\\\n  gitty global\",\\n        formatter_class=argparse.RawTextHelpFormatter,\\n    )\\n    parser.add_argument(\\n        \"command\", choices=[\"issue\", \"pr\", \"fetch\", \"local\", \"global\"], nargs=\"?\", help=\"Command to execute\"\\n    )\\n    parser.add_argument(\"number\", type=int, nargs=\"?\", help=\"Issue or PR number (if applicable)\")\\n\\n    if len(sys.argv) == 1:\\n        parser.print_help()\\n        sys.exit(0)\\n\\n    args = parser.parse_args()\\n    command = args.command\\n\\n    # Check for gh CLI installation before processing commands that need it\\n    if command in [\"issue\", \"pr\", \"fetch\"]:\\n        check_gh_cli()\\n\\n    # Check for OpenAI API key before processing commands that need it\\n    if command in [\"issue\", \"pr\"]:\\n        check_openai_key()\\n\\n    if command in [\"issue\", \"pr\"]:\\n        # Always auto-detect repository\\n        pipe = subprocess.run(\\n            [\\n                \"gh\",\\n                \"repo\",\\n                \"view\",\\n                \"--json\",\\n                \"owner,name\",\\n                \"-q\",\\n                \\'.owner.login + \"/\" + .name\\',\\n            ],\\n            check=True,\\n            capture_output=True,\\n        )\\n        owner, repo = pipe.stdout.decode().strip().split(\"/\")\\n        number = args.number\\n        if command == \"issue\":\\n            asyncio.run(run_gitty(owner, repo, command, number))\\n        else:\\n            print(f\"Command \\'{command}\\' is not implemented.\")\\n            sys.exit(1)\\n    elif command == \"fetch\":\\n        pipe = subprocess.run(\\n            [\\n                \"gh\",\\n                \"repo\",\\n                \"view\",\\n                \"--json\",\\n                \"owner,name\",\\n                \"-q\",\\n                \\'.owner.login + \"/\" + .name\\',\\n            ],\\n            check=True,\\n            capture_output=True,\\n        )\\n        owner, repo = pipe.stdout.decode().strip().split(\"/\")\\n        gitty_dir = get_gitty_dir()\\n        db_path = os.path.join(gitty_dir, \"issues.db\")\\n        fetch_and_update_issues(owner, repo, db_path)\\n    elif command == \"local\":\\n        gitty_dir = get_gitty_dir()\\n        local_config_path = os.path.join(gitty_dir, \"config\")\\n        edit_config_file(local_config_path)\\n    elif command == \"global\":\\n        global_config_dir = os.path.expanduser(\"~/.gitty\")\\n        os.makedirs(global_config_dir, exist_ok=True)\\n        global_config_path = os.path.join(global_config_dir, \"config\")\\n        edit_config_file(global_config_path)\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'filename': 'python/samples/gitty/src/gitty/__main__.py'}, {'content': '# AutoGen Python Development Guide\\n\\n[![Docs (dev)](https://img.shields.io/badge/Docs-dev-blue)](https://microsoft.github.io/autogen/dev/)\\n[![Docs (latest release)](https://img.shields.io/badge/Docs-latest%20release-blue)](https://microsoft.github.io/autogen/dev/)\\n[![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/)\\n\\nThis directory works as a single `uv` workspace containing all project packages, including:\\n\\n- `packages/autogen-core`: interface definitions and reference implementations of agent runtime, model, tool, workbench, memory, tracing.\\n- `packages/autogen-agentchat`: single and multi-agent workflows built on top of `autogen-core`.\\n- `packages/autogen-ext`: implementations for ecosystem integrations. For example, `autogen-ext[openai]` provides the OpenAI model client.\\n- `packages/autogen-studio`: a web-based IDE for building and running AutoGen agents.\\n\\n## Migrating from 0.2.x?\\n\\nPlease refer to the [migration guide](./migration_guide.md) for how to migrate your code from 0.2.x to 0.4.x.\\n\\n## Quick Start\\n\\n**TL;DR**, run all checks with:\\n\\n```sh\\nuv sync --all-extras\\nsource .venv/bin/activate\\npoe check\\n```\\n\\n## Setup\\n\\n`uv` is a package manager that assists in creating the necessary environment and installing packages to run AutoGen.\\n\\n- [Install `uv`](https://docs.astral.sh/uv/getting-started/installation/).\\n\\nTo upgrade `uv` to the latest version, run:\\n\\n```sh\\nuv self update\\n```\\n\\n## Virtual Environment\\n\\nDuring development, you may need to test changes made to any of the packages.\\\\\\nTo do so, create a virtual environment where the AutoGen packages are installed based on the current state of the directory.\\\\\\nRun the following commands at the root level of the Python directory:\\n\\n```sh\\nuv sync --all-extras\\nsource .venv/bin/activate\\n```\\n\\n- `uv sync --all-extras` will create a `.venv` directory at the current level and install packages from the current directory along with any other dependencies. The `all-extras` flag adds optional dependencies.\\n- `source .venv/bin/activate` activates the virtual environment.\\n\\n## Common Tasks\\n\\nTo create a pull request (PR), ensure the following checks are met. You can run each check individually:\\n\\n- Format: `poe format`\\n- Lint: `poe lint`\\n- Test: `poe test`\\n- Mypy: `poe mypy`\\n- Pyright: `poe pyright`\\n- Build docs: `poe docs-build`\\n- Check docs: `poe docs-check`\\n- Clean docs: `poe docs-clean`\\n- Check code blocks in API references: `poe docs-check-examples`\\n- Auto rebuild+serve docs: `poe docs-serve`\\n- Check samples in `python/samples`: `poe samples-code-check`\\n  Alternatively, you can run all the checks with:\\n- `poe check`\\n\\n> [!NOTE]\\n> These need to be run in the virtual environment.\\n\\n## Syncing Dependencies\\n\\nWhen you pull new changes, you may need to update the dependencies.\\nTo do so, first make sure you are in the virtual environment, and then in the `python` directory, run:\\n\\n```sh\\nuv sync --all-extras\\n```\\n\\nThis will update the dependencies in the virtual environment.\\n\\n## Building Documentation\\n\\nThe documentation source directory is located at `docs/src/`.\\n\\nTo build the documentation, run this from the root of the Python directory:\\n\\n```sh\\npoe docs-build\\n```\\n\\nTo serve the documentation locally, run:\\n\\n```sh\\npoe docs-serve\\n```\\n\\nWhen you make changes to the doc strings or add new modules, you may need to\\nrefresh the API references in the documentation by first cleaning the docs and\\nthen building them again:\\n\\n```sh\\npoe docs-clean # This will remove the build directory and the reference directory\\npoe docs-build # This will rebuild the documentation from scratch\\n```\\n\\n## Writing Documentation\\n\\nWhen you add a new public class or function, you should always add a docstring\\nto it. The docstring should follow the\\n[Google style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) layout\\nand the Sphinx RST format for Python docstrings.\\n\\nThe docstring for a public class or function should include:\\n\\n- A short description of the class or function at the beginning immediately after the `\"\"\"`.\\n- A longer description if necessary, explaining the purpose and usage.\\n- A list of arguments with their types and descriptions, using the `Args` section.\\n  Each argument should be listed with its name, type, and a brief description.\\n- A description of the return value and its type, using the `Returns` section.\\n  If the function does not return anything, you can omit this section.\\n- A list of exceptions that the function may raise, with descriptions,\\n  using the `Raises` section. This is optional but recommended if the function can raise exceptions that users should be aware of.\\n- Examples of how to use the class or function, using the `Examples` section,\\n  and formatted using `.. code-block:: python` directive. Optionally, also include the output of the example using\\n  `.. code-block:: text` directive.\\n\\nHere is an example of a docstring for `McpWorkbench` class:\\n\\n```python\\nclass McpWorkbench(Workbench, Component[McpWorkbenchConfig]):\\n    \"\"\"A workbench that wraps an MCP server and provides an interface\\n    to list and call tools provided by the server.\\n\\n    This workbench should be used as a context manager to ensure proper\\n    initialization and cleanup of the underlying MCP session.\\n\\n    Args:\\n        server_params (McpServerParams): The parameters to connect to the MCP server.\\n            This can be either a :class:`StdioServerParams` or :class:`SseServerParams`.\\n        tool_overrides (Optional[Dict[str, ToolOverride]]): Optional mapping of original tool\\n            names to override configurations for name and/or description. This allows\\n            customizing how server tools appear to consumers while maintaining the underlying\\n            tool functionality.\\n\\n    Raises:\\n        ValueError: If there are conflicts in tool override names.\\n\\n    Examples:\\n\\n        Here is a simple example of how to use the workbench with a `mcp-server-fetch` server:\\n\\n        .. code-block:: python\\n\\n            import asyncio\\n\\n            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\\n\\n\\n            async def main() -> None:\\n                params = StdioServerParams(\\n                    command=\"uvx\",\\n                    args=[\"mcp-server-fetch\"],\\n                    read_timeout_seconds=60,\\n                )\\n\\n                # You can also use `start()` and `stop()` to manage the session.\\n                async with McpWorkbench(server_params=params) as workbench:\\n                    tools = await workbench.list_tools()\\n                    print(tools)\\n                    result = await workbench.call_tool(tools[0][\"name\"], {\"url\": \"https://github.com/\"})\\n                    print(result)\\n\\n\\n            asyncio.run(main())\\n```\\n\\nThe code blocks with `.. code-block:: python` is checked by the `docs-check-examples` task using Pyright,\\nso make sure the code is valid. Running the code as a script and checking it using `pyright`\\nis a good way to ensure the code examples are correct.\\n\\nWhen you reference a class, method, or function in the docstring, you should always\\nuse the `:class:`, `:meth:`, or `:func:` directive to create a link to the class or function.\\nAlways use the fully qualified name of the class or function, including the package name, but\\nprefix it with a `~` for shorter rendering in the documentation.\\nFor example, if you are referencing the `AssistantAgent` class in the `autogen-agentchat` package,\\nyou should write it as `:class:~autogen_agentchat.AssistantAgent`.\\n\\nFor a public data class, including those that are Pydantic models, you should also include docstrings\\nfor each field in the class.\\n\\n## Writing Tests\\n\\nWhen you add a new public class or function, you should also always add tests for it.\\nWe track test coverage and aim for not reducing the coverage percentage with new changes.\\n\\nWe use `pytest` for testing, and you should always use fixtures to set up the test dependencies.\\n\\nUse mock objects to simulate dependencies and avoid making real API calls or database queries in tests.\\nSee existing tests for examples of how to use fixtures and mocks.\\n\\nFor model clients, use `autogen_ext.models.replay.ReplayChatCompletionClient` as a\\ndrop-in replacement for the model client to simulate responses without making real API calls.\\n\\nWhen certain tests requires interaction with actual model APIs or other external services,\\nyou should configure the tests to be skipped if the required services are not available.\\nFor example, if you are testing a model client that requires an OpenAI API key,\\nyou can use the `pytest.mark.skipif` decorator to skip the test if the environment variable for the API key is not set.\\n\\n## Creating a New Package\\n\\nTo create a new package, similar to `autogen-core` or `autogen-chat`, use the following:\\n\\n```sh\\nuv sync --python 3.12\\nsource .venv/bin/activate\\ncookiecutter ./templates/new-package/\\n```', 'filename': 'python/README.md'}, {'content': \"## AutoGen FAQs\\n\\n### What is AutoGen 0.4?\\n\\nAutoGen v0.4 is a rewrite of AutoGen from the ground up to create a more robust,\\nscalable, easier to use, cross-language library for building AI Agents.\\nSome key features include asynchronous messaging, support for scalable distributed agents,\\nmodular extensible design (bring your own agents, implement behaviors however you like),\\ncross-language support, improved observability, and full typing integration.\\nIt is a breaking change.\\n\\n### Why these changes?\\n\\nWe listened to our AutoGen users, learned from what was working, and adapted to fix what wasn't.\\nWe brought together wide-ranging teams working on many different types of AI Agents\\nand collaborated to design an improved framework with a more flexible\\nprogramming model and better scalability.\\n\\n### Is this project still maintained?\\n\\nWe want to reaffirm our commitment to supporting both the original version of AutoGen (0.2) and the redesign (0.4). AutoGen 0.4 is still work-in-progress, and we shared the code now to build with the community. There are no plans to deprecate the original AutoGen anytime soon, and both versions will be actively maintained.\\n\\n### Who should use it 0.4?\\n\\nThis code is still experimental, so expect changes and bugs while we work towards a stable 0.4 release. We encourage early adopters to\\ntry it out, give us feedback, and contribute.\\nFor those looking for a stable version we recommend to continue using 0.2\\n\\n### I'm using AutoGen 0.2, should I upgrade?\\n\\nIf you consider yourself an early adopter, you are comfortable making some\\nchanges to your code, and are willing to try it out, then yes.\\n\\n### How do I still use AutoGen 0.2?\\n\\nAutoGen 0.2 can be installed with:\\n\\n```sh\\npip install autogen-agentchat~=0.2\\n```\\n\\n### Will AutoGen Studio be supported in 0.4?\\n\\nYes, this is on the [roadmap](#roadmap).\\nOur current plan is to enable an implementation of AutoGen Studio\\non the AgentChat high level API which implements a set of agent functionalities\\n(agents, teams, etc).\\n\\n### How do I migrate?\\n\\nFor users familiar with AutoGen, the AgentChat library in 0.4 provides similar concepts.\\nWe are working on a migration guide.\\n\\n### Is 0.4 done?\\n\\nWe are still actively developing AutoGen 0.4. One exciting new feature is the emergence of new SDKs for .NET. The python SDKs are further ahead at this time but our goal is to achieve parity. We aim to add additional languages in future releases.\\n\\n### What is happening next? When will this release be ready?\\n\\nWe are still working on improving the documentation, samples, and enhancing the code. We are hoping to release before the end of the year when things are ready.\\n\\n### What is the history of this project?\\n\\nThe rearchitecture of the framework started with multiple Microsoft teams coming together\\nto address the gaps and learnings from AutoGen 0.2 - merging ideas from several predecessor projects.\\nThe team worked on this internally for some time to ensure alignment before moving work back to the open in October 2024.\\n\\n### What is the official channel for support?\\n\\nUse GitHub [Issues](https://github.com/microsoft/autogen/issues) for bug reports and feature requests.\\nUse GitHub [Discussions](https://github.com/microsoft/autogen/discussions) for general questions and discussions.\\n\\n### Do you use Discord for communications?\\n\\nWe are unable to use the old Discord for project discussions, many of the maintainers no longer have viewing or posting rights there. Therefore, we request that all discussions take place on <https://github.com/microsoft/autogen/discussions/>  or the [new discord server](https://aka.ms/autogen-discord).\\n\\n### What about forks?\\n\\n<https://github.com/microsoft/autogen/> remains the only official repo for development and support of AutoGen.\\nWe are aware that there are thousands of forks of AutoGen, including many for personal development and startups building with or on top of the library. We are not involved with any of these forks and are not aware of any plans related to them.\\n\\n### What is the status of the license and open source?\\n\\nOur project remains fully open-source and accessible to everyone. We understand that some forks use different licenses to align with different interests. We will continue to use the most permissive license (MIT) for the project.\\n\\n### Can you clarify the current state of the packages?\\n\\nCurrently, we are unable to make releases to the `pyautogen` package via Pypi due to a change to package ownership that was done without our involvement. Additionally, we are moving to using multiple packages to align with the new design. Please see details [here](https://microsoft.github.io/autogen/dev/packages/index.html).\\n\\n### Can I still be involved?\\n\\nWe are grateful to all the contributors to AutoGen 0.2 and we look forward to continuing to collaborate with everyone in the AutoGen community.\", 'filename': 'FAQ.md'}]\n"
     ]
    }
   ],
   "source": [
    "final_results = final_results[:5]\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbdb17-f207-4cf8-b574-7c7b82de8472",
   "metadata": {},
   "source": [
    "#### PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42cc79e5-1d51-4ddc-a96a-d65865da575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return aut_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return aut_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79968293-27f2-41e1-a70e-a21f2ad23ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af941d0f-6e60-457c-a76f-473d52ad8d75",
   "metadata": {},
   "source": [
    "### Day 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8b152-ca19-4a16-ae2c-259744a6009d",
   "metadata": {},
   "source": [
    "#### No Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89046a93-2928-4fbf-9a6f-91c3d17a5a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a pull request (PR), several checks and criteria typically need to be met. These can vary by organization, workflow, or version control system, but common checks include:\n",
      "\n",
      "1. **Code Review**: At least one (or more) reviewers must approve the changes in the PR.\n",
      "\n",
      "2. **Continuous Integration (CI) Checks**: Automated tests must pass, including:\n",
      "   - Unit tests\n",
      "   - Integration tests\n",
      "   - End-to-end tests\n",
      "\n",
      "3. **Static Code Analysis**: Code quality checks must be conducted, including:\n",
      "   - Linter checks\n",
      "   - Security scans\n",
      "   - Code formatting\n",
      "\n",
      "4. **Branch Policies**: The PR needs to comply with branch protection rules, such as:\n",
      "   - No direct commits to the main branch\n",
      "   - Mandatory PR for merging changes\n",
      "\n",
      "5. **Conflict Resolution**: The PR must be free of merge conflicts with the target branch.\n",
      "\n",
      "6. **Documentation Updates**: If changes require updates to documentation, those should be included or noted.\n",
      "\n",
      "7. **Commit Standards**: Commits should follow a specific message format or squashing guidelines if required.\n",
      "\n",
      "8. **Project Management Integration**: If using tools like Jira, Trello, etc., related issues should be linked or referenced in the PR.\n",
      "\n",
      "9. **Deployment Readiness**: In some workflows, the code might need to meet deployment criteria (e.g., staging testing).\n",
      "\n",
      "10. **Approval from Stakeholders**: Sometimes, additional approvals from project leads or product owners may be necessary.\n",
      "\n",
      "Ensuring these checks are in place helps maintain code quality and ensure smooth collaboration among team members.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This line loads the variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now the OpenAI client will be able to find the key in your environment\n",
    "openai_client = OpenAI()\n",
    "\n",
    "#user_prompt = \"I just discovered the course, can I join now?\"\n",
    "user_prompt = 'what checks must be met to run a PR?'\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545bf9a-2774-4f16-bdea-6103f2f9198b",
   "metadata": {},
   "source": [
    "#### Function Calling with OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b17398-82a6-42f4-8380-edf9d3798801",
   "metadata": {},
   "source": [
    "Cannot pass this function to openai llm hence we describe the capability of the function using json\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "aut_index = Index(\n",
    "    text_fields=[\"filename\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "def text_search(query):\n",
    "    return aut_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a69b03-4dbb-4a75-b13e-c83ce18d1e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"query\":\"PR checks AutoGen repository\"}', call_id='call_ckhE8MhfepyMexexjHHVldQ0', name='text_search', type='function_call', id='fc_68d8699d5e10819f96df2ef3cadf8aa30e6f59c642a49773', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the autogen github repository\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the autogen github repository.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "#question = 'what checks must be met to run a PR?'\n",
    "question = 'what checks must be met to run a PR in the **AutoGen** repository?'\n",
    "# OR\n",
    "# question3 = 'How do I check the build status for a PR in AutoGen?'\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool] # TOOL\n",
    ")\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d91b7-0cb6-46a6-8eac-9fb6ad3e5041",
   "metadata": {},
   "source": [
    "invoke the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169abc27-c4f3-45d3-99af-56420b137224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from minsearch import Index\n",
    "\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "aut_index = Index(\n",
    "    text_fields=[\"filename\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "def text_search(query):\n",
    "    return aut_index.search(query, num_results=5)\n",
    "    \n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05994e7-b88a-423f-b201-c6078cd48678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems I couldn't find specific information about the checks required to run a pull request (PR) in the AutoGen repository. However, common checks for PRs in many repositories typically include:\n",
      "\n",
      "1. **Code Review**: Ensuring that other developers review the code changes.\n",
      "2. **Automated Tests**: Running unit tests and integration tests to verify that the changes do not break existing functionality.\n",
      "3. **Linting**: Code style checks to enforce style guidelines and best practices.\n",
      "4. **CI/CD Pipeline**: Successful completion of continuous integration and deployment processes.\n",
      "5. **Documentation**: Updates to any relevant documentation that may be affected by the changes.\n",
      "6. **Issue Link**: Reference to a related issue in the repository to provide context for the changes.\n",
      "\n",
      "For accurate and detailed information, it's best to check the AutoGen repository's CONTRIBUTING.md or relevant documentation. Would you like help finding or reviewing those specific files?\n"
     ]
    }
   ],
   "source": [
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d26bc2e-48fc-4e86-b01e-48bcd0cb7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7600787-70f3-48c4-b3e1-7e03419f7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return aut_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f504e205-51ca-441b-a123-0aa6ec457aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"aut_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b030d5a5-3781-4a24-bff0-cf4887be0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'what checks must be met to run a PR?'\n",
    "question = 'what checks must be met to run a PR in the **AutoGen** repository?'\n",
    "# OR\n",
    "# question3 = 'How do I check the build status for a PR in AutoGen?'\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27452090-9686-4580-a725-25d6196319d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output=\"It seems I wasn't able to find specific information regarding the checks that must be met to run a pull request (PR) in the AutoGen repository through a search in the available resources. \\n\\nFor the most accurate and up-to-date information, I recommend checking the AutoGen repository directly on GitHub. Typically, PRs may have checks related to:\\n\\n1. **Code Review**: A certain number of approvals from maintainers or team members.\\n2. **Continuous Integration**: Automated tests must pass (e.g., unit tests, integration tests).\\n3. **Code Style**: Adherence to coding standards and formatting checks.\\n4. **Documentation**: Any changes may require updates to documentation.\\n5. **Issue Linking**: Linked issues or features that the PR addresses.\\n\\nIf specific checks are implemented in the AutoGen repository, they would usually be detailed in the repository's contribution guidelines or README file. Would you like assistance with something else?\")\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79766f87-b1f9-4f40-906d-e67361ebab50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='what checks must be met to run a PR in the **AutoGen** repository?', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 40, 445625, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen repository PR checks\"}', tool_call_id='call_kHtsYNArGwL1xkIY0Z786BQ7')], usage=RequestUsage(input_tokens=152, output_tokens=18, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 42, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7KxP3AjxZAsag2R6O34epYHkzC', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_kHtsYNArGwL1xkIY0Z786BQ7', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 42, 464252, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen repository pull request requirements\"}', tool_call_id='call_3RkbPgCoBEd7TwnsD0ZWSSD4')], usage=RequestUsage(input_tokens=179, output_tokens=19, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 43, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7LyGCCnaFIlxJswrLFyzP5kQ2x', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_3RkbPgCoBEd7TwnsD0ZWSSD4', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 43, 337090, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen repository PR check requirements\"}', tool_call_id='call_VEgNyouCTLN6NciFfz5HExEz')], usage=RequestUsage(input_tokens=207, output_tokens=19, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 44, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7MToSG0fZAoDM9CG000l1bAqbU', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_VEgNyouCTLN6NciFfz5HExEz', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 44, 233554, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"required checks for pull requests AutoGen repository\"}', tool_call_id='call_AuKfHs21JKtL5G2tWM5CgRQF')], usage=RequestUsage(input_tokens=235, output_tokens=21, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 46, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7OKxGtOR1fJtrbOe5pZddgY3e7', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_AuKfHs21JKtL5G2tWM5CgRQF', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 45, 616626, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen repository contribution guidelines\"}', tool_call_id='call_W0wzi6CNfTvnKuyvaQmmrTCS')], usage=RequestUsage(input_tokens=265, output_tokens=18, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 46, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7O3N944sQNZvfFMA4HrOWV8If2', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_W0wzi6CNfTvnKuyvaQmmrTCS', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 46, 403968, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen repository PR validation checks\"}', tool_call_id='call_V9oGwNFlbRcvGXsDakYQlUJw')], usage=RequestUsage(input_tokens=292, output_tokens=19, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 48, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7QyFOMe4Euga4koCNiLtH0kARm', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_V9oGwNFlbRcvGXsDakYQlUJw', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 47, 905281, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"AutoGen GitHub repository pull request checks\"}', tool_call_id='call_hUOuiaQNamlIceLCPzmeaDGm')], usage=RequestUsage(input_tokens=320, output_tokens=21, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 49, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CKY7RXixzs4miVaQgEVTeQyvpHm9u', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[], tool_call_id='call_hUOuiaQNamlIceLCPzmeaDGm', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 49, 767415, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\"),\n",
       " ModelResponse(parts=[TextPart(content=\"It seems I wasn't able to find specific information regarding the checks that must be met to run a pull request (PR) in the AutoGen repository through a search in the available resources. \\n\\nFor the most accurate and up-to-date information, I recommend checking the AutoGen repository directly on GitHub. Typically, PRs may have checks related to:\\n\\n1. **Code Review**: A certain number of approvals from maintainers or team members.\\n2. **Continuous Integration**: Automated tests must pass (e.g., unit tests, integration tests).\\n3. **Code Style**: Adherence to coding standards and formatting checks.\\n4. **Documentation**: Any changes may require updates to documentation.\\n5. **Issue Linking**: Linked issues or features that the PR addresses.\\n\\nIf specific checks are implemented in the AutoGen repository, they would usually be detailed in the repository's contribution guidelines or README file. Would you like assistance with something else?\")], usage=RequestUsage(input_tokens=350, output_tokens=191, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 27, 22, 55, 50, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CKY7SA5XR19BwkTksCepvHFIJEAC9', finish_reason='stop')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1232244-5891-49ed-a120-0c29171d0b09",
   "metadata": {},
   "source": [
    "#### HYBRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73c7c73f-490f-4aaf-9776-fb03ed6e9710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceead1b51dbd46c49dfbff424800d8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output='To run a pull request (PR) in the AutoGen repository, the following checks must be met:\\n\\n1. **Documentation Changes**: You should include any necessary documentation changes for the project. Reference the [documentation](https://microsoft.github.io/autogen/) and ensure you can build and test it locally as described in the [CONTRIBUTING guide](https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md).\\n\\n2. **Tests**: If your changes introduce any new functionality or modify existing behavior, you should add corresponding tests.\\n\\n3. **All Auto Checks**: You must make sure that all automated checks have passed before the PR can be merged.\\n\\nThese checks ensure a certain level of quality and maintainability in the codebase before changes are integrated.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from minsearch import Index, VectorSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # Import tqdm for progress bar\n",
    "\n",
    "# Assuming read_repo_data is defined elsewhere\n",
    "# (Note: This is the data loading step)\n",
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"Chunks text into overlapping sections.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk_content = seq[i:i+size]\n",
    "        # Stores the chunked text under the 'content' key\n",
    "        result.append({'start': i, 'content': chunk_content}) \n",
    "        if i + size >= n:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# --- Data Preparation and Chunking ---\n",
    "autogen_data_chunks = []\n",
    "\n",
    "for doc in autogen_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        # Merge chunk content with document metadata (filename, etc.)\n",
    "        chunk.update(doc_copy) \n",
    "    autogen_data_chunks.extend(chunks)\n",
    "\n",
    "# --- 1. Text Search Index (minsearch Index) ---\n",
    "# FIX: Define and fit the text index\n",
    "aut_index = Index(\n",
    "    text_fields=[\"filename\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "aut_index.fit(autogen_data_chunks)\n",
    "\n",
    "# --- 2. Vector Search Index (minsearch VectorSearch) ---\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "autogen_embeddings = []\n",
    "\n",
    "# FIX: Use the correct list name: autogen_data_chunks\n",
    "for d in tqdm(autogen_data_chunks):\n",
    "    # d['content'] is the correct key holding the chunked text\n",
    "    v = embedding_model.encode(d['content'])\n",
    "    autogen_embeddings.append(v)\n",
    "\n",
    "autogen_embeddings = np.array(autogen_embeddings)\n",
    "\n",
    "autogen_vindex = VectorSearch()\n",
    "# FIX: Use the correct list name: autogen_data_chunks\n",
    "autogen_vindex.fit(autogen_embeddings, autogen_data_chunks)\n",
    "\n",
    "\n",
    "# --- HYBRID SEARCH FUNCTION (The Agent's Tool) ---\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Performs a Hybrid Search combining Text (Keyword) and Vector (Semantic) search.\n",
    "    This is the function the agent will call.\n",
    "    \"\"\"\n",
    "    # 1. Text Search (Keyword matching)\n",
    "    text_results = aut_index.search(query, num_results=5)\n",
    "    \n",
    "    # 2. Vector Search (Semantic matching)\n",
    "    q_vector = embedding_model.encode(query)\n",
    "    vector_results = autogen_vindex.search(q_vector, num_results=5)\n",
    "    \n",
    "    # 3. Combine and Deduplicate Results\n",
    "    seen_filenames = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        # Deduplicate based on 'filename'\n",
    "        if result['filename'] not in seen_filenames:\n",
    "            seen_filenames.add(result['filename'])\n",
    "            # Return only the essential information for the LLM\n",
    "            combined_results.append({\n",
    "                \"filename\": result['filename'],\n",
    "                \"content\": result['content']\n",
    "            })\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "\n",
    "# --- AGENT SETUP AND EXECUTION ---\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for the AutoGen course.\n",
    "Your sole purpose is to answer questions using the 'hybrid_search' tool, which queries the official AutoGen documentation.\n",
    "Always use the tool before answering.\n",
    "\"\"\"\n",
    "\n",
    "# Assuming pydantic_ai and Agent are defined/imported correctly\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"autogen_agent\",\n",
    "    instructions=system_prompt,\n",
    "    # FIX: Pass the HYBRID search function as the tool\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# Use the specific, enforced question to trigger the tool call\n",
    "question = 'what checks must be met to run a PR in the **AutoGen** repository?'\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed337821-cb09-42dd-b6ea-2e13b4d7c798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
