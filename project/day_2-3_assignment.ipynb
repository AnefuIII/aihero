{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown and code files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\" \n",
    "    url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/main.zip'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc2c3b1-5bb7-4dc1-8524-8474d67432f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 749 files from microsoft/autogen.\n"
     ]
    }
   ],
   "source": [
    "autogen_data = read_repo_data('microsoft', 'autogen')\n",
    "\n",
    "# This will print the number of files you successfully processed\n",
    "print(f'Processed {len(autogen_data)} files from microsoft/autogen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for record in autogen_data:\n",
    "    index[record['filename']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {
    "_sphinx_cell_id": "d4ce051b-aaae-4725-9eef-869cf50e26fb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "exporter = MarkdownExporter()\n",
    "exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "def format_notebook_as_md(raw_notebook: str) -> str:\n",
    "    nb_parsed = nbformat.reads(\n",
    "        raw_notebook,\n",
    "        as_version=nbformat.NO_CONVERT,\n",
    "    )\n",
    "    md_body, _ = exporter.from_notebook_node(nb_parsed)\n",
    "    return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8d3523-688c-4724-8b6c-47247388b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    if not text.startswith(\"```\"):\n",
    "        return text\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This line loads the variables from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now the OpenAI client will be able to find the key in your environment\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# You can also explicitly pass the key if you prefer\n",
    "# import os\n",
    "# openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_editing_instructions = \"\"\"\n",
    "You're a professional coding editor.\n",
    "\n",
    "You are given a Markdown file that was converted from a Jupyter notebook.  \n",
    "The file already contains code blocks and inline comments.  \n",
    "\n",
    "Your task:\n",
    "\n",
    "- Turn it into clear, well-structured documentation.  \n",
    "- Add section headers (##) where appropriate. Keep sections relatively large (8-10 paragraphs and code blocks)\n",
    "- Add concise, high-level explanations for each code block.  \n",
    "- Summarize what the code is doing without being overly verbose.  \n",
    "- Keep the formatting in Markdown.\n",
    "- Aim for a balance: clear enough to guide someone new, but not overloaded with detail. \n",
    "\n",
    "Output the improved Markdown file with the new documentation.\n",
    "\"\"\".strip()\n",
    "\n",
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, open the file and read its content\n",
    "# with open('data-processing-code.ipynb', 'r', encoding='utf-8') as f:\n",
    "#     raw_notebook_content = f.read()\n",
    "\n",
    "# result = llm(notebook_editing_instructions, md_body)\n",
    "# print(result)\n",
    "# #result = llm(system_prompt, md_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a456b4aa-40f2-428c-973c-4d40f0005213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0f975c-f603-4a3b-9a32-c261a44d1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 49 jupyter notebooks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38326f6876044754b80642b625da26a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipynb_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') == True and record['filename'].endswith('.ipynb'):\n",
    "        ipynb_data.append(record)\n",
    "\n",
    "\n",
    "print(f'processing {len(ipynb_data)} jupyter notebooks...')\n",
    "\n",
    "for record in tqdm(ipynb_data):\n",
    "    md_body = format_notebook_as_md(record['content'])\n",
    "    new_content = llm(notebook_editing_instructions, md_body)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d791ab0-a49b-4439-a143-bd3e8c38067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941e67d0-9260-48ca-bcdf-8abb6401c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 539 code files...\n"
     ]
    }
   ],
   "source": [
    "code_data = []\n",
    "\n",
    "for record in autogen_data:\n",
    "    if record.get('code') != True:\n",
    "        continue\n",
    "\n",
    "    path = record['filename']\n",
    "    ext = path.split('.')[-1]\n",
    "\n",
    "    if ext not in code_extensions:\n",
    "        continue\n",
    "\n",
    "    if ext == 'ipynb':\n",
    "        continue\n",
    "\n",
    "    # print(path)\n",
    "    code_data.append(record)\n",
    "\n",
    "print(f'processing {len(code_data)} code files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5755e9-1f70-4510-b75c-6b952ad303fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb84bc7c21b4d61b42c54699dd38536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(code_data):\n",
    "    code = record['content']\n",
    "\n",
    "    new_content = llm(code_doc_instructions, code)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d50b9af-207b-421c-a567-3aacc0bfacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5aeaab-dc9c-4b1d-a00b-ce29bbf9b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da555e70-19ba-4996-9f69-9cfd18a477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/autogen_data_processed.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(autogen_data, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8984a6-78ac-4c05-b065-76f7dc910382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"<!-- Thank you for your contribution! Please review https://microsoft.github.io/autogen/docs/Contribute before opening a pull request. -->\\n\\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\\n\\n## Why are these changes needed?\\n\\n<!-- Please give a short summary of the change and the problem this solves. -->\\n\\n## Related issue number\\n\\n<!-- For example: \\\"Closes #1234\\\" -->\\n\\n## Checks\\n\\n- [ ] I've included any doc changes needed for <https://microsoft.github.io/autogen/>. See <https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md> to build and test documentation locally.\\n- [ ] I've added tests (if relevant) corresponding to the changes introduced in this PR.\\n- [ ] I've made sure all auto checks have passed.\",\n",
      "    \"filename\": \".github/PULL_REQUEST_TEMPLATE.md\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"# AutoGen Multi-Agent AI Framework\\n\\nAutoGen is a multi-language framework for creating AI agents that can act autonomously or work alongside humans. The project has separate Python and .NET implementations with their own development workflows.\\n\\nAlways reference these instructions first and fallback to search or bash commands only when you encounter unexpected information that does not match the info here.\\n\\n## Working Effectively\\n\\n### Prerequisites and Environment Setup\\n\\n**CRITICAL**: Install both .NET 8.0 and 9.0 for full compatibility:\\n- Install uv package manager: `python3 -m pip install uv` \\n- Install .NET 9.0 SDK: `wget https://dot.net/v1/dotnet-install.sh && chmod +x dotnet-install.sh && ./dotnet-install.sh --channel 9.0`\\n- Install .NET 8.0 runtime: `./dotnet-install.sh --channel 8.0 --runtime dotnet && ./dotnet-install.sh --channel 8.0 --runtime aspnetcore`\\n- Update PATH: `export PATH=\\\"$HOME/.dotnet:$PATH\\\"`\\n\\n### Python Development Workflow\\n\\n**Bootstrap and build Python environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nuv sync --all-extras  # NEVER CANCEL: Takes 2 minutes. Set timeout to 300+ seconds.\\nsource .venv/bin/activate\\n```\\n\\n**Validate Python development:**\\n```bash\\n# Quick validation (under 1 second each)\\npoe format  # Code formatting\\npoe lint    # Linting with ruff\\n\\n# Type checking - NEVER CANCEL these commands\\npoe mypy     # Takes 6 minutes. Set timeout to 600+ seconds.\\npoe pyright  # Takes 41 seconds. Set timeout to 120+ seconds.\\n\\n# Individual package testing (core package example)\\npoe --directory ./packages/autogen-core test  # Takes 10 seconds. Set timeout to 60+ seconds.\\n\\n# Documentation - NEVER CANCEL\\npoe docs-build  # Takes 1 minute 16 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: Python environment setup takes 2 minutes minimum\\n- **NEVER CANCEL**: mypy type checking takes 6 minutes \\n- **NEVER CANCEL**: Documentation build takes 1+ minutes\\n- Format/lint tasks complete in under 1 second\\n- Individual package tests typically complete in 10-60 seconds\\n\\n### .NET Development Workflow\\n\\n**Bootstrap and build .NET environment:**\\n```bash\\ncd /home/runner/work/autogen/autogen/dotnet\\nexport PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet restore  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\ndotnet build --configuration Release  # NEVER CANCEL: Takes 53 seconds. Set timeout to 300+ seconds.\\n```\\n\\n**Validate .NET development:**\\n```bash\\n# Unit tests - NEVER CANCEL\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Takes 25 seconds. Set timeout to 120+ seconds.\\n\\n# Format check (if build fails) \\ndotnet format --verify-no-changes\\n\\n# Run samples\\ncd samples/Hello\\ndotnet run\\n```\\n\\n**CRITICAL TIMING EXPECTATIONS:**\\n- **NEVER CANCEL**: .NET restore takes 53 seconds minimum\\n- **NEVER CANCEL**: .NET build takes 53 seconds minimum  \\n- **NEVER CANCEL**: .NET unit tests take 25 seconds minimum\\n- All build and test commands require appropriate timeouts\\n\\n### Complete Validation Workflow\\n\\n**Run full check suite (Python):**\\n```bash\\ncd /home/runner/work/autogen/autogen/python\\nsource .venv/bin/activate\\npoe check  # NEVER CANCEL: Runs all checks. Takes 7+ minutes total. Set timeout to 900+ seconds.\\n```\\n\\n## Validation Scenarios\\n\\n### Manual Validation Requirements\\nAlways manually validate changes by running complete user scenarios after making modifications:\\n\\n**Python validation scenarios:**\\n1. **Import test**: Verify core imports work:\\n   ```python\\n   from autogen_agentchat.agents import AssistantAgent\\n   from autogen_core import AgentRuntime\\n   from autogen_ext.models.openai import OpenAIChatCompletionClient\\n   ```\\n\\n2. **AutoGen Studio test**: Verify web interface can start:\\n   ```bash\\n   autogenstudio ui --help  # Should show help without errors\\n   ```\\n\\n3. **Documentation test**: Build and verify docs generate without errors:\\n   ```bash\\n   poe docs-build && ls docs/build/index.html\\n   ```\\n\\n**.NET validation scenarios:**\\n1. **Sample execution**: Run Hello sample to verify runtime works:\\n   ```bash\\n   cd dotnet/samples/Hello && dotnet run --help\\n   ```\\n\\n2. **Build validation**: Ensure all projects compile:\\n   ```bash\\n   dotnet build --configuration Release --no-restore\\n   ```\\n\\n3. **Test execution**: Run unit tests to verify functionality:\\n   ```bash\\n   dotnet test --filter \\\"Category=UnitV2\\\" --configuration Release --no-build\\n   ```\\n\\n## Common Issues and Workarounds\\n\\n### Network-Related Issues\\n- **Python tests may fail** with network errors (tiktoken downloads, Playwright browser downloads) in sandboxed environments - this is expected\\n- **Documentation intersphinx warnings** due to inability to reach external documentation sites - this is expected\\n- **Individual package tests work better** than full test suite in network-restricted environments\\n\\n### .NET Runtime Issues  \\n- **Requires both .NET 8.0 and 9.0**: Build uses 9.0 SDK but tests need 8.0 runtime\\n- **Global.json specifies 9.0.100**: Must install exact .NET 9.0 version or later\\n- **Path configuration critical**: Ensure `$HOME/.dotnet` is in PATH before system .NET\\n\\n### Python Package Issues\\n- **Use uv exclusively**: Do not use pip/conda for dependency management\\n- **Virtual environment required**: Always activate `.venv` before running commands\\n- **Package workspace structure**: Project uses uv workspace with multiple packages\\n\\n## Timing Reference\\n\\n### Python Commands\\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `uv sync --all-extras` | 2 minutes | 300+ seconds | NEVER CANCEL |\\n| `poe mypy` | 6 minutes | 600+ seconds | NEVER CANCEL |\\n| `poe pyright` | 41 seconds | 120+ seconds | NEVER CANCEL |\\n| `poe docs-build` | 1 min 16 sec | 300+ seconds | NEVER CANCEL |\\n| `poe format` | <1 second | 30 seconds | Quick |\\n| `poe lint` | <1 second | 30 seconds | Quick |\\n| Individual package test | 10 seconds | 60+ seconds | May have network failures |\\n\\n### .NET Commands  \\n| Command | Expected Time | Timeout | Notes |\\n|---------|---------------|---------|-------|\\n| `dotnet restore` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet build --configuration Release` | 53 seconds | 300+ seconds | NEVER CANCEL |\\n| `dotnet test --filter \\\"Category=UnitV2\\\"` | 25 seconds | 120+ seconds | NEVER CANCEL |\\n| `dotnet format --verify-no-changes` | 5-10 seconds | 60 seconds | Quick validation |\\n\\n## Repository Structure\\n\\n### Python Packages (`python/packages/`)\\n- `autogen-core`: Core agent runtime, model interfaces, and base components\\n- `autogen-agentchat`: High-level multi-agent conversation APIs  \\n- `autogen-ext`: Extensions for specific model providers and tools\\n- `autogen-studio`: Web-based IDE for agent workflows\\n- `agbench`: Benchmarking suite for agent performance\\n- `magentic-one-cli`: Multi-agent team CLI application\\n\\n### .NET Projects (`dotnet/src/`)\\n- `AutoGen`: Legacy 0.2-style .NET packages (being deprecated)\\n- `Microsoft.AutoGen.*`: New event-driven .NET packages\\n- `AutoGen.Core`: Core .NET agent functionality\\n- Multiple provider packages: OpenAI, Anthropic, Ollama, etc.\\n\\n### Key Configuration Files\\n- `python/pyproject.toml`: Python workspace and tool configuration\\n- `dotnet/global.json`: .NET SDK version requirements  \\n- `dotnet/AutoGen.sln`: .NET solution file\\n- `python/uv.lock`: Locked Python dependencies\\n\\n## Development Best Practices\\n\\n### Before Committing Changes\\n**ALWAYS run these validation steps:**\\n\\n**Python:**\\n```bash\\ncd python && source .venv/bin/activate\\npoe format    # Fix formatting\\npoe lint      # Check code quality  \\npoe mypy      # Type checking (6 minutes)\\npoe docs-build # Verify docs build (1+ minutes)\\n```\\n\\n**.NET:**\\n```bash  \\ncd dotnet && export PATH=\\\"$HOME/.dotnet:$PATH\\\"\\ndotnet format --verify-no-changes  # Check formatting\\ndotnet build --configuration Release --no-restore  # Build (53 seconds)\\ndotnet test --configuration Release --filter \\\"Category=UnitV2\\\" --no-build  # Test (25 seconds)\\n```\\n\\n### Key Directories Reference\\n```\\nautogen/\\n\\u251c\\u2500\\u2500 python/                    # Python implementation\\n\\u2502   \\u251c\\u2500\\u2500 packages/             # Individual Python packages\\n\\u2502   \\u251c\\u2500\\u2500 docs/                 # Sphinx documentation\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Example code\\n\\u2502   \\u2514\\u2500\\u2500 pyproject.toml        # Workspace configuration\\n\\u251c\\u2500\\u2500 dotnet/                   # .NET implementation  \\n\\u2502   \\u251c\\u2500\\u2500 src/                  # Source projects\\n\\u2502   \\u251c\\u2500\\u2500 test/                 # Test projects\\n\\u2502   \\u251c\\u2500\\u2500 samples/              # Sample applications\\n\\u2502   \\u2514\\u2500\\u2500 AutoGen.sln          # Solution file\\n\\u251c\\u2500\\u2500 .github/workflows/        # CI/CD pipelines\\n\\u2514\\u2500\\u2500 docs/                     # Additional documentation\\n```\\n\\nThis framework supports creating both simple single-agent applications and complex multi-agent workflows with support for various LLM providers, tools, and deployment patterns.\",\n",
      "    \"filename\": \".github/copilot-instructions.md\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head data/autogen_data_processed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "966c577e-28da-401a-84ff-9c69ddabb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d46bee-ed92-4f7c-bac3-3168560d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen_data_chunks = []\n",
    "\n",
    "for doc in autogen_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    autogen_data_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655c97c2-d5cc-4b53-a31a-fccffb8424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autogen_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb350c9-ffb1-4d2a-be36-d961e5bd450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'chunk': \"## Prerequisites\\n\\n- Access to gpt3.5-turbo or preferably gpt4 - [Get access here](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai)\\n- [Setup a Github app](#how-do-i-setup-the-github-app)\\n- [Install the Github app](https://docs.github.com/en/apps/using-github-apps/installing-your-own-github-app)\\n- [Provision the azure resources](#how-do-I-deploy-the-azure-bits)\\n- [Create labels for the dev team skills](#which-labels-should-i-create)\\n\\n### How do I setup the Github app?\\n\\n- [Register a Github app](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app), with the options listed below:\\n    - Give your App a name and add a description\\n    - Homepage URL: Can be anything (Example: repository URL)\\n    - Add a dummy value for the webhook url, we'll come back to this setting\\n    - Enter a webhook secret, which you'll need later on when filling in the `WebhookSecret` property in the `appsettings.json` file\\n    - Setup the following permissions\\n        - Repository \\n            - Contents - read and write\\n            - Issues - read and write\\n            - Metadata - read only\\n            - Pull requests - read and write\\n    - Subscribe to the following events:\\n        - Issues\\n        - Issue comment\\n    - Allow this app to be installed by any user or organization\\n    \\n- After the app is created, generate a private key, we'll use it later for authentication to Github from the app\\n\\n### Which labels should I create?\\n\\nIn order for us to know which skill and persona we need to talk with, we are using Labels in Github Issues.\\n\\nThe default bunch of skills and personnas are as follow:\\n- PM.Readme\\n- Do.It\\n- DevLead.Plan\\n- Developer.Implement\\n\\nAdd them to your repository (They are not there by default).\\n\\nOnce you start adding your own skills, just remember to add the corresponding label to your repository.\\n\\n## How do I run this locally?\\n\\nCodespaces are preset for this repo. For codespa\",\n",
       " 'filename': 'dotnet/samples/dev-team/docs/github-flow-getting-started.md'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autogen_data_chunks[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dfd46-106f-4f7d-8ed8-30d9b92389e5",
   "metadata": {},
   "source": [
    "### Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b39ee7-b0c1-41cf-8954-74fcfd61d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from data/autogen_data_processed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'data/autogen_data_processed.json'\n",
    "autogen_data_chunk = None # Initialize the variable\n",
    "\n",
    "try:\n",
    "    # Open the file in 'r' (read) mode\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        # Use json.load() to read the JSON data from the file object (f_in)\n",
    "        autogen_data_chunk = json.load(f_in)\n",
    "        \n",
    "    print(f\"Successfully loaded data from {input_file}\")\n",
    "    # You can now work with autogen_data_chunk\n",
    "    # print(type(autogen_data_chunk)) \n",
    "    # print(autogen_data_chunk)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {input_file} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Failed to decode JSON from {input_file}. The file might be malformed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8a3f9-3103-491a-8af4-88a0d441a4fe",
   "metadata": {},
   "source": [
    "#### TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cfced2-b5ea-41cd-a997-3eda055456b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x79c9594ca0e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(autogen_data_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05660303-3c4e-4dd4-9435-0a716d0a202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what does interface definitions and reference implementations of agent runtime, model, tool, workbench, memory, tracing'\n",
    "query2 = 'tell me about packages/autogen-core'\n",
    "results = index.search(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bc6927-ead6-4236-84b8-6951b0a6bd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '# AutoGen Core\\n\\n- [Documentation](https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html)\\n\\nAutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.',\n",
       "  'filename': 'python/packages/autogen-core/README.md'},\n",
       " {'code': False,\n",
       "  'content': '# Overview of `autogen_core`\\n\\nThe `autogen_core` module serves as the foundation for a system centered around agents, interoperability, and component management. This module facilitates the creation and management of agents, provides tools for data serialization, defines message handling protocols, and manages logging functionalities. By importing various submodules and defining global constants and classes, it offers an organized layout for the internal structure of the agent management system.\\n\\n## Versioning Information\\n\\nThe module starts by retrieving its version using the `importlib.metadata` package. This is a common practice to enable tracking of the module’s version which helps in ensuring compatibility and debugging:\\n\\n```python\\n__version__ = importlib.metadata.version(\"autogen_core\")\\n```\\n\\n## Core Agent Functionality\\n\\nThe module defines several core classes representing different aspects of agent functionality including:\\n\\n- **Agent**: Central class representing an agent that encapsulates core behaviors and properties.\\n- **AgentId**: A class that likely manages unique identifiers for agents, which is essential for tracking and communication.\\n- **AgentInstantiationContext**: Manages context information during the instantiation of agents.\\n- **AgentMetadata**: Stores metadata related to agents such as attributes or configurations.\\n\\nThese components contribute to defining how agents operate within this framework, maintaining unique states and handling interactions.\\n\\n## Agent Management and Runtime\\n\\nThe module includes a range of classes designed to handle the runtime behavior of agents:\\n\\n- **AgentRuntime**: Likely manages the lifecycle of agents, including their execution and state management.\\n- **SingleThreadedAgentRuntime**: A special runtime for executing agents in a single-threaded context, ensuring sequential processing without concurrency issues.\\n\\nAdditionally, **AgentProxy** allows for interfacing with agents from potentially different execution contexts or systems, facilitating remote communication.\\n\\n## Component System\\n\\nA significant portion of the module revolves around component management:\\n\\n- **Component** and its related classes (e.g., **ComponentBase**, **ComponentLoader**) allow the system to dynamically load and configure components at runtime. \\n- This includes defining schemas (**ComponentSchemaType**) and potentially handling configurations through **ComponentFromConfig** and **ComponentToConfig**.\\n\\nThese components likely enable extensibility and modularity, where new features can be integrated without significant modifications to the core system.\\n\\n## Messaging and Interventions\\n\\nThe module also defines mechanisms for handling messages and interventions:\\n\\n- **MessageContext** and **MessageHandlerContext** encapsulate information necessary for message processing and handler states.\\n- **InterventionHandler** and **DefaultInterventionHandler** are utilized to manage disruptions or changes in message flows, which help in error handling or providing alternate routes for message processing.\\n\\nThis aids in maintaining the reliability of communication in agent interactions.\\n\\n## Subscription Mechanism\\n\\nTo support an event-driven architecture, the module provides several classes and functions related to subscriptions:\\n\\n- **Subscription**: Central class for managing subscriptions to events or message types.\\n- **TypeSubscription** and **TypePrefixSubscription** allow for more specific subscription behaviors based on message types, enabling agents to respond to targeted messages.\\n\\nThis mechanism enhances the responsiveness of agents to specific events, promoting efficient processing.\\n\\n## Serialization Handling\\n\\nData serialization is crucial in this context, and the module defines elements for handling various data formats:\\n\\n- **MessageSerializer**: Implements serialization logic, allowing messages to be converted to and from different formats.\\n- Constants like **JSON_DATA_CONTENT_TYPE** and **PROTOBUF_DATA_CONTENT_TYPE** define the expected data types for serialization.\\n\\nBy supporting multiple formats, the system can handle a broader range of data interactions, making it versatile for different use cases.\\n\\n## Logging and Telemetry\\n\\nThe module includes robust logging capabilities:\\n\\n- **EVENT_LOGGER_NAME**, **ROOT_LOGGER_NAME**, and **TRACE_LOGGER_NAME** provide structured logging support, enabling tracking and debugging of agent activities.\\n- Functions like **trace_create_agent_span** and **trace_invoke_agent_span** help in monitoring performance and analyzing agent behavior over time.\\n\\nThis focus on logging ensures that users can gather insights into system operations, which is essential for maintenance and troubleshooting.\\n\\n## Exposed API \\n\\nLastly, the module exports a well-defined set of public interfaces with the `__all__` variable, outlining which classes and functions are accessible for use in other modules. This approach aids in defining a clear API for users of the `autogen_core`, ensuring encapsulation and reducing the risk of interference with internal workings. Each of these exported components plays a distinct role in agent management, message processing, or system integration, enabling a comprehensive ecosystem of functionalities tailored for developing agent-based applications.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/__init__.py'},\n",
       " {'code': False,\n",
       "  'content': \"# Module Overview\\n\\nThis module serves as an interface for a toolkit that likely facilitates the handling of various tools, including stream processing and function execution within a broader workflow or workbench context. It imports and organizes several classes and schemas from different submodules, making them accessible for external use. The use of `__all__` allows for the specification of a public API for the module.\\n\\n## Imports\\n\\nThe module starts by importing a set of classes and schemas from different parts of the package. Here's a breakdown of the imports:\\n\\n- **Base Classes**  \\n  The `BaseStreamTool`, `BaseTool`, and `BaseToolWithState` classes are foundational tools meant to be extended for specific purposes within the toolkit. They likely provide common functionality that can be shared among various tool implementations.\\n\\n- **Schemas**  \\n  `ParametersSchema` and `ToolSchema` probably define the structure or validation requirements for parameters and tools, respectively.\\n\\n- **Specific Tools**  \\n  `StreamTool` could represent a specialized tool designed for processing streaming data, while `FunctionTool` likely wraps around functions to provide them as tools within the workbench environment.\\n\\n- **Workbench Components**  \\n  The `Workbench`, `StaticWorkbench`, `StaticStreamWorkbench`, `ToolResult`, `TextResultContent`, and `ImageResultContent` classes are likely designed to manage and present results from the tools executed within a workbench setup. They may handle different data types, encapsulating the results of tool executions along with relevant metadata.\\n\\n## Public API\\n\\nThe `__all__` list explicitly declares the public API of the module, outlining which classes and functions can be accessed when using `from module import *`. This serves several purposes:\\n\\n- **Encapsulation**  \\n  By controlling what is exposed, the module can prevent direct access to internal components that are not intended for use outside of the module, encouraging better practices and reducing the chance of incorrect usage.\\n\\n- **Ease of Access**  \\n  Users of the module can easily see which tools and classes are available for them to use, streamlining their interaction with the toolkit.\\n\\n## Key Classes and Their Roles\\n\\n### Base Classes\\n\\n- **BaseTool**: This class probably provides a framework for creating tools within the module, encapsulating common functionalities needed by all tools. \\n- **BaseToolWithState**: This likely extends `BaseTool` to support maintaining internal state information across invocations, useful for tools that need to remember context or progress.\\n- **BaseStreamTool**: This may provide specialized functionalities needed for streaming tools, such as buffering or continuous processing capabilities.\\n\\n### Tool Classes\\n\\n- **Tool**: This is likely a more generalized utility for representing a tool within the workbench.\\n- **StreamTool**: Designed for tools that work with data streams, it may encapsulate the logic and necessary configurations to handle streaming data effectively.\\n- **FunctionTool**: This acts as a wrapper for standard functions, turning them into tools that can be used within the workbench framework.\\n\\n### Workbench Components\\n\\n- **Workbench**: This may be the main hub for executing and managing tools, providing infrastructure for input/output as well as result handling.\\n- **StaticWorkbench**: A variant of `Workbench`, potentially designed for static data rather than streaming, giving users a different set of capabilities for non-streaming tasks.\\n- **StaticStreamWorkbench**: Presumably combines aspects of both static and streaming workbenches, perhaps designed for scenarios that require both paradigms.\\n\\n## Result Handling Classes\\n\\n- **ToolResult**: Likely encapsulates the results returned from tool executions, including success/failure statuses and relevant output data.\\n- **TextResultContent**: This class is expected to manage the textual output produced by tools, providing a way to access and format the results.\\n- **ImageResultContent**: Similar to `TextResultContent`, but specifically designed for visual data outputs, allowing users to handle and display images generated as a result of tool execution.\\n\\n## Conclusion\\n\\nThis module provides a structured foundation for tool integration and management within a workflow, facilitating both streaming and static data processing. It abstracts common functionalities through base classes and organizes tool results in a way that's accessible and easy to interact with. Overall, the design suggests a robust toolkit aimed at enhancing productivity in contexts where various tools need to be systematically utilized and managed. This modular approach allows for flexibility in extending and customizing functionality according to user requirements.\",\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/tools/__init__.py'},\n",
       " {'code': False,\n",
       "  'content': '# Code Documentation\\n\\n## Overview\\n\\nThe given code is an import statement from a Python module, which outlines how various classes, types, and functions are structured and made available for use in the current module. This code is organized into several logical sections that include both model client functionalities and type definitions. It utilizes an `__all__` list to explicitly define which components should be accessible when the module is imported.\\n\\n## Module Imports\\n\\nThe code begins by importing several components from internal modules (`._model_client` and `._types`):\\n\\n### From `_model_client`\\n\\n- **ChatCompletionClient**: This likely represents a client that manages interactions with a chat model, possibly facilitating the initiation and handling of chat completions.\\n- **ModelCapabilities**: This class or structure may provide information about the capabilities of different models available through the client.\\n- **ModelFamily**: This could categorize different models under broader classes or groups.\\n- **ModelInfo**: Would likely contain metadata or specifics about a particular model, including its characteristics and performance metrics.\\n- **validate_model_info**: A function designed to validate the provided model information to ensure it meets certain specifications.\\n\\n### From `_types`\\n\\n- **AssistantMessage**: This type probably encapsulates messages generated by an AI assistant during a chat session.\\n- **ChatCompletionTokenLogprob**: This type may handle the token probabilities for generated completions, useful for analyzing responses.\\n- **CreateResult**: Presumably represents the outcomes of a create action concerning chat generation.\\n- **FinishReasons**: Likely enumerates various conditions or reasons for the termination of a chat session or message generation.\\n- **FunctionExecutionResult** and **FunctionExecutionResultMessage**: These would represent results and associated messages from executing certain functions.\\n- **LLMMessage**: This type could encapsulate messages sent or received by a large language model (LLM).\\n- **RequestUsage**: Probably tracks the usage metrics or statistics for requests made to the client or model.\\n- **SystemMessage**: Likely categorizes messages intended for system-level interactions or commands.\\n- **UserMessage**: Represents messages provided by the user interacting with the AI model.\\n- **TopLogprob**: This may represent the top log probabilities associated with token outputs from the model.\\n\\n## `__all__` Declaration\\n\\nThe `__all__` list is defined, which controls what is exported when the module is imported using `from module import *`. By including the various components in this list, the developer is promoting a controlled interface. This enforcement of public API helps in maintaining a clean namespace and provides clear expectations to users of the module.\\n\\n### Listed Components\\n\\nThe components included in the `__all__` list include both classes and function references that provide the primary functionality for users. \\n\\n- Model-related classes and functions (`ModelInfo`, `ModelCapabilities`, etc.) are essential for interacting with the models.\\n- Message types such as `UserMessage`, `AssistantMessage`, and `SystemMessage` form the core of the chat interaction protocols.\\n\\n## Conclusion\\n\\nThis module appears to be part of a larger system designed to facilitate interactions with a chat model, particularly an AI-driven system. The imports suggest an organized architecture that separates model management and message type definitions. The `__all__` declaration indicates a commitment to a logical interface design, improving usability and reducing the potential for conflicts in a larger application.\\n\\nUsers of this module would utilize the imported classes and types for tasks related to chat message construction, model interaction, and information handling, contributing to the development of AI-enhanced conversational applications.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/models/__init__.py'},\n",
       " {'code': False,\n",
       "  'content': '# Module Documentation\\n\\nThis module serves as an interface for functionalities related to JSON schema and Pydantic models. It imports specific functions from separate modules to facilitate these capabilities and provides them for external access.\\n\\n## Imports\\n\\n### `schema_to_pydantic_model`\\n\\n- **Source**: Imported from the module `._json_to_pydantic`.\\n- **Purpose**: This function is intended to convert a JSON schema into a Pydantic model. Pydantic is a data validation and settings management library for Python, leveraging Python type annotations.\\n- **Usage**: When invoked, it takes a JSON schema as input and outputs a corresponding Pydantic model, which can be used for data validation and serialization.\\n\\n### `extract_json_from_str`\\n\\n- **Source**: Imported from the module `._load_json`.\\n- **Purpose**: This function is designed to extract JSON data from a string format.\\n- **Usage**: It accepts a string that represents JSON data and returns a Python object. This is useful for converting JSON strings into usable data structures, such as dictionaries or lists.\\n\\n## Exposed Interfaces\\n\\n### `__all__`\\n\\n- **Definition**: The `__all__` variable is defined as a list containing the names of the functions that are intended to be publicly accessible from this module.\\n- **Contents**: \\n  - `\"schema_to_pydantic_model\"`\\n  - `\"extract_json_from_str\"`\\n- **Functionality**: By defining `__all__`, the module specifies which elements can be imported when a user employs the `from module import *` syntax. This helps in maintaining a clean namespace and avoiding unintended access to internal functions.\\n\\n## Summary\\n\\nThis module integrates JSON handling and Pydantic model generation capabilities by importing two specific functions from their respective modules. It sets up a clean interface for users, allowing them to:\\n- Convert JSON schemas into Pydantic models for application development.\\n- Extract JSON data from string representations for manipulation and processing in Python.\\n\\nThe overall architecture promotes modularity and reusability of code, providing key functionalities without exposing inner workings, thus aligning with good software design principles.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/utils/__init__.py'},\n",
       " {'code': False,\n",
       "  'content': \"# Module Documentation\\n\\nThis module is part of a package that defines memory-related functionalities. It imports various components necessary for memory operations and exposes these components for external use. Below is a high-level description of the code and its structure.\\n\\n## Imports\\n\\nThe module begins with several import statements that load specific classes and types from other components within the package. The imported elements include:\\n\\n- **Memory**: Likely a class that handles memory storage and retrieval operations.\\n- **MemoryContent**: Presumably a class or type that defines the content that can be stored in memory.\\n- **MemoryMimeType**: This could represent various MIME types associated with the memory content, indicating the format of the data.\\n- **MemoryQueryResult**: Probably a structure that encapsulates the results obtained from querying the memory.\\n- **UpdateContextResult**: This might handle the context result of an update operation in memory.\\n- **ListMemory**: Another class that may offer functionalities specifically related to memory operations utilizing lists.\\n\\n## `__all__` Declaration\\n\\nThe `__all__` list is defined next in the module. This list serves as an interface for the module, dictating which components are to be exposed and accessible when the module is imported using the `from module import *` syntax. The inclusion of these components suggests that they are considered essential parts of the module's public API:\\n\\n- `Memory`\\n- `MemoryContent`\\n- `MemoryQueryResult`\\n- `UpdateContextResult`\\n- `MemoryMimeType`\\n- `ListMemory`\\n\\nThis structure indicates that users of this module can import these classes directly, promoting better usability and organization.\\n\\n## Purpose and Role of Imports\\n\\nEach import likely serves a specific role within the overall architecture of the memory handling system:\\n\\n### Memory\\n\\nThe `Memory` class is foundational, providing core functionalities for memory management. It may handle tasks such as saving, retrieving, and deleting data within a certain memory structure.\\n\\n### MemoryContent\\n\\n`MemoryContent` likely defines the data structure that can be stored in memory, including attributes that describe the nature of the content.\\n\\n### MemoryMimeType\\n\\n`MemoryMimeType` would be instrumental in managing the formats of data being processed. This may include types for images, texts, and other media forms, helping ensure the correct handling of different data types.\\n\\n### MemoryQueryResult\\n\\nThis class encapsulates the results from memory queries. Users could leverage this to understand what data was retrieved, whether it was found, and potentially other metadata about the query operations.\\n\\n### UpdateContextResult\\n\\n`UpdateContextResult` probably deals with the aftermath of updating actions within a memory context, indicating whether updates were successful and providing any relevant context information.\\n\\n### ListMemory\\n\\nThe `ListMemory` class may focus on memory management operations that specifically deal with lists, allowing for handling collections of memory entries efficiently and intuitively. It could provide specialized methods for manipulating lists, such as addition, removal, or querying of list items.\\n\\n## Conclusion\\n\\nIn summary, this module is designed for memory handling within an application, offering a structured way to manage various types of data. By carefully importing essential components and exposing them through the `__all__` list, it provides a clean API that facilitates the use of these memory functionalities in other parts of the application. The role of each imported class or type is critical for the coherent operation and usability of the overall memory management system.\",\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/memory/__init__.py'},\n",
       " {'code': False,\n",
       "  'content': '# Code Analysis and Documentation\\n\\n## Overview\\n\\nThis code snippet defines a simple data structure using Python\\'s `dataclasses` module. The primary purpose of this structure is to encapsulate the details of a function call, including its identifier, the arguments being passed to the function, and the name of the function itself.\\n\\n## Imports\\n\\n### `from __future__ import annotations`\\n\\nThis import statement is used to enable postponed evaluation of type annotations. It allows type hints to be represented as strings, which can be beneficial in avoiding certain circular import issues or improving performance.\\n\\n### `from dataclasses import dataclass`\\n\\nThis import brings in the `dataclass` decorator from the `dataclasses` module, which streamlines the creation of classes that are primarily used to store data. It reduces boilerplate code significantly by automatically generating methods such as `__init__()`, `__repr__()`, and others based on the class attributes.\\n\\n## Class Definition: `FunctionCall`\\n\\n### Purpose\\n\\nThe `FunctionCall` class serves as a blueprint for creating objects that store the necessary information to represent a single function call in a structured manner. This is particularly useful in contexts such as logging, serialization, or managing tasks where functions with specific parameters need to be executed or tracked.\\n\\n### Attributes\\n\\n- **`id: str`**  \\n  - This attribute holds a unique identifier for the function call. It can be used to reference or log the specific instance of the function call accurately.\\n\\n- **`arguments: str`**  \\n  - This attribute is meant to contain the arguments for the function call in a JSON-compatible string format. This design allows easy serialization and deserialization of the function\\'s parameters.\\n\\n- **`name: str`**  \\n  - This attribute specifies the name of the function that is to be called. It is essential to identify which function is being referenced when the `FunctionCall` instance is invoked or processed.\\n\\n## Use Cases\\n\\nThe `FunctionCall` class can be used in various scenarios, including:\\n\\n- **Task Scheduling**: In applications that involve scheduling tasks, this class can encapsulate the necessary details of what needs to be executed at a certain time.\\n\\n- **Remote Procedure Call (RPC)**: By structuring the function calls this way, they can easily be serialized and transported over a network for execution on a remote server.\\n\\n- **Event Handling**: Implementing an event-driven architecture can benefit from using this class to define actions triggered by specific events.\\n\\n- **Code Generation**: In scenarios where dynamic code execution is required, such as in frameworks or libraries, this structure can help represent and conduct function calls.\\n\\n## Example Usage\\n\\nWhile the provided code does not include an example usage, one could visualize creating an instance of `FunctionCall` as follows:\\n\\n```python\\ncall = FunctionCall(id=\"1\", arguments=\\'{\"param1\": \"value1\", \"param2\": \"value2\"}\\', name=\"my_function\")\\n```\\n\\nThis example demonstrates how a `FunctionCall` object could be created, encapsulating a function named `my_function` with specific JSON arguments.\\n\\n## Summary\\n\\nIn summary, this code snippet effectively sets up a simple yet flexible infrastructure to represent function calls in Python. By leveraging data classes, it promotes cleaner and more maintainable code by automating the typical patterns involved in defining data structures.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/_types.py'},\n",
       " {'code': False,\n",
       "  'content': \"# Documentation for Logging Events in the LLM Framework\\n\\nThis module defines a series of event classes intended to log various interactions related to Large Language Models (LLMs) within a framework. Each class encapsulates specific types of events, allowing for structured logging of activities such as calls to LLMs, tool executions, message transmissions, and exceptions encountered during processing.\\n\\n## LLM Call Events\\n\\n### LLMCallEvent\\n\\nThe `LLMCallEvent` class is designed for logging an entire call made to a Language Model (LLM). It captures vital information related to the request and the response received.\\n\\n#### Parameters:\\n- **messages**: A list of dictionaries representing the messages sent during the call, which should be JSON serializable.\\n- **response**: A dictionary capturing the response from the LLM, also JSON serializable.\\n- **prompt_tokens**: An integer representing the number of tokens used in the user's prompt.\\n- **completion_tokens**: An integer indicating the number of tokens used in the model's response.\\n\\n#### Properties:\\n- **prompt_tokens**: Returns the number of tokens used in the prompt.\\n- **completion_tokens**: Returns the number of tokens used in the completion.\\n\\nThis class includes a `__str__` method that outputs the encapsulated data in a JSON serializable format, making it easy to log in a structured manner.\\n\\n### LLMStreamStartEvent\\n\\nThe `LLMStreamStartEvent` class logs the initiation of a streaming call. This is important for real-time applications where data is sent and received in a stream.\\n\\n#### Parameters:\\n- **messages**: Similar to `LLMCallEvent`, this parameter captures the messages involved in the stream. They also need to be JSON serializable.\\n\\nThe JSON serializable output is ensured via the `__str__` method, enabling structured logging.\\n\\n### LLMStreamEndEvent\\n\\nThe `LLMStreamEndEvent` class is used for logging the completion of a stream. It captures data similar to the `LLMCallEvent` but focuses on the completion aspect of a streaming session.\\n\\n#### Parameters:\\n- **response**: The output received at the end of the stream, which must be JSON serializable.\\n- **prompt_tokens**: Number indicating prompt token usage.\\n- **completion_tokens**: Number indicating completion token usage.\\n\\nMuch like the previous event classes, it also allows for structured logging through its `__str__` method, which produces a JSON serializable string.\\n\\n## Tool Call Event\\n\\n### ToolCallEvent\\n\\nThe `ToolCallEvent` class is designed to log the execution of various tools that might be part of the process.\\n\\n#### Parameters:\\n- **tool_name**: A string representing the name of the tool being logged.\\n- **arguments**: A dictionary specifying the arguments passed to the tool, which should be JSON serializable.\\n- **result**: A string that denotes the result returned by the executed tool.\\n\\nThis class follows the same pattern as preceding classes, focusing on providing structured logging through its `__str__` method.\\n\\n## Message Events\\n\\n### MessageEvent\\n\\nThe `MessageEvent` class encapsulates events related to message transmission within the system.\\n\\n#### Parameters:\\n- **payload**: A string representing the message content.\\n- **sender**: An `AgentId` or `None` to indicate who sent the message.\\n- **receiver**: An `AgentId`, `TopicId`, or `None` indicating the recipient of the message.\\n- **kind**: An enum indicating the type of message (e.g., direct, publish, or respond).\\n- **delivery_stage**: An enum indicating at which stage of delivery the message is (e.g., sent or delivered).\\n\\nThis class supports structured logging like others, outputting the event in a JSON serializable format via its `__str__` method.\\n\\n### MessageDroppedEvent\\n\\nSimilar to `MessageEvent`, the `MessageDroppedEvent` class is used to log instances where messages were not delivered. \\n\\n#### Parameters:\\n- **payload**: The message that was dropped.\\n- **sender**: The sender of the message.\\n- **receiver**: The intended recipient of the message.\\n- **kind**: The type of the message.\\n\\nIt also produces a structured JSON serializable output for logging purposes.\\n\\n## Exception Events\\n\\n### MessageHandlerExceptionEvent\\n\\nThis class captures exceptions raised during message handling.\\n\\n#### Parameters:\\n- **payload**: The content of the message that caused the exception.\\n- **handling_agent**: The `AgentId` of the agent that was handling the message when the exception occurred.\\n- **exception**: The exception object that was raised.\\n\\nThe JSON serializable output is managed in the same way as with previous classes.\\n\\n### AgentConstructionExceptionEvent\\n\\nThe `AgentConstructionExceptionEvent` class logs any exceptions that occur during the construction of an agent.\\n\\n#### Parameters:\\n- **agent_id**: The `AgentId` associated with the agent construction process.\\n- **exception**: The exception that was raised during construction.\\n\\nThe class's JSON serializable output is created through its `__str__` method, allowing for structured error logging.\\n\\n## Conclusion\\n\\nThis module encapsulates a robust logging framework specifically targeted at events within LLM interactions and agent messaging systems. By leveraging structured logging through JSON serialization, it facilitates better tracking and debugging of the system's dynamic, real-time events.\",\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/logging.py'},\n",
       " {'code': False,\n",
       "  'content': '# Documentation for Logging Configuration\\n\\nThis section provides a high-level description of the logging configuration used in the code snippet. It defines various logger names utilized throughout the application, which are essential for tracking and debugging purposes.\\n\\n## Purpose of Logger Names\\n\\nThe defined logger names help categorize logging messages into different contexts. This segmentation is crucial for both developers and users to easily interpret log outputs, understanding where the messages originated from. Each logger serves a specific purpose, and proper logging practices can greatly enhance the maintainability of the code.\\n\\n## Root Logger\\n\\n```python\\nROOT_LOGGER_NAME = \"autogen_core\"\\n```\\n\\n### Description\\nThe root logger serves as the primary logging point in the application. It aggregates all log messages generated within the core framework. Having a dedicated name for the root logger allows for consistent logging across various modules of the application under a unified namespace.\\n\\n### Usage\\nBy naming this logger as \"autogen_core\", developers can configure multiple handlers and formatters under this root logger. This is particularly useful for setting logging levels or output destinations (e.g., console, files) for all log messages originating from the core functionalities of the application.\\n\\n## Event Logger\\n\\n```python\\nEVENT_LOGGER_NAME = \"autogen_core.events\"\\n```\\n\\n### Description\\nThe event logger is specifically designed for structured event logging. This format is typically used to record significant application events, such as user actions or system events, allowing easier tracking of application behavior.\\n\\n### Usage\\nUsing a distinct logger name like \"autogen_core.events\" enables filtering of logs related to events separately from other types of logs. It makes it easier to manage and analyze application behavior by allowing developers to focus on event-driven logging without mixing it with general runtime logs.\\n\\n## Trace Logger\\n\\n```python\\nTRACE_LOGGER_NAME = \"autogen_core.trace\"\\n```\\n\\n### Description\\nThe trace logger is intended for more detailed, developer-oriented tracing of application execution. Logs generated by this logger do not need to conform to a structured format and are mainly aimed at developers when debugging.\\n\\n### Usage\\nBy using \"autogen_core.trace\" as the logger name, developers can track specific operations or states within the application without introducing a structured logging format. This logger should be utilized for messages that are not expected to contribute to the logging output for end-users, but are crucial for debugging purposes.\\n\\n## Conclusion\\n\\nThis simple yet effective logging configuration allows for enhanced clarity and structure in logging practices. By defining specific logger names, the codebase facilitates better monitoring and debugging, ultimately leading to more maintainable and robust software. Each logger serves its own purpose, ensuring that log messages are appropriate for their intended audiences, whether that be end-users, administrators, or developers.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/_constants.py'},\n",
       " {'code': False,\n",
       "  'content': '# Module Overview\\n\\nThis module is part of a larger codebase, likely dealing with the context management of chat-based interactions. It imports multiple classes from different modules and defines an `__all__` list, which specifies the public symbols of the module. This approach helps in organizing code and controlling the scope of what can be accessed when the module is imported.\\n\\n## Imports\\n\\nThe module imports the following classes:\\n\\n1. **BufferedChatCompletionContext**:  This class is likely designed to manage chat completions in a buffered manner, storing intermediate results for efficiency or for access purposes.\\n\\n2. **ChatCompletionContext**: This class probably encapsulates the state and behavior related to processing chat completions, handling various scenarios that may arise during chat interactions.\\n\\n3. **ChatCompletionContextState**: It seems to represent the state of a `ChatCompletionContext`, allowing for management of various possible states during chat operations (e.g., initial, processing, completed).\\n\\n4. **HeadAndTailChatCompletionContext**: This context class likely manages chat completions with a focus on both the beginning (head) and the end (tail) of the conversation, which could be useful for understanding and generating relevant responses.\\n\\n5. **TokenLimitedChatCompletionContext**: This class appears to implement constraints on the size of chat completions based on token limits, facilitating resource management and efficiency in processing.\\n\\n6. **UnboundedChatCompletionContext**: Unlike the token-limited context, this class probably allows for an unlimited size in chat completions, which could be suitable for less constrained scenarios.\\n\\n## Public Interface\\n\\nThe `__all__` declaration is important for setting the public API of the module. It restricts the import of only the specified classes when `from module import *` is used, promoting encapsulation and keeping potentially sensitive or internal classes hidden from users. \\n\\nThe listed public classes are:\\n\\n- **ChatCompletionContext**\\n- **ChatCompletionContextState**\\n- **UnboundedChatCompletionContext**\\n- **BufferedChatCompletionContext**\\n- **TokenLimitedChatCompletionContext**\\n- **HeadAndTailChatCompletionContext**\\n\\nThis structure indicates a well-organized class hierarchy or relationships, where each context class serves a different purpose, potentially tailored to different scenarios in chat processing.\\n\\n## Summary of Functionality\\n\\n1. **Context Management**: The module is focused on various chat completion contexts, which likely handle and process the state of chat interactions. \\n\\n2. **State Handling**: With the inclusion of `ChatCompletionContextState`, the module probably supports complex state transitions, allowing for robust management of the conversation lifecycle.\\n\\n3. **Efficiency Mechanisms**: The classes that imply buffering or token limitations suggest that the code is designed to optimize resource use and possibly enhance performance during chat interactions.\\n\\n4. **Flexibility**: The different context classes provide flexibility for developers to choose an appropriate context based on specific use cases, whether that be for minimal token usage or handling large amounts of data without constraints.\\n\\n5. **Modular Design**: Each context class appears to serve a distinct role, contributing to a more modular design that enhances maintainability and usability.\\n\\n6. **Exported Classes**: By exporting only certain classes, the module keeps its interface clean and manageable, allowing developers to focus on the essential components without being overwhelmed by internal workings.\\n\\n## Conclusion\\n\\nOverall, this module offers a structured approach to handling various chat contexts, employing classes that cater to specific requirements in terms of state management, efficiency, and flexibility. This design is beneficial for applications involving complex chat interactions, where the context of each conversation can dynamically shift. The explicit control over what is included in the public interface showcases thoughtful code organization and encapsulation practices.',\n",
       "  'filename': 'python/packages/autogen-core/src/autogen_core/model_context/__init__.py'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d1aa6-c151-4413-b977-ee63a4eb6415",
   "metadata": {},
   "source": [
    "#### VECTOR SEARCH\n",
    "uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e004786-e664-46af-8950-073b8425aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831ead1-9fe7-4c0f-8324-366df601cab9",
   "metadata": {},
   "source": [
    "The multi-qa-distilbert-cos-v1 model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
    "Other popular models include:\n",
    "* all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
    "* all-mpnet-base-v2 - Higher quality, slower\n",
    "* Check Sentence Transformers documentation for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828fb0a-7dff-447f-9dc5-a368e8249d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a8db1-f461-4638-9205-a9d8d2dc929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf69888-09fb-4c5c-b2de-73e1b8be44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ca284-9b2e-4c8c-8272-414aa5aac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d912c-9238-4701-abd4-953e7791752b",
   "metadata": {},
   "source": [
    "###### vector search usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e647-50e0-4197-943c-034e46d71fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad5300-8d4a-46f0-b78d-d5cf82b0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500de25d-9730-4a0d-bbaf-4a2e703577ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen_embeddings = []\n",
    "\n",
    "for d in tqdm(autogen_data_chunk):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    autogen_embeddings.append(v)\n",
    "\n",
    "autogen_embeddings = np.array(autogen_embeddings)\n",
    "\n",
    "autogen_vindex = VectorSearch()\n",
    "autogen_vindex.fit(autogen_embeddings, autogen_data_chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ddd3a2-bc82-49b5-ad1c-0be3e260f8f6",
   "metadata": {},
   "source": [
    "#### HYBRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76976855-b25c-4130-a01f-e5c7338f5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbdb17-f207-4cf8-b574-7c7b82de8472",
   "metadata": {},
   "source": [
    "#### PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc79e5-1d51-4ddc-a96a-d65865da575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79968293-27f2-41e1-a70e-a21f2ad23ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89046a93-2928-4fbf-9a6f-91c3d17a5a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
