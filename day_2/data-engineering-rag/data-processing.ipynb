{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deea5eb6-4af4-415b-8acd-9e02066523e8",
   "metadata": {
    "_sphinx_cell_id": "89577dfe-ecc1-4b36-a665-58185dcaa4cf"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf69474d-d31f-4945-b440-95c68f5c30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_extensions = {'md', 'mdx'}\n",
    "code_extensions = {'py', 'sql', 'java', 'ipynb'}\n",
    "\n",
    "extensions = doc_extensions | code_extensions\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filepath = file_info.filename\n",
    "        filepath_lower = filepath.lower()\n",
    "\n",
    "        if filepath_lower.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        filename = filepath_lower.split('/')[-1]\n",
    "\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        ext = filename.split('.')[-1]\n",
    "\n",
    "        if ext not in extensions:\n",
    "            continue\n",
    "\n",
    "        filepath_edited = filepath.split('/', maxsplit=1)[1]\n",
    "\n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                if ext in doc_extensions:\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filepath_edited\n",
    "                elif ext in code_extensions:\n",
    "                    data = {\n",
    "                        'code': True,\n",
    "                        'content': content,\n",
    "                        'filename': filepath_edited\n",
    "                    }\n",
    "\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    zf.close()\n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef88a6ec-8511-4dd5-86ad-2ad9f967626a",
   "metadata": {
    "_sphinx_cell_id": "125c4d5b-7113-484d-aca0-a83fde964a48"
   },
   "outputs": [],
   "source": [
    "de_zoomcamp_data = read_repo_data('DataTalksClub', 'data-engineering-zoomcamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031e517-b52b-4779-b463-16e2f6b04599",
   "metadata": {
    "_sphinx_cell_id": "f956d5ae-868c-4ded-a0fa-a3bad88c6b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77a382b-edc7-49c9-95a3-a40b05160159",
   "metadata": {
    "_sphinx_cell_id": "e55121aa-c222-4ac3-9911-37161a43f2ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    index[record['filename']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f185a28c-2355-4d0f-94ca-3ba126244f14",
   "metadata": {
    "_sphinx_cell_id": "d4ce051b-aaae-4725-9eef-869cf50e26fb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "exporter = MarkdownExporter()\n",
    "exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "def format_notebook_as_md(raw_notebook: str) -> str:\n",
    "    nb_parsed = nbformat.reads(\n",
    "        raw_notebook,\n",
    "        as_version=nbformat.NO_CONVERT,\n",
    "    )\n",
    "    md_body, _ = exporter.from_notebook_node(nb_parsed)\n",
    "    return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8d3523-688c-4724-8b6c-47247388b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    text = text.strip()\n",
    "\n",
    "    if not text.startswith(\"```\"):\n",
    "        return text\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f42422-e0e9-4dba-a75e-b1465e7b2cd4",
   "metadata": {
    "_sphinx_cell_id": "c933285c-97e4-43f6-9f7d-caba91beaeea"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c320a93c-1d82-4a15-aa53-a6961ea89b68",
   "metadata": {
    "_sphinx_cell_id": "8021b298-87b3-4f8f-8ee7-b675e66bf691"
   },
   "outputs": [],
   "source": [
    "def llm(instructions, content, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da62b0a0-62b4-4f38-8e85-d2b5ca3e4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_editing_instructions = \"\"\"\n",
    "You're a professional coding editor.\n",
    "\n",
    "You are given a Markdown file that was converted from a Jupyter notebook.  \n",
    "The file already contains code blocks and inline comments.  \n",
    "\n",
    "Your task:\n",
    "\n",
    "- Turn it into clear, well-structured documentation.  \n",
    "- Add section headers (##) where appropriate. Keep sections relatively large (8-10 paragraphs and code blocks)\n",
    "- Add concise, high-level explanations for each code block.  \n",
    "- Summarize what the code is doing without being overly verbose.  \n",
    "- Keep the formatting in Markdown.\n",
    "- Aim for a balance: clear enough to guide someone new, but not overloaded with detail. \n",
    "\n",
    "Output the improved Markdown file with the new documentation.\n",
    "\"\"\".strip()\n",
    "\n",
    "code_doc_instructions = \"\"\"\n",
    "You are given a piece of source code.  \n",
    "\n",
    "Your task:  \n",
    "- Analyze the code and produce a clear, high-level description of what it does.  \n",
    "- If the code defines functions, methods, or classes, describe their purpose and role.  \n",
    "- If it’s just a script without explicit functions/classes, summarize what the script does step by step at a high level.  \n",
    "- Add logical sections or headings (##) if needed. Sections must be relatively large (8-10 paragraphs and code blocks)\n",
    "- Keep explanations concise and clear — avoid unnecessary verbosity.  \n",
    "- Output the result in Markdown, structured like documentation.  \n",
    "- Do not rewrite or modify the code itself, only provide descriptive documentation.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c829f4e5-663b-45ee-becd-caf6b05f8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "# Kafka Data Producer for NYC Taxi Data\n",
      "\n",
      "This document describes a Python script that produces NYC taxi trip data to a Kafka topic. The script leverages the `KafkaProducer` class from the `kafka` library and processes data from a CSV file of green taxi trips.\n",
      "\n",
      "## Step 1: Importing Required Libraries\n",
      "\n",
      "To start, we import the necessary libraries. The `json` library is utilized for data serialization, while `KafkaProducer` is used to send messages to a Kafka topic.\n",
      "\n",
      "```python\n",
      "import json\n",
      "from kafka import KafkaProducer\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `json`: This module helps in converting Python objects into JSON format, making them suitable for transmission.\n",
      "- `KafkaProducer`: This part of the Kafka library provides the functionality to send records to a Kafka topic.\n",
      "\n",
      "## Step 2: Defining the JSON Serializer\n",
      "\n",
      "Next, a function is defined to serialize data into JSON format. This will be necessary for sending messages to Kafka.\n",
      "\n",
      "```python\n",
      "def json_serializer(data):\n",
      "    return json.dumps(data).encode('utf-8')\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `json_serializer`: This function takes a Python dictionary as input, converts it into a JSON string, and then encodes it into bytes, which is a format suitable for Kafka.\n",
      "\n",
      "## Step 3: Configuring Kafka Producer\n",
      "\n",
      "We then configure the Kafka producer, specifying the server details and the value serializer.\n",
      "\n",
      "```python\n",
      "server = 'localhost:9092'\n",
      "\n",
      "producer = KafkaProducer(\n",
      "    bootstrap_servers=[server],\n",
      "    value_serializer=json_serializer\n",
      ")\n",
      "\n",
      "producer.bootstrap_connected()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `server`: Specifies the Kafka server address.\n",
      "- `KafkaProducer`: Initializes the producer with the server address and the previously defined serializer.\n",
      "- `bootstrap_connected()`: This method checks if the producer is connected to the Kafka server.\n",
      "\n",
      "## Step 4: Downloading the Data\n",
      "\n",
      "We now download the NYC taxi trip data from a provided GitHub link. This dataset will be used for sending messages to Kafka.\n",
      "\n",
      "```python\n",
      "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `!wget`: This command is used to download the gzipped CSV file containing the taxi trip data directly from the internet.\n",
      "\n",
      "## Step 5: Loading the Data into a DataFrame\n",
      "\n",
      "After downloading the data, we use the `pandas` library to read the CSV file into a DataFrame.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('green_tripdata_2019-10.csv.gz')\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `pd.read_csv()`: This function reads the compressed CSV file and loads it into a Pandas DataFrame, enabling easy data manipulation and analysis.\n",
      "\n",
      "## Step 6: Exploring the Data\n",
      "\n",
      "To understand the structure of the dataset, we view the first few rows of the DataFrame.\n",
      "\n",
      "```python\n",
      "df.head()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `df.head()`: This function call displays the first five rows of the DataFrame. It's useful for quickly checking the data and its structure.\n",
      "\n",
      "## Step 7: Selecting Relevant Columns\n",
      "\n",
      "We filter the DataFrame to select only the columns relevant to our analysis.\n",
      "\n",
      "```python\n",
      "columns = [\n",
      "    'lpep_pickup_datetime',\n",
      "    'lpep_dropoff_datetime',\n",
      "    'PULocationID',\n",
      "    'DOLocationID',\n",
      "    'passenger_count',\n",
      "    'trip_distance',\n",
      "    'tip_amount'\n",
      "]\n",
      "\n",
      "df = df[columns]\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- The `columns` list defines which attributes to keep in the DataFrame. \n",
      "- The filtering operation (`df = df[columns]`) ensures that only relevant data is retained for sending to Kafka.\n",
      "\n",
      "## Step 8: Preparing for Streaming\n",
      "\n",
      "Before sending data, necessary imports for timing and progress indication are included.\n",
      "\n",
      "```python\n",
      "from time import time\n",
      "from tqdm.auto import tqdm\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `time`: This module will be used if we need to perform any timing operations.\n",
      "- `tqdm`: This library provides a progress bar for loops, which allows us to visualize the data processing.\n",
      "\n",
      "## Step 9: Converting DataFrame to Dictionary\n",
      "\n",
      "The DataFrame is then converted into a list of dictionaries. Each dictionary corresponds to a row of data.\n",
      "\n",
      "```python\n",
      "messages = df.to_dict(orient='records')\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `to_dict(orient='records')`: This method converts the DataFrame into a list of dictionaries, making it easier to send each row as a separate message to Kafka.\n",
      "\n",
      "## Step 10: Sending Messages to Kafka\n",
      "\n",
      "Finally, we loop through each message and send it to the specified Kafka topic.\n",
      "\n",
      "```python\n",
      "topic_name = 'green-trips'\n",
      "\n",
      "for message in tqdm(messages):\n",
      "    producer.send(topic_name, value=message)\n",
      "\n",
      "producer.flush()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- `topic_name`: This variable defines the Kafka topic where messages will be sent.\n",
      "- The `for` loop iterates over each message, sending it to the Kafka topic.\n",
      "- `producer.flush()`: This command ensures that all buffered messages are sent. It's important to call this to make sure data is not lost.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In summary, this script sets up a Kafka producer to send NYC green taxi trip data to a specified Kafka topic. By following the structured steps outlined, users can easily modify the code to fit other datasets or Kafka configurations as needed.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# First, open the file and read its content\n",
    "with open('data-processing-code.ipynb', 'r', encoding='utf-8') as f:\n",
    "    raw_notebook_content = f.read()\n",
    "\n",
    "result = llm(notebook_editing_instructions, md_body)\n",
    "print(result)\n",
    "#result = llm(system_prompt, md_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a456b4aa-40f2-428c-973c-4d40f0005213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f0f975c-f603-4a3b-9a32-c261a44d1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 19 jupyter notebooks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60cab1c2b2747b1bf67af06cbbe08d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipynb_data = []\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    if record.get('code') == True and record['filename'].endswith('.ipynb'):\n",
    "        ipynb_data.append(record)\n",
    "\n",
    "\n",
    "print(f'processing {len(ipynb_data)} jupyter notebooks...')\n",
    "\n",
    "for record in tqdm(ipynb_data):\n",
    "    md_body = format_notebook_as_md(record['content'])\n",
    "    new_content = llm(notebook_editing_instructions, md_body)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d791ab0-a49b-4439-a143-bd3e8c38067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "941e67d0-9260-48ca-bcdf-8abb6401c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 78 code files...\n"
     ]
    }
   ],
   "source": [
    "code_data = []\n",
    "\n",
    "for record in de_zoomcamp_data:\n",
    "    if record.get('code') != True:\n",
    "        continue\n",
    "\n",
    "    path = record['filename']\n",
    "    ext = path.split('.')[-1]\n",
    "\n",
    "    if ext not in code_extensions:\n",
    "        continue\n",
    "\n",
    "    if ext == 'ipynb':\n",
    "        continue\n",
    "\n",
    "    # print(path)\n",
    "    code_data.append(record)\n",
    "\n",
    "print(f'processing {len(code_data)} code files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5755e9-1f70-4510-b75c-6b952ad303fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187640344414b88b3c2bcbc87a39173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(code_data):\n",
    "    code = record['content']\n",
    "\n",
    "    new_content = llm(code_doc_instructions, code)\n",
    "    new_content = strip_code_fence(new_content)\n",
    "\n",
    "    record['content'] = new_content\n",
    "    record['code'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d50b9af-207b-421c-a567-3aacc0bfacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e5aeaab-dc9c-4b1d-a00b-ce29bbf9b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da555e70-19ba-4996-9f69-9cfd18a477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/de-zoomcamp-processed.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(de_zoomcamp_data, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e8984a6-78ac-4c05-b065-76f7dc910382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"## Terraform Overview\\n\\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\\n\\n### Concepts\\n\\n#### Introduction\\n\\n1. What is [Terraform](https://www.terraform.io)?\\n   * open-source tool by [HashiCorp](https://www.hashicorp.com), used for provisioning infrastructure resources\\n   * supports DevOps best practices for change management\\n   * Managing configuration files in source control to maintain an ideal provisioning state \\n     for testing and production environments\\n2. What is IaC?\\n   * Infrastructure-as-Code\\n   * build, change, and manage your infrastructure in a safe, consistent, and repeatable way \\n     by defining resource configurations that you can version, reuse, and share.\\n3. Some advantages\\n   * Infrastructure lifecycle management\\n   * Version control commits\\n   * Very useful for stack-based deployments, and with cloud providers such as AWS, GCP, Azure, K8S\\u2026\\n   * State-based approach to track resource changes throughout deployments\\n\\n\\n#### Files\\n\\n* `main.tf`\\n* `variables.tf`\\n* Optional: `resources.tf`, `output.tf`\\n* `.tfstate`\\n\\n#### Declarations\\n* `terraform`: configure basic Terraform settings to provision your infrastructure\\n   * `required_version`: minimum Terraform version to apply to your configuration\\n   * `backend`: stores Terraform's \\\"state\\\" snapshots, to map real-world resources to your configuration.\\n      * `local`: stores state file locally as `terraform.tfstate`\\n   * `required_providers`: specifies the providers required by the current module\\n* `provider`:\\n   * adds a set of resource types and/or data sources that Terraform can manage\\n   * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms.\\n* `resource`\\n  * blocks to define components of your infrastructure\\n  * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table\\n* `variable` & `locals`\\n  * runtime arguments and constants\\n\\n\\n#### Execution steps\\n1. `terraform init`: \\n    * Initializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control \\n2. `terraform plan`:\\n    * Matches/previews local changes against a remote state, and proposes an Execution Plan.\\n3. `terraform apply`: \\n    * Asks for approval to the proposed plan, and applies changes to cloud\\n4. `terraform destroy`\\n    * Removes your stack from the Cloud\\n\\n\\n### Terraform Workshop to create GCP Infra\\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`\\n\\n\\n### References\\nhttps://learn.hashicorp.com/collections/terraform/gcp-get-started\",\n",
      "    \"filename\": \"01-docker-terraform/1_terraform_gcp/1_terraform_overview.md\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"## GCP Overview\\n\\n[Video](https://www.youtube.com/watch?v=18jIzE41fJ4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=2)\\n\\n\\n### Project infrastructure modules in GCP:\\n* Google Cloud Storage (GCS): Data Lake\\n* BigQuery: Data Warehouse\\n\\n(Concepts explained in Week 2 - Data Ingestion)\\n\\n### Initial Setup\\n\\nFor this course, we'll use a free version (upto EUR 300 credits). \\n\\n1. Create an account with your Google email ID \\n2. Setup your first [project](https://console.cloud.google.com/) if you haven't already\\n    * eg. \\\"DTC DE Course\\\", and note down the \\\"Project ID\\\" (we'll use this later when deploying infra with TF)\\n3. Setup [service account & authentication](https://cloud.google.com/docs/authentication/getting-started) for this project\\n    * Grant `Viewer` role to begin with.\\n    * Download service-account-keys (.json) for auth.\\n4. Download [SDK](https://cloud.google.com/sdk/docs/quickstart) for local setup\\n5. Set environment variable to point to your downloaded GCP keys:\\n   ```shell\\n   export GOOGLE_APPLICATION_CREDENTIALS=\\\"<path/to/your/service-account-authkeys>.json\\\"\\n   \\n   # Refresh token/session, and verify authentication\\n   gcloud auth application-default login\\n   ```\\n   \\n### Setup for Access\\n \\n1. [IAM Roles](https://cloud.google.com/storage/docs/access-control/iam-roles) for Service account:\\n   * Go to the *IAM* section of *IAM & Admin* https://console.cloud.google.com/iam-admin/iam\\n   * Click the *Edit principal* icon for your service account.\\n   * Add these roles in addition to *Viewer* : **Storage Admin** + **Storage Object Admin** + **BigQuery Admin**\\n   \\n2. Enable these APIs for your project:\\n   * https://console.cloud.google.com/apis/library/iam.googleapis.com\\n   * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com\\n   \\n3. Please ensure `GOOGLE_APPLICATION_CREDENTIALS` env-var is set.\\n   ```shell\\n   export GOOGLE_APPLICATION_CREDENTIALS=\\\"<path/to/your/service-account-authkeys>.json\\\"\\n   ```\\n \\n### Terraform Workshop to create GCP Infra\\nContinue [here](./terraform): `week_1_basics_n_setup/1_terraform_gcp/terraform`\",\n",
      "    \"filename\": \"01-docker-terraform/1_terraform_gcp/2_gcp_overview.md\"\n",
      "  },\n",
      "  {\n"
     ]
    }
   ],
   "source": [
    "!head data/de-zoomcamp-processed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "966c577e-28da-401a-84ff-9c69ddabb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56d46bee-ed92-4f7c-bac3-3168560d0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zoomcamp_chunks = []\n",
    "\n",
    "for doc in de_zoomcamp_data:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    de_zoomcamp_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "655c97c2-d5cc-4b53-a31a-fccffb8424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_zoomcamp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb350c9-ffb1-4d2a-be36-d961e5bd450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 3000,\n",
       " 'chunk': \"ile based on the specified service and year.\\n   - It builds the full request URL to download the file and retrieves it using the `requests` library. The content is saved locally.\\n   \\n3. After downloading, the script reads the CSV file into a pandas DataFrame while handling gzip compression.\\n   \\n4. The data is then converted into a Parquet file, and the original file name is modified accordingly.\\n   \\n5. Using the `upload_to_gcs` function, the Parquet file is uploaded to GCS at the specified path.\\n\\n6. It prints messages to inform the user of each step (local file download, Parquet conversion, and upload to GCS).\\n\\n## Main Script Execution\\n\\nAt the end of the script, the `web_to_gcs` function is called twice for different years and a specific service ('green'):\\n- It processes data for 'green' taxis for the years '2019' and '2020'.\\n\\n### Optional Execution\\n\\nThere are additional commented-out calls to `web_to_gcs` that would process data for 'yellow' taxis for the same years, indicating that the script can be easily extended for other services by uncommenting those lines.\\n\\n## Conclusion\\n\\nThis script efficiently automates the process of downloading, converting, and uploading NYC taxi trip data to Google Cloud Storage. By utilizing pandas for data handling and Google Cloud Storage for hosting the files, it offers a seamless way to manage large datasets. The script can be extended easily for additional services or years, making it a flexible solution for data engineers and analysts working with NYC TLC data.\",\n",
       " 'code': False,\n",
       " 'filename': '03-data-warehouse/extras/web_to_gcs.py'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_zoomcamp_chunks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b39ee7-b0c1-41cf-8954-74fcfd61d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
